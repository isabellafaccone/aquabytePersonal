#!/usr/bin/env python
# vim: tabstop=2 shiftwidth=2 expandtab

DESC = """
mft-cli - This tool serves to both automate and document the MFT-PG
development workflow.  This tool is designed to work on any host
with Python 2.7, and on either an x86 host or an Arm host (e.g. Jetson TX2).

## Example Usage
$ ./mft-cli --pull-s3-artifacts
Ensure the latest version of all S3 data & model artifacts are available.

$ ./mft-cli --build-docker
Build all available dockerized environments

"""

import os
import subprocess
import sys

## Logging
import logging
LOG_FORMAT = "%(asctime)s\t%(name)-4s %(process)d : %(message)s"
log = logging.getLogger("op")
log.setLevel(logging.INFO)
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setFormatter(logging.Formatter(LOG_FORMAT))
log.addHandler(console_handler)


def run_cmd(cmd, collect=False):
  cmd = cmd.replace('\n', '').strip()
  log.info("Running %s ..." % cmd)
  if collect:
    out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
  else:
    subprocess.check_call(cmd, shell=True)
    out = None
  log.info("... done with %s " % cmd)
  return out


def enter_dockerized_shell(
        image_name,
        container_name='',
        src_root='',
        mnt_local_host_root=True,
        start_only=False,
        include_trt=False):
  
  if not container_name:
    container_name = (
      os.environ.get('USER', 'anon') + '-' + image_name)
  local_mounts = ''
  if src_root:
    local_mounts = (
      local_mounts + ' -v {src_root}:/opt/mft-pg:z'.format(src_root=src_root))
  if mnt_local_host_root:
    local_mounts = (local_mounts + ' -v /:/outer_root')
  if include_trt:
    trt_path = run_cmd(
      "python3 -c 'import tensorrt as trt; print(trt.__file__)'", collect=True)
    trt_path = trt_path.strip()
    trt_path = os.path.dirname(trt_path)
    local_mounts = (local_mounts + ' -v %s:%s' % (trt_path, trt_path))

  CMD = """
      docker run
        --runtime=nvidia
        --name {container_name}
        -d -it -P --net=host
        -w /opt/mft-pg
        {local_mounts}
          {docker_image} sleep infinity || docker start {container_name} || true
    """.format(
          container_name=container_name,
          local_mounts=local_mounts,
          docker_image=image_name)
  run_cmd(CMD)
  
  log.info(
    "To delete this container, use: $ docker rm -f %s" % container_name)
  
  if not start_only:
    EXEC_CMD = 'docker exec -it %s bash' % container_name
    os.execvp("docker", EXEC_CMD.split(' '))
  return container_name

def create_arg_parser():
  import argparse
  
  parser = argparse.ArgumentParser(
                      description=DESC,
                      formatter_class=argparse.RawDescriptionHelpFormatter)
  
  # Configuration
  parser.add_argument(
    '--root', default=os.path.dirname(os.path.abspath(__file__)),
    help='Use source at this root directory [default %(default)s]')

  # Actions
  parser.add_argument(
    '--pull-s3-artifacts', default=False, action='store_true',
    help='Pull the latest versions of all S3 artifacts')
  parser.add_argument(
    '--build-docker', default=False, action='store_true',
    help='Build all available Docker environments')

  parser.add_argument(
    '--shell-inference', default=False, action='store_true',
    help='Drop into a containerized inference shell')

  parser.add_argument(
    '--shell-darknet-train', default=False, action='store_true',
    help='Drop into a containerized shell for training via darknet')

  parser.add_argument(
    '--mlflow-ui', default='',
    help='Start the MLFlow UI on the given port number.  Serves the '
         'local `mlruns` directory')
  parser.add_argument(
    '--jupyter-ui', default='',
    help='Start Jupyter on the given port number.  Serves the '
         'local source root directory')

  return parser


def main(args=None):
  if not args:
    parser = create_arg_parser()
    args = parser.parse_args()
  
  if args.pull_s3_artifacts:
    try:
      run_cmd("aws --version")
    except Exception as e:
      raise ValueError("Host needs the awscli python package; %s" % (e,))
    run_cmd("""
      cd {src_root} && \
        cd detection/models && \
        aws s3 sync --size-only s3://aquabyte-research/pwais/mft-pg/detection_models_s3/ ./detection_models_s3/ && \
        cd - && \

        cd datasets && \
        aws s3 sync --size-only s3://aquabyte-research/pwais/mft-pg/datasets_s3/ ./datasets_s3/ && \
        cd -
      """.format(src_root=args.root))
  
  if args.build_docker:
    host_arch = run_cmd("dpkg --print-architecture", collect=True)
    host_arch = host_arch.strip()
    if host_arch == 'arm64':
      run_cmd("""
        echo "Building for ARM" && \
          cd {src_root} && \
            cd detection/inference/docker_jetson && \
            docker build -t mft-pg-inference-arm64 . && \
            cd -
        """.format(src_root=args.root))
    elif host_arch in ('amd64', 'x86_64'):
      run_cmd("""
        echo "Building for x86_64" && \
          cd {src_root} && \
            cd detection/inference/docker_x86 && \
            docker build -t mft-pg-inference-x86 . && \
            cd - && \

            cd detection/training/darknet/docker && \
            docker build -t mft-pg-darknet-train . && \
            cd - 

        """.format(src_root=args.root))
        # cd /tmp && \
        #     git clone https://github.com/mlflow/mlflow mlflow && \
        #     cd mlflow && \
        #     git checkout v1.17.0 && \
        #     docker build -t mft-pg-mlflow-ui .
    else:
      raise ValueError("Don't know how to build for %s" % host_arch)

  if args.shell_inference:
    host_arch = run_cmd("dpkg --print-architecture", collect=True)
    host_arch = host_arch.strip()
    
    if host_arch == 'arm64':
      enter_dockerized_shell(
        'mft-pg-inference-arm64',
        src_root=args.root,
        include_trt=True)
    elif host_arch in ('amd64', 'x86_64'):
      enter_dockerized_shell('mft-pg-inference-x86', src_root=args.root)
  
  if args.shell_darknet_train:
    enter_dockerized_shell('mft-pg-darknet-train', src_root=args.root)

  elif args.mlflow_ui:
    container_name = os.environ.get('USER', 'anon') + '-' + 'mlflow-ui'
    container_name = enter_dockerized_shell(
                        'mft-pg-inference-x86',
                        container_name=container_name,
                        src_root=args.root,
                        start_only=True)
    run_cmd("""
        docker exec -it {container_name} bash -c "mlflow ui --host=0.0.0.0 --port={port}"
      """.format(container_name=container_name, port=args.mlflow_ui))
    
  elif args.jupyter_ui:
    container_name = os.environ.get('USER', 'anon') + '-' + 'jupyter-ui'
    container_name = enter_dockerized_shell(
                        'mft-pg-inference-x86',
                        container_name=container_name,
                        src_root=args.root,
                        start_only=True)
    run_cmd("""
        echo "FIXME ENV" && 
        docker exec -it {container_name} bash -c "PYTHONPATH=$PYTHONPATH:/opt/mft-pg:/opt/oarphpy PYSPARK_PYTHON=python3 PYSPARK_DRIVER_PYTHON=python3 jupyter notebook --no-browser --allow-root --ip='*' --port={port}"
      """.format(container_name=container_name, port=args.jupyter_ui))

if __name__ == '__main__':
  main()
