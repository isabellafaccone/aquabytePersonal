{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils import get_matching_s3_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "annotations_s3_bucket = 'aquabyte-annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_world_point(x, y, d, pixel_count_width, \n",
    "                           pixel_count_height, image_sensor_width, \n",
    "                           image_sensor_height, focal_length):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    \n",
    "    image_center_x = pixel_count_width / 2.0  \n",
    "    image_center_y = pixel_count_height / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (image_sensor_width / 4096)\n",
    "    sensor_z = px_z * (image_sensor_height / 3000)\n",
    "\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / focal_length\n",
    "    world_z = (world_y * sensor_z) / focal_length\n",
    "    return [world_x, world_y, world_z]\n",
    "\n",
    "\n",
    "\n",
    "def depth_from_disp(disp, focal_length_pixel, baseline):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    depth = focal_length_pixel*baseline / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_f = '/app/data/cloud_data_service/annotations.json'\n",
    "\n",
    "def cogito_main(s3_client, annotations_s3_bucket, image_s3_bucket):\n",
    "    \"\"\" every hour check s3 folder for new files\"\"\"\n",
    "    generator = get_matching_s3_keys(s3_client,\n",
    "                                     annotations_s3_bucket,\n",
    "                                     prefix='cogito/gtsf_keypoint_annotations',\n",
    "                                     suffix='.json')\n",
    "\n",
    "    for key in generator:\n",
    "        print(key)\n",
    "        date = str(key.replace('.json', '').split('/')[-1])\n",
    "        # check if it's already been processed -- if it has, continue\n",
    "        # logic to be added\n",
    "        \n",
    "        s3_client.download_file(annotations_s3_bucket, key, annotations_f)\n",
    "        annotation_objects = json.load(open(annotations_f))\n",
    "        \n",
    "        # remove duplicates\n",
    "        \n",
    "        annotations_by_datarow_id = {}\n",
    "        for obj in annotation_objects:\n",
    "            datarow_id = obj['DataRow ID']\n",
    "            if datarow_id not in list(annotations_by_datarow_id.keys()):\n",
    "                annotations_by_datarow_id[datarow_id] = obj\n",
    "            else:\n",
    "                if obj['Created At'] > annotations_by_datarow_id[datarow_id]:\n",
    "                    annotations_by_datarow_id[datarow_id] = obj\n",
    "                    \n",
    "        anns = annotations_by_datarow_id.values()\n",
    "        \n",
    "        # get body parts list\n",
    "        body_parts = []\n",
    "        for k, v in anns[0]['Label'].items():\n",
    "            body_parts.append(k)\n",
    "            \n",
    "        # create keypoints dataframe\n",
    "        keypoints_df = pd.DataFrame()\n",
    "\n",
    "        gtsf_data_collections = session.query(GtsfDataCollection).filter(GtsfDataCollection.date == date).all()\n",
    "        \n",
    "        fish_ids = []\n",
    "        for idx, obj in enumerate(anns):\n",
    "            if obj['Label'] == 'Skip':\n",
    "                continue\n",
    "\n",
    "            # get fish_id\n",
    "            image_url = str(obj['Labeled Data'])\n",
    "            s3_bucket_key = image_url[image_url.index(image_s3_bucket):]\n",
    "            s3_key = os.path.join(*s3_bucket_key.split('/')[1:])\n",
    "            fish_id = image_url.split('/')[-3]\n",
    "            if not fish_id in fish_ids:\n",
    "                fish_ids.append(fish_id)\n",
    "\n",
    "            # get image file name and epoch\n",
    "            image_f_name = image_url.split('/')[-1]\n",
    "            epoch = int(image_f_name.replace('.jpg', '').split('_')[-1])\n",
    "\n",
    "            camera = str(image_f_name.split('_')[0])\n",
    "\n",
    "            for body_part in body_parts:\n",
    "                kp_dict = obj['Label'][body_part][0]['geometry']\n",
    "                kp = (kp_dict['x'], kp_dict['y'])\n",
    "\n",
    "                row = {\n",
    "                    'fish_id': fish_id,\n",
    "                    'epoch': epoch,\n",
    "                    'image_s3_key': s3_key,\n",
    "                    'body_part': body_part,\n",
    "                    'camera': camera,\n",
    "                    'keypoint': kp\n",
    "\n",
    "                }\n",
    "\n",
    "                keypoints_df = keypoints_df.append(row, ignore_index=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # get camera details\n",
    "        gtsf_data_collection = session.query(GtsfDataCollection) \\\n",
    "                                      .filter(GtsfDataCollection.gtsf_fish_identifier==fish_ids[0]) \\\n",
    "                                      .one()\n",
    "\n",
    "        calibration = session.query(Calibration) \\\n",
    "                     .filter(Calibration.enclosure_id == gtsf_data_collection.enclosure_id) \\\n",
    "                     .order_by(Calibration.utc_timestamp.desc()) \\\n",
    "                     .first()\n",
    "                \n",
    "        enclosure = session.query(Enclosure).get(calibration.enclosure_id)\n",
    "            \n",
    "        \n",
    "        focal_length = float(calibration.predicted_focal_length_mm) / (1e3)\n",
    "        baseline = float(calibration.predicted_baseline_mm) / (1e3)\n",
    "        pixel_size_m = float(enclosure.pixel_width_um) / (1e6)\n",
    "        focal_length_pixel = focal_length / pixel_size_m\n",
    "        image_sensor_width = float(enclosure.sensor_width_mm) / (1e3)\n",
    "        image_sensor_height = float(enclosure.sensor_height_mm) / (1e3)\n",
    "        pixel_count_width = enclosure.image_num_pixels_width\n",
    "        pixel_count_height = enclosure.image_num_pixels_height\n",
    "        \n",
    "        # create stereo frame pairs df\n",
    "        \n",
    "        stereo_frame_pairs_df = pd.DataFrame()\n",
    "        for fish_id in fish_ids:\n",
    "            gtsf_data_collection = session.query(GtsfDataCollection) \\\n",
    "                                          .filter(GtsfDataCollection.gtsf_fish_identifier==fish_id) \\\n",
    "                                          .one()\n",
    "\n",
    "            calibration = session.query(Calibration) \\\n",
    "                                 .filter(Calibration.enclosure_id==gtsf_data_collection.enclosure_id) \\\n",
    "                                 .one()\n",
    "\n",
    "            fish_id_mask = keypoints_df.fish_id == fish_id\n",
    "            epochs = keypoints_df[fish_id_mask].epoch.unique()\n",
    "            for epoch in epochs:\n",
    "                epoch_mask = keypoints_df.epoch == epoch\n",
    "                row = {}\n",
    "                row['gtsf_data_collection_id'] = gtsf_data_collection.id\n",
    "                row['gtsf_fish_identifier'] = fish_id\n",
    "                row['epoch'] = epoch\n",
    "\n",
    "                row['left_image_s3_key'] = keypoints_df[fish_id_mask \\\n",
    "                                                        & epoch_mask \\\n",
    "                                                        & (keypoints_df.camera == 'left')\n",
    "                                                       ].image_s3_key.iloc[0]\n",
    "\n",
    "                row['right_image_s3_key'] = keypoints_df[fish_id_mask \\\n",
    "                                                        & epoch_mask \\\n",
    "                                                        & (keypoints_df.camera == 'right')\n",
    "                                                       ].image_s3_key.iloc[0]\n",
    "\n",
    "                left_keypoints, right_keypoints, world_keypoints = {}, {}, {}\n",
    "\n",
    "                for body_part in body_parts:\n",
    "                    left_row = keypoints_df[\n",
    "                        (keypoints_df.epoch == epoch) & (keypoints_df.camera == 'left') & (keypoints_df.body_part == body_part)\n",
    "                    ].iloc[0]\n",
    "\n",
    "                    lkp = left_row['keypoint']\n",
    "                    left_keypoints[body_part] = lkp\n",
    "\n",
    "                    right_row = keypoints_df[\n",
    "                        (keypoints_df.epoch == epoch) & (keypoints_df.camera == 'right') & (keypoints_df.body_part == body_part)\n",
    "                    ].iloc[0]\n",
    "\n",
    "                    rkp = right_row['keypoint']\n",
    "                    right_keypoints[body_part] = rkp\n",
    "\n",
    "                    d = abs(lkp[0] - rkp[0])\n",
    "\n",
    "                    # compute world key point\n",
    "                    depth = depth_from_disp(d, focal_length_pixel, baseline)\n",
    "                    wkp = convert_to_world_point(lkp[0], lkp[1], depth, pixel_count_width, \n",
    "                                                 pixel_count_height, image_sensor_width, \n",
    "                                                 image_sensor_height, focal_length)\n",
    "\n",
    "                    world_keypoints[body_part] = wkp\n",
    "\n",
    "                row['left_keypoints'] = left_keypoints\n",
    "                row['right_keypoints'] = right_keypoints\n",
    "                row['world_keypoints'] = world_keypoints\n",
    "\n",
    "                stereo_frame_pairs_df = stereo_frame_pairs_df.append(row, ignore_index=True)\n",
    "                \n",
    "        # add stereo frame pairs to database\n",
    "        \n",
    "        for idx, row in stereo_frame_pairs_df.iterrows():\n",
    "            stereo_frame_pair = StereoFramePair(\n",
    "                gtsf_data_collection_id=row['gtsf_data_collection_id'],\n",
    "                gtsf_fish_identifier=row['gtsf_fish_identifier'],\n",
    "                date=date,\n",
    "                epoch=int(row['epoch']),\n",
    "                left_image_s3_key=row['left_image_s3_key'],\n",
    "                right_image_s3_key=row['right_image_s3_key'],\n",
    "                image_s3_bucket=image_s3_bucket,\n",
    "                left_image_keypoint_coordinates=json.dumps(row['left_keypoints']),\n",
    "                right_image_keypoint_coordinates=json.dumps(row['right_keypoints']),\n",
    "                world_keypoint_coordinates=json.dumps(row['world_keypoints']),\n",
    "                annotations_project_name=annotation_objects[0]['Project Name'],\n",
    "                annotations_file_s3_key=key,\n",
    "                annotations_s3_bucket=annotations_s3_bucket\n",
    "            )\n",
    "            \n",
    "            session.add(stereo_frame_pair)\n",
    "            session.commit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_main(s3_client, 'aquabyte-annotations', 'aquabyte-groundtruths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkp = df[df.gtsf_fish_identifier == '190226010001'].world_keypoints.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
