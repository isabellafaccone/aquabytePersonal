{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/thomas/blender/volumes_all.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.array(data[\"dimensions\"])[:, 1], data[\"volume\"])\n",
    "# plt.ylabel(\"Volume (cm^3)\")\n",
    "# plt.xlabel(\"Length (mm)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"volume\"])\n",
    "plt.title(\"Blender volume histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pairwise distances from blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = data[\"mapping\"]\n",
    "reverse_mapping = data[\"reverse_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_parts = max(list(mapping.values()))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"volume\":[]}\n",
    "dataset_np = []\n",
    "kfactors = []\n",
    "for (coord, vol) in zip(data[\"coordinates\"], data[\"volume\"]):\n",
    "    row = []\n",
    "    for k in range(number_of_parts):\n",
    "        v = coord[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = coord[reverse_mapping[str(k0)]]\n",
    "            dist = np.sqrt((v[2]-v0[2])**2 + (v[1]-v0[1])**2)\n",
    "            cname = \"{}-{}\".format(k, k0)\n",
    "            row.append(dist)\n",
    "            if cname not in dataset:\n",
    "                dataset[cname] = []\n",
    "            dataset[cname].append(dist)\n",
    "    dataset_np.append(row)\n",
    "    dataset[\"volume\"].append(vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"2-3\"], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the gtsf data\n",
    "\n",
    "Loading the gtsf data points and creating the pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = ['/root/data/rds/formatted.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for jsonpath in jsonfiles:\n",
    "    with open(jsonpath, \"r\") as f:\n",
    "        jfile = json.load(f)\n",
    "        annotations += jfile\n",
    "print(\"Number of annotations: {}\".format(len(annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs per timestamp\n",
    "pairs = {}\n",
    "new_annotations = []\n",
    "for ann in annotations:\n",
    "    if ann['pen_id'] != 4:\n",
    "        continue\n",
    "    if ann['site_id'] != 23:\n",
    "        continue\n",
    "    if ann[\"species\"] != \"salmon\":\n",
    "        continue\n",
    "    if ann[\"kfactor\"] < 0.3:\n",
    "        continue\n",
    "    timestamp = ann[\"timestamp\"]\n",
    "    side = os.path.basename(ann[\"local_path\"]).split(\"_\")[0]\n",
    "    ann[\"side\"] = side\n",
    "    if timestamp not in pairs:\n",
    "        pairs[timestamp] = {}\n",
    "    pairs[timestamp][side] = ann\n",
    "    new_annotations.append(ann)\n",
    "full_pairs = [k for (k, v)in pairs.items() if \"left\" in v and \"right\" in v]\n",
    "print(\"Number of full pairs: {}\".format(len(full_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D to 3D \n",
    "\n",
    "Move from 2d pixel coordinates to 3d world coordinates. First, need to create pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pairs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquabyte.biomass import BiomassAnnotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = BiomassAnnotation(new_annotations, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair = bio.plot_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the keypoints and create world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquabyte.optics import depth_from_disp, convert_to_world_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'FOCAL_LENGTH': 0.0084366,\n",
    "#           'BASELINE': 0.128096,\n",
    "#           'PIXEL_SIZE_M': 3.45 * 1e-6,\n",
    "#           'FOCAL_LENGTH_PIXEL': 0.0084366 / (3.45 * 1e-6),\n",
    "#           'IMAGE_SENSOR_WIDTH': 0.01412,\n",
    "#           'IMAGE_SENSOR_HEIGHT': 0.01035,\n",
    "#           'PIXEL_COUNT_WIDTH': 4096,\n",
    "#           'PIXEL_COUNT_HEIGHT': 3000\n",
    "#          }\n",
    "params = {'BASELINE' : 0.10019751688037272,\n",
    "'FOCAL_LENGTH' : 0.013658357173918818,\n",
    "'FOCAL_LENGTH_PIXEL' :  3958.944108382266,\n",
    "'IMAGE_SENSOR_HEIGHT' : 0.01035,\n",
    "'IMAGE_SENSOR_WIDTH' : 0.01412,\n",
    "'PIXEL_COUNT_HEIGHT' : 3000,\n",
    "'PIXEL_COUNT_WIDTH' : 4096}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = {\"jitter\": False, \"delta\": 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = {}\n",
    "for ts in bio.full_pairs:\n",
    "    left_keypoints = bio.load_keypoints(ts, 'left', jitter)\n",
    "    right_keypoints = bio.load_keypoints(ts, 'right', jitter)\n",
    "    \n",
    "    # calculate disparities\n",
    "    disparities = left_keypoints[:, 0] - right_keypoints[:, 0]\n",
    "    # print(disparities)\n",
    "    # compute world key point\n",
    "    world_keypoints = {}\n",
    "    for (i, d) in enumerate(disparities):\n",
    "        depth = depth_from_disp(d, params)\n",
    "        world_coord = convert_to_world_point(left_keypoints[i, 0], left_keypoints[i, 1], depth, params)\n",
    "        world_keypoints[list(mapping.keys())[i]] = world_coord\n",
    "    world[ts] = world_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(left_keypoints[:, 0], left_keypoints[:, 1])\n",
    "# for i in range(number_of_parts):\n",
    "#     plt.text(left_keypoints[i, 0], left_keypoints[i, 1], list(mapping.keys())[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 10))\n",
    "# for (k, v) in world['190226010005'].items():\n",
    "#     plt.scatter(v[0], v[2])\n",
    "#     plt.text(v[0]+0.003, v[2]+0.003, k)\n",
    "#     plt.axis(\"scaled\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "\n",
    "First, let's calculate the pairwise distances for the gtsf data. Second let's find the closest Blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions_average = []\n",
    "ground_truth = []\n",
    "ids = []\n",
    "for ts in world:\n",
    "    # load keypoints\n",
    "    world_keypoints = world[ts]\n",
    "    # calculate distances\n",
    "    measurements= []\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = np.linalg.norm(v - v0)*1000 \n",
    "            measurements.append(dist)\n",
    "    \n",
    "    # find closest blender volume\n",
    "    # calculate l1 distance\n",
    "    diff = np.nanmean(np.abs(np.array(df)[:, :-1] - measurements), axis=1)\n",
    "    closest = np.argsort(diff)\n",
    "    idx = 10\n",
    "    closest5 = np.array(df)[closest[:idx], -1]\n",
    "#     print(\"closest volumes\", closest5)\n",
    "#     print(\"standard dev:\", np.std(closest5))\n",
    "#     print(\"estimated length\", measurements[13])\n",
    "    closest_length = np.array(list(df[\"2-3\"].iloc()[closest[:idx]]))\n",
    "    kfactor = 10**5*closest5 / closest_length**3\n",
    "#     print(\"closest length\", closest_length)\n",
    "#     print(\"closest kfactor\", kfactor)\n",
    "#     print(\"closest height\", list(df[\"4-6\"].iloc()[closest[:idx]]))\n",
    "#     print(\"#\"*50)\n",
    "    pred_volume = np.array(df)[closest[0], -1]\n",
    "    predictions.append(pred_volume)\n",
    "    predictions_average.append(np.mean(closest5))\n",
    "    ids.append(bio.pairs[ts]['left']['keypoint_annotation_id'])\n",
    "    # ground truth\n",
    "#     ground_truth_weight = [ann[\"weight\"] for ann in annotations if ann[\"timestamp\"] == ts][0]\n",
    "#     ground_truth_kfactor = [ann[\"kfactor\"] for ann in annotations if ann[\"timestamp\"] == ts][0]\n",
    "#     ground_truth.append([ground_truth_weight, ground_truth_kfactor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "predictions_average = np.array(predictions_average)\n",
    "ground_truth = np.array(ground_truth)\n",
    "# gt_weight = ground_truth[:, 0]\n",
    "# gt_kfactor = ground_truth[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = data['coeff'][0]\n",
    "intercept = data['coeff'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = predictions*slope + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = {}\n",
    "i = 0\n",
    "for ts in full_pairs:\n",
    "    date = pairs[ts]['left']['local_path'].split('date=')[1].split('/')[0]\n",
    "    if date not in dates:\n",
    "        dates[date] = []\n",
    "    dates[date].append(predictions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = sorted(dates.keys())\n",
    "# daily_mean = [(np.median(dates[d]) + np.mean(dates[d])) /2.0 for d in days]\n",
    "daily_mean = [np.mean(dates[d]) for d in days]\n",
    "daily_count = [len(dates[d]) for d in days]\n",
    "\n",
    "plt.plot(days, daily_count)\n",
    "plt.title('Daily count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(days, daily_mean)\n",
    "plt.title('Daily average biomass (g)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['coeff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(daily_mean)\n",
    "X = np.arange(1, len(daily_mean)+1)[:, np.newaxis]\n",
    "logY = np.log(Y)\n",
    "logX = np.log(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X, logY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, logY)\n",
    "plt.plot(X, X*reg.coef_ + reg.intercept_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.exp(reg.intercept_)\n",
    "coef = reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = intercept*np.exp(coef*X)\n",
    "plt.plot(X, Y, 'b')\n",
    "plt.plot(X, intercept*np.exp(coef*X), 'r')\n",
    "plt.legend(['data', 'fit'])\n",
    "plt.title('{} exp^{}t'.format(intercept, coef[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = preds.squeeze() - Y\n",
    "relative_error = (preds.squeeze() - Y) / Y * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(0, 101, 5)\n",
    "percentiles = np.percentile(np.abs(relative_error), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(percentiles, values)\n",
    "plt.yticks(np.arange(0,101,5))\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Absolute relative error (%)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPOPULATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sql_credentials = json.load(open('/root/thomas/sqlcredentials.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_credentials['user'] = 'thomas_the_fixer'\n",
    "sql_credentials['password'] = 'thomas2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_engine = create_engine(\n",
    "    \"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                                         sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                                         sql_credentials[\"database\"]))\n",
    "metadata = MetaData()\n",
    "biomass_table = Table('biomass_computations', metadata, autoload=True, autoload_with=sql_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(ids))):\n",
    "    query = biomass_table.update().where(biomass_table.c.keypoint_annotation_id == ids[i]).values(estimated_biomass_g = predictions[i])\n",
    "\n",
    "    connection = sql_engine.connect()\n",
    "    ex = None\n",
    "    try:\n",
    "        ex = connection.execute(query)\n",
    "    except exc.IntegrityError as e:\n",
    "        print(\"ERROR: failed query, {}\", query)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLD CODE**\n",
    "\n",
    "Quick OLS. \n",
    "\n",
    "$\\hat{\\beta} = (X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "(just for Alok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth = ground_truth[:, np.newaxis]\n",
    "# ground_truth.shape\n",
    "# A = np.linalg.inv(np.matmul(ground_truth.transpose(), ground_truth))\n",
    "# B = np.matmul(ground_truth.transpose(), predictions)\n",
    "# coeff = 1 / (A*B)\n",
    "# print(\"Reg coeff: {}\".format(coeff))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "# plt.scatter(ground_truth, predictions*coeff)\n",
    "# #plt.scatter(ground_truth, predictions)\n",
    "# plt.xlabel(\"Ground truth weight\")\n",
    "# plt.ylabel(\"Predicted weight\")\n",
    "# plt.axis(\"scaled\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear reg New code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from aquabyte.biomass import BiomassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc = BiomassAccuracy(ground_truth, predictions, 'test', split_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_kf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = bioacc.calculate_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_with_density(errors[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_with_density(errors[\"relative_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_sample_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
