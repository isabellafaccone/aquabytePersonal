{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_order = {\"TAIL_NOTCH\":0,\n",
    "                \"ADIPOSE_FIN\":1,\n",
    "                \"UPPER_LIP\":2,\n",
    "                \"ANAL_FIN\":3,\n",
    "                \"PELVIC_FIN\":4,\n",
    "                \"EYE\":5,\n",
    "                \"PECTORAL_FIN\":6,\n",
    "                \"DORSAL_FIN\":7}\n",
    "kps = [\"TAIL_NOTCH\",\n",
    "                \"ADIPOSE_FIN\",\n",
    "                \"UPPER_LIP\",\n",
    "                \"ANAL_FIN\",\n",
    "                \"PELVIC_FIN\",\n",
    "                \"EYE\",\n",
    "                \"PECTORAL_FIN\",\n",
    "                \"DORSAL_FIN\"]\n",
    "rkps = {str(v): k for (k,v) in keypoints_order.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/thomas/blender/volumes_all.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.array(data[\"dimensions\"])[:, 1], data[\"volume\"])\n",
    "# plt.ylabel(\"Volume (cm^3)\")\n",
    "# plt.xlabel(\"Length (mm)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"volume\"])\n",
    "plt.title(\"Blender volume histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pairwise distances from blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = data[\"mapping\"]\n",
    "reverse_mapping = data[\"reverse_mapping\"]\n",
    "reverse_mapping = rkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_parts = max(list(mapping.values()))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"volume\":[]}\n",
    "dataset_np = []\n",
    "kfactors = []\n",
    "for (coord, vol) in zip(data[\"coordinates\"], data[\"volume\"]):\n",
    "    row = []\n",
    "    for k in range(number_of_parts):\n",
    "        v = coord[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = coord[reverse_mapping[str(k0)]]\n",
    "            dist = np.sqrt((v[2]-v0[2])**2 + (v[1]-v0[1])**2)\n",
    "            cname = \"{}-{}\".format(k, k0)\n",
    "            row.append(dist)\n",
    "            if cname not in dataset:\n",
    "                dataset[cname] = []\n",
    "            dataset[cname].append(dist)\n",
    "    dataset_np.append(row)\n",
    "    dataset[\"volume\"].append(vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"2-3\"], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the gtsf data\n",
    "\n",
    "Loading the gtsf data points and creating the pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = ['/root/thomas/biomass_kp_predictions_val.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for jsonpath in jsonfiles:\n",
    "    with open(jsonpath, \"r\") as f:\n",
    "        jfile = json.load(f)\n",
    "        annotations += jfile\n",
    "print(\"Number of annotations: {}\".format(len(annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the local path for ease and rename the body parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann in annotations:\n",
    "    local_path = os.path.join(\"/root/data/gtsf_phase_I/\", \n",
    "                  \"/\".join(ann[\"Labeled Data\"].split(\"/\")[7:]))\n",
    "    ann[\"local_path\"] = local_path\n",
    "    if not os.path.isfile(local_path):\n",
    "        print(local_path)  \n",
    "        print(\"missing image!!\")\n",
    "    for body_part in ann[\"Label\"].keys():\n",
    "        new_body_part = \"_\".join(body_part.replace(\":\", \"\").split()).upper()\n",
    "        ann[\"Label\"][new_body_part] = ann[\"Label\"].pop(body_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table, select, func, and_, insert, delete, update, or_\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_credentials = json.load(open(\"/root/thomas/sql_research_credentials.json\"))\n",
    "\n",
    "sql_engine = create_engine(\n",
    "    \"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                                         sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                                         sql_credentials[\"database\"]))\n",
    "\n",
    "metadata = MetaData()\n",
    "gtsf = Table('gtsf_data_collections', metadata, autoload=True, autoload_with=sql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "for ann in annotations:\n",
    "    timestamp = ann[\"local_path\"].split(\"/\")[-3]\n",
    "    ann[\"timestamp\"] = timestamp\n",
    "    timestamps.append(ann[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = select([gtsf.c.ground_truth_metadata,\n",
    "                gtsf.c.gtsf_fish_identifier]).select_from(gtsf).where(gtsf.c.gtsf_fish_identifier.in_(timestamps))\n",
    "connection = sql_engine.connect()\n",
    "q = connection.execute(query)\n",
    "results = [(eval(r[0]), r[1]) for r in q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann in annotations:\n",
    "    for r in results:\n",
    "        if r[1] == ann[\"timestamp\"]:\n",
    "            ann[\"weight\"] = r[0][\"data\"][\"weight\"]\n",
    "            ann[\"breath\"] = r[0][\"data\"][\"breath\"]\n",
    "            ann[\"length\"] = r[0][\"data\"][\"length\"]\n",
    "            ann[\"width\"] = r[0][\"data\"][\"width\"]\n",
    "            ann[\"kfactor\"] = 10**5*ann[\"weight\"] / ann[\"length\"]**3\n",
    "            ann[\"species\"] = r[0][\"data\"].get(\"species\", \"salmon\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfactor = np.array([ann[\"kfactor\"] for ann in annotations])\n",
    "plt.hist(kfactor)\n",
    "plt.title(\"K factor distribution of GTSF data\")\n",
    "plt.xlabel(\"K factor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D to 3D \n",
    "\n",
    "Move from 2d pixel coordinates to 3d world coordinates. First, need to create pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pairs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquabyte.biomass import BiomassAnnotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = BiomassAnnotation(annotations, kps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = bio.plot_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the keypoints and create world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquabyte.optics import depth_from_disp, convert_to_world_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'FOCAL_LENGTH': 0.0084366,\n",
    "          'BASELINE': 0.128096,\n",
    "          'PIXEL_SIZE_M': 3.45 * 1e-6,\n",
    "          'FOCAL_LENGTH_PIXEL': 0.0084366 / (3.45 * 1e-6),\n",
    "          'IMAGE_SENSOR_WIDTH': 0.01412,\n",
    "          'IMAGE_SENSOR_HEIGHT': 0.01035,\n",
    "          'PIXEL_COUNT_WIDTH': 4096,\n",
    "          'PIXEL_COUNT_HEIGHT': 3000\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = {\"jitter\": False, \"delta\": 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = {}\n",
    "for ts in bio.full_pairs:\n",
    "#     left_keypoints = bio.load_keypoints(ts, 'left', jitter)\n",
    "#     right_keypoints = bio.load_keypoints(ts, 'right', jitter)\n",
    "    left_keypoints = np.array(bio.pairs[ts]['left']['predictions'])\n",
    "    right_keypoints = np.array(bio.pairs[ts]['right']['predictions'])\n",
    "    \n",
    "    # calculate disparities\n",
    "#     disparities = left_keypoints[:, 0] - right_keypoints[:, 0]\n",
    "    disparities = left_keypoints[:, 0] - right_keypoints[:, 0]\n",
    "#     print(disparities)\n",
    "#     print(pdisparities)\n",
    "#     print(disparities - pdisparities)\n",
    "#     print('#'*50)\n",
    "    # compute world key point\n",
    "    world_keypoints = {}\n",
    "    for (i, d) in enumerate(disparities):\n",
    "        depth = depth_from_disp(d, params)\n",
    "        world_coord = convert_to_world_point(left_keypoints[i, 0], left_keypoints[i, 1], depth, params)\n",
    "        world_keypoints[kps[i]] = world_coord\n",
    "    world[ts] = world_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(left_keypoints[:, 0], left_keypoints[:, 1])\n",
    "for i in range(number_of_parts):\n",
    "    plt.text(left_keypoints[i, 0], left_keypoints[i, 1], kps[i])\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in world.keys():\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for (k, v) in world[ts].items():\n",
    "        plt.scatter(v[0], v[2])\n",
    "        plt.text(v[0]+0.003, v[2]+0.003, k)\n",
    "        plt.axis(\"scaled\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "\n",
    "First, let's calculate the pairwise distances for the gtsf data. Second let's find the closest Blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions_average = []\n",
    "ground_truth = []\n",
    "\n",
    "for ts in world:\n",
    "    # load keypoints\n",
    "    world_keypoints = world[ts]\n",
    "    # calculate distances\n",
    "    measurements= []\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[rkps[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[rkps[str(k0)]]\n",
    "            dist = np.linalg.norm(v - v0)*1000 \n",
    "            measurements.append(dist)\n",
    "    print(measurements)\n",
    "    # find closest blender volume\n",
    "    # calculate l1 distance\n",
    "    diff = np.nanmean(np.abs(np.array(df)[:, :-1] - measurements), axis=1)\n",
    "    closest = np.argsort(diff)\n",
    "    idx = 10\n",
    "    closest5 = np.array(df)[closest[:idx], -1]\n",
    "    print(\"closest volumes\", closest5)\n",
    "    print(\"standard dev:\", np.std(closest5))\n",
    "    print(\"estimated length\", measurements[13])\n",
    "    closest_length = np.array(list(df[\"2-3\"].iloc()[closest[:idx]]))\n",
    "    kfactor = 10**5*closest5 / closest_length**3\n",
    "    print(\"closest length\", closest_length)\n",
    "    print(\"closest kfactor\", kfactor)\n",
    "    print(\"closest height\", list(df[\"4-6\"].iloc()[closest[:idx]]))\n",
    "    print(\"#\"*50)\n",
    "    pred_volume = np.array(df)[closest[0], -1]\n",
    "    predictions.append(pred_volume)\n",
    "    predictions_average.append(np.mean(closest5))\n",
    "    \n",
    "    # ground truth\n",
    "    ground_truth_weight = [ann[\"weight\"] for ann in annotations if ann[\"timestamp\"] == ts][0]\n",
    "    ground_truth_kfactor = [ann[\"kfactor\"] for ann in annotations if ann[\"timestamp\"] == ts][0]\n",
    "    ground_truth.append([ground_truth_weight, ground_truth_kfactor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "predictions_average = np.array(predictions_average)\n",
    "ground_truth = np.array(ground_truth)\n",
    "gt_weight = ground_truth[:, 0]\n",
    "gt_kfactor = ground_truth[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gt_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLD CODE**\n",
    "\n",
    "Quick OLS. \n",
    "\n",
    "$\\hat{\\beta} = (X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "(just for Alok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth = ground_truth[:, np.newaxis]\n",
    "# ground_truth.shape\n",
    "# A = np.linalg.inv(np.matmul(ground_truth.transpose(), ground_truth))\n",
    "# B = np.matmul(ground_truth.transpose(), predictions)\n",
    "# coeff = 1 / (A*B)\n",
    "# print(\"Reg coeff: {}\".format(coeff))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "# plt.scatter(ground_truth, predictions*coeff)\n",
    "# #plt.scatter(ground_truth, predictions)\n",
    "# plt.xlabel(\"Ground truth weight\")\n",
    "# plt.ylabel(\"Predicted weight\")\n",
    "# plt.axis(\"scaled\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions[:, np.newaxis]\n",
    "reg = LinearRegression().fit(predictions, gt_weight)\n",
    "print(reg.coef_, reg.intercept_)\n",
    "print(\"R2 : {}\".format(reg.score(predictions, gt_weight)))\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gt_weight, predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "plt.scatter(gt_weight, predictions*reg.coef_ + reg.intercept_, c=gt_kfactor)\n",
    "#plt.scatter(ground_truth, predictions)\n",
    "plt.xlabel(\"Ground truth weight\")\n",
    "plt.ylabel(\"Predicted weight\")\n",
    "plt.colorbar()\n",
    "plt.clim([0.8, 1.6])\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear reg New code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from aquabyte.biomass import BiomassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'Biomass prediction from keypoint predictions. Total population {} pairs, split {}'.format(len(bio.full_pairs), 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc = BiomassAccuracy(ground_truth, predictions, description, split_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_kf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = bioacc.calculate_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_with_density(errors[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_with_density(errors[\"relative_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioacc.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bioacc.plot_sample_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
