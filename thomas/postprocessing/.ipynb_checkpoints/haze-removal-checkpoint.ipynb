{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr2rgb(img):\n",
    "    b,g,r = cv.split(img)\n",
    "    return cv.merge([r,g,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of a hazed image that we would be working with. The haze faintly obscures the foreground. We want to recover the unhazed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = bgr2rgb(cv.imread('/root/data/aquabyte-images/erko-rotoy/2018-09-24/1/left_erko-rotoy_1_1537795190521.jpg'))\n",
    "img = cv.resize(img, (1000, 750))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the formation of the hazed image, we use the equation \n",
    "\n",
    "$$I(x) = J(x)t(x) + A(1 - t(x))$$\n",
    "\n",
    "where $I$ is the observed intensity, $J$ is the scene radiance, $A$ is the global atmospheric light, and $t$ is the medium transmission. Intuitively, the haze image is a linear combination of the scene radiance and scattered atmospheric light. If we can recover $A$ and $t$ from the hazed image, then we can use this model to solve for $J$, the unhazed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Dark Channel Prior\n",
    "The dark channel prior is defined as: $$J^{dark}(x) = \\min_{c \\in \\{r,g,b\\}}(\\min_{y \\in \\Omega(x)}(J^{c}(y)))$$\n",
    "\n",
    "$\\mathbf{J}$ is our image, $J^{c}$ is defined as a color channel of our image (one of red, blue, or green), and $\\Omega(x)$ is a patch of pixels centered at $x$. \n",
    "\n",
    "Intuitively, $J^{dark}$ should be low in a haze-free region of the image. Colorful objects will lack intensity in at least one color channel, resulting in a low $J^{dark}$. However, in the lighter colors in the sky regions of the image will have higher $J^{dark}$. We'll use this information later to estimate the atmospheric light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dark_channel_prior(img, w_size=15):\n",
    "    \"\"\"\n",
    "    img    -> 3D tensor in RGB format\n",
    "    w_size -> size of patch to consider (default is 15)\n",
    "    \"\"\"    \n",
    "    J_dark = ndimage.minimum_filter(img, footprint=np.ones((w_size,w_size,3)), mode='nearest')\n",
    "            \n",
    "    return J_dark[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underwater_get_dark_channel_prior(img, w_size=15):\n",
    "    \"\"\"\n",
    "    img    -> 3D tensor in RGB format\n",
    "    w_size -> size of patch to consider (default is 15)\n",
    "    \"\"\"    \n",
    "    J_dark = ndimage.minimum_filter(img, footprint=np.ones((w_size,w_size,3)), mode='nearest')\n",
    "            \n",
    "    return J_dark[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dark channel prior from the image. In the foreground, the dark channel prior has low intensity, while in the sky regions, the intensity is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tian_jdark = get_dark_channel_prior(img)\n",
    "plt.imshow(tian_jdark, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of Atmospheric Light\n",
    "We assume that the atmospheric light is a constant for each channel. Also, we calculate the atmospheric light for each channel in the image since Equation (6) from the paper references $A^{c}$, indicating that $A$ is different depending on what channel that is being used in the calculations.\n",
    "\n",
    "The procedure is as follows:\n",
    "1. Calculate the dark channel prior.\n",
    "2. Pick the 0.1% brightest pixels in the dark channel.\n",
    "3. Find the maximum intensity among the pixels in the original image $I$ for each of the channels.\n",
    "\n",
    "A naive approach would be to simply take the highest intensities in all three color channels of the image and use that as the atmospheric light. However, this method is susceptible to incorrect results if there is a white object in the foreground. \n",
    "\n",
    "To circumvent this, we use the method of calculating the dark channel prior since it will limit our search for atmospheric light in the sky region of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_atmospheric_light(img, w_size=15):\n",
    "    \"\"\"\n",
    "    img -> 3D tensor in RGB format\n",
    "    \n",
    "    ret -> \n",
    "        A_r |\n",
    "        A_g | -> estimated atmospheric light in the RGB channels\n",
    "        A_c |\n",
    "    \"\"\"\n",
    "    size = img.shape[:2]\n",
    "    k = int(0.001*np.prod(size))\n",
    "    j_dark = get_dark_channel_prior(img, w_size=w_size)\n",
    "    idx = np.argpartition(-j_dark.ravel(),k)[:k]\n",
    "    x, y = np.hsplit(np.column_stack(np.unravel_index(idx, size)), 2)\n",
    "    \n",
    "    A = np.array([img[x,y,0].max(), img[x,y,1].max(), img[x,y,2].max()])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we replicate the detection of atmospheric light, similar to figure 7 (first row, third column). The pixels we inspect for the atmospheric light (shown in red) is in the same region as that of the paper's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = img.shape[:2]\n",
    "k = int(0.001*np.prod(size))\n",
    "idx = np.argpartition(-tian_jdark.ravel(),k)[:k]\n",
    "zz = np.column_stack(np.unravel_index(idx, tian_jdark.shape))\n",
    "x, y = np.hsplit(zz,2)\n",
    "plt.imshow(img)\n",
    "plt.scatter(y, x, c='r')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Transmission\n",
    "We calculate the transmission $\\bar{t}(x)$ through\n",
    "\n",
    "$$\\bar{t}(x) = 1 - \\omega\\min_{c \\in \\{r,g,b\\}}(\\min_{y \\in \\Omega(x)}(\\frac{J^{c}(y)}{A^{c}}))$$\n",
    "\n",
    "Intuitively, after normalizing the image using the atmospheric light, we know the fraction of atmospheric light that defines each pixel. From the model of the hazed image, we know that the hazed image is a linear combination of the atmospheric light and the original unhazed image. So, we can do 1 - this fraction to recover the fraction of the original image that defines each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transmission(img, omega=0.95, w_size=15):\n",
    "    \"\"\"\n",
    "    Estimates the transmission map using the dark channel prior of the normalized image. \n",
    "    A small fraction, omega, of the haze is kept to retain depth perspective after haze removal.\n",
    "    \n",
    "    img   -> 3D Tensor in RGB format\n",
    "    omega -> fraction of haze to keep in image (default is 0.95)\n",
    "    \"\"\"\n",
    "    A= estimate_atmospheric_light(img)\n",
    "    norm_img = img / A\n",
    "    norm_img_dc = get_dark_channel_prior(norm_img, w_size=w_size)\n",
    "\n",
    "    return 1 - omega*norm_img_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_map = estimate_transmission(img)\n",
    "plt.imshow(alpha_map, cmap='gray')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Matting\n",
    "In this stage, we refine the alpha map to outline the profile of the objects in the image, allowing our transmission map to be correctly expressed. The paper uses a Matting Laplacian matrix to implement this refinement, but in another paper by the author he proposes a guided filter to achieve refinement. We will use this approach for our project.\n",
    "\n",
    "We can vectorize the calculation of the guided filter by using two facts to calculate the covariance matrix for each patch:\n",
    "1. $\\Sigma_{ij} = cov(X_i, X_j) = \\mathbb{E}[(X_i - \\mu_i)(X_j - \\mu_j)] = \\mathbb{E}[X_iX_j] - \\mu_i\\mu_j$ [[1]](https://en.wikipedia.org/wiki/Covariance_matrix)\n",
    "2. $\\Sigma = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} & \\Sigma_{13} \\\\ \\Sigma_{21} & \\Sigma_{22} & \\Sigma_{23} \\\\ \\Sigma_{31} & \\Sigma_{32} & \\Sigma_{33}\\end{bmatrix}$. Note that $\\Sigma_{ij} = \\Sigma_{ji}$.\n",
    "\n",
    "This will generate the covariance matrix for each patch in equation (14) from [He et. al's guided filter paper](http://kaiminghe.com/publications/eccv10guidedfilter.pdf). With this done, the subsequent calculations are easy to implement.\n",
    "\n",
    "Originally, we tried to use the Laplacian Soft Matting covered in the paper, but the implementation was too tedious. Further, the actual implementation of Laplacian Soft Matting is computationally inefficent. As we do **not** need an exact solution to the Matting Laplacian, and [He et. al's guided filter paper](http://kaiminghe.com/publications/eccv10guidedfilter.pdf) shows that the guided filter is an approximate solution, we chose not to implement it. The guided filter was easier to reason about and gave outstanding results in refining the transmission map.\n",
    "\n",
    "Interestingly, in our testing there was little differnce between a guided filter based on the grayscale version of each photo vs using all 3 channels was minor. On some level this makes sense as most features can be distinguished clearly based on just intensity, however, it is clear that there can be boundaries that will be missed in some images due to foreground and background intensities blending together in the grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_filter(I, p, omega=60, eps=0.01):\n",
    "    \"\"\"\n",
    "    from http://kaiminghe.com/publications/eccv10guidedfilter.pdf\n",
    "    and  https://arxiv.org/pdf/1505.00996.pdf\n",
    "    \n",
    "    I     -> guidance image, 3D Tensor in RGB format\n",
    "    p     -> filtering input image, \n",
    "    omega -> window size (default is 60)\n",
    "    eps   -> regularization parameter (default 0.01)\n",
    "    \"\"\"\n",
    "    \n",
    "    w_size = (omega,omega)\n",
    "    I = I/255\n",
    "    I_r, I_g, I_b = I[:,:,0], I[:,:,1], I[:,:,2]\n",
    "    \n",
    "    mean_I_r = cv.blur(I_r, w_size)\n",
    "    mean_I_g = cv.blur(I_g, w_size)\n",
    "    mean_I_b = cv.blur(I_b, w_size)\n",
    "    \n",
    "    mean_p = cv.blur(p, w_size)\n",
    "    \n",
    "    mean_Ip_r = cv.blur(I_r*p, w_size)\n",
    "    mean_Ip_g = cv.blur(I_g*p, w_size)\n",
    "    mean_Ip_b = cv.blur(I_b*p, w_size)\n",
    "         \n",
    "    cov_Ip_r =  mean_Ip_r - mean_I_r*mean_p\n",
    "    cov_Ip_g =  mean_Ip_g - mean_I_g*mean_p\n",
    "    cov_Ip_b =  mean_Ip_b - mean_I_b*mean_p\n",
    "    cov_Ip = np.stack([cov_Ip_r, cov_Ip_g, cov_Ip_b], axis=-1)\n",
    "    \n",
    "    var_I_rr = cv.blur(I_r*I_r, w_size) - mean_I_r*mean_I_r\n",
    "    var_I_rg = cv.blur(I_r*I_g, w_size) - mean_I_r*mean_I_g\n",
    "    var_I_rb = cv.blur(I_r*I_b, w_size) - mean_I_r*mean_I_b\n",
    "    var_I_gb = cv.blur(I_g*I_b, w_size) - mean_I_g*mean_I_b\n",
    "    var_I_gg = cv.blur(I_g*I_g, w_size) - mean_I_g*mean_I_g\n",
    "    var_I_bb = cv.blur(I_b*I_b, w_size) - mean_I_b*mean_I_b\n",
    "    \n",
    "    a = np.zeros(I.shape)\n",
    "    for x, y in np.ndindex(I.shape[:2]):\n",
    "        Sigma = np.array([\n",
    "            [var_I_rr[x,y], var_I_rg[x,y], var_I_rb[x,y]],\n",
    "            [var_I_rg[x,y], var_I_gg[x,y], var_I_gb[x,y]],\n",
    "            [var_I_rb[x,y], var_I_gb[x,y], var_I_bb[x,y]]\n",
    "        ])\n",
    "        c = cov_Ip[x,y,:]\n",
    "        \n",
    "        a[x,y,:] = np.linalg.inv(Sigma + eps*np.eye(3)).dot(c)\n",
    "        \n",
    "    mean_a = np.stack([cv.blur(a[:,:,0], w_size), cv.blur(a[:,:,1], w_size), cv.blur(a[:,:,2], w_size)], axis=-1)\n",
    "    mean_I = np.stack([mean_I_r, mean_I_g, mean_I_b], axis=-1)\n",
    "    \n",
    "    b = mean_p - np.sum(a*mean_I, axis=2)\n",
    "    mean_b = cv.blur(b, w_size)\n",
    "    q = np.sum(mean_a*I, axis=2) + mean_b\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_alpha_map = guided_filter(img, alpha_map, omega=200, eps=1e-3)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ax1.imshow(alpha_map, cmap=\"gray\")\n",
    "ax1.set_title(\"Original\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(f_alpha_map, cmap=\"gray\")\n",
    "ax2.set_title(\"w/ Guided Filter\")\n",
    "ax2.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the Scene Radiance\n",
    "We now implement the full pipeline of haze removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haze_removal(img, w_size=15, a_omega=0.95, gf_w_size=200, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Implements the haze removal pipeline from \n",
    "    Single Image Haze Removal Using Dark Channel Prior by He et al. (2009)\n",
    "    \n",
    "    I       -> 3D tensor in RGB format\n",
    "    w_size  -> window size of local patch (default is 15)\n",
    "    a_omega -> fraction of haze to keep in image (default is 0.95)\n",
    "    omega   -> window size for guided filter (default is 200)\n",
    "    eps     -> regularization parameter for guided filter(default 1e-6)\n",
    "    \"\"\"\n",
    "    img = img.astype(np.int16)\n",
    "    A = estimate_atmospheric_light(img, w_size=w_size)\n",
    "    alpha_map = estimate_transmission(img, omega=a_omega, w_size=w_size)\n",
    "    f_alpha_map = guided_filter(img, alpha_map, omega=gf_w_size, eps=eps)\n",
    "    \n",
    "    img[:,:,0] -= A[0]\n",
    "    img[:,:,1] -= A[1]\n",
    "    img[:,:,2] -= A[2]\n",
    "    z = np.maximum(f_alpha_map, 0.1)\n",
    "    img[:,:,0] = img[:,:,0]/z\n",
    "    img[:,:,1] = img[:,:,1]/z\n",
    "    img[:,:,2] = img[:,:,2]/z\n",
    "\n",
    "    img[:,:,0] += A[0]\n",
    "    img[:,:,1] += A[1]\n",
    "    img[:,:,2] += A[2]\n",
    "\n",
    "    img = np.maximum(img, 0)\n",
    "    img = np.minimum(img, 255)\n",
    "    \n",
    "    return img, f_alpha_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, _ = haze_removal(img, w_size=15, a_omega=0.95, gf_w_size=200, eps=1e-6)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title(\"Original\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(l)\n",
    "ax2.set_title(\"After Haze Removal\")\n",
    "ax2.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replicate our pipeline on the images used in [He's paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206515&tag=1). The left column is the original image with haze, the middle column is the refined transmission map, generated from the dark channel prior and guided filter, and on the right is the image with the haze removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "for idx, file in enumerate(glob.glob(\"./images/*\")):\n",
    "    img = bgr2rgb(cv.imread(file))    \n",
    "    dehazed, f_alpha_map = haze_removal(img)\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,15))\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(f_alpha_map, cmap=\"gray\")\n",
    "    ax2.axis(\"off\")\n",
    "    ax3.imshow(dehazed)\n",
    "    ax3.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
