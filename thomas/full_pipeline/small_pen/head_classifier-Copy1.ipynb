{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = [\"/root/data/small_pen_data_collection/body_parts_detection_merged.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_center_data = []\n",
    "for jfile in jsonfiles:\n",
    "    coco = COCO(jfile)\n",
    "    image_ids = coco.getImgIds()\n",
    "    cats = coco.cats\n",
    "\n",
    "    for image_id in image_ids:\n",
    "        image_data = coco.loadImgs([image_id])[0]\n",
    "        if \"local_path\" not in image_data:\n",
    "            continue\n",
    "\n",
    "        annotation_ids = coco.getAnnIds(imgIds=[image_data['id']] )   \n",
    "        headid = coco.getAnnIds(imgIds=[image_data['id']], catIds=[2])\n",
    "        if len(coco.loadAnns(headid)) > 0:\n",
    "            head = coco.loadAnns(headid)\n",
    "            for h in head:\n",
    "                tmp = {}\n",
    "                tmp['local_path'] = image_data['local_path'].replace(\"sotra-small-pen_0\", \"small-pen-test-site_1\")\n",
    "\n",
    "                tmp['head_bbox'] = h['bbox']\n",
    "                tmp['head_mask'] = h['segmentation']\n",
    "                tmp['jfile'] = jfile\n",
    "\n",
    "                eye_center_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ec in tqdm(eye_center_data):\n",
    "    # load the image\n",
    "    new_path = '/root/data/reidentification/heads/{}.head.jpg'.format(os.path.basename(ec['local_path']))\n",
    "    if os.path.isfile(new_path):\n",
    "        continue\n",
    "    image = io.imread(ec['local_path'])\n",
    "    \n",
    "    x1, y1, width, height = [int(c) for c in ec['head_bbox']]\n",
    "    head = image[y1:y1+height, x1:x1+width, :]\n",
    "    \n",
    "    io.imsave(new_path, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from skimage.transform import resize\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "from keras.applications.mobilenet import MobileNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = glob.glob('/root/data/reidentification/heads_with_eye/small-pen-test-site/*')\n",
    "print(len(heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(heads[0]).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingdic = {}\n",
    "all_path = glob.glob('/root/data/small_pen_data_collection/1*/*.jpg')\n",
    "for path in all_path:\n",
    "    mappingdic[os.path.basename(path).split('.')[0]] = path.split('/')[-2].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiencedic = {}\n",
    "for head in heads:\n",
    "    k = os.path.basename(head).split('.')[0]\n",
    "    exp = mappingdic[k]\n",
    "    if exp not in experiencedic:\n",
    "        experiencedic[exp] = []\n",
    "    experiencedic[exp].append(head)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k, v) in experiencedic.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nclasses = len(experiencedic.keys())\n",
    "print(nclasses)\n",
    "input_size = (128, 128, 3)\n",
    "experiences = list(experiencedic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(experiencedic, batch_size, input_size):\n",
    "    \n",
    "    while True:\n",
    "        x_batch = np.zeros((batch_size, input_size[0], input_size[1], input_size[2]))\n",
    "        y_batch = np.zeros((batch_size, nclasses))\n",
    "        for i in range(batch_size):\n",
    "            random_exp = np.random.choice(list(experiencedic.keys()))\n",
    "            random_head = np.random.choice(experiencedic[random_exp])\n",
    "            img = io.imread(random_head)\n",
    "            head = resize(img, input_size)\n",
    "            xscale = input_size[0] / img.shape[0]\n",
    "            yscale = input_size[1] / img.shape[1]\n",
    "            x_batch[i, ...] = head\n",
    "            y_batch[i, experiences.index(random_exp)] = 1\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(experiencedic, batch_size, input_size)\n",
    "# val_gen = generator(experiencedic, batch_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(train_gen)\n",
    "for i in range(batch_size):\n",
    "    f, ax = plt.subplots(1)\n",
    "    ax.imshow(xb[i, ...])\n",
    "    ax.set_title(experiences[np.argmax(yb[i, ...])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "dropout=1e-3\n",
    "classes = nclasses\n",
    "shape = (1, 1, int(1024 * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet = MobileNet(input_shape=input_size, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.GlobalAveragePooling2D()(mnet.output)\n",
    "x = layers.Reshape(shape, name='reshape_1')(x)\n",
    "x = layers.Dropout(dropout, name='dropout')(x)\n",
    "x = layers.Conv2D(classes, (1, 1),\n",
    "                  padding='same',\n",
    "                  name='conv_preds')(x)\n",
    "x = layers.Activation('softmax', name='act_softmax')(x)\n",
    "x = layers.Reshape((classes,), name='reshape_2')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=mnet.inputs, outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\", \"top_k_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch= len(heads) // batch_size, \n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'conv1'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "# # build the VGG16 network with ImageNet weights\n",
    "# model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "# print('Model loaded.')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(32):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 4\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        width_margin = (img_width + margin) * i\n",
    "        height_margin = (img_height + margin) * j\n",
    "        stitched_filters[\n",
    "            width_margin: width_margin + img_width,\n",
    "            height_margin: height_margin + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "plt.imshow(stitched_filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kept_filters[10][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
