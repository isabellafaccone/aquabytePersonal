{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from fancyimpute import KNN, SimpleFill\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high values\n",
    "df[df.iloc[:, 1:-2] > 1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477, 39)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/477 with 0 missing, elapsed time: 0.059\n",
      "Imputing row 101/477 with 0 missing, elapsed time: 0.063\n",
      "Imputing row 201/477 with 8 missing, elapsed time: 0.068\n",
      "Imputing row 301/477 with 0 missing, elapsed time: 0.073\n",
      "Imputing row 401/477 with 0 missing, elapsed time: 0.078\n"
     ]
    }
   ],
   "source": [
    "imputation_types = 'knn' # 'knn', 'mean', 'median'\n",
    "if imputation_types == 'mean':\n",
    "    df_filled = SimpleFill().fit_transform(df.iloc[:, 1:-2])\n",
    "    df.iloc[:, 1:-2] = df_filled\n",
    "elif imputation_types == 'knn':\n",
    "    df_filled = KNN().fit_transform(df.iloc[:, 1:-2])\n",
    "    df.iloc[:, 1:-2] = df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 357\n"
     ]
    }
   ],
   "source": [
    "# get predictors and target\n",
    "train_x = np.array(train.iloc[:, 1:-2])\n",
    "train_y = np.array(train.iloc[:, -2])\n",
    "print(\"Number of training samples: {}\".format(train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing samples: 120\n"
     ]
    }
   ],
   "source": [
    "# get predictors and target\n",
    "test_x = np.array(test.iloc[:, 1:-2])\n",
    "test_y = np.array(test.iloc[:, -2])\n",
    "print(\"Number of testing samples: {}\".format(test_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "clf = gnb.fit(train_x, train_y)\n",
    "train_y_predicted = clf.predict(train_x)\n",
    "test_y_predicted = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801462.868347339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(train_y, train_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962014.25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_y, test_y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression - train - test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training sets\n",
    "regr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ -5179.40628865  -1481.28408009   2160.1110729   13237.15040679\n",
      "   7208.05062561 -16483.18149984   5608.85950899  -6227.90241946\n",
      "   3541.75173936  -1910.53038573  -5399.77517226   1309.03873278\n",
      "   8999.79450257  -3496.29449176   2287.41369568  -6549.05684787\n",
      "   1893.0635046   -1290.12186116  -6697.14519353   1408.53674814\n",
      "  12466.08460914   4839.17072163   1797.69989056   2545.64701048\n",
      "  -1181.52354469   4816.95789985   4011.30215605   -499.38309433\n",
      "  15004.87588038 -16872.92460598  -2163.8565021   -5980.51402239\n",
      "  -5352.10228886  -5742.89616763  13636.48857734  -5566.75279206]\n",
      "Mean squared error: 518405.45\n",
      "Variance score: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "pred_train = regr.predict(train_x)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(train_y, pred_train))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(train_y, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ -5179.40628865  -1481.28408009   2160.1110729   13237.15040679\n",
      "   7208.05062561 -16483.18149984   5608.85950899  -6227.90241946\n",
      "   3541.75173936  -1910.53038573  -5399.77517226   1309.03873278\n",
      "   8999.79450257  -3496.29449176   2287.41369568  -6549.05684787\n",
      "   1893.0635046   -1290.12186116  -6697.14519353   1408.53674814\n",
      "  12466.08460914   4839.17072163   1797.69989056   2545.64701048\n",
      "  -1181.52354469   4816.95789985   4011.30215605   -499.38309433\n",
      "  15004.87588038 -16872.92460598  -2163.8565021   -5980.51402239\n",
      "  -5352.10228886  -5742.89616763  13636.48857734  -5566.75279206]\n",
      "Mean squared error: 1536626.45\n",
      "Variance score: -0.05\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "pred_test = regr.predict(test_x)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(test_y, pred_test))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_y, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, pval = f_regression(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.83500495e-01, 8.41780155e-33, 1.99954118e-24, 3.39158112e-40,\n",
       "       4.84595985e-25, 1.15851909e-05, 4.22184641e-01, 1.47824164e-09,\n",
       "       1.36736245e-33, 7.48601683e-26, 1.75691523e-38, 2.80092686e-21,\n",
       "       2.29043309e-05, 5.69737702e-01, 1.47836387e-07, 9.62245735e-16,\n",
       "       2.90218353e-05, 2.72828992e-02, 2.28541290e-08, 3.03100885e-18,\n",
       "       2.77177355e-34, 2.99415260e-23, 1.70111932e-06, 4.47079451e-04,\n",
       "       2.70817155e-10, 1.21866129e-06, 6.05748077e-02, 2.18678106e-06,\n",
       "       4.68605699e-23, 8.19923037e-26, 4.13402919e-10, 1.97875236e-10,\n",
       "       2.30221092e-07, 6.86957854e-02, 6.31613362e-02, 5.68082911e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick neural net in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import math\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=train_x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(100, input_dim=train_x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(50, input_dim=train_x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(25, input_dim=train_x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.adam(lr=0.01, decay=0.0)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 500.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    print(lrate)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.lr.append(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print(K.eval(lr_with_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100)\n",
    "lh = LossHistory()\n",
    "pl = MyCallback()\n",
    "lrate = keras.callbacks.LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 357 samples, validate on 120 samples\n",
      "Epoch 1/2000\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 3269.5786 - val_loss: 3301.7460\n",
      "Epoch 2/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 3241.6996 - val_loss: 3253.0656\n",
      "Epoch 3/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 3174.5377 - val_loss: 3157.1335\n",
      "Epoch 4/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 3055.6576 - val_loss: 3003.5503\n",
      "Epoch 5/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 2876.0711 - val_loss: 2785.1849\n",
      "Epoch 6/2000\n",
      "357/357 [==============================] - 0s 69us/step - loss: 2629.6721 - val_loss: 2496.4934\n",
      "Epoch 7/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 2313.6551 - val_loss: 2134.8866\n",
      "Epoch 8/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 1923.6864 - val_loss: 1702.4199\n",
      "Epoch 9/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 1473.3098 - val_loss: 1233.8945\n",
      "Epoch 10/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 1044.2303 - val_loss: 918.0749\n",
      "Epoch 11/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 792.0629 - val_loss: 763.5878\n",
      "Epoch 12/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 714.2843 - val_loss: 765.8436\n",
      "Epoch 13/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 724.2177 - val_loss: 765.2110\n",
      "Epoch 14/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 719.4750 - val_loss: 757.8717\n",
      "Epoch 15/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 717.9589 - val_loss: 756.9452\n",
      "Epoch 16/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 714.3075 - val_loss: 758.0670\n",
      "Epoch 17/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 715.2237 - val_loss: 757.3015\n",
      "Epoch 18/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 714.9560 - val_loss: 756.3982\n",
      "Epoch 19/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 714.9276 - val_loss: 756.1985\n",
      "Epoch 20/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 714.0598 - val_loss: 755.9793\n",
      "Epoch 21/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 714.3729 - val_loss: 756.0422\n",
      "Epoch 22/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 713.5013 - val_loss: 755.2511\n",
      "Epoch 23/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 715.3426 - val_loss: 755.1220\n",
      "Epoch 24/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 714.2112 - val_loss: 756.7464\n",
      "Epoch 25/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 714.4102 - val_loss: 756.2894\n",
      "Epoch 26/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 712.8369 - val_loss: 754.2709\n",
      "Epoch 27/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 713.2818 - val_loss: 754.0169\n",
      "Epoch 28/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 712.7677 - val_loss: 754.7463\n",
      "Epoch 29/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 712.0054 - val_loss: 754.2856\n",
      "Epoch 30/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 711.4513 - val_loss: 753.7936\n",
      "Epoch 31/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 711.6032 - val_loss: 754.1627\n",
      "Epoch 32/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 711.1962 - val_loss: 753.6524\n",
      "Epoch 33/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 711.1856 - val_loss: 753.0531\n",
      "Epoch 34/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 711.8213 - val_loss: 754.4038\n",
      "Epoch 35/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 711.4399 - val_loss: 754.7511\n",
      "Epoch 36/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 711.0669 - val_loss: 752.6712\n",
      "Epoch 37/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 709.8251 - val_loss: 752.3959\n",
      "Epoch 38/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 710.6515 - val_loss: 751.5188\n",
      "Epoch 39/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 708.8714 - val_loss: 752.5819\n",
      "Epoch 40/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 710.8296 - val_loss: 753.4592\n",
      "Epoch 41/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 709.8672 - val_loss: 751.8114\n",
      "Epoch 42/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 708.7077 - val_loss: 750.5153\n",
      "Epoch 43/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 709.0534 - val_loss: 750.7435\n",
      "Epoch 44/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 707.7699 - val_loss: 750.6598\n",
      "Epoch 45/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 707.6370 - val_loss: 750.2105\n",
      "Epoch 46/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 707.5181 - val_loss: 750.9813\n",
      "Epoch 47/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 708.1617 - val_loss: 749.1815\n",
      "Epoch 48/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 706.9008 - val_loss: 750.5275\n",
      "Epoch 49/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 709.0821 - val_loss: 752.4112\n",
      "Epoch 50/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 708.7244 - val_loss: 749.5389\n",
      "Epoch 51/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 708.3925 - val_loss: 748.2808\n",
      "Epoch 52/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 705.5971 - val_loss: 748.5984\n",
      "Epoch 53/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 705.6422 - val_loss: 748.0942\n",
      "Epoch 54/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 706.3695 - val_loss: 747.5762\n",
      "Epoch 55/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 705.7093 - val_loss: 747.5919\n",
      "Epoch 56/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 705.0265 - val_loss: 748.2359\n",
      "Epoch 57/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 703.9004 - val_loss: 747.1847\n",
      "Epoch 58/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 703.9928 - val_loss: 747.8273\n",
      "Epoch 59/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 704.1918 - val_loss: 746.7969\n",
      "Epoch 60/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 703.2635 - val_loss: 746.6678\n",
      "Epoch 61/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 703.3407 - val_loss: 745.8638\n",
      "Epoch 62/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 703.2629 - val_loss: 746.3331\n",
      "Epoch 63/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 703.4320 - val_loss: 749.1541\n",
      "Epoch 64/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 704.3867 - val_loss: 746.1350\n",
      "Epoch 65/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 703.4449 - val_loss: 744.7376\n",
      "Epoch 66/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 701.4267 - val_loss: 746.2205\n",
      "Epoch 67/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 703.8120 - val_loss: 744.9729\n",
      "Epoch 68/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 701.3455 - val_loss: 744.0744\n",
      "Epoch 69/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 703.4465 - val_loss: 743.9011\n",
      "Epoch 70/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 701.8742 - val_loss: 744.7208\n",
      "Epoch 71/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 701.1719 - val_loss: 743.9575\n",
      "Epoch 72/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 701.5038 - val_loss: 743.9683\n",
      "Epoch 73/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 700.0895 - val_loss: 743.6309\n",
      "Epoch 74/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 699.7845 - val_loss: 743.1629\n",
      "Epoch 75/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 701.1613 - val_loss: 742.4274\n",
      "Epoch 76/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 90us/step - loss: 699.2970 - val_loss: 744.2123\n",
      "Epoch 77/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 699.3116 - val_loss: 741.8797\n",
      "Epoch 78/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 699.1455 - val_loss: 741.7734\n",
      "Epoch 79/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 698.9146 - val_loss: 742.3774\n",
      "Epoch 80/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 707.1698 - val_loss: 748.6769\n",
      "Epoch 81/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 698.6733 - val_loss: 741.3281\n",
      "Epoch 82/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 697.9314 - val_loss: 741.1772\n",
      "Epoch 83/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 697.1773 - val_loss: 741.2212\n",
      "Epoch 84/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 700.2030 - val_loss: 740.3652\n",
      "Epoch 85/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 696.5175 - val_loss: 741.3512\n",
      "Epoch 86/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 700.1976 - val_loss: 745.8071\n",
      "Epoch 87/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 701.4070 - val_loss: 745.2562\n",
      "Epoch 88/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 699.9630 - val_loss: 741.5450\n",
      "Epoch 89/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 697.9260 - val_loss: 739.2342\n",
      "Epoch 90/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 695.7501 - val_loss: 741.0428\n",
      "Epoch 91/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 700.3536 - val_loss: 741.0293\n",
      "Epoch 92/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 697.4493 - val_loss: 738.9154\n",
      "Epoch 93/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 695.8818 - val_loss: 739.3093\n",
      "Epoch 94/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 696.8417 - val_loss: 740.6594\n",
      "Epoch 95/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 695.0933 - val_loss: 738.1392\n",
      "Epoch 96/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 694.9078 - val_loss: 737.8657\n",
      "Epoch 97/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 694.3830 - val_loss: 739.7243\n",
      "Epoch 98/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 694.8372 - val_loss: 737.4523\n",
      "Epoch 99/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 695.1379 - val_loss: 737.2599\n",
      "Epoch 100/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 693.8272 - val_loss: 737.5339\n",
      "Epoch 101/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 693.7527 - val_loss: 738.1040\n",
      "Epoch 102/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 694.2725 - val_loss: 737.7645\n",
      "Epoch 103/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 696.6922 - val_loss: 738.9660\n",
      "Epoch 104/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 692.2981 - val_loss: 736.2026\n",
      "Epoch 105/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 694.6172 - val_loss: 736.0759\n",
      "Epoch 106/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 692.8465 - val_loss: 735.7722\n",
      "Epoch 107/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 693.4172 - val_loss: 739.9884\n",
      "Epoch 108/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 695.9623 - val_loss: 738.7558\n",
      "Epoch 109/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 693.9228 - val_loss: 736.4211\n",
      "Epoch 110/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 694.7928 - val_loss: 734.9773\n",
      "Epoch 111/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 692.3698 - val_loss: 734.7158\n",
      "Epoch 112/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 694.0139 - val_loss: 738.3935\n",
      "Epoch 113/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 694.1747 - val_loss: 734.3383\n",
      "Epoch 114/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 690.7678 - val_loss: 734.5084\n",
      "Epoch 115/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 691.2399 - val_loss: 735.6677\n",
      "Epoch 116/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 690.1114 - val_loss: 733.6640\n",
      "Epoch 117/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 692.3784 - val_loss: 734.0356\n",
      "Epoch 118/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 689.1912 - val_loss: 734.5131\n",
      "Epoch 119/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 690.5505 - val_loss: 733.8399\n",
      "Epoch 120/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 689.7917 - val_loss: 732.8390\n",
      "Epoch 121/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 691.9840 - val_loss: 732.6241\n",
      "Epoch 122/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 690.2034 - val_loss: 733.0805\n",
      "Epoch 123/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 688.6573 - val_loss: 732.1499\n",
      "Epoch 124/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 689.4625 - val_loss: 731.9045\n",
      "Epoch 125/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 690.3765 - val_loss: 733.9054\n",
      "Epoch 126/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 688.6531 - val_loss: 731.6981\n",
      "Epoch 127/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 688.2692 - val_loss: 731.8021\n",
      "Epoch 128/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 688.4000 - val_loss: 734.0107\n",
      "Epoch 129/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 689.5994 - val_loss: 730.7882\n",
      "Epoch 130/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 687.4873 - val_loss: 730.5655\n",
      "Epoch 131/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 686.8037 - val_loss: 730.4332\n",
      "Epoch 132/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 687.2788 - val_loss: 730.1711\n",
      "Epoch 133/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 686.2453 - val_loss: 729.8719\n",
      "Epoch 134/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 688.1584 - val_loss: 729.6000\n",
      "Epoch 135/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 686.8339 - val_loss: 731.6130\n",
      "Epoch 136/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 687.4654 - val_loss: 729.2396\n",
      "Epoch 137/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 685.6954 - val_loss: 729.0134\n",
      "Epoch 138/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 685.1155 - val_loss: 728.9104\n",
      "Epoch 139/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 686.1549 - val_loss: 728.6706\n",
      "Epoch 140/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 683.6616 - val_loss: 728.6181\n",
      "Epoch 141/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 684.3723 - val_loss: 728.3862\n",
      "Epoch 142/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 684.3173 - val_loss: 727.7624\n",
      "Epoch 143/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 685.6570 - val_loss: 727.4860\n",
      "Epoch 144/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 682.6484 - val_loss: 728.0160\n",
      "Epoch 145/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 683.7750 - val_loss: 727.7819\n",
      "Epoch 146/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 683.3174 - val_loss: 726.8385\n",
      "Epoch 147/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 682.9353 - val_loss: 726.5256\n",
      "Epoch 148/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 684.3861 - val_loss: 726.5330\n",
      "Epoch 149/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 682.1420 - val_loss: 727.1530\n",
      "Epoch 150/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 683.4369 - val_loss: 727.2212\n",
      "Epoch 151/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 84us/step - loss: 685.2390 - val_loss: 727.6793\n",
      "Epoch 152/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 684.1367 - val_loss: 726.8980\n",
      "Epoch 153/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 682.2255 - val_loss: 725.6002\n",
      "Epoch 154/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 682.3701 - val_loss: 725.7855\n",
      "Epoch 155/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 681.6642 - val_loss: 725.2461\n",
      "Epoch 156/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 681.7750 - val_loss: 724.5602\n",
      "Epoch 157/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 680.2541 - val_loss: 724.3419\n",
      "Epoch 158/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 680.2962 - val_loss: 725.2908\n",
      "Epoch 159/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 680.5300 - val_loss: 724.0516\n",
      "Epoch 160/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 679.6609 - val_loss: 723.7246\n",
      "Epoch 161/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 679.5550 - val_loss: 723.6602\n",
      "Epoch 162/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 679.1216 - val_loss: 723.1052\n",
      "Epoch 163/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 681.0886 - val_loss: 726.3311\n",
      "Epoch 164/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 678.9081 - val_loss: 722.4797\n",
      "Epoch 165/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 678.2160 - val_loss: 722.5643\n",
      "Epoch 166/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 677.7999 - val_loss: 723.8732\n",
      "Epoch 167/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 677.9120 - val_loss: 722.0289\n",
      "Epoch 168/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 678.0628 - val_loss: 721.6302\n",
      "Epoch 169/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 676.9832 - val_loss: 721.9332\n",
      "Epoch 170/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 678.9956 - val_loss: 725.3350\n",
      "Epoch 171/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 676.6959 - val_loss: 720.9429\n",
      "Epoch 172/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 676.8575 - val_loss: 720.9157\n",
      "Epoch 173/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 677.0014 - val_loss: 720.6578\n",
      "Epoch 174/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 676.5719 - val_loss: 720.8793\n",
      "Epoch 175/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 676.4380 - val_loss: 720.3625\n",
      "Epoch 176/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 677.8082 - val_loss: 724.9611\n",
      "Epoch 177/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 681.0301 - val_loss: 723.3883\n",
      "Epoch 178/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 674.6115 - val_loss: 719.4274\n",
      "Epoch 179/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 678.2016 - val_loss: 720.0690\n",
      "Epoch 180/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 674.0211 - val_loss: 718.7170\n",
      "Epoch 181/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 674.1495 - val_loss: 718.4684\n",
      "Epoch 182/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 673.6213 - val_loss: 718.7695\n",
      "Epoch 183/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 674.1267 - val_loss: 718.3088\n",
      "Epoch 184/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 675.2065 - val_loss: 717.6959\n",
      "Epoch 185/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 675.6082 - val_loss: 718.5562\n",
      "Epoch 186/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 671.9675 - val_loss: 719.0867\n",
      "Epoch 187/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 675.3266 - val_loss: 723.6156\n",
      "Epoch 188/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 670.9816 - val_loss: 717.5133\n",
      "Epoch 189/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 672.6564 - val_loss: 717.1148\n",
      "Epoch 190/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 670.8530 - val_loss: 716.1936\n",
      "Epoch 191/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 672.0490 - val_loss: 716.0713\n",
      "Epoch 192/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 670.0498 - val_loss: 716.0295\n",
      "Epoch 193/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 672.5068 - val_loss: 716.3497\n",
      "Epoch 194/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 670.0883 - val_loss: 715.3860\n",
      "Epoch 195/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 669.8329 - val_loss: 716.2425\n",
      "Epoch 196/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 674.1694 - val_loss: 719.0564\n",
      "Epoch 197/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 668.2800 - val_loss: 715.0977\n",
      "Epoch 198/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 669.0914 - val_loss: 714.6246\n",
      "Epoch 199/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 670.5391 - val_loss: 716.2841\n",
      "Epoch 200/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 668.3029 - val_loss: 714.1422\n",
      "Epoch 201/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 667.8489 - val_loss: 714.5021\n",
      "Epoch 202/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 668.5833 - val_loss: 714.3983\n",
      "Epoch 203/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 667.8245 - val_loss: 714.5747\n",
      "Epoch 204/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 668.2811 - val_loss: 715.7275\n",
      "Epoch 205/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 666.5733 - val_loss: 713.0444\n",
      "Epoch 206/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 669.7415 - val_loss: 713.2483\n",
      "Epoch 207/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 666.5146 - val_loss: 712.9791\n",
      "Epoch 208/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 667.2042 - val_loss: 713.8355\n",
      "Epoch 209/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 666.0306 - val_loss: 712.1601\n",
      "Epoch 210/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 668.1183 - val_loss: 713.1417\n",
      "Epoch 211/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 668.2009 - val_loss: 712.3908\n",
      "Epoch 212/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 666.1592 - val_loss: 715.0214\n",
      "Epoch 213/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 666.9535 - val_loss: 711.3353\n",
      "Epoch 214/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 664.5701 - val_loss: 711.1823\n",
      "Epoch 215/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 666.0625 - val_loss: 710.8534\n",
      "Epoch 216/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 663.1796 - val_loss: 712.0038\n",
      "Epoch 217/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 663.0352 - val_loss: 710.2462\n",
      "Epoch 218/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 663.4819 - val_loss: 710.5545\n",
      "Epoch 219/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 663.5191 - val_loss: 709.7834\n",
      "Epoch 220/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 663.9196 - val_loss: 717.6279\n",
      "Epoch 221/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 664.0445 - val_loss: 710.5366\n",
      "Epoch 222/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 662.9751 - val_loss: 709.0490\n",
      "Epoch 223/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 661.2919 - val_loss: 709.3384\n",
      "Epoch 224/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 662.0617 - val_loss: 708.7493\n",
      "Epoch 225/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 661.7065 - val_loss: 708.4889\n",
      "Epoch 226/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 92us/step - loss: 660.3715 - val_loss: 708.9101\n",
      "Epoch 227/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 665.2915 - val_loss: 709.8832\n",
      "Epoch 228/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 660.8477 - val_loss: 709.8534\n",
      "Epoch 229/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 661.1714 - val_loss: 707.5543\n",
      "Epoch 230/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 659.9953 - val_loss: 714.6651\n",
      "Epoch 231/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 662.9059 - val_loss: 710.6882\n",
      "Epoch 232/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 658.1214 - val_loss: 707.5135\n",
      "Epoch 233/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 660.6096 - val_loss: 707.8037\n",
      "Epoch 234/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 658.2427 - val_loss: 706.8069\n",
      "Epoch 235/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 658.5105 - val_loss: 710.8129\n",
      "Epoch 236/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 659.2195 - val_loss: 707.8679\n",
      "Epoch 237/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 657.6562 - val_loss: 706.2805\n",
      "Epoch 238/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 658.5884 - val_loss: 706.2164\n",
      "Epoch 239/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 656.2910 - val_loss: 706.5769\n",
      "Epoch 240/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 656.1686 - val_loss: 705.6476\n",
      "Epoch 241/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 655.6108 - val_loss: 705.7123\n",
      "Epoch 242/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 656.0946 - val_loss: 705.3210\n",
      "Epoch 243/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 655.1723 - val_loss: 705.0720\n",
      "Epoch 244/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 657.3058 - val_loss: 707.3453\n",
      "Epoch 245/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 654.3768 - val_loss: 705.2400\n",
      "Epoch 246/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 657.3907 - val_loss: 704.9586\n",
      "Epoch 247/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 655.5467 - val_loss: 709.0419\n",
      "Epoch 248/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 656.0812 - val_loss: 706.0456\n",
      "Epoch 249/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 652.5112 - val_loss: 705.2943\n",
      "Epoch 250/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 655.6916 - val_loss: 704.1963\n",
      "Epoch 251/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 654.2497 - val_loss: 706.2555\n",
      "Epoch 252/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 653.6264 - val_loss: 703.5828\n",
      "Epoch 253/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 653.0418 - val_loss: 703.3944\n",
      "Epoch 254/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 651.9586 - val_loss: 703.5422\n",
      "Epoch 255/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 651.0879 - val_loss: 704.8002\n",
      "Epoch 256/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 651.6164 - val_loss: 703.6772\n",
      "Epoch 257/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 651.0236 - val_loss: 704.4746\n",
      "Epoch 258/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 651.2683 - val_loss: 704.1360\n",
      "Epoch 259/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 651.5164 - val_loss: 704.6039\n",
      "Epoch 260/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 650.1139 - val_loss: 703.0325\n",
      "Epoch 261/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 649.7240 - val_loss: 701.9961\n",
      "Epoch 262/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 651.6913 - val_loss: 702.6507\n",
      "Epoch 263/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 648.8449 - val_loss: 701.6015\n",
      "Epoch 264/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 648.4072 - val_loss: 702.4474\n",
      "Epoch 265/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 648.0948 - val_loss: 703.7715\n",
      "Epoch 266/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 648.3046 - val_loss: 701.5310\n",
      "Epoch 267/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 650.6838 - val_loss: 702.5339\n",
      "Epoch 268/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 649.3271 - val_loss: 701.6606\n",
      "Epoch 269/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 647.0890 - val_loss: 702.2687\n",
      "Epoch 270/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 651.4510 - val_loss: 704.6205\n",
      "Epoch 271/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 652.4827 - val_loss: 701.6744\n",
      "Epoch 272/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 647.2421 - val_loss: 703.7642\n",
      "Epoch 273/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 647.1782 - val_loss: 702.1804\n",
      "Epoch 274/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 646.4599 - val_loss: 700.2235\n",
      "Epoch 275/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 646.0599 - val_loss: 700.4604\n",
      "Epoch 276/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 645.1830 - val_loss: 706.3010\n",
      "Epoch 277/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 651.0797 - val_loss: 704.8496\n",
      "Epoch 278/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 645.7846 - val_loss: 699.8644\n",
      "Epoch 279/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 644.4819 - val_loss: 700.1401\n",
      "Epoch 280/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 644.5706 - val_loss: 704.7741\n",
      "Epoch 281/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 645.6800 - val_loss: 700.7186\n",
      "Epoch 282/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 644.4296 - val_loss: 699.5903\n",
      "Epoch 283/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 644.5171 - val_loss: 699.1680\n",
      "Epoch 284/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 643.6452 - val_loss: 700.3347\n",
      "Epoch 285/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 642.8171 - val_loss: 698.6200\n",
      "Epoch 286/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 642.5676 - val_loss: 698.0850\n",
      "Epoch 287/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 643.2636 - val_loss: 699.0856\n",
      "Epoch 288/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 647.6051 - val_loss: 698.0978\n",
      "Epoch 289/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 650.1520 - val_loss: 717.9094\n",
      "Epoch 290/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 645.3023 - val_loss: 698.4242\n",
      "Epoch 291/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 640.5986 - val_loss: 697.6472\n",
      "Epoch 292/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 641.3368 - val_loss: 697.3204\n",
      "Epoch 293/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 640.3008 - val_loss: 697.8053\n",
      "Epoch 294/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 640.6273 - val_loss: 697.2662\n",
      "Epoch 295/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 641.5054 - val_loss: 700.3080\n",
      "Epoch 296/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 646.9639 - val_loss: 696.9246\n",
      "Epoch 297/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 639.3134 - val_loss: 698.9368\n",
      "Epoch 298/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 640.3146 - val_loss: 696.5486\n",
      "Epoch 299/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 639.5549 - val_loss: 696.3908\n",
      "Epoch 300/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 641.7847 - val_loss: 696.3125\n",
      "Epoch 301/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 95us/step - loss: 639.2236 - val_loss: 696.8365\n",
      "Epoch 302/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 638.1334 - val_loss: 696.0022\n",
      "Epoch 303/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 640.1698 - val_loss: 696.1452\n",
      "Epoch 304/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 638.5362 - val_loss: 701.7462\n",
      "Epoch 305/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 641.9782 - val_loss: 699.6113\n",
      "Epoch 306/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 637.1870 - val_loss: 695.3643\n",
      "Epoch 307/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 642.6565 - val_loss: 695.7479\n",
      "Epoch 308/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 638.9166 - val_loss: 696.9422\n",
      "Epoch 309/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 638.6620 - val_loss: 694.9514\n",
      "Epoch 310/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 637.3818 - val_loss: 695.1088\n",
      "Epoch 311/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 637.3443 - val_loss: 694.7193\n",
      "Epoch 312/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 637.0265 - val_loss: 694.7419\n",
      "Epoch 313/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 638.8723 - val_loss: 695.0501\n",
      "Epoch 314/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 638.5648 - val_loss: 694.2923\n",
      "Epoch 315/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 635.9697 - val_loss: 695.6244\n",
      "Epoch 316/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 638.1314 - val_loss: 696.7618\n",
      "Epoch 317/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 640.0416 - val_loss: 694.2885\n",
      "Epoch 318/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 636.0379 - val_loss: 698.7310\n",
      "Epoch 319/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 637.5182 - val_loss: 696.2868\n",
      "Epoch 320/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 636.5047 - val_loss: 693.2964\n",
      "Epoch 321/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 636.4699 - val_loss: 696.4934\n",
      "Epoch 322/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 635.5643 - val_loss: 694.7144\n",
      "Epoch 323/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 635.0206 - val_loss: 693.0057\n",
      "Epoch 324/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 634.7853 - val_loss: 694.1000\n",
      "Epoch 325/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 635.1293 - val_loss: 693.4540\n",
      "Epoch 326/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 634.2946 - val_loss: 696.1773\n",
      "Epoch 327/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 635.3887 - val_loss: 692.3282\n",
      "Epoch 328/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 635.6081 - val_loss: 693.4535\n",
      "Epoch 329/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 634.1538 - val_loss: 692.0844\n",
      "Epoch 330/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 635.3390 - val_loss: 693.7505\n",
      "Epoch 331/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 634.3773 - val_loss: 692.8012\n",
      "Epoch 332/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 634.4488 - val_loss: 697.5399\n",
      "Epoch 333/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 634.2002 - val_loss: 691.9477\n",
      "Epoch 334/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 639.6463 - val_loss: 691.7319\n",
      "Epoch 335/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 634.3023 - val_loss: 703.3593\n",
      "Epoch 336/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 638.4403 - val_loss: 691.2666\n",
      "Epoch 337/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 633.5471 - val_loss: 693.0738\n",
      "Epoch 338/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 633.1236 - val_loss: 691.5123\n",
      "Epoch 339/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 633.6451 - val_loss: 691.0788\n",
      "Epoch 340/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 633.7167 - val_loss: 691.6379\n",
      "Epoch 341/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 632.6618 - val_loss: 696.4169\n",
      "Epoch 342/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 632.7118 - val_loss: 690.3961\n",
      "Epoch 343/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 632.7739 - val_loss: 692.1821\n",
      "Epoch 344/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 632.5663 - val_loss: 690.1294\n",
      "Epoch 345/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 635.9904 - val_loss: 690.2707\n",
      "Epoch 346/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 638.4674 - val_loss: 698.5488\n",
      "Epoch 347/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 633.0655 - val_loss: 692.5525\n",
      "Epoch 348/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 632.2719 - val_loss: 692.8380\n",
      "Epoch 349/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 632.4870 - val_loss: 690.1911\n",
      "Epoch 350/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 633.1123 - val_loss: 696.5586\n",
      "Epoch 351/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 632.3811 - val_loss: 689.4472\n",
      "Epoch 352/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 636.1016 - val_loss: 689.2582\n",
      "Epoch 353/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 631.7002 - val_loss: 690.6287\n",
      "Epoch 354/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 631.9397 - val_loss: 692.8827\n",
      "Epoch 355/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 630.9393 - val_loss: 689.1323\n",
      "Epoch 356/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 634.3288 - val_loss: 689.1170\n",
      "Epoch 357/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 630.1234 - val_loss: 697.3629\n",
      "Epoch 358/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 631.8005 - val_loss: 691.4425\n",
      "Epoch 359/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 630.9213 - val_loss: 688.7057\n",
      "Epoch 360/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 631.1412 - val_loss: 691.7743\n",
      "Epoch 361/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 631.0345 - val_loss: 690.8986\n",
      "Epoch 362/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 631.6342 - val_loss: 688.4612\n",
      "Epoch 363/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 629.7174 - val_loss: 694.8555\n",
      "Epoch 364/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 630.4545 - val_loss: 693.5555\n",
      "Epoch 365/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 630.1095 - val_loss: 691.6452\n",
      "Epoch 366/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 632.9319 - val_loss: 688.2252\n",
      "Epoch 367/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 629.9282 - val_loss: 689.0138\n",
      "Epoch 368/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 629.6211 - val_loss: 693.4143\n",
      "Epoch 369/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 629.4070 - val_loss: 690.3498\n",
      "Epoch 370/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 628.9594 - val_loss: 692.0401\n",
      "Epoch 371/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 632.8656 - val_loss: 696.2240\n",
      "Epoch 372/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 630.1610 - val_loss: 688.0617\n",
      "Epoch 373/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 630.1915 - val_loss: 689.1636\n",
      "Epoch 374/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 628.5660 - val_loss: 690.2179\n",
      "Epoch 375/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 628.5681 - val_loss: 687.9619\n",
      "Epoch 376/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 90us/step - loss: 628.3449 - val_loss: 687.1242\n",
      "Epoch 377/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 628.8935 - val_loss: 688.4215\n",
      "Epoch 378/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 628.0156 - val_loss: 690.2625\n",
      "Epoch 379/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 630.3379 - val_loss: 694.0544\n",
      "Epoch 380/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 630.0967 - val_loss: 686.7769\n",
      "Epoch 381/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 627.7177 - val_loss: 689.9085\n",
      "Epoch 382/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 627.6726 - val_loss: 691.2358\n",
      "Epoch 383/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 628.4673 - val_loss: 693.9135\n",
      "Epoch 384/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 628.7846 - val_loss: 690.3995\n",
      "Epoch 385/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 630.1791 - val_loss: 686.1958\n",
      "Epoch 386/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 627.2854 - val_loss: 689.3882\n",
      "Epoch 387/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 630.3546 - val_loss: 695.6799\n",
      "Epoch 388/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 629.6779 - val_loss: 685.9114\n",
      "Epoch 389/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 626.5822 - val_loss: 692.6333\n",
      "Epoch 390/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 626.8303 - val_loss: 686.8822\n",
      "Epoch 391/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 627.0583 - val_loss: 688.6940\n",
      "Epoch 392/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 626.7805 - val_loss: 688.9934\n",
      "Epoch 393/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 626.1201 - val_loss: 687.0032\n",
      "Epoch 394/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 625.9480 - val_loss: 686.3005\n",
      "Epoch 395/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 628.3764 - val_loss: 685.4620\n",
      "Epoch 396/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 630.4715 - val_loss: 688.5084\n",
      "Epoch 397/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 629.6814 - val_loss: 685.2231\n",
      "Epoch 398/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 627.7676 - val_loss: 692.3835\n",
      "Epoch 399/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 626.9546 - val_loss: 689.1663\n",
      "Epoch 400/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 625.5514 - val_loss: 686.3752\n",
      "Epoch 401/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 625.0182 - val_loss: 686.8208\n",
      "Epoch 402/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 625.3509 - val_loss: 687.0994\n",
      "Epoch 403/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 625.2810 - val_loss: 686.0786\n",
      "Epoch 404/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 628.3512 - val_loss: 695.5667\n",
      "Epoch 405/2000\n",
      "357/357 [==============================] - 0s 70us/step - loss: 625.4076 - val_loss: 685.7302\n",
      "Epoch 406/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 626.1205 - val_loss: 684.4951\n",
      "Epoch 407/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 626.6075 - val_loss: 685.2840\n",
      "Epoch 408/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 625.8973 - val_loss: 688.4234\n",
      "Epoch 409/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 625.2778 - val_loss: 687.5354\n",
      "Epoch 410/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 627.9155 - val_loss: 684.1858\n",
      "Epoch 411/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 624.9151 - val_loss: 690.2615\n",
      "Epoch 412/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 626.1355 - val_loss: 688.7033\n",
      "Epoch 413/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 631.4116 - val_loss: 693.9653\n",
      "Epoch 414/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 628.4140 - val_loss: 685.9743\n",
      "Epoch 415/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 625.4597 - val_loss: 695.3828\n",
      "Epoch 416/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 624.4630 - val_loss: 686.5348\n",
      "Epoch 417/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 624.0084 - val_loss: 684.3141\n",
      "Epoch 418/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 624.0131 - val_loss: 685.6532\n",
      "Epoch 419/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 626.3046 - val_loss: 683.0352\n",
      "Epoch 420/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 624.2718 - val_loss: 683.9782\n",
      "Epoch 421/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 624.6873 - val_loss: 689.8579\n",
      "Epoch 422/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 623.3286 - val_loss: 687.1328\n",
      "Epoch 423/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 622.7749 - val_loss: 687.0771\n",
      "Epoch 424/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 623.4552 - val_loss: 685.6516\n",
      "Epoch 425/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 622.8369 - val_loss: 686.1157\n",
      "Epoch 426/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 622.3681 - val_loss: 686.2673\n",
      "Epoch 427/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 622.3777 - val_loss: 683.0054\n",
      "Epoch 428/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 623.4227 - val_loss: 685.6998\n",
      "Epoch 429/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 622.4683 - val_loss: 688.2108\n",
      "Epoch 430/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 624.8136 - val_loss: 682.7198\n",
      "Epoch 431/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 621.3041 - val_loss: 688.5972\n",
      "Epoch 432/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 621.8010 - val_loss: 684.6838\n",
      "Epoch 433/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 624.1122 - val_loss: 682.6511\n",
      "Epoch 434/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 621.9679 - val_loss: 685.7852\n",
      "Epoch 435/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 622.2235 - val_loss: 683.5869\n",
      "Epoch 436/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 621.3306 - val_loss: 681.4303\n",
      "Epoch 437/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 625.3067 - val_loss: 688.4870\n",
      "Epoch 438/2000\n",
      "357/357 [==============================] - 0s 68us/step - loss: 621.8753 - val_loss: 687.8478\n",
      "Epoch 439/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 621.9752 - val_loss: 687.2432\n",
      "Epoch 440/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 620.6662 - val_loss: 681.8044\n",
      "Epoch 441/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 621.2569 - val_loss: 686.1112\n",
      "Epoch 442/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 620.7085 - val_loss: 684.6394\n",
      "Epoch 443/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 620.9048 - val_loss: 683.0912\n",
      "Epoch 444/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 619.8654 - val_loss: 688.2976\n",
      "Epoch 445/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 621.2182 - val_loss: 685.7333\n",
      "Epoch 446/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 623.1488 - val_loss: 681.0514\n",
      "Epoch 447/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 624.1916 - val_loss: 688.2435\n",
      "Epoch 448/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 622.3676 - val_loss: 679.8662\n",
      "Epoch 449/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 624.6057 - val_loss: 681.1365\n",
      "Epoch 450/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 619.3281 - val_loss: 687.7351\n",
      "Epoch 451/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 83us/step - loss: 620.0542 - val_loss: 686.2869\n",
      "Epoch 452/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 627.4046 - val_loss: 680.4283\n",
      "Epoch 453/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 618.8317 - val_loss: 690.5768\n",
      "Epoch 454/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 621.6327 - val_loss: 685.6333\n",
      "Epoch 455/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 619.6049 - val_loss: 686.7942\n",
      "Epoch 456/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 617.9386 - val_loss: 680.0243\n",
      "Epoch 457/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 619.8779 - val_loss: 680.7053\n",
      "Epoch 458/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 619.3614 - val_loss: 684.0785\n",
      "Epoch 459/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 618.9606 - val_loss: 683.4697\n",
      "Epoch 460/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 617.9970 - val_loss: 680.5603\n",
      "Epoch 461/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 618.3221 - val_loss: 682.7137\n",
      "Epoch 462/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 619.3698 - val_loss: 686.1271\n",
      "Epoch 463/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 618.9389 - val_loss: 683.7253\n",
      "Epoch 464/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 619.4725 - val_loss: 679.0224\n",
      "Epoch 465/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 618.6054 - val_loss: 682.1397\n",
      "Epoch 466/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 619.0549 - val_loss: 687.7854\n",
      "Epoch 467/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 619.9199 - val_loss: 678.6088\n",
      "Epoch 468/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 618.4020 - val_loss: 683.6708\n",
      "Epoch 469/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 618.4492 - val_loss: 678.1989\n",
      "Epoch 470/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 617.8174 - val_loss: 682.4400\n",
      "Epoch 471/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 618.7474 - val_loss: 684.7933\n",
      "Epoch 472/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 617.9575 - val_loss: 681.7019\n",
      "Epoch 473/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 620.0517 - val_loss: 677.6224\n",
      "Epoch 474/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 618.8372 - val_loss: 681.9873\n",
      "Epoch 475/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 616.9026 - val_loss: 681.6074\n",
      "Epoch 476/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 617.2146 - val_loss: 676.8239\n",
      "Epoch 477/2000\n",
      "357/357 [==============================] - 0s 69us/step - loss: 618.7998 - val_loss: 681.3462\n",
      "Epoch 478/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 616.1216 - val_loss: 679.9199\n",
      "Epoch 479/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 618.0515 - val_loss: 678.9579\n",
      "Epoch 480/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 615.5276 - val_loss: 683.4526\n",
      "Epoch 481/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 617.1705 - val_loss: 684.6269\n",
      "Epoch 482/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 615.1967 - val_loss: 677.3291\n",
      "Epoch 483/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 624.3143 - val_loss: 675.9210\n",
      "Epoch 484/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 614.7560 - val_loss: 686.7573\n",
      "Epoch 485/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 619.5440 - val_loss: 688.2128\n",
      "Epoch 486/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 617.7020 - val_loss: 679.8408\n",
      "Epoch 487/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 616.5527 - val_loss: 680.7963\n",
      "Epoch 488/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 616.1531 - val_loss: 678.5270\n",
      "Epoch 489/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 616.0474 - val_loss: 682.5137\n",
      "Epoch 490/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 615.8252 - val_loss: 679.9130\n",
      "Epoch 491/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 615.6687 - val_loss: 678.5933\n",
      "Epoch 492/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 615.1449 - val_loss: 680.5391\n",
      "Epoch 493/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 615.3392 - val_loss: 676.9633\n",
      "Epoch 494/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 615.2401 - val_loss: 682.9667\n",
      "Epoch 495/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 615.5184 - val_loss: 682.5839\n",
      "Epoch 496/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 616.0275 - val_loss: 684.8707\n",
      "Epoch 497/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 614.7324 - val_loss: 678.0999\n",
      "Epoch 498/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 614.0782 - val_loss: 679.4109\n",
      "Epoch 499/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 616.1789 - val_loss: 682.6737\n",
      "Epoch 500/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 615.3196 - val_loss: 677.2697\n",
      "Epoch 501/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 613.6655 - val_loss: 680.8370\n",
      "Epoch 502/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 616.6880 - val_loss: 686.4603\n",
      "Epoch 503/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 615.7809 - val_loss: 679.4191\n",
      "Epoch 504/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 613.6078 - val_loss: 676.6840\n",
      "Epoch 505/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 613.9153 - val_loss: 686.6931\n",
      "Epoch 506/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 615.2501 - val_loss: 678.3255\n",
      "Epoch 507/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 615.6009 - val_loss: 681.0424\n",
      "Epoch 508/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 613.4873 - val_loss: 681.3175\n",
      "Epoch 509/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 614.5588 - val_loss: 678.5373\n",
      "Epoch 510/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 613.0641 - val_loss: 676.3819\n",
      "Epoch 511/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 613.2197 - val_loss: 676.5753\n",
      "Epoch 512/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 612.6706 - val_loss: 681.1763\n",
      "Epoch 513/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 612.4152 - val_loss: 675.6775\n",
      "Epoch 514/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 613.0226 - val_loss: 674.4955\n",
      "Epoch 515/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 613.9720 - val_loss: 680.6644\n",
      "Epoch 516/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 610.8168 - val_loss: 674.0967\n",
      "Epoch 517/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 614.4029 - val_loss: 673.6067\n",
      "Epoch 518/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 611.0550 - val_loss: 681.4086\n",
      "Epoch 519/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 616.1076 - val_loss: 674.6595\n",
      "Epoch 520/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 611.8397 - val_loss: 678.4386\n",
      "Epoch 521/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 612.2690 - val_loss: 683.4639\n",
      "Epoch 522/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 614.1965 - val_loss: 671.6627\n",
      "Epoch 523/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 614.3004 - val_loss: 676.8028\n",
      "Epoch 524/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 613.0268 - val_loss: 677.3289\n",
      "Epoch 525/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 613.1889 - val_loss: 671.5369\n",
      "Epoch 526/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 87us/step - loss: 612.0905 - val_loss: 681.2405\n",
      "Epoch 527/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 612.9242 - val_loss: 678.9593\n",
      "Epoch 528/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 612.4570 - val_loss: 671.4024\n",
      "Epoch 529/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 611.6629 - val_loss: 674.8286\n",
      "Epoch 530/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 610.9906 - val_loss: 676.8463\n",
      "Epoch 531/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 611.0848 - val_loss: 673.4911\n",
      "Epoch 532/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 610.6175 - val_loss: 678.1275\n",
      "Epoch 533/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 613.0678 - val_loss: 673.1427\n",
      "Epoch 534/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 611.1192 - val_loss: 676.0704\n",
      "Epoch 535/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 610.3353 - val_loss: 670.5326\n",
      "Epoch 536/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 614.3461 - val_loss: 670.5526\n",
      "Epoch 537/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 611.1863 - val_loss: 677.6382\n",
      "Epoch 538/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 610.6044 - val_loss: 675.4510\n",
      "Epoch 539/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 609.4885 - val_loss: 673.0484\n",
      "Epoch 540/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 610.2958 - val_loss: 671.5813\n",
      "Epoch 541/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 608.5548 - val_loss: 676.7320\n",
      "Epoch 542/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 612.6893 - val_loss: 682.2405\n",
      "Epoch 543/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 610.1571 - val_loss: 672.7249\n",
      "Epoch 544/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 609.8716 - val_loss: 670.4264\n",
      "Epoch 545/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 610.8052 - val_loss: 675.1295\n",
      "Epoch 546/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 608.5669 - val_loss: 671.8169\n",
      "Epoch 547/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 608.0914 - val_loss: 676.4703\n",
      "Epoch 548/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 609.4627 - val_loss: 675.0809\n",
      "Epoch 549/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 610.2458 - val_loss: 671.3415\n",
      "Epoch 550/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 611.6096 - val_loss: 677.4398\n",
      "Epoch 551/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 607.4184 - val_loss: 670.3316\n",
      "Epoch 552/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 609.1358 - val_loss: 671.1415\n",
      "Epoch 553/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 608.4981 - val_loss: 671.7521\n",
      "Epoch 554/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 608.0516 - val_loss: 674.4982\n",
      "Epoch 555/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 609.5928 - val_loss: 675.7938\n",
      "Epoch 556/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 607.3136 - val_loss: 670.5296\n",
      "Epoch 557/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 607.8001 - val_loss: 671.2881\n",
      "Epoch 558/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 608.5119 - val_loss: 668.2072\n",
      "Epoch 559/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 607.6682 - val_loss: 671.7215\n",
      "Epoch 560/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 608.6382 - val_loss: 670.0708\n",
      "Epoch 561/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 608.6057 - val_loss: 682.1699\n",
      "Epoch 562/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 611.9228 - val_loss: 679.3414\n",
      "Epoch 563/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 606.7344 - val_loss: 670.9618\n",
      "Epoch 564/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 607.3814 - val_loss: 672.9038\n",
      "Epoch 565/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 607.7915 - val_loss: 674.6975\n",
      "Epoch 566/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 607.0158 - val_loss: 672.0510\n",
      "Epoch 567/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 608.4278 - val_loss: 668.1126\n",
      "Epoch 568/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 605.3187 - val_loss: 678.7315\n",
      "Epoch 569/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 608.1154 - val_loss: 675.0304\n",
      "Epoch 570/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 607.8086 - val_loss: 669.8764\n",
      "Epoch 571/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 608.6144 - val_loss: 680.7514\n",
      "Epoch 572/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 608.0024 - val_loss: 672.9142\n",
      "Epoch 573/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 608.0564 - val_loss: 675.9709\n",
      "Epoch 574/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 606.1848 - val_loss: 668.0563\n",
      "Epoch 575/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 605.6370 - val_loss: 672.3039\n",
      "Epoch 576/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 608.7912 - val_loss: 678.8595\n",
      "Epoch 577/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 605.7368 - val_loss: 666.9765\n",
      "Epoch 578/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 604.7748 - val_loss: 673.4772\n",
      "Epoch 579/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 605.7961 - val_loss: 671.1140\n",
      "Epoch 580/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 606.9556 - val_loss: 665.8168\n",
      "Epoch 581/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 604.9518 - val_loss: 674.6159\n",
      "Epoch 582/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 607.2294 - val_loss: 672.3216\n",
      "Epoch 583/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 606.9675 - val_loss: 675.4040\n",
      "Epoch 584/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 609.0523 - val_loss: 665.0981\n",
      "Epoch 585/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 602.5012 - val_loss: 676.4890\n",
      "Epoch 586/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 606.3995 - val_loss: 677.5999\n",
      "Epoch 587/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 604.8053 - val_loss: 665.6952\n",
      "Epoch 588/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 609.7392 - val_loss: 672.8489\n",
      "Epoch 589/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 608.7643 - val_loss: 663.6936\n",
      "Epoch 590/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 603.6552 - val_loss: 678.2795\n",
      "Epoch 591/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 604.1905 - val_loss: 667.0387\n",
      "Epoch 592/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 608.8003 - val_loss: 663.7402\n",
      "Epoch 593/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 602.7281 - val_loss: 676.2796\n",
      "Epoch 594/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 606.5654 - val_loss: 671.6066\n",
      "Epoch 595/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 603.1185 - val_loss: 667.1272\n",
      "Epoch 596/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 607.2932 - val_loss: 664.0125\n",
      "Epoch 597/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 605.4623 - val_loss: 667.9844\n",
      "Epoch 598/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 605.5977 - val_loss: 674.4628\n",
      "Epoch 599/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 605.1473 - val_loss: 666.8057\n",
      "Epoch 600/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 603.3534 - val_loss: 669.5729\n",
      "Epoch 601/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 91us/step - loss: 604.1380 - val_loss: 668.7765\n",
      "Epoch 602/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 602.9894 - val_loss: 670.3553\n",
      "Epoch 603/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 602.4938 - val_loss: 668.4486\n",
      "Epoch 604/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 603.6242 - val_loss: 667.1823\n",
      "Epoch 605/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 603.3783 - val_loss: 665.1351\n",
      "Epoch 606/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 603.1209 - val_loss: 674.1530\n",
      "Epoch 607/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 606.4514 - val_loss: 666.0169\n",
      "Epoch 608/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 602.2332 - val_loss: 673.0646\n",
      "Epoch 609/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 602.8548 - val_loss: 665.4720\n",
      "Epoch 610/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 602.2113 - val_loss: 667.7044\n",
      "Epoch 611/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 605.8761 - val_loss: 672.4152\n",
      "Epoch 612/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 601.9428 - val_loss: 662.8025\n",
      "Epoch 613/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 603.7501 - val_loss: 664.7910\n",
      "Epoch 614/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 603.3306 - val_loss: 664.0760\n",
      "Epoch 615/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 602.1423 - val_loss: 666.3276\n",
      "Epoch 616/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 601.5857 - val_loss: 666.6453\n",
      "Epoch 617/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 601.2132 - val_loss: 668.6386\n",
      "Epoch 618/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 602.1813 - val_loss: 667.5202\n",
      "Epoch 619/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 601.3400 - val_loss: 668.0211\n",
      "Epoch 620/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 602.6847 - val_loss: 674.0641\n",
      "Epoch 621/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 602.4125 - val_loss: 662.7053\n",
      "Epoch 622/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 602.3324 - val_loss: 669.3781\n",
      "Epoch 623/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 600.4981 - val_loss: 662.6762\n",
      "Epoch 624/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 605.5072 - val_loss: 661.1376\n",
      "Epoch 625/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 600.7625 - val_loss: 670.7231\n",
      "Epoch 626/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 602.1537 - val_loss: 663.4058\n",
      "Epoch 627/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 607.6617 - val_loss: 661.0191\n",
      "Epoch 628/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 600.9675 - val_loss: 666.1118\n",
      "Epoch 629/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 599.8294 - val_loss: 666.0046\n",
      "Epoch 630/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 599.8716 - val_loss: 665.1691\n",
      "Epoch 631/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 600.9867 - val_loss: 666.7482\n",
      "Epoch 632/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 600.3178 - val_loss: 661.1553\n",
      "Epoch 633/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 599.5876 - val_loss: 668.5636\n",
      "Epoch 634/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 603.6492 - val_loss: 672.4614\n",
      "Epoch 635/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 599.6859 - val_loss: 662.4568\n",
      "Epoch 636/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 600.7838 - val_loss: 663.2719\n",
      "Epoch 637/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 602.8654 - val_loss: 669.8931\n",
      "Epoch 638/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 601.0358 - val_loss: 660.1935\n",
      "Epoch 639/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 602.0171 - val_loss: 666.2137\n",
      "Epoch 640/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 604.6629 - val_loss: 660.7291\n",
      "Epoch 641/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 599.5404 - val_loss: 673.2645\n",
      "Epoch 642/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 600.0685 - val_loss: 662.7269\n",
      "Epoch 643/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 608.2695 - val_loss: 659.9544\n",
      "Epoch 644/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 607.8569 - val_loss: 669.4780\n",
      "Epoch 645/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 600.4981 - val_loss: 660.8698\n",
      "Epoch 646/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 600.8345 - val_loss: 671.8594\n",
      "Epoch 647/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 603.3068 - val_loss: 663.5961\n",
      "Epoch 648/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 598.8496 - val_loss: 664.2778\n",
      "Epoch 649/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 599.3770 - val_loss: 666.4757\n",
      "Epoch 650/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 598.3648 - val_loss: 662.7754\n",
      "Epoch 651/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 600.9415 - val_loss: 659.7371\n",
      "Epoch 652/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 597.7287 - val_loss: 672.1240\n",
      "Epoch 653/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 600.8571 - val_loss: 664.8667\n",
      "Epoch 654/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 598.5760 - val_loss: 658.9889\n",
      "Epoch 655/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 601.9760 - val_loss: 659.6061\n",
      "Epoch 656/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 596.5075 - val_loss: 667.0519\n",
      "Epoch 657/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 603.7739 - val_loss: 674.9316\n",
      "Epoch 658/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 598.8396 - val_loss: 661.6292\n",
      "Epoch 659/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 604.2729 - val_loss: 657.3882\n",
      "Epoch 660/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 598.6934 - val_loss: 665.5676\n",
      "Epoch 661/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 598.4186 - val_loss: 666.7874\n",
      "Epoch 662/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 597.6639 - val_loss: 661.0132\n",
      "Epoch 663/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 598.8730 - val_loss: 663.6879\n",
      "Epoch 664/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 599.7705 - val_loss: 667.8284\n",
      "Epoch 665/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 598.3374 - val_loss: 664.6921\n",
      "Epoch 666/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 598.4556 - val_loss: 657.4123\n",
      "Epoch 667/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 599.5718 - val_loss: 666.2337\n",
      "Epoch 668/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 598.4615 - val_loss: 659.2441\n",
      "Epoch 669/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 598.0846 - val_loss: 663.5534\n",
      "Epoch 670/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 596.8736 - val_loss: 659.6810\n",
      "Epoch 671/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 598.9587 - val_loss: 666.0914\n",
      "Epoch 672/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 596.8803 - val_loss: 660.9192\n",
      "Epoch 673/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 596.0078 - val_loss: 663.6049\n",
      "Epoch 674/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 596.5035 - val_loss: 661.1057\n",
      "Epoch 675/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 598.8060 - val_loss: 657.2332\n",
      "Epoch 676/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 88us/step - loss: 597.8670 - val_loss: 661.7972\n",
      "Epoch 677/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 595.6718 - val_loss: 669.5693\n",
      "Epoch 678/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 598.3221 - val_loss: 661.6090\n",
      "Epoch 679/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 595.7899 - val_loss: 662.1447\n",
      "Epoch 680/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 595.3835 - val_loss: 663.6341\n",
      "Epoch 681/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 595.4226 - val_loss: 662.4153\n",
      "Epoch 682/2000\n",
      "357/357 [==============================] - 0s 70us/step - loss: 596.9956 - val_loss: 657.8330\n",
      "Epoch 683/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 596.1017 - val_loss: 662.8509\n",
      "Epoch 684/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 599.2206 - val_loss: 657.6387\n",
      "Epoch 685/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 600.8101 - val_loss: 672.2165\n",
      "Epoch 686/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 595.4302 - val_loss: 657.8503\n",
      "Epoch 687/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 599.4955 - val_loss: 656.2601\n",
      "Epoch 688/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 596.5646 - val_loss: 664.8818\n",
      "Epoch 689/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 595.2371 - val_loss: 658.3329\n",
      "Epoch 690/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 594.7846 - val_loss: 661.6785\n",
      "Epoch 691/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 594.4314 - val_loss: 660.7417\n",
      "Epoch 692/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 596.7980 - val_loss: 657.6239\n",
      "Epoch 693/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 594.9726 - val_loss: 664.4000\n",
      "Epoch 694/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 595.6826 - val_loss: 660.3358\n",
      "Epoch 695/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 597.0701 - val_loss: 664.8686\n",
      "Epoch 696/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 595.2099 - val_loss: 658.1886\n",
      "Epoch 697/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 593.2084 - val_loss: 664.1399\n",
      "Epoch 698/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 596.1635 - val_loss: 660.8393\n",
      "Epoch 699/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 604.9477 - val_loss: 654.9232\n",
      "Epoch 700/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 593.0920 - val_loss: 665.3651\n",
      "Epoch 701/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 596.7667 - val_loss: 662.1499\n",
      "Epoch 702/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 595.8970 - val_loss: 656.1313\n",
      "Epoch 703/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 598.3428 - val_loss: 665.5702\n",
      "Epoch 704/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 592.1560 - val_loss: 655.7537\n",
      "Epoch 705/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 596.7287 - val_loss: 658.1199\n",
      "Epoch 706/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 593.9267 - val_loss: 659.1608\n",
      "Epoch 707/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 596.8589 - val_loss: 659.6311\n",
      "Epoch 708/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 597.6288 - val_loss: 654.6405\n",
      "Epoch 709/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 594.4915 - val_loss: 662.2036\n",
      "Epoch 710/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 596.7860 - val_loss: 664.2156\n",
      "Epoch 711/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 592.7186 - val_loss: 656.9261\n",
      "Epoch 712/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 592.8975 - val_loss: 661.9132\n",
      "Epoch 713/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 594.9454 - val_loss: 661.6066\n",
      "Epoch 714/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 595.3152 - val_loss: 656.7535\n",
      "Epoch 715/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 592.6132 - val_loss: 660.4343\n",
      "Epoch 716/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 592.6925 - val_loss: 656.5129\n",
      "Epoch 717/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 593.2156 - val_loss: 660.5838\n",
      "Epoch 718/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 595.1833 - val_loss: 657.7108\n",
      "Epoch 719/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 592.5792 - val_loss: 659.5216\n",
      "Epoch 720/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 593.1509 - val_loss: 657.6574\n",
      "Epoch 721/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 592.8745 - val_loss: 661.4932\n",
      "Epoch 722/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 592.9665 - val_loss: 656.4560\n",
      "Epoch 723/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 595.5344 - val_loss: 654.6336\n",
      "Epoch 724/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 592.4379 - val_loss: 658.1223\n",
      "Epoch 725/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 592.2059 - val_loss: 659.4986\n",
      "Epoch 726/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 592.6975 - val_loss: 659.9743\n",
      "Epoch 727/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 595.1676 - val_loss: 661.9640\n",
      "Epoch 728/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 594.1905 - val_loss: 654.2196\n",
      "Epoch 729/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 592.6598 - val_loss: 659.0497\n",
      "Epoch 730/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 592.0076 - val_loss: 660.1949\n",
      "Epoch 731/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 594.8635 - val_loss: 654.3380\n",
      "Epoch 732/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 595.9706 - val_loss: 656.9389\n",
      "Epoch 733/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 591.7620 - val_loss: 657.8495\n",
      "Epoch 734/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 592.2880 - val_loss: 660.7242\n",
      "Epoch 735/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 592.3864 - val_loss: 658.4510\n",
      "Epoch 736/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 591.8011 - val_loss: 653.6221\n",
      "Epoch 737/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 594.3546 - val_loss: 654.4391\n",
      "Epoch 738/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 599.5212 - val_loss: 665.3001\n",
      "Epoch 739/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 593.4619 - val_loss: 653.7689\n",
      "Epoch 740/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 593.4220 - val_loss: 660.2240\n",
      "Epoch 741/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 593.1830 - val_loss: 656.9754\n",
      "Epoch 742/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 592.1025 - val_loss: 652.8718\n",
      "Epoch 743/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 591.9023 - val_loss: 656.9399\n",
      "Epoch 744/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 591.0456 - val_loss: 653.6177\n",
      "Epoch 745/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 593.3307 - val_loss: 655.6247\n",
      "Epoch 746/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 597.4495 - val_loss: 651.7035\n",
      "Epoch 747/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 592.4123 - val_loss: 659.0201\n",
      "Epoch 748/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 592.8593 - val_loss: 657.4087\n",
      "Epoch 749/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 592.6382 - val_loss: 652.2951\n",
      "Epoch 750/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 591.4638 - val_loss: 656.9206\n",
      "Epoch 751/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 79us/step - loss: 593.5680 - val_loss: 651.7471\n",
      "Epoch 752/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 591.3551 - val_loss: 657.9856\n",
      "Epoch 753/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 590.1423 - val_loss: 653.8999\n",
      "Epoch 754/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 590.7117 - val_loss: 652.8970\n",
      "Epoch 755/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 590.9150 - val_loss: 652.9295\n",
      "Epoch 756/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 591.3520 - val_loss: 653.8283\n",
      "Epoch 757/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 591.0610 - val_loss: 651.8546\n",
      "Epoch 758/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 593.7405 - val_loss: 654.2858\n",
      "Epoch 759/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 590.8895 - val_loss: 655.9545\n",
      "Epoch 760/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 590.7758 - val_loss: 653.4684\n",
      "Epoch 761/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 590.0959 - val_loss: 656.0220\n",
      "Epoch 762/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 592.0522 - val_loss: 652.4527\n",
      "Epoch 763/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 589.6697 - val_loss: 654.6023\n",
      "Epoch 764/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 589.4102 - val_loss: 654.9233\n",
      "Epoch 765/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 589.7883 - val_loss: 654.1051\n",
      "Epoch 766/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 592.3741 - val_loss: 650.9864\n",
      "Epoch 767/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 591.2486 - val_loss: 656.2390\n",
      "Epoch 768/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 590.3907 - val_loss: 652.2709\n",
      "Epoch 769/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 588.9373 - val_loss: 655.9685\n",
      "Epoch 770/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 592.1585 - val_loss: 655.8868\n",
      "Epoch 771/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 588.6149 - val_loss: 651.6979\n",
      "Epoch 772/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 590.0836 - val_loss: 653.6721\n",
      "Epoch 773/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 592.3579 - val_loss: 656.3975\n",
      "Epoch 774/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 588.1734 - val_loss: 650.4362\n",
      "Epoch 775/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 595.3587 - val_loss: 651.1780\n",
      "Epoch 776/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 589.3859 - val_loss: 663.8429\n",
      "Epoch 777/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 594.0310 - val_loss: 654.8997\n",
      "Epoch 778/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 589.4416 - val_loss: 652.2430\n",
      "Epoch 779/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 590.0278 - val_loss: 652.8382\n",
      "Epoch 780/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 588.5544 - val_loss: 656.3638\n",
      "Epoch 781/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 589.4419 - val_loss: 654.0536\n",
      "Epoch 782/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 591.1911 - val_loss: 652.0484\n",
      "Epoch 783/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 589.1615 - val_loss: 657.7303\n",
      "Epoch 784/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 592.8980 - val_loss: 650.0190\n",
      "Epoch 785/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 588.9311 - val_loss: 653.5938\n",
      "Epoch 786/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 588.7024 - val_loss: 653.0475\n",
      "Epoch 787/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 589.5047 - val_loss: 652.6655\n",
      "Epoch 788/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 589.7651 - val_loss: 652.1335\n",
      "Epoch 789/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 588.9488 - val_loss: 650.9470\n",
      "Epoch 790/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 588.0964 - val_loss: 652.7310\n",
      "Epoch 791/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 589.0548 - val_loss: 654.2306\n",
      "Epoch 792/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 592.0329 - val_loss: 657.7946\n",
      "Epoch 793/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 589.6407 - val_loss: 649.2047\n",
      "Epoch 794/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 591.2272 - val_loss: 655.0833\n",
      "Epoch 795/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 594.8684 - val_loss: 655.2681\n",
      "Epoch 796/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 591.5640 - val_loss: 648.9312\n",
      "Epoch 797/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 589.3927 - val_loss: 657.6239\n",
      "Epoch 798/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 588.8304 - val_loss: 652.9022\n",
      "Epoch 799/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 589.0712 - val_loss: 651.0331\n",
      "Epoch 800/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 596.6839 - val_loss: 659.8784\n",
      "Epoch 801/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 594.0066 - val_loss: 650.1411\n",
      "Epoch 802/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 587.4381 - val_loss: 654.8129\n",
      "Epoch 803/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 588.3208 - val_loss: 652.8866\n",
      "Epoch 804/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 587.8006 - val_loss: 650.7151\n",
      "Epoch 805/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 587.9661 - val_loss: 658.0066\n",
      "Epoch 806/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 589.3294 - val_loss: 649.6922\n",
      "Epoch 807/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 588.1358 - val_loss: 653.9487\n",
      "Epoch 808/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 589.9097 - val_loss: 650.0203\n",
      "Epoch 809/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 587.5977 - val_loss: 653.8749\n",
      "Epoch 810/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 587.9385 - val_loss: 650.0448\n",
      "Epoch 811/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 586.7309 - val_loss: 653.6873\n",
      "Epoch 812/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 588.3527 - val_loss: 655.1173\n",
      "Epoch 813/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 587.1286 - val_loss: 649.6612\n",
      "Epoch 814/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 588.0057 - val_loss: 653.1270\n",
      "Epoch 815/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 588.0171 - val_loss: 649.5225\n",
      "Epoch 816/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 588.3816 - val_loss: 652.5495\n",
      "Epoch 817/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 587.7724 - val_loss: 648.7409\n",
      "Epoch 818/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 587.1164 - val_loss: 652.4629\n",
      "Epoch 819/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 587.8564 - val_loss: 651.1055\n",
      "Epoch 820/2000\n",
      "357/357 [==============================] - 0s 63us/step - loss: 587.3805 - val_loss: 649.5194\n",
      "Epoch 821/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 591.8951 - val_loss: 652.4920\n",
      "Epoch 822/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 590.2405 - val_loss: 648.9068\n",
      "Epoch 823/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 587.7655 - val_loss: 651.3583\n",
      "Epoch 824/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 587.0463 - val_loss: 649.6006\n",
      "Epoch 825/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 589.8025 - val_loss: 649.2799\n",
      "Epoch 826/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 82us/step - loss: 586.2857 - val_loss: 651.2478\n",
      "Epoch 827/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 590.3257 - val_loss: 655.2780\n",
      "Epoch 828/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 586.8057 - val_loss: 648.0040\n",
      "Epoch 829/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 587.6208 - val_loss: 649.2540\n",
      "Epoch 830/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 585.9740 - val_loss: 652.6944\n",
      "Epoch 831/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 593.5414 - val_loss: 656.7306\n",
      "Epoch 832/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 589.2874 - val_loss: 647.5274\n",
      "Epoch 833/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 588.9462 - val_loss: 658.7201\n",
      "Epoch 834/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 589.6201 - val_loss: 649.5113\n",
      "Epoch 835/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 585.9132 - val_loss: 649.4893\n",
      "Epoch 836/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 586.0994 - val_loss: 648.0340\n",
      "Epoch 837/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 587.5434 - val_loss: 653.7071\n",
      "Epoch 838/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 587.4534 - val_loss: 652.5501\n",
      "Epoch 839/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 586.1529 - val_loss: 647.8539\n",
      "Epoch 840/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 584.6149 - val_loss: 652.7780\n",
      "Epoch 841/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 586.3707 - val_loss: 648.5671\n",
      "Epoch 842/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 586.5736 - val_loss: 648.3237\n",
      "Epoch 843/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 585.9772 - val_loss: 649.5078\n",
      "Epoch 844/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 586.6091 - val_loss: 650.8345\n",
      "Epoch 845/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 586.7257 - val_loss: 653.8284\n",
      "Epoch 846/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 586.1264 - val_loss: 649.0527\n",
      "Epoch 847/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 587.1862 - val_loss: 648.1110\n",
      "Epoch 848/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 585.9594 - val_loss: 651.8582\n",
      "Epoch 849/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 584.6670 - val_loss: 646.7087\n",
      "Epoch 850/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 587.6237 - val_loss: 646.8054\n",
      "Epoch 851/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 591.3597 - val_loss: 647.6306\n",
      "Epoch 852/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 584.8294 - val_loss: 652.8191\n",
      "Epoch 853/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 585.9987 - val_loss: 650.8765\n",
      "Epoch 854/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 586.7522 - val_loss: 648.8790\n",
      "Epoch 855/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 592.1944 - val_loss: 656.5346\n",
      "Epoch 856/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 587.9682 - val_loss: 647.9641\n",
      "Epoch 857/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 587.2661 - val_loss: 650.1062\n",
      "Epoch 858/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 595.1855 - val_loss: 650.8907\n",
      "Epoch 859/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 588.6309 - val_loss: 645.8293\n",
      "Epoch 860/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 585.1224 - val_loss: 650.0727\n",
      "Epoch 861/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 585.1655 - val_loss: 648.1400\n",
      "Epoch 862/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.8704 - val_loss: 647.3895\n",
      "Epoch 863/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 586.0450 - val_loss: 650.8920\n",
      "Epoch 864/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 585.1889 - val_loss: 648.4553\n",
      "Epoch 865/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 588.4423 - val_loss: 646.9608\n",
      "Epoch 866/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 583.8530 - val_loss: 653.8555\n",
      "Epoch 867/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 585.5176 - val_loss: 648.0985\n",
      "Epoch 868/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 586.1781 - val_loss: 649.3988\n",
      "Epoch 869/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 584.9367 - val_loss: 648.0273\n",
      "Epoch 870/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.1491 - val_loss: 649.7211\n",
      "Epoch 871/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 583.8725 - val_loss: 647.7190\n",
      "Epoch 872/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 586.5905 - val_loss: 654.1368\n",
      "Epoch 873/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 587.6682 - val_loss: 652.3546\n",
      "Epoch 874/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.8290 - val_loss: 647.9692\n",
      "Epoch 875/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 585.7005 - val_loss: 648.4279\n",
      "Epoch 876/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 583.9712 - val_loss: 647.9686\n",
      "Epoch 877/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 587.0613 - val_loss: 645.3700\n",
      "Epoch 878/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 587.2715 - val_loss: 656.0391\n",
      "Epoch 879/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 587.7639 - val_loss: 646.0368\n",
      "Epoch 880/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 585.2897 - val_loss: 650.4382\n",
      "Epoch 881/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 584.1957 - val_loss: 651.8798\n",
      "Epoch 882/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 586.5752 - val_loss: 646.0526\n",
      "Epoch 883/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 584.5138 - val_loss: 647.0956\n",
      "Epoch 884/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 585.7503 - val_loss: 647.0113\n",
      "Epoch 885/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 584.0402 - val_loss: 646.9523\n",
      "Epoch 886/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 584.4410 - val_loss: 647.8313\n",
      "Epoch 887/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 584.5679 - val_loss: 651.4080\n",
      "Epoch 888/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 584.6088 - val_loss: 648.8165\n",
      "Epoch 889/2000\n",
      "357/357 [==============================] - 0s 68us/step - loss: 584.0471 - val_loss: 646.5735\n",
      "Epoch 890/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 583.7327 - val_loss: 649.1898\n",
      "Epoch 891/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 585.3633 - val_loss: 651.1966\n",
      "Epoch 892/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.4908 - val_loss: 646.8032\n",
      "Epoch 893/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 584.4440 - val_loss: 646.1391\n",
      "Epoch 894/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 586.0638 - val_loss: 645.2510\n",
      "Epoch 895/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 589.3894 - val_loss: 657.4895\n",
      "Epoch 896/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 583.7546 - val_loss: 645.4778\n",
      "Epoch 897/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 590.0497 - val_loss: 645.4394\n",
      "Epoch 898/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 584.5662 - val_loss: 647.8842\n",
      "Epoch 899/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 584.1645 - val_loss: 644.9089\n",
      "Epoch 900/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 583.8008 - val_loss: 646.5567\n",
      "Epoch 901/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 89us/step - loss: 587.1207 - val_loss: 647.5853\n",
      "Epoch 902/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 584.8986 - val_loss: 649.5683\n",
      "Epoch 903/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 586.0655 - val_loss: 648.2880\n",
      "Epoch 904/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.7588 - val_loss: 643.5203\n",
      "Epoch 905/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 583.7694 - val_loss: 648.6209\n",
      "Epoch 906/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 582.7585 - val_loss: 645.0957\n",
      "Epoch 907/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 585.8140 - val_loss: 644.2797\n",
      "Epoch 908/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 586.2056 - val_loss: 648.2528\n",
      "Epoch 909/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 581.6383 - val_loss: 642.8203\n",
      "Epoch 910/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 589.9506 - val_loss: 644.1546\n",
      "Epoch 911/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 585.2892 - val_loss: 653.7018\n",
      "Epoch 912/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.4595 - val_loss: 644.2097\n",
      "Epoch 913/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 586.1869 - val_loss: 646.0347\n",
      "Epoch 914/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 593.7783 - val_loss: 658.1572\n",
      "Epoch 915/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 586.7460 - val_loss: 643.5701\n",
      "Epoch 916/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 583.6349 - val_loss: 647.0010\n",
      "Epoch 917/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 584.3399 - val_loss: 647.3607\n",
      "Epoch 918/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 582.7586 - val_loss: 646.7520\n",
      "Epoch 919/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 583.2335 - val_loss: 648.6524\n",
      "Epoch 920/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 584.4441 - val_loss: 646.7178\n",
      "Epoch 921/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 583.6000 - val_loss: 644.2251\n",
      "Epoch 922/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 586.9879 - val_loss: 649.1282\n",
      "Epoch 923/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 582.2637 - val_loss: 643.5382\n",
      "Epoch 924/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 583.5326 - val_loss: 645.3836\n",
      "Epoch 925/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 582.9247 - val_loss: 646.8115\n",
      "Epoch 926/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 583.1971 - val_loss: 643.4769\n",
      "Epoch 927/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 582.4105 - val_loss: 647.0843\n",
      "Epoch 928/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 582.4724 - val_loss: 645.0799\n",
      "Epoch 929/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 582.5347 - val_loss: 646.1915\n",
      "Epoch 930/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 582.8058 - val_loss: 647.1052\n",
      "Epoch 931/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 586.3282 - val_loss: 645.1849\n",
      "Epoch 932/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 585.3491 - val_loss: 652.6907\n",
      "Epoch 933/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 584.2580 - val_loss: 645.1577\n",
      "Epoch 934/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 583.9999 - val_loss: 643.2129\n",
      "Epoch 935/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 582.4317 - val_loss: 646.9068\n",
      "Epoch 936/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 583.6724 - val_loss: 649.1099\n",
      "Epoch 937/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 582.2669 - val_loss: 643.8171\n",
      "Epoch 938/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 585.6363 - val_loss: 642.1867\n",
      "Epoch 939/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 582.7512 - val_loss: 644.6901\n",
      "Epoch 940/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 582.1406 - val_loss: 645.0528\n",
      "Epoch 941/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 583.0412 - val_loss: 647.1819\n",
      "Epoch 942/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 583.6974 - val_loss: 640.7846\n",
      "Epoch 943/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 582.9525 - val_loss: 648.1521\n",
      "Epoch 944/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 585.9634 - val_loss: 649.5434\n",
      "Epoch 945/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 582.5524 - val_loss: 642.8033\n",
      "Epoch 946/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 584.7652 - val_loss: 643.5516\n",
      "Epoch 947/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 587.8687 - val_loss: 648.3667\n",
      "Epoch 948/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 586.3350 - val_loss: 640.9661\n",
      "Epoch 949/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 583.3508 - val_loss: 648.3379\n",
      "Epoch 950/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 585.7144 - val_loss: 642.5350\n",
      "Epoch 951/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 584.1330 - val_loss: 653.3987\n",
      "Epoch 952/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 585.8346 - val_loss: 646.4298\n",
      "Epoch 953/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 586.4514 - val_loss: 649.5828\n",
      "Epoch 954/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 584.5393 - val_loss: 640.5776\n",
      "Epoch 955/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 582.0228 - val_loss: 648.9004\n",
      "Epoch 956/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 581.7454 - val_loss: 643.5941\n",
      "Epoch 957/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 582.1935 - val_loss: 643.5589\n",
      "Epoch 958/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 582.8612 - val_loss: 641.7326\n",
      "Epoch 959/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 580.5274 - val_loss: 648.9078\n",
      "Epoch 960/2000\n",
      "357/357 [==============================] - 0s 60us/step - loss: 584.8223 - val_loss: 647.7103\n",
      "Epoch 961/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 581.8609 - val_loss: 643.7092\n",
      "Epoch 962/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 581.1660 - val_loss: 646.3406\n",
      "Epoch 963/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 584.0724 - val_loss: 650.1789\n",
      "Epoch 964/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 583.1513 - val_loss: 641.4310\n",
      "Epoch 965/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 583.2265 - val_loss: 644.3664\n",
      "Epoch 966/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 586.8749 - val_loss: 653.7312\n",
      "Epoch 967/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 581.7324 - val_loss: 640.9796\n",
      "Epoch 968/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 582.0466 - val_loss: 647.6180\n",
      "Epoch 969/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 582.6682 - val_loss: 643.9979\n",
      "Epoch 970/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 582.1055 - val_loss: 639.7092\n",
      "Epoch 971/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 581.7648 - val_loss: 645.7195\n",
      "Epoch 972/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 580.9939 - val_loss: 644.2626\n",
      "Epoch 973/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 580.5440 - val_loss: 642.3872\n",
      "Epoch 974/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 581.7828 - val_loss: 643.2389\n",
      "Epoch 975/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 580.7394 - val_loss: 643.6765\n",
      "Epoch 976/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 87us/step - loss: 580.9040 - val_loss: 644.4671\n",
      "Epoch 977/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 583.6232 - val_loss: 648.8731\n",
      "Epoch 978/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 583.6025 - val_loss: 645.7426\n",
      "Epoch 979/2000\n",
      "357/357 [==============================] - 0s 113us/step - loss: 581.2368 - val_loss: 643.2984\n",
      "Epoch 980/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.2279 - val_loss: 642.7044\n",
      "Epoch 981/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 580.8015 - val_loss: 645.2542\n",
      "Epoch 982/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 582.0371 - val_loss: 647.7829\n",
      "Epoch 983/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 583.9099 - val_loss: 641.2288\n",
      "Epoch 984/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.2807 - val_loss: 644.8521\n",
      "Epoch 985/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 580.4238 - val_loss: 640.2994\n",
      "Epoch 986/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.4451 - val_loss: 642.1357\n",
      "Epoch 987/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 586.6957 - val_loss: 650.8013\n",
      "Epoch 988/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 582.0795 - val_loss: 641.9845\n",
      "Epoch 989/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 580.8023 - val_loss: 644.5523\n",
      "Epoch 990/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 582.8303 - val_loss: 641.6595\n",
      "Epoch 991/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 580.8677 - val_loss: 640.5026\n",
      "Epoch 992/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.2960 - val_loss: 640.2906\n",
      "Epoch 993/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 580.4785 - val_loss: 643.0113\n",
      "Epoch 994/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 581.3764 - val_loss: 643.7220\n",
      "Epoch 995/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 582.5936 - val_loss: 643.5660\n",
      "Epoch 996/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 584.4306 - val_loss: 640.6996\n",
      "Epoch 997/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 580.1340 - val_loss: 646.9440\n",
      "Epoch 998/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 583.1452 - val_loss: 643.4344\n",
      "Epoch 999/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 584.2646 - val_loss: 642.3112\n",
      "Epoch 1000/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 581.4313 - val_loss: 649.2835\n",
      "Epoch 1001/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 582.9982 - val_loss: 640.5437\n",
      "Epoch 1002/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 579.7595 - val_loss: 645.8734\n",
      "Epoch 1003/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 589.0437 - val_loss: 653.0194\n",
      "Epoch 1004/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 586.6670 - val_loss: 637.6041\n",
      "Epoch 1005/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 584.2462 - val_loss: 644.4337\n",
      "Epoch 1006/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 581.6467 - val_loss: 644.1592\n",
      "Epoch 1007/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 581.1254 - val_loss: 643.4461\n",
      "Epoch 1008/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 580.4199 - val_loss: 642.4994\n",
      "Epoch 1009/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 580.3330 - val_loss: 643.5148\n",
      "Epoch 1010/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 583.4134 - val_loss: 646.6253\n",
      "Epoch 1011/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 580.8097 - val_loss: 641.1500\n",
      "Epoch 1012/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 580.6675 - val_loss: 646.7611\n",
      "Epoch 1013/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 580.2025 - val_loss: 640.8324\n",
      "Epoch 1014/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 580.1422 - val_loss: 645.0627\n",
      "Epoch 1015/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 586.8697 - val_loss: 651.6003\n",
      "Epoch 1016/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 581.3138 - val_loss: 640.9742\n",
      "Epoch 1017/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 581.3751 - val_loss: 643.2684\n",
      "Epoch 1018/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 581.3992 - val_loss: 654.5643\n",
      "Epoch 1019/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 583.4560 - val_loss: 640.1272\n",
      "Epoch 1020/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 582.6173 - val_loss: 640.9789\n",
      "Epoch 1021/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 581.6815 - val_loss: 641.7126\n",
      "Epoch 1022/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 580.8280 - val_loss: 641.8091\n",
      "Epoch 1023/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 579.6858 - val_loss: 640.9247\n",
      "Epoch 1024/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 580.9791 - val_loss: 644.4970\n",
      "Epoch 1025/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 583.0155 - val_loss: 647.1639\n",
      "Epoch 1026/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 579.8101 - val_loss: 639.8878\n",
      "Epoch 1027/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 584.3751 - val_loss: 640.9604\n",
      "Epoch 1028/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 587.6363 - val_loss: 650.9028\n",
      "Epoch 1029/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 581.8282 - val_loss: 637.2498\n",
      "Epoch 1030/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 578.5740 - val_loss: 647.7980\n",
      "Epoch 1031/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 583.9114 - val_loss: 649.1012\n",
      "Epoch 1032/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 580.8381 - val_loss: 640.6765\n",
      "Epoch 1033/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 580.5424 - val_loss: 641.4333\n",
      "Epoch 1034/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 579.5116 - val_loss: 642.9473\n",
      "Epoch 1035/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 581.0505 - val_loss: 647.6519\n",
      "Epoch 1036/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 584.8026 - val_loss: 651.8887\n",
      "Epoch 1037/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 584.1781 - val_loss: 643.4473\n",
      "Epoch 1038/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 580.7303 - val_loss: 642.7264\n",
      "Epoch 1039/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 581.2824 - val_loss: 645.4514\n",
      "Epoch 1040/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 578.7269 - val_loss: 639.3247\n",
      "Epoch 1041/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 580.5254 - val_loss: 643.1796\n",
      "Epoch 1042/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 583.3874 - val_loss: 636.5083\n",
      "Epoch 1043/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 581.6511 - val_loss: 646.2577\n",
      "Epoch 1044/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 585.1587 - val_loss: 650.9246\n",
      "Epoch 1045/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 579.9549 - val_loss: 636.9219\n",
      "Epoch 1046/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 581.3917 - val_loss: 644.4507\n",
      "Epoch 1047/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 579.0299 - val_loss: 640.4995\n",
      "Epoch 1048/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 580.5698 - val_loss: 641.7352\n",
      "Epoch 1049/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 578.9979 - val_loss: 644.6631\n",
      "Epoch 1050/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 582.2604 - val_loss: 645.1913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1051/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 579.5043 - val_loss: 641.7685\n",
      "Epoch 1052/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 579.3798 - val_loss: 644.7322\n",
      "Epoch 1053/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 582.4983 - val_loss: 642.6530\n",
      "Epoch 1054/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 579.4951 - val_loss: 642.1656\n",
      "Epoch 1055/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 578.6968 - val_loss: 641.5998\n",
      "Epoch 1056/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 580.1207 - val_loss: 641.3884\n",
      "Epoch 1057/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 582.0070 - val_loss: 637.1501\n",
      "Epoch 1058/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 580.9894 - val_loss: 645.2441\n",
      "Epoch 1059/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 580.6761 - val_loss: 640.3503\n",
      "Epoch 1060/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 579.0798 - val_loss: 641.5339\n",
      "Epoch 1061/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 579.5791 - val_loss: 639.9104\n",
      "Epoch 1062/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 579.0779 - val_loss: 641.5086\n",
      "Epoch 1063/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 579.5021 - val_loss: 641.0997\n",
      "Epoch 1064/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 581.9827 - val_loss: 638.0924\n",
      "Epoch 1065/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 584.3447 - val_loss: 651.5548\n",
      "Epoch 1066/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 580.4023 - val_loss: 635.8841\n",
      "Epoch 1067/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 583.4552 - val_loss: 641.9039\n",
      "Epoch 1068/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.5256 - val_loss: 641.6079\n",
      "Epoch 1069/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 578.4245 - val_loss: 640.6701\n",
      "Epoch 1070/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 579.1429 - val_loss: 640.3450\n",
      "Epoch 1071/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 578.4146 - val_loss: 643.7110\n",
      "Epoch 1072/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 579.5284 - val_loss: 642.6460\n",
      "Epoch 1073/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 579.5742 - val_loss: 641.6025\n",
      "Epoch 1074/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 578.3385 - val_loss: 640.1868\n",
      "Epoch 1075/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 578.4311 - val_loss: 644.0184\n",
      "Epoch 1076/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 579.3822 - val_loss: 644.3853\n",
      "Epoch 1077/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 579.7999 - val_loss: 643.0306\n",
      "Epoch 1078/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 579.2201 - val_loss: 642.2283\n",
      "Epoch 1079/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.7270 - val_loss: 643.3834\n",
      "Epoch 1080/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 579.0539 - val_loss: 640.4610\n",
      "Epoch 1081/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 579.1616 - val_loss: 641.2119\n",
      "Epoch 1082/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 585.4515 - val_loss: 639.6676\n",
      "Epoch 1083/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 593.0161 - val_loss: 657.8452\n",
      "Epoch 1084/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.6350 - val_loss: 635.4422\n",
      "Epoch 1085/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 579.9029 - val_loss: 645.0299\n",
      "Epoch 1086/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 578.9799 - val_loss: 641.8612\n",
      "Epoch 1087/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.6231 - val_loss: 639.9308\n",
      "Epoch 1088/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 579.7022 - val_loss: 638.7346\n",
      "Epoch 1089/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 580.0101 - val_loss: 648.7745\n",
      "Epoch 1090/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 581.1907 - val_loss: 642.1882\n",
      "Epoch 1091/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 578.5773 - val_loss: 638.4990\n",
      "Epoch 1092/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 580.1911 - val_loss: 639.5272\n",
      "Epoch 1093/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 580.0146 - val_loss: 641.2951\n",
      "Epoch 1094/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 578.0370 - val_loss: 644.3736\n",
      "Epoch 1095/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 579.0909 - val_loss: 644.7091\n",
      "Epoch 1096/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 579.6029 - val_loss: 641.7093\n",
      "Epoch 1097/2000\n",
      "357/357 [==============================] - 0s 70us/step - loss: 578.5694 - val_loss: 647.2565\n",
      "Epoch 1098/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 579.6671 - val_loss: 644.7527\n",
      "Epoch 1099/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 579.0659 - val_loss: 639.1765\n",
      "Epoch 1100/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 578.0398 - val_loss: 642.7290\n",
      "Epoch 1101/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 583.2222 - val_loss: 637.9126\n",
      "Epoch 1102/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 580.7250 - val_loss: 642.6409\n",
      "Epoch 1103/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 580.9071 - val_loss: 638.0751\n",
      "Epoch 1104/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 580.5752 - val_loss: 642.6775\n",
      "Epoch 1105/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 580.2743 - val_loss: 639.3547\n",
      "Epoch 1106/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 581.0645 - val_loss: 646.1842\n",
      "Epoch 1107/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 580.1270 - val_loss: 640.6469\n",
      "Epoch 1108/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 580.9804 - val_loss: 645.5392\n",
      "Epoch 1109/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 577.9433 - val_loss: 638.4108\n",
      "Epoch 1110/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 580.9489 - val_loss: 638.7711\n",
      "Epoch 1111/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 578.8274 - val_loss: 641.6755\n",
      "Epoch 1112/2000\n",
      "357/357 [==============================] - 0s 66us/step - loss: 579.2475 - val_loss: 638.6704\n",
      "Epoch 1113/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 580.7903 - val_loss: 641.0297\n",
      "Epoch 1114/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 578.6120 - val_loss: 637.2465\n",
      "Epoch 1115/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 578.8464 - val_loss: 642.9036\n",
      "Epoch 1116/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 585.4240 - val_loss: 646.1190\n",
      "Epoch 1117/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 584.3770 - val_loss: 635.4531\n",
      "Epoch 1118/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 585.8639 - val_loss: 655.5139\n",
      "Epoch 1119/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 582.3597 - val_loss: 640.9994\n",
      "Epoch 1120/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 577.6873 - val_loss: 637.3035\n",
      "Epoch 1121/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 580.7818 - val_loss: 639.7033\n",
      "Epoch 1122/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 578.7736 - val_loss: 642.0769\n",
      "Epoch 1123/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 577.4150 - val_loss: 640.8353\n",
      "Epoch 1124/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 578.5867 - val_loss: 641.7877\n",
      "Epoch 1125/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 93us/step - loss: 577.7266 - val_loss: 643.7886\n",
      "Epoch 1126/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.3356 - val_loss: 637.4281\n",
      "Epoch 1127/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.6710 - val_loss: 642.4214\n",
      "Epoch 1128/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 587.6194 - val_loss: 651.9407\n",
      "Epoch 1129/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 585.3839 - val_loss: 635.4025\n",
      "Epoch 1130/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 582.9358 - val_loss: 646.0229\n",
      "Epoch 1131/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.9132 - val_loss: 639.0635\n",
      "Epoch 1132/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.5634 - val_loss: 644.6969\n",
      "Epoch 1133/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 577.9665 - val_loss: 636.6278\n",
      "Epoch 1134/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.4892 - val_loss: 641.8023\n",
      "Epoch 1135/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 577.2749 - val_loss: 643.3111\n",
      "Epoch 1136/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 578.2801 - val_loss: 642.2118\n",
      "Epoch 1137/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 578.4476 - val_loss: 640.1473\n",
      "Epoch 1138/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 577.0038 - val_loss: 638.5098\n",
      "Epoch 1139/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 577.9680 - val_loss: 638.5690\n",
      "Epoch 1140/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 580.6297 - val_loss: 638.2419\n",
      "Epoch 1141/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 579.5085 - val_loss: 647.5335\n",
      "Epoch 1142/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 578.3739 - val_loss: 636.7579\n",
      "Epoch 1143/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 577.3978 - val_loss: 642.9147\n",
      "Epoch 1144/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 578.4789 - val_loss: 640.5715\n",
      "Epoch 1145/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 577.2107 - val_loss: 644.1679\n",
      "Epoch 1146/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 578.8957 - val_loss: 640.5541\n",
      "Epoch 1147/2000\n",
      "357/357 [==============================] - 0s 69us/step - loss: 577.8515 - val_loss: 637.2154\n",
      "Epoch 1148/2000\n",
      "357/357 [==============================] - 0s 63us/step - loss: 580.9885 - val_loss: 637.8741\n",
      "Epoch 1149/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 579.4779 - val_loss: 643.6638\n",
      "Epoch 1150/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 578.5170 - val_loss: 638.2765\n",
      "Epoch 1151/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.0672 - val_loss: 642.2159\n",
      "Epoch 1152/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 584.2936 - val_loss: 640.7887\n",
      "Epoch 1153/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 585.3003 - val_loss: 648.0203\n",
      "Epoch 1154/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 577.3509 - val_loss: 635.5956\n",
      "Epoch 1155/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 579.6536 - val_loss: 640.8404\n",
      "Epoch 1156/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 576.8719 - val_loss: 638.5331\n",
      "Epoch 1157/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 578.1534 - val_loss: 638.4921\n",
      "Epoch 1158/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 576.7466 - val_loss: 646.3803\n",
      "Epoch 1159/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 579.8918 - val_loss: 642.8447\n",
      "Epoch 1160/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 578.4158 - val_loss: 636.1846\n",
      "Epoch 1161/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 587.1436 - val_loss: 648.9227\n",
      "Epoch 1162/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 575.2277 - val_loss: 635.8919\n",
      "Epoch 1163/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 581.9545 - val_loss: 640.0244\n",
      "Epoch 1164/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 587.8444 - val_loss: 652.1869\n",
      "Epoch 1165/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 578.1073 - val_loss: 636.2946\n",
      "Epoch 1166/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 579.1958 - val_loss: 641.4724\n",
      "Epoch 1167/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.5837 - val_loss: 646.5593\n",
      "Epoch 1168/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 578.3972 - val_loss: 639.6740\n",
      "Epoch 1169/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 577.4907 - val_loss: 638.9889\n",
      "Epoch 1170/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 576.7631 - val_loss: 639.6086\n",
      "Epoch 1171/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 576.7990 - val_loss: 641.8746\n",
      "Epoch 1172/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 579.7440 - val_loss: 645.7585\n",
      "Epoch 1173/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.5926 - val_loss: 639.9612\n",
      "Epoch 1174/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 585.8479 - val_loss: 637.5072\n",
      "Epoch 1175/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 575.4494 - val_loss: 644.9827\n",
      "Epoch 1176/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 580.6796 - val_loss: 640.6597\n",
      "Epoch 1177/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 579.3185 - val_loss: 650.5624\n",
      "Epoch 1178/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 582.5955 - val_loss: 642.9099\n",
      "Epoch 1179/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 577.3139 - val_loss: 641.3334\n",
      "Epoch 1180/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.4866 - val_loss: 637.4662\n",
      "Epoch 1181/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.4579 - val_loss: 639.2791\n",
      "Epoch 1182/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 578.1508 - val_loss: 638.6577\n",
      "Epoch 1183/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 577.2378 - val_loss: 644.6348\n",
      "Epoch 1184/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 580.0209 - val_loss: 640.6404\n",
      "Epoch 1185/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 577.5213 - val_loss: 636.4717\n",
      "Epoch 1186/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.1023 - val_loss: 644.0269\n",
      "Epoch 1187/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 581.5368 - val_loss: 647.1363\n",
      "Epoch 1188/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 579.0324 - val_loss: 638.0710\n",
      "Epoch 1189/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 575.8509 - val_loss: 645.9938\n",
      "Epoch 1190/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 578.1060 - val_loss: 642.7102\n",
      "Epoch 1191/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 575.6624 - val_loss: 635.2750\n",
      "Epoch 1192/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 578.9679 - val_loss: 642.5322\n",
      "Epoch 1193/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 579.5212 - val_loss: 636.9758\n",
      "Epoch 1194/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.7834 - val_loss: 643.4840\n",
      "Epoch 1195/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.5438 - val_loss: 641.1966\n",
      "Epoch 1196/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 578.5160 - val_loss: 636.2282\n",
      "Epoch 1197/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 578.7335 - val_loss: 651.2592\n",
      "Epoch 1198/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 580.8344 - val_loss: 642.0916\n",
      "Epoch 1199/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 102us/step - loss: 578.0945 - val_loss: 637.5172\n",
      "Epoch 1200/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 578.6997 - val_loss: 641.1300\n",
      "Epoch 1201/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 577.7192 - val_loss: 640.7963\n",
      "Epoch 1202/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 580.8516 - val_loss: 637.5750\n",
      "Epoch 1203/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 579.1129 - val_loss: 642.4581\n",
      "Epoch 1204/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.9965 - val_loss: 637.5136\n",
      "Epoch 1205/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 575.9270 - val_loss: 641.9712\n",
      "Epoch 1206/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 580.2250 - val_loss: 635.9315\n",
      "Epoch 1207/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 578.1101 - val_loss: 641.3570\n",
      "Epoch 1208/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 578.1108 - val_loss: 643.2103\n",
      "Epoch 1209/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 578.1429 - val_loss: 635.5703\n",
      "Epoch 1210/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.0409 - val_loss: 641.9719\n",
      "Epoch 1211/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 580.9854 - val_loss: 646.5793\n",
      "Epoch 1212/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 578.9289 - val_loss: 638.4521\n",
      "Epoch 1213/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 576.2662 - val_loss: 642.1934\n",
      "Epoch 1214/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 579.3111 - val_loss: 642.7134\n",
      "Epoch 1215/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 578.4683 - val_loss: 636.8867\n",
      "Epoch 1216/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 577.0866 - val_loss: 642.5403\n",
      "Epoch 1217/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 578.9271 - val_loss: 643.5864\n",
      "Epoch 1218/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 575.5337 - val_loss: 635.6140\n",
      "Epoch 1219/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 578.7693 - val_loss: 641.6344\n",
      "Epoch 1220/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.1668 - val_loss: 651.0818\n",
      "Epoch 1221/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 585.1849 - val_loss: 645.9429\n",
      "Epoch 1222/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 579.1694 - val_loss: 636.0977\n",
      "Epoch 1223/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 576.8262 - val_loss: 644.1389\n",
      "Epoch 1224/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 578.9630 - val_loss: 640.5332\n",
      "Epoch 1225/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.6029 - val_loss: 641.4983\n",
      "Epoch 1226/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 577.4140 - val_loss: 641.6323\n",
      "Epoch 1227/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 588.5413 - val_loss: 634.1702\n",
      "Epoch 1228/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 578.0550 - val_loss: 646.2847\n",
      "Epoch 1229/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.7669 - val_loss: 636.8935\n",
      "Epoch 1230/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.9407 - val_loss: 642.2874\n",
      "Epoch 1231/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 577.7485 - val_loss: 636.1191\n",
      "Epoch 1232/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 574.9648 - val_loss: 646.5331\n",
      "Epoch 1233/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 578.1671 - val_loss: 635.6397\n",
      "Epoch 1234/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 576.3257 - val_loss: 642.1471\n",
      "Epoch 1235/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 579.9580 - val_loss: 642.8786\n",
      "Epoch 1236/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 576.5402 - val_loss: 636.2453\n",
      "Epoch 1237/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 576.7059 - val_loss: 642.4428\n",
      "Epoch 1238/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 576.8346 - val_loss: 639.2763\n",
      "Epoch 1239/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 578.4145 - val_loss: 635.5549\n",
      "Epoch 1240/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 579.4670 - val_loss: 646.6712\n",
      "Epoch 1241/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 577.5444 - val_loss: 636.1640\n",
      "Epoch 1242/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 576.7307 - val_loss: 638.5458\n",
      "Epoch 1243/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 578.6511 - val_loss: 641.0634\n",
      "Epoch 1244/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 579.5512 - val_loss: 635.9393\n",
      "Epoch 1245/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 575.6707 - val_loss: 642.7685\n",
      "Epoch 1246/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 576.0565 - val_loss: 637.4765\n",
      "Epoch 1247/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 578.6087 - val_loss: 640.5326\n",
      "Epoch 1248/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.3316 - val_loss: 639.9538\n",
      "Epoch 1249/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 575.8540 - val_loss: 640.1386\n",
      "Epoch 1250/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 576.3607 - val_loss: 641.6766\n",
      "Epoch 1251/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 579.6838 - val_loss: 644.5212\n",
      "Epoch 1252/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.7180 - val_loss: 633.3355\n",
      "Epoch 1253/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 583.3578 - val_loss: 636.3656\n",
      "Epoch 1254/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.0551 - val_loss: 644.3673\n",
      "Epoch 1255/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 576.6623 - val_loss: 635.7496\n",
      "Epoch 1256/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 575.5675 - val_loss: 641.8074\n",
      "Epoch 1257/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 575.7974 - val_loss: 635.1256\n",
      "Epoch 1258/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 580.5363 - val_loss: 634.9622\n",
      "Epoch 1259/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 576.6364 - val_loss: 639.2012\n",
      "Epoch 1260/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 575.3109 - val_loss: 639.4724\n",
      "Epoch 1261/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 577.5645 - val_loss: 641.9679\n",
      "Epoch 1262/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.1064 - val_loss: 635.3318\n",
      "Epoch 1263/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.1542 - val_loss: 640.9744\n",
      "Epoch 1264/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 576.0947 - val_loss: 635.7217\n",
      "Epoch 1265/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 579.1548 - val_loss: 636.8098\n",
      "Epoch 1266/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 575.8903 - val_loss: 641.5992\n",
      "Epoch 1267/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 576.9082 - val_loss: 637.6923\n",
      "Epoch 1268/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 577.1727 - val_loss: 638.0502\n",
      "Epoch 1269/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 581.4237 - val_loss: 643.9096\n",
      "Epoch 1270/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 579.6767 - val_loss: 633.5481\n",
      "Epoch 1271/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.4839 - val_loss: 642.3321\n",
      "Epoch 1272/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 576.6293 - val_loss: 640.4184\n",
      "Epoch 1273/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 94us/step - loss: 580.9606 - val_loss: 636.8025\n",
      "Epoch 1274/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 575.1209 - val_loss: 645.4176\n",
      "Epoch 1275/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 578.3066 - val_loss: 644.3084\n",
      "Epoch 1276/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 574.4175 - val_loss: 635.7690\n",
      "Epoch 1277/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 576.3259 - val_loss: 640.8289\n",
      "Epoch 1278/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.4254 - val_loss: 639.2548\n",
      "Epoch 1279/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 577.4068 - val_loss: 635.8145\n",
      "Epoch 1280/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 578.3814 - val_loss: 644.9157\n",
      "Epoch 1281/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 576.6012 - val_loss: 637.5777\n",
      "Epoch 1282/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 575.4084 - val_loss: 640.7324\n",
      "Epoch 1283/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 575.3965 - val_loss: 640.0700\n",
      "Epoch 1284/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 576.8906 - val_loss: 639.6699\n",
      "Epoch 1285/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 580.9262 - val_loss: 647.8040\n",
      "Epoch 1286/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 576.9099 - val_loss: 635.0376\n",
      "Epoch 1287/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 574.8733 - val_loss: 642.9693\n",
      "Epoch 1288/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 576.4788 - val_loss: 635.2930\n",
      "Epoch 1289/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 575.7765 - val_loss: 642.9801\n",
      "Epoch 1290/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 577.3005 - val_loss: 641.7367\n",
      "Epoch 1291/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.2108 - val_loss: 638.2338\n",
      "Epoch 1292/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.8404 - val_loss: 637.3652\n",
      "Epoch 1293/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.1244 - val_loss: 648.9349\n",
      "Epoch 1294/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.2917 - val_loss: 637.2124\n",
      "Epoch 1295/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.2715 - val_loss: 644.4221\n",
      "Epoch 1296/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 575.9591 - val_loss: 635.7789\n",
      "Epoch 1297/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 577.6210 - val_loss: 640.3810\n",
      "Epoch 1298/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.7556 - val_loss: 634.1179\n",
      "Epoch 1299/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 577.2725 - val_loss: 636.8263\n",
      "Epoch 1300/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 573.6949 - val_loss: 648.1931\n",
      "Epoch 1301/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 577.2440 - val_loss: 640.9617\n",
      "Epoch 1302/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 575.6946 - val_loss: 638.2811\n",
      "Epoch 1303/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 573.7017 - val_loss: 632.8668\n",
      "Epoch 1304/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 578.5059 - val_loss: 641.0940\n",
      "Epoch 1305/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 576.3540 - val_loss: 640.0539\n",
      "Epoch 1306/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.1803 - val_loss: 644.6025\n",
      "Epoch 1307/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 578.0355 - val_loss: 639.1647\n",
      "Epoch 1308/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 578.4969 - val_loss: 635.9149\n",
      "Epoch 1309/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 575.2843 - val_loss: 640.1155\n",
      "Epoch 1310/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 575.1163 - val_loss: 635.9009\n",
      "Epoch 1311/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 576.9961 - val_loss: 637.4922\n",
      "Epoch 1312/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 575.8946 - val_loss: 641.2205\n",
      "Epoch 1313/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 575.3949 - val_loss: 639.1057\n",
      "Epoch 1314/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.8758 - val_loss: 640.4649\n",
      "Epoch 1315/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 579.8893 - val_loss: 645.3317\n",
      "Epoch 1316/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 574.6161 - val_loss: 634.0675\n",
      "Epoch 1317/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 580.5734 - val_loss: 640.4457\n",
      "Epoch 1318/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 576.0367 - val_loss: 637.3587\n",
      "Epoch 1319/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 578.0605 - val_loss: 642.4562\n",
      "Epoch 1320/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 578.4001 - val_loss: 633.1740\n",
      "Epoch 1321/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 574.9229 - val_loss: 649.7014\n",
      "Epoch 1322/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.1587 - val_loss: 636.9374\n",
      "Epoch 1323/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 576.0383 - val_loss: 639.0590\n",
      "Epoch 1324/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 577.1231 - val_loss: 648.6966\n",
      "Epoch 1325/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 579.5996 - val_loss: 638.0405\n",
      "Epoch 1326/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 576.3466 - val_loss: 643.5203\n",
      "Epoch 1327/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 575.9951 - val_loss: 637.4410\n",
      "Epoch 1328/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.7556 - val_loss: 636.5441\n",
      "Epoch 1329/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 577.3174 - val_loss: 645.7943\n",
      "Epoch 1330/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.0421 - val_loss: 633.9567\n",
      "Epoch 1331/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 575.7345 - val_loss: 640.7736\n",
      "Epoch 1332/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 576.2358 - val_loss: 638.9696\n",
      "Epoch 1333/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 575.5439 - val_loss: 634.2028\n",
      "Epoch 1334/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 575.7336 - val_loss: 640.0725\n",
      "Epoch 1335/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 575.3726 - val_loss: 641.6710\n",
      "Epoch 1336/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.7835 - val_loss: 637.5809\n",
      "Epoch 1337/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 575.1654 - val_loss: 636.5148\n",
      "Epoch 1338/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.0927 - val_loss: 635.7140\n",
      "Epoch 1339/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 575.7325 - val_loss: 640.2475\n",
      "Epoch 1340/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.1398 - val_loss: 635.3585\n",
      "Epoch 1341/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 579.4760 - val_loss: 635.9168\n",
      "Epoch 1342/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 577.0736 - val_loss: 641.1138\n",
      "Epoch 1343/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 581.2656 - val_loss: 633.6437\n",
      "Epoch 1344/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 574.9621 - val_loss: 637.9222\n",
      "Epoch 1345/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 574.7210 - val_loss: 641.7578\n",
      "Epoch 1346/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 575.5736 - val_loss: 639.7251\n",
      "Epoch 1347/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 91us/step - loss: 573.6586 - val_loss: 635.9930\n",
      "Epoch 1348/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 574.4532 - val_loss: 637.4809\n",
      "Epoch 1349/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 574.0348 - val_loss: 638.2071\n",
      "Epoch 1350/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 574.9090 - val_loss: 637.8196\n",
      "Epoch 1351/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.9846 - val_loss: 635.5276\n",
      "Epoch 1352/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 574.7897 - val_loss: 639.2365\n",
      "Epoch 1353/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 575.4618 - val_loss: 640.1149\n",
      "Epoch 1354/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 574.8037 - val_loss: 641.5194\n",
      "Epoch 1355/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 575.4118 - val_loss: 636.4830\n",
      "Epoch 1356/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.6342 - val_loss: 636.8339\n",
      "Epoch 1357/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.9525 - val_loss: 639.4090\n",
      "Epoch 1358/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 574.5161 - val_loss: 639.6327\n",
      "Epoch 1359/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 574.1412 - val_loss: 634.9559\n",
      "Epoch 1360/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 573.6665 - val_loss: 640.4550\n",
      "Epoch 1361/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 576.4513 - val_loss: 636.6649\n",
      "Epoch 1362/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 575.9118 - val_loss: 642.2393\n",
      "Epoch 1363/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 574.6521 - val_loss: 635.9654\n",
      "Epoch 1364/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 575.1742 - val_loss: 642.5449\n",
      "Epoch 1365/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.2181 - val_loss: 637.0212\n",
      "Epoch 1366/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 580.6801 - val_loss: 645.2827\n",
      "Epoch 1367/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 574.5790 - val_loss: 633.4505\n",
      "Epoch 1368/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.7841 - val_loss: 643.8184\n",
      "Epoch 1369/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 578.8324 - val_loss: 644.7763\n",
      "Epoch 1370/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 578.9012 - val_loss: 633.2552\n",
      "Epoch 1371/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 575.1753 - val_loss: 645.6073\n",
      "Epoch 1372/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 577.4936 - val_loss: 639.5382\n",
      "Epoch 1373/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 578.0409 - val_loss: 634.8549\n",
      "Epoch 1374/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 575.4828 - val_loss: 637.2259\n",
      "Epoch 1375/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.9912 - val_loss: 634.1613\n",
      "Epoch 1376/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 573.6863 - val_loss: 640.4406\n",
      "Epoch 1377/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 574.1433 - val_loss: 638.3055\n",
      "Epoch 1378/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 574.0250 - val_loss: 636.5355\n",
      "Epoch 1379/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 573.9749 - val_loss: 640.3488\n",
      "Epoch 1380/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 576.2460 - val_loss: 636.3404\n",
      "Epoch 1381/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 574.1527 - val_loss: 636.6212\n",
      "Epoch 1382/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 574.9301 - val_loss: 634.9477\n",
      "Epoch 1383/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 575.9173 - val_loss: 639.5512\n",
      "Epoch 1384/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 573.9473 - val_loss: 639.6995\n",
      "Epoch 1385/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 574.1846 - val_loss: 636.0772\n",
      "Epoch 1386/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 576.4602 - val_loss: 635.4851\n",
      "Epoch 1387/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 573.9512 - val_loss: 645.9251\n",
      "Epoch 1388/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.9705 - val_loss: 636.7838\n",
      "Epoch 1389/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 573.9114 - val_loss: 636.8074\n",
      "Epoch 1390/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 579.7447 - val_loss: 634.7403\n",
      "Epoch 1391/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.5413 - val_loss: 644.3865\n",
      "Epoch 1392/2000\n",
      "357/357 [==============================] - 0s 104us/step - loss: 574.7392 - val_loss: 635.3653\n",
      "Epoch 1393/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 573.9968 - val_loss: 642.2906\n",
      "Epoch 1394/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.7534 - val_loss: 635.7162\n",
      "Epoch 1395/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 574.7013 - val_loss: 635.6600\n",
      "Epoch 1396/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.0572 - val_loss: 640.4083\n",
      "Epoch 1397/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 578.5402 - val_loss: 642.1517\n",
      "Epoch 1398/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.7620 - val_loss: 636.0132\n",
      "Epoch 1399/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 574.6736 - val_loss: 635.1539\n",
      "Epoch 1400/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 574.8937 - val_loss: 638.6349\n",
      "Epoch 1401/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 573.9386 - val_loss: 639.2929\n",
      "Epoch 1402/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 579.2024 - val_loss: 642.0416\n",
      "Epoch 1403/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 572.2524 - val_loss: 632.8567\n",
      "Epoch 1404/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.0931 - val_loss: 640.5663\n",
      "Epoch 1405/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 574.1682 - val_loss: 639.0341\n",
      "Epoch 1406/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.0269 - val_loss: 642.1741\n",
      "Epoch 1407/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 574.8848 - val_loss: 638.6620\n",
      "Epoch 1408/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 573.3490 - val_loss: 640.5312\n",
      "Epoch 1409/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 573.5669 - val_loss: 636.8582\n",
      "Epoch 1410/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 574.1700 - val_loss: 636.2865\n",
      "Epoch 1411/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 574.0639 - val_loss: 635.5871\n",
      "Epoch 1412/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.9052 - val_loss: 642.5900\n",
      "Epoch 1413/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 574.4163 - val_loss: 636.1427\n",
      "Epoch 1414/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 577.4425 - val_loss: 633.9587\n",
      "Epoch 1415/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 573.8630 - val_loss: 636.8004\n",
      "Epoch 1416/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.3725 - val_loss: 636.2496\n",
      "Epoch 1417/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.0925 - val_loss: 638.1105\n",
      "Epoch 1418/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 574.1620 - val_loss: 638.0758\n",
      "Epoch 1419/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 573.7791 - val_loss: 636.1460\n",
      "Epoch 1420/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.9884 - val_loss: 636.6780\n",
      "Epoch 1421/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 90us/step - loss: 575.4225 - val_loss: 643.6955\n",
      "Epoch 1422/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.8915 - val_loss: 634.3520\n",
      "Epoch 1423/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 573.8859 - val_loss: 635.9783\n",
      "Epoch 1424/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.4955 - val_loss: 638.3729\n",
      "Epoch 1425/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 577.5184 - val_loss: 643.6866\n",
      "Epoch 1426/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.0126 - val_loss: 636.9631\n",
      "Epoch 1427/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.9577 - val_loss: 642.5756\n",
      "Epoch 1428/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 573.3195 - val_loss: 634.5264\n",
      "Epoch 1429/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 574.1035 - val_loss: 640.3588\n",
      "Epoch 1430/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 576.4459 - val_loss: 638.3078\n",
      "Epoch 1431/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 574.8032 - val_loss: 640.6582\n",
      "Epoch 1432/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 576.2283 - val_loss: 634.9695\n",
      "Epoch 1433/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 575.3329 - val_loss: 648.9839\n",
      "Epoch 1434/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.7316 - val_loss: 635.6021\n",
      "Epoch 1435/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 577.4582 - val_loss: 637.0506\n",
      "Epoch 1436/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 574.8278 - val_loss: 637.4487\n",
      "Epoch 1437/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.2781 - val_loss: 642.2447\n",
      "Epoch 1438/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.0659 - val_loss: 636.7599\n",
      "Epoch 1439/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.1368 - val_loss: 635.0846\n",
      "Epoch 1440/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 578.5857 - val_loss: 649.5105\n",
      "Epoch 1441/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 578.0070 - val_loss: 632.3830\n",
      "Epoch 1442/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 574.0981 - val_loss: 636.3785\n",
      "Epoch 1443/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.0983 - val_loss: 636.0711\n",
      "Epoch 1444/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 574.3113 - val_loss: 635.6220\n",
      "Epoch 1445/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.7187 - val_loss: 641.5959\n",
      "Epoch 1446/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.6881 - val_loss: 633.6762\n",
      "Epoch 1447/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 574.9211 - val_loss: 640.1194\n",
      "Epoch 1448/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 574.6414 - val_loss: 637.4271\n",
      "Epoch 1449/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.1453 - val_loss: 632.7289\n",
      "Epoch 1450/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.9705 - val_loss: 635.1506\n",
      "Epoch 1451/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 573.2436 - val_loss: 638.5149\n",
      "Epoch 1452/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.5363 - val_loss: 642.3990\n",
      "Epoch 1453/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 573.4407 - val_loss: 635.0938\n",
      "Epoch 1454/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.9986 - val_loss: 632.7365\n",
      "Epoch 1455/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 573.2929 - val_loss: 639.4069\n",
      "Epoch 1456/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.9591 - val_loss: 643.9092\n",
      "Epoch 1457/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 574.7589 - val_loss: 631.9333\n",
      "Epoch 1458/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 571.6353 - val_loss: 641.6311\n",
      "Epoch 1459/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 575.4137 - val_loss: 639.3347\n",
      "Epoch 1460/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.1220 - val_loss: 640.4706\n",
      "Epoch 1461/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 575.2276 - val_loss: 635.4203\n",
      "Epoch 1462/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 573.2391 - val_loss: 635.5754\n",
      "Epoch 1463/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 573.1165 - val_loss: 639.7728\n",
      "Epoch 1464/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.1011 - val_loss: 636.1892\n",
      "Epoch 1465/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 575.4689 - val_loss: 637.3306\n",
      "Epoch 1466/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 573.5601 - val_loss: 636.8775\n",
      "Epoch 1467/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.8266 - val_loss: 636.8461\n",
      "Epoch 1468/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 572.9123 - val_loss: 638.8341\n",
      "Epoch 1469/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 572.5337 - val_loss: 638.7700\n",
      "Epoch 1470/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 574.1818 - val_loss: 638.5303\n",
      "Epoch 1471/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 571.8357 - val_loss: 633.5104\n",
      "Epoch 1472/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 573.3618 - val_loss: 633.7778\n",
      "Epoch 1473/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 576.8866 - val_loss: 635.7098\n",
      "Epoch 1474/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 574.5781 - val_loss: 643.5790\n",
      "Epoch 1475/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 577.4346 - val_loss: 630.9335\n",
      "Epoch 1476/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 575.8264 - val_loss: 638.8675\n",
      "Epoch 1477/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 573.7159 - val_loss: 634.8420\n",
      "Epoch 1478/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 576.3601 - val_loss: 642.5962\n",
      "Epoch 1479/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.9107 - val_loss: 632.3723\n",
      "Epoch 1480/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 573.1791 - val_loss: 641.6361\n",
      "Epoch 1481/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 574.0252 - val_loss: 632.0785\n",
      "Epoch 1482/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.5007 - val_loss: 639.7704\n",
      "Epoch 1483/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 572.7412 - val_loss: 639.8146\n",
      "Epoch 1484/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 571.8439 - val_loss: 632.6119\n",
      "Epoch 1485/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 575.8758 - val_loss: 633.7437\n",
      "Epoch 1486/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.6843 - val_loss: 640.4414\n",
      "Epoch 1487/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 574.4949 - val_loss: 632.1121\n",
      "Epoch 1488/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 572.3540 - val_loss: 638.9378\n",
      "Epoch 1489/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 572.2476 - val_loss: 634.4982\n",
      "Epoch 1490/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.0523 - val_loss: 634.5012\n",
      "Epoch 1491/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 572.5957 - val_loss: 637.2992\n",
      "Epoch 1492/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 574.2670 - val_loss: 639.9302\n",
      "Epoch 1493/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 572.1300 - val_loss: 633.5703\n",
      "Epoch 1494/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.4277 - val_loss: 636.6612\n",
      "Epoch 1495/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 89us/step - loss: 573.6536 - val_loss: 636.2150\n",
      "Epoch 1496/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 576.5633 - val_loss: 640.0947\n",
      "Epoch 1497/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 571.6359 - val_loss: 634.5098\n",
      "Epoch 1498/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 575.0093 - val_loss: 636.3227\n",
      "Epoch 1499/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 572.4884 - val_loss: 638.2225\n",
      "Epoch 1500/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 578.9520 - val_loss: 644.4841\n",
      "Epoch 1501/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 573.3408 - val_loss: 634.9896\n",
      "Epoch 1502/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 575.1205 - val_loss: 639.6177\n",
      "Epoch 1503/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 573.5685 - val_loss: 636.4259\n",
      "Epoch 1504/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 573.2094 - val_loss: 642.9931\n",
      "Epoch 1505/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.3230 - val_loss: 634.9973\n",
      "Epoch 1506/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 572.8468 - val_loss: 637.2769\n",
      "Epoch 1507/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.4946 - val_loss: 644.7985\n",
      "Epoch 1508/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 578.3764 - val_loss: 635.6448\n",
      "Epoch 1509/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 573.3220 - val_loss: 638.3343\n",
      "Epoch 1510/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 571.5271 - val_loss: 632.8009\n",
      "Epoch 1511/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 576.7012 - val_loss: 636.0141\n",
      "Epoch 1512/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 572.8121 - val_loss: 645.4553\n",
      "Epoch 1513/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.5174 - val_loss: 633.9094\n",
      "Epoch 1514/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 572.6489 - val_loss: 636.9409\n",
      "Epoch 1515/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 577.1717 - val_loss: 638.0706\n",
      "Epoch 1516/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 573.5149 - val_loss: 633.1475\n",
      "Epoch 1517/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 579.7619 - val_loss: 641.1009\n",
      "Epoch 1518/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 583.4317 - val_loss: 630.2011\n",
      "Epoch 1519/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 574.4614 - val_loss: 639.9811\n",
      "Epoch 1520/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 572.3561 - val_loss: 634.5800\n",
      "Epoch 1521/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 573.1692 - val_loss: 640.2743\n",
      "Epoch 1522/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 574.6614 - val_loss: 644.2242\n",
      "Epoch 1523/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 573.6367 - val_loss: 633.7886\n",
      "Epoch 1524/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 574.3400 - val_loss: 637.0690\n",
      "Epoch 1525/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 572.7578 - val_loss: 640.2085\n",
      "Epoch 1526/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 572.9254 - val_loss: 635.5416\n",
      "Epoch 1527/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 573.2235 - val_loss: 640.5634\n",
      "Epoch 1528/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 578.9205 - val_loss: 632.7068\n",
      "Epoch 1529/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.3494 - val_loss: 642.4045\n",
      "Epoch 1530/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 576.1971 - val_loss: 640.3222\n",
      "Epoch 1531/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 575.1829 - val_loss: 634.2911\n",
      "Epoch 1532/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.7682 - val_loss: 640.8352\n",
      "Epoch 1533/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 579.0064 - val_loss: 639.7868\n",
      "Epoch 1534/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 573.1125 - val_loss: 629.7829\n",
      "Epoch 1535/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 574.4035 - val_loss: 645.8333\n",
      "Epoch 1536/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 579.4166 - val_loss: 640.0488\n",
      "Epoch 1537/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 586.5552 - val_loss: 629.2603\n",
      "Epoch 1538/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 582.9769 - val_loss: 649.7043\n",
      "Epoch 1539/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 572.7901 - val_loss: 632.1707\n",
      "Epoch 1540/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 582.6476 - val_loss: 634.8851\n",
      "Epoch 1541/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 574.4691 - val_loss: 644.6261\n",
      "Epoch 1542/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 575.3782 - val_loss: 634.2864\n",
      "Epoch 1543/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.4648 - val_loss: 642.1830\n",
      "Epoch 1544/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 572.0779 - val_loss: 636.4579\n",
      "Epoch 1545/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 574.7194 - val_loss: 643.6866\n",
      "Epoch 1546/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.5007 - val_loss: 632.0180\n",
      "Epoch 1547/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 575.3609 - val_loss: 639.8076\n",
      "Epoch 1548/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.0606 - val_loss: 639.4852\n",
      "Epoch 1549/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 575.2660 - val_loss: 637.6102\n",
      "Epoch 1550/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 572.7075 - val_loss: 642.2406\n",
      "Epoch 1551/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 571.2633 - val_loss: 633.3330\n",
      "Epoch 1552/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.1022 - val_loss: 642.1770\n",
      "Epoch 1553/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 572.3380 - val_loss: 634.0470\n",
      "Epoch 1554/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.3065 - val_loss: 641.3837\n",
      "Epoch 1555/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 571.5619 - val_loss: 635.3167\n",
      "Epoch 1556/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.4028 - val_loss: 637.0228\n",
      "Epoch 1557/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 576.9357 - val_loss: 632.0109\n",
      "Epoch 1558/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.5282 - val_loss: 646.9409\n",
      "Epoch 1559/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 575.0779 - val_loss: 635.9744\n",
      "Epoch 1560/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 579.8265 - val_loss: 629.8974\n",
      "Epoch 1561/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 574.3957 - val_loss: 639.6419\n",
      "Epoch 1562/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 572.1453 - val_loss: 637.6574\n",
      "Epoch 1563/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 576.3225 - val_loss: 634.3692\n",
      "Epoch 1564/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 571.1295 - val_loss: 640.3340\n",
      "Epoch 1565/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 576.6663 - val_loss: 632.9863\n",
      "Epoch 1566/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 572.3525 - val_loss: 642.1736\n",
      "Epoch 1567/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 574.1630 - val_loss: 632.4469\n",
      "Epoch 1568/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 572.3372 - val_loss: 638.0676\n",
      "Epoch 1569/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 95us/step - loss: 571.9011 - val_loss: 634.4495\n",
      "Epoch 1570/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 572.2986 - val_loss: 635.5574\n",
      "Epoch 1571/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 571.8054 - val_loss: 640.1116\n",
      "Epoch 1572/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 572.1906 - val_loss: 638.6957\n",
      "Epoch 1573/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 573.4476 - val_loss: 637.5232\n",
      "Epoch 1574/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 571.9669 - val_loss: 644.8699\n",
      "Epoch 1575/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 575.9500 - val_loss: 640.2321\n",
      "Epoch 1576/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 574.2282 - val_loss: 636.9768\n",
      "Epoch 1577/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 575.9336 - val_loss: 642.3184\n",
      "Epoch 1578/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 573.4524 - val_loss: 631.1966\n",
      "Epoch 1579/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.6163 - val_loss: 639.1543\n",
      "Epoch 1580/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.8450 - val_loss: 635.6649\n",
      "Epoch 1581/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 573.7569 - val_loss: 640.8540\n",
      "Epoch 1582/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 571.0543 - val_loss: 633.2199\n",
      "Epoch 1583/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 578.7229 - val_loss: 636.5980\n",
      "Epoch 1584/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.7660 - val_loss: 638.5967\n",
      "Epoch 1585/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 573.6596 - val_loss: 637.1293\n",
      "Epoch 1586/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 571.5208 - val_loss: 644.3731\n",
      "Epoch 1587/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 572.5203 - val_loss: 630.9423\n",
      "Epoch 1588/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 571.1189 - val_loss: 642.1161\n",
      "Epoch 1589/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 574.9455 - val_loss: 643.9091\n",
      "Epoch 1590/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 572.6667 - val_loss: 632.1858\n",
      "Epoch 1591/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 572.1404 - val_loss: 635.6221\n",
      "Epoch 1592/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 573.1013 - val_loss: 639.6082\n",
      "Epoch 1593/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 576.2249 - val_loss: 647.5232\n",
      "Epoch 1594/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.3540 - val_loss: 631.6021\n",
      "Epoch 1595/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 572.1637 - val_loss: 641.1878\n",
      "Epoch 1596/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 575.6418 - val_loss: 641.3796\n",
      "Epoch 1597/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 572.4729 - val_loss: 632.1979\n",
      "Epoch 1598/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.6521 - val_loss: 642.2049\n",
      "Epoch 1599/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 576.9597 - val_loss: 642.0724\n",
      "Epoch 1600/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 573.8177 - val_loss: 632.3898\n",
      "Epoch 1601/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 571.4942 - val_loss: 645.0144\n",
      "Epoch 1602/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.2334 - val_loss: 635.7392\n",
      "Epoch 1603/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 570.7544 - val_loss: 630.9631\n",
      "Epoch 1604/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 575.7440 - val_loss: 638.7893\n",
      "Epoch 1605/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 572.9191 - val_loss: 634.8554\n",
      "Epoch 1606/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 572.6869 - val_loss: 638.7636\n",
      "Epoch 1607/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 571.9451 - val_loss: 634.0449\n",
      "Epoch 1608/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 570.9197 - val_loss: 638.3731\n",
      "Epoch 1609/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 572.6209 - val_loss: 638.8254\n",
      "Epoch 1610/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.5203 - val_loss: 632.6301\n",
      "Epoch 1611/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 573.6543 - val_loss: 638.5746\n",
      "Epoch 1612/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 580.6610 - val_loss: 643.4658\n",
      "Epoch 1613/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 575.2401 - val_loss: 633.4698\n",
      "Epoch 1614/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 573.3328 - val_loss: 645.0023\n",
      "Epoch 1615/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 572.1395 - val_loss: 635.5175\n",
      "Epoch 1616/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 575.0850 - val_loss: 635.3153\n",
      "Epoch 1617/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 571.5425 - val_loss: 640.7667\n",
      "Epoch 1618/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 571.2597 - val_loss: 634.4173\n",
      "Epoch 1619/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 572.5486 - val_loss: 638.2908\n",
      "Epoch 1620/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 571.9778 - val_loss: 638.3959\n",
      "Epoch 1621/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 575.4315 - val_loss: 630.0525\n",
      "Epoch 1622/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 571.9402 - val_loss: 639.7441\n",
      "Epoch 1623/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 571.9174 - val_loss: 635.7442\n",
      "Epoch 1624/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 571.7200 - val_loss: 638.2449\n",
      "Epoch 1625/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 571.7954 - val_loss: 634.9460\n",
      "Epoch 1626/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 571.0415 - val_loss: 639.1781\n",
      "Epoch 1627/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 571.4237 - val_loss: 633.4719\n",
      "Epoch 1628/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 576.5674 - val_loss: 633.2398\n",
      "Epoch 1629/2000\n",
      "357/357 [==============================] - 0s 74us/step - loss: 570.2652 - val_loss: 641.2648\n",
      "Epoch 1630/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 572.9848 - val_loss: 642.0607\n",
      "Epoch 1631/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 573.5417 - val_loss: 639.6725\n",
      "Epoch 1632/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 571.3304 - val_loss: 635.5751\n",
      "Epoch 1633/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 571.4256 - val_loss: 641.2337\n",
      "Epoch 1634/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.6002 - val_loss: 635.1854\n",
      "Epoch 1635/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 573.1471 - val_loss: 644.6776\n",
      "Epoch 1636/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 570.9101 - val_loss: 633.3199\n",
      "Epoch 1637/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 572.3699 - val_loss: 638.3345\n",
      "Epoch 1638/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 571.4278 - val_loss: 641.6010\n",
      "Epoch 1639/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.3453 - val_loss: 640.2400\n",
      "Epoch 1640/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 572.2140 - val_loss: 633.9055\n",
      "Epoch 1641/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 570.9881 - val_loss: 636.5418\n",
      "Epoch 1642/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 570.5856 - val_loss: 640.0766\n",
      "Epoch 1643/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 93us/step - loss: 570.0401 - val_loss: 635.7089\n",
      "Epoch 1644/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 570.5614 - val_loss: 638.5686\n",
      "Epoch 1645/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 570.6824 - val_loss: 638.7190\n",
      "Epoch 1646/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 571.2347 - val_loss: 637.7498\n",
      "Epoch 1647/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 571.0015 - val_loss: 635.8375\n",
      "Epoch 1648/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 570.6678 - val_loss: 634.1486\n",
      "Epoch 1649/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 570.6984 - val_loss: 636.7113\n",
      "Epoch 1650/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 573.8713 - val_loss: 639.8472\n",
      "Epoch 1651/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 571.1987 - val_loss: 638.5008\n",
      "Epoch 1652/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.8346 - val_loss: 640.4093\n",
      "Epoch 1653/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.7239 - val_loss: 638.9279\n",
      "Epoch 1654/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.4907 - val_loss: 635.4375\n",
      "Epoch 1655/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 571.2076 - val_loss: 632.7239\n",
      "Epoch 1656/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 572.2855 - val_loss: 636.9048\n",
      "Epoch 1657/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 570.8822 - val_loss: 636.9823\n",
      "Epoch 1658/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 570.7887 - val_loss: 633.6401\n",
      "Epoch 1659/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 571.2679 - val_loss: 635.5925\n",
      "Epoch 1660/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 571.2683 - val_loss: 641.4432\n",
      "Epoch 1661/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 573.0903 - val_loss: 633.1116\n",
      "Epoch 1662/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 571.0113 - val_loss: 639.7932\n",
      "Epoch 1663/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 571.2156 - val_loss: 641.2075\n",
      "Epoch 1664/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 573.1599 - val_loss: 639.8177\n",
      "Epoch 1665/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 570.8511 - val_loss: 632.8560\n",
      "Epoch 1666/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.8624 - val_loss: 638.5737\n",
      "Epoch 1667/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.2811 - val_loss: 634.8756\n",
      "Epoch 1668/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 571.8650 - val_loss: 633.0811\n",
      "Epoch 1669/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 570.9115 - val_loss: 636.3102\n",
      "Epoch 1670/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.5774 - val_loss: 635.2072\n",
      "Epoch 1671/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.5583 - val_loss: 640.6846\n",
      "Epoch 1672/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.0198 - val_loss: 637.5659\n",
      "Epoch 1673/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.1157 - val_loss: 637.3964\n",
      "Epoch 1674/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.9206 - val_loss: 631.8482\n",
      "Epoch 1675/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 571.3982 - val_loss: 634.6420\n",
      "Epoch 1676/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 570.9530 - val_loss: 636.6038\n",
      "Epoch 1677/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 570.6341 - val_loss: 633.6463\n",
      "Epoch 1678/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 570.0847 - val_loss: 639.0915\n",
      "Epoch 1679/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.1540 - val_loss: 638.9883\n",
      "Epoch 1680/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 572.7628 - val_loss: 643.0788\n",
      "Epoch 1681/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 571.3033 - val_loss: 635.1609\n",
      "Epoch 1682/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.3468 - val_loss: 634.3319\n",
      "Epoch 1683/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 570.9106 - val_loss: 635.5353\n",
      "Epoch 1684/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 570.8227 - val_loss: 634.5392\n",
      "Epoch 1685/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.1263 - val_loss: 638.5352\n",
      "Epoch 1686/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 572.1226 - val_loss: 641.9944\n",
      "Epoch 1687/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 574.0526 - val_loss: 632.0781\n",
      "Epoch 1688/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.1862 - val_loss: 636.8974\n",
      "Epoch 1689/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 570.0324 - val_loss: 636.0398\n",
      "Epoch 1690/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.0109 - val_loss: 637.7096\n",
      "Epoch 1691/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 570.0286 - val_loss: 634.5302\n",
      "Epoch 1692/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 572.1801 - val_loss: 634.6334\n",
      "Epoch 1693/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.5814 - val_loss: 642.5838\n",
      "Epoch 1694/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 572.5091 - val_loss: 638.6768\n",
      "Epoch 1695/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.4693 - val_loss: 634.5170\n",
      "Epoch 1696/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 570.0731 - val_loss: 637.3976\n",
      "Epoch 1697/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 570.2757 - val_loss: 640.6881\n",
      "Epoch 1698/2000\n",
      "357/357 [==============================] - 0s 69us/step - loss: 571.7810 - val_loss: 641.6453\n",
      "Epoch 1699/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 571.4993 - val_loss: 637.9790\n",
      "Epoch 1700/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 571.3969 - val_loss: 633.7735\n",
      "Epoch 1701/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.4241 - val_loss: 636.8700\n",
      "Epoch 1702/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.3928 - val_loss: 640.1069\n",
      "Epoch 1703/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 571.5418 - val_loss: 639.1232\n",
      "Epoch 1704/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 571.1553 - val_loss: 638.5032\n",
      "Epoch 1705/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.3557 - val_loss: 636.3486\n",
      "Epoch 1706/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 575.3567 - val_loss: 631.1901\n",
      "Epoch 1707/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 574.3368 - val_loss: 645.7621\n",
      "Epoch 1708/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 572.2330 - val_loss: 637.1401\n",
      "Epoch 1709/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.5766 - val_loss: 630.7620\n",
      "Epoch 1710/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 571.6754 - val_loss: 636.7546\n",
      "Epoch 1711/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 572.0788 - val_loss: 640.1554\n",
      "Epoch 1712/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 571.3671 - val_loss: 631.9812\n",
      "Epoch 1713/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.5166 - val_loss: 639.6209\n",
      "Epoch 1714/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 571.1791 - val_loss: 642.1519\n",
      "Epoch 1715/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 571.0111 - val_loss: 637.4581\n",
      "Epoch 1716/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 571.2953 - val_loss: 634.9928\n",
      "Epoch 1717/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 97us/step - loss: 569.3023 - val_loss: 642.5011\n",
      "Epoch 1718/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 572.3690 - val_loss: 639.8509\n",
      "Epoch 1719/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 571.6720 - val_loss: 639.9421\n",
      "Epoch 1720/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.8844 - val_loss: 636.1964\n",
      "Epoch 1721/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 572.6506 - val_loss: 644.3215\n",
      "Epoch 1722/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 571.9256 - val_loss: 631.4262\n",
      "Epoch 1723/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 571.2046 - val_loss: 637.4232\n",
      "Epoch 1724/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 571.7675 - val_loss: 645.2869\n",
      "Epoch 1725/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 571.6461 - val_loss: 638.9418\n",
      "Epoch 1726/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 570.0099 - val_loss: 633.4667\n",
      "Epoch 1727/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.8219 - val_loss: 635.7259\n",
      "Epoch 1728/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 570.6989 - val_loss: 634.3216\n",
      "Epoch 1729/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 569.9331 - val_loss: 638.7759\n",
      "Epoch 1730/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 571.7040 - val_loss: 638.3402\n",
      "Epoch 1731/2000\n",
      "357/357 [==============================] - 0s 103us/step - loss: 571.5610 - val_loss: 639.6054\n",
      "Epoch 1732/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 570.5641 - val_loss: 638.9287\n",
      "Epoch 1733/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 570.0892 - val_loss: 633.9378\n",
      "Epoch 1734/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 569.9753 - val_loss: 642.4160\n",
      "Epoch 1735/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.4317 - val_loss: 637.7366\n",
      "Epoch 1736/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 570.0881 - val_loss: 635.2316\n",
      "Epoch 1737/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.5486 - val_loss: 639.6286\n",
      "Epoch 1738/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.6196 - val_loss: 634.0274\n",
      "Epoch 1739/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 570.2221 - val_loss: 633.8742\n",
      "Epoch 1740/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.2066 - val_loss: 634.5298\n",
      "Epoch 1741/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.9708 - val_loss: 637.9115\n",
      "Epoch 1742/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 570.8460 - val_loss: 641.2762\n",
      "Epoch 1743/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.1090 - val_loss: 641.9504\n",
      "Epoch 1744/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 571.4182 - val_loss: 641.4876\n",
      "Epoch 1745/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 571.5036 - val_loss: 638.5995\n",
      "Epoch 1746/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 570.1616 - val_loss: 636.5565\n",
      "Epoch 1747/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 570.3238 - val_loss: 637.7255\n",
      "Epoch 1748/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 570.0305 - val_loss: 636.5896\n",
      "Epoch 1749/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 573.0594 - val_loss: 641.9843\n",
      "Epoch 1750/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 570.6297 - val_loss: 637.2292\n",
      "Epoch 1751/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 570.0271 - val_loss: 635.9769\n",
      "Epoch 1752/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.5319 - val_loss: 638.3976\n",
      "Epoch 1753/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.1543 - val_loss: 635.1830\n",
      "Epoch 1754/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 570.2320 - val_loss: 634.2417\n",
      "Epoch 1755/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.8277 - val_loss: 638.1792\n",
      "Epoch 1756/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 570.8663 - val_loss: 640.3958\n",
      "Epoch 1757/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 571.1544 - val_loss: 637.2864\n",
      "Epoch 1758/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 570.0792 - val_loss: 638.3462\n",
      "Epoch 1759/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.4807 - val_loss: 634.9663\n",
      "Epoch 1760/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.8468 - val_loss: 635.0818\n",
      "Epoch 1761/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.6526 - val_loss: 637.6133\n",
      "Epoch 1762/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.4031 - val_loss: 634.4852\n",
      "Epoch 1763/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 570.0962 - val_loss: 633.4445\n",
      "Epoch 1764/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 570.0627 - val_loss: 638.7522\n",
      "Epoch 1765/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 570.0960 - val_loss: 637.8542\n",
      "Epoch 1766/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 570.3092 - val_loss: 639.8701\n",
      "Epoch 1767/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 569.8462 - val_loss: 636.5449\n",
      "Epoch 1768/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.7123 - val_loss: 635.8670\n",
      "Epoch 1769/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 570.2247 - val_loss: 635.0323\n",
      "Epoch 1770/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.1652 - val_loss: 640.5003\n",
      "Epoch 1771/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 570.1070 - val_loss: 638.0666\n",
      "Epoch 1772/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 570.1612 - val_loss: 635.8880\n",
      "Epoch 1773/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.8788 - val_loss: 638.1088\n",
      "Epoch 1774/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 570.0626 - val_loss: 637.0782\n",
      "Epoch 1775/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 569.6506 - val_loss: 636.2729\n",
      "Epoch 1776/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.6283 - val_loss: 636.3750\n",
      "Epoch 1777/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.7131 - val_loss: 638.3422\n",
      "Epoch 1778/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 570.0364 - val_loss: 640.9476\n",
      "Epoch 1779/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 571.2031 - val_loss: 641.1646\n",
      "Epoch 1780/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.8156 - val_loss: 636.9702\n",
      "Epoch 1781/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.5992 - val_loss: 633.9154\n",
      "Epoch 1782/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 570.4371 - val_loss: 632.8415\n",
      "Epoch 1783/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 570.6667 - val_loss: 635.9078\n",
      "Epoch 1784/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 569.6793 - val_loss: 636.1284\n",
      "Epoch 1785/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.8924 - val_loss: 637.0227\n",
      "Epoch 1786/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.6135 - val_loss: 637.1281\n",
      "Epoch 1787/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.9103 - val_loss: 638.9015\n",
      "Epoch 1788/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 570.2719 - val_loss: 637.1615\n",
      "Epoch 1789/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.7702 - val_loss: 636.5400\n",
      "Epoch 1790/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.5658 - val_loss: 633.6983\n",
      "Epoch 1791/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 87us/step - loss: 570.1122 - val_loss: 635.4917\n",
      "Epoch 1792/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.6218 - val_loss: 635.7191\n",
      "Epoch 1793/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.5114 - val_loss: 637.0846\n",
      "Epoch 1794/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.7271 - val_loss: 637.0788\n",
      "Epoch 1795/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.5281 - val_loss: 637.1866\n",
      "Epoch 1796/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.6836 - val_loss: 636.7373\n",
      "Epoch 1797/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.7447 - val_loss: 635.1720\n",
      "Epoch 1798/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.6999 - val_loss: 636.0755\n",
      "Epoch 1799/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.9691 - val_loss: 639.7743\n",
      "Epoch 1800/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 571.2323 - val_loss: 634.6421\n",
      "Epoch 1801/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.6873 - val_loss: 638.0541\n",
      "Epoch 1802/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 570.2647 - val_loss: 639.0355\n",
      "Epoch 1803/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 570.3864 - val_loss: 637.5918\n",
      "Epoch 1804/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.5644 - val_loss: 635.0939\n",
      "Epoch 1805/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.6072 - val_loss: 634.8109\n",
      "Epoch 1806/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.0117 - val_loss: 635.4072\n",
      "Epoch 1807/2000\n",
      "357/357 [==============================] - 0s 102us/step - loss: 569.5937 - val_loss: 635.2686\n",
      "Epoch 1808/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.5951 - val_loss: 636.4398\n",
      "Epoch 1809/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4650 - val_loss: 636.3476\n",
      "Epoch 1810/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.4603 - val_loss: 635.6230\n",
      "Epoch 1811/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.5223 - val_loss: 635.6930\n",
      "Epoch 1812/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.4138 - val_loss: 634.7834\n",
      "Epoch 1813/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 570.0889 - val_loss: 634.6351\n",
      "Epoch 1814/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.4703 - val_loss: 639.7488\n",
      "Epoch 1815/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 570.0195 - val_loss: 638.2851\n",
      "Epoch 1816/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4230 - val_loss: 636.0866\n",
      "Epoch 1817/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.6982 - val_loss: 636.7434\n",
      "Epoch 1818/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.9455 - val_loss: 634.6189\n",
      "Epoch 1819/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 571.1741 - val_loss: 639.9514\n",
      "Epoch 1820/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.3836 - val_loss: 636.6734\n",
      "Epoch 1821/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 570.1511 - val_loss: 637.6402\n",
      "Epoch 1822/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 571.8209 - val_loss: 632.0292\n",
      "Epoch 1823/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 571.3439 - val_loss: 632.8269\n",
      "Epoch 1824/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 570.3112 - val_loss: 634.5912\n",
      "Epoch 1825/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.3209 - val_loss: 638.5002\n",
      "Epoch 1826/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.7629 - val_loss: 638.2698\n",
      "Epoch 1827/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.8515 - val_loss: 636.5317\n",
      "Epoch 1828/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.4046 - val_loss: 636.7491\n",
      "Epoch 1829/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 569.2713 - val_loss: 634.9054\n",
      "Epoch 1830/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.7374 - val_loss: 634.2917\n",
      "Epoch 1831/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 570.0123 - val_loss: 633.6950\n",
      "Epoch 1832/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 571.0394 - val_loss: 638.8263\n",
      "Epoch 1833/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 569.9986 - val_loss: 639.2173\n",
      "Epoch 1834/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 570.0575 - val_loss: 634.1672\n",
      "Epoch 1835/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 569.4467 - val_loss: 637.1338\n",
      "Epoch 1836/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.4285 - val_loss: 638.4371\n",
      "Epoch 1837/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 570.3219 - val_loss: 638.8976\n",
      "Epoch 1838/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.6409 - val_loss: 637.8177\n",
      "Epoch 1839/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.4808 - val_loss: 635.4800\n",
      "Epoch 1840/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.9423 - val_loss: 634.0414\n",
      "Epoch 1841/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.5950 - val_loss: 635.7497\n",
      "Epoch 1842/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.4737 - val_loss: 636.8442\n",
      "Epoch 1843/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.3060 - val_loss: 635.2437\n",
      "Epoch 1844/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.3900 - val_loss: 636.5242\n",
      "Epoch 1845/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 569.6137 - val_loss: 637.3535\n",
      "Epoch 1846/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.3026 - val_loss: 634.8031\n",
      "Epoch 1847/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.2614 - val_loss: 636.0910\n",
      "Epoch 1848/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.3108 - val_loss: 636.3616\n",
      "Epoch 1849/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4269 - val_loss: 636.0946\n",
      "Epoch 1850/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.2905 - val_loss: 636.1737\n",
      "Epoch 1851/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.3643 - val_loss: 636.7145\n",
      "Epoch 1852/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2999 - val_loss: 636.5413\n",
      "Epoch 1853/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.3057 - val_loss: 636.3723\n",
      "Epoch 1854/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 569.4222 - val_loss: 636.2955\n",
      "Epoch 1855/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.4863 - val_loss: 635.3456\n",
      "Epoch 1856/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4205 - val_loss: 635.2786\n",
      "Epoch 1857/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.2664 - val_loss: 636.0785\n",
      "Epoch 1858/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.8054 - val_loss: 637.5953\n",
      "Epoch 1859/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.4276 - val_loss: 636.0488\n",
      "Epoch 1860/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.2557 - val_loss: 635.4485\n",
      "Epoch 1861/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 569.5650 - val_loss: 634.0099\n",
      "Epoch 1862/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.7590 - val_loss: 633.8275\n",
      "Epoch 1863/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 569.9199 - val_loss: 635.2459\n",
      "Epoch 1864/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.3107 - val_loss: 635.7301\n",
      "Epoch 1865/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 90us/step - loss: 569.4104 - val_loss: 636.8422\n",
      "Epoch 1866/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.2954 - val_loss: 636.5067\n",
      "Epoch 1867/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.4288 - val_loss: 636.5556\n",
      "Epoch 1868/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 569.4190 - val_loss: 637.8500\n",
      "Epoch 1869/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.6722 - val_loss: 638.2552\n",
      "Epoch 1870/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.7292 - val_loss: 635.1746\n",
      "Epoch 1871/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.6487 - val_loss: 636.0782\n",
      "Epoch 1872/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.7239 - val_loss: 634.8485\n",
      "Epoch 1873/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.3549 - val_loss: 635.5788\n",
      "Epoch 1874/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.2362 - val_loss: 636.9344\n",
      "Epoch 1875/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 570.5055 - val_loss: 639.6633\n",
      "Epoch 1876/2000\n",
      "357/357 [==============================] - 0s 101us/step - loss: 570.0623 - val_loss: 637.0837\n",
      "Epoch 1877/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.2389 - val_loss: 636.5324\n",
      "Epoch 1878/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2951 - val_loss: 636.7914\n",
      "Epoch 1879/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2946 - val_loss: 636.5258\n",
      "Epoch 1880/2000\n",
      "357/357 [==============================] - 0s 67us/step - loss: 569.3710 - val_loss: 637.9049\n",
      "Epoch 1881/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.7207 - val_loss: 638.5177\n",
      "Epoch 1882/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 569.5472 - val_loss: 636.2428\n",
      "Epoch 1883/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4132 - val_loss: 634.5012\n",
      "Epoch 1884/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.5167 - val_loss: 634.4956\n",
      "Epoch 1885/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.5071 - val_loss: 635.1792\n",
      "Epoch 1886/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.6163 - val_loss: 636.7112\n",
      "Epoch 1887/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.8121 - val_loss: 635.0655\n",
      "Epoch 1888/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.2105 - val_loss: 636.0210\n",
      "Epoch 1889/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.1167 - val_loss: 637.1057\n",
      "Epoch 1890/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.2691 - val_loss: 635.2976\n",
      "Epoch 1891/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.3127 - val_loss: 634.2760\n",
      "Epoch 1892/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.8827 - val_loss: 634.0876\n",
      "Epoch 1893/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.5332 - val_loss: 634.7423\n",
      "Epoch 1894/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 570.2444 - val_loss: 637.3174\n",
      "Epoch 1895/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.0844 - val_loss: 635.0044\n",
      "Epoch 1896/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.4526 - val_loss: 635.3209\n",
      "Epoch 1897/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.3745 - val_loss: 636.0504\n",
      "Epoch 1898/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.1214 - val_loss: 637.0022\n",
      "Epoch 1899/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.2412 - val_loss: 637.1631\n",
      "Epoch 1900/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.2829 - val_loss: 635.6267\n",
      "Epoch 1901/2000\n",
      "357/357 [==============================] - 0s 106us/step - loss: 569.2522 - val_loss: 636.6937\n",
      "Epoch 1902/2000\n",
      "357/357 [==============================] - 0s 71us/step - loss: 569.4264 - val_loss: 637.4508\n",
      "Epoch 1903/2000\n",
      "357/357 [==============================] - 0s 75us/step - loss: 569.6083 - val_loss: 637.6209\n",
      "Epoch 1904/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.1300 - val_loss: 635.4700\n",
      "Epoch 1905/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.3413 - val_loss: 635.4689\n",
      "Epoch 1906/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.3046 - val_loss: 635.1752\n",
      "Epoch 1907/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.5521 - val_loss: 636.1868\n",
      "Epoch 1908/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.6067 - val_loss: 635.2486\n",
      "Epoch 1909/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.7824 - val_loss: 636.9055\n",
      "Epoch 1910/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.4846 - val_loss: 635.4644\n",
      "Epoch 1911/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.2580 - val_loss: 637.0766\n",
      "Epoch 1912/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.4988 - val_loss: 636.1679\n",
      "Epoch 1913/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2896 - val_loss: 636.9129\n",
      "Epoch 1914/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.4586 - val_loss: 637.1910\n",
      "Epoch 1915/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.2795 - val_loss: 637.0564\n",
      "Epoch 1916/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 570.1263 - val_loss: 638.4267\n",
      "Epoch 1917/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.2594 - val_loss: 635.3723\n",
      "Epoch 1918/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 569.2787 - val_loss: 635.0643\n",
      "Epoch 1919/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 570.1278 - val_loss: 634.1872\n",
      "Epoch 1920/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.4452 - val_loss: 634.6448\n",
      "Epoch 1921/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.4123 - val_loss: 636.1987\n",
      "Epoch 1922/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.1934 - val_loss: 636.7901\n",
      "Epoch 1923/2000\n",
      "357/357 [==============================] - 0s 73us/step - loss: 569.2521 - val_loss: 636.5794\n",
      "Epoch 1924/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.2908 - val_loss: 637.3021\n",
      "Epoch 1925/2000\n",
      "357/357 [==============================] - 0s 86us/step - loss: 569.2436 - val_loss: 636.2652\n",
      "Epoch 1926/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.2341 - val_loss: 636.1077\n",
      "Epoch 1927/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.7359 - val_loss: 634.4238\n",
      "Epoch 1928/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.3380 - val_loss: 635.7855\n",
      "Epoch 1929/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.2097 - val_loss: 635.9250\n",
      "Epoch 1930/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.1629 - val_loss: 636.4787\n",
      "Epoch 1931/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.2025 - val_loss: 637.0552\n",
      "Epoch 1932/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.2667 - val_loss: 637.0675\n",
      "Epoch 1933/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.5489 - val_loss: 637.6982\n",
      "Epoch 1934/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.4210 - val_loss: 635.7364\n",
      "Epoch 1935/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.1900 - val_loss: 636.1076\n",
      "Epoch 1936/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.2042 - val_loss: 636.5563\n",
      "Epoch 1937/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.1521 - val_loss: 636.8781\n",
      "Epoch 1938/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2067 - val_loss: 636.0644\n",
      "Epoch 1939/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 0s 89us/step - loss: 569.2065 - val_loss: 636.5946\n",
      "Epoch 1940/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.0876 - val_loss: 636.1159\n",
      "Epoch 1941/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.1862 - val_loss: 636.1808\n",
      "Epoch 1942/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.0947 - val_loss: 635.6661\n",
      "Epoch 1943/2000\n",
      "357/357 [==============================] - 0s 76us/step - loss: 569.1064 - val_loss: 635.2757\n",
      "Epoch 1944/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.6180 - val_loss: 634.8760\n",
      "Epoch 1945/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.2597 - val_loss: 635.9476\n",
      "Epoch 1946/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.2739 - val_loss: 635.9345\n",
      "Epoch 1947/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 569.1968 - val_loss: 636.6721\n",
      "Epoch 1948/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.2084 - val_loss: 637.1848\n",
      "Epoch 1949/2000\n",
      "357/357 [==============================] - 0s 79us/step - loss: 569.2884 - val_loss: 636.1269\n",
      "Epoch 1950/2000\n",
      "357/357 [==============================] - 0s 80us/step - loss: 569.1093 - val_loss: 636.2262\n",
      "Epoch 1951/2000\n",
      "357/357 [==============================] - 0s 77us/step - loss: 569.0960 - val_loss: 636.2430\n",
      "Epoch 1952/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.0777 - val_loss: 635.8930\n",
      "Epoch 1953/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.1845 - val_loss: 635.2530\n",
      "Epoch 1954/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.3012 - val_loss: 636.3394\n",
      "Epoch 1955/2000\n",
      "357/357 [==============================] - 0s 89us/step - loss: 569.2859 - val_loss: 635.5350\n",
      "Epoch 1956/2000\n",
      "357/357 [==============================] - 0s 100us/step - loss: 569.0779 - val_loss: 636.3607\n",
      "Epoch 1957/2000\n",
      "357/357 [==============================] - 0s 97us/step - loss: 569.1006 - val_loss: 636.8966\n",
      "Epoch 1958/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.0684 - val_loss: 635.9479\n",
      "Epoch 1959/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.1160 - val_loss: 635.8375\n",
      "Epoch 1960/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.1597 - val_loss: 635.1611\n",
      "Epoch 1961/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 569.2204 - val_loss: 635.2831\n",
      "Epoch 1962/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.1605 - val_loss: 635.9610\n",
      "Epoch 1963/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.2587 - val_loss: 637.0014\n",
      "Epoch 1964/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.1959 - val_loss: 636.3440\n",
      "Epoch 1965/2000\n",
      "357/357 [==============================] - 0s 84us/step - loss: 569.1729 - val_loss: 636.7167\n",
      "Epoch 1966/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.1493 - val_loss: 636.8553\n",
      "Epoch 1967/2000\n",
      "357/357 [==============================] - 0s 95us/step - loss: 569.2208 - val_loss: 636.7266\n",
      "Epoch 1968/2000\n",
      "357/357 [==============================] - 0s 99us/step - loss: 569.0983 - val_loss: 636.3432\n",
      "Epoch 1969/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.1053 - val_loss: 636.5830\n",
      "Epoch 1970/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.0957 - val_loss: 636.8543\n",
      "Epoch 1971/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 569.0828 - val_loss: 636.4298\n",
      "Epoch 1972/2000\n",
      "357/357 [==============================] - 0s 72us/step - loss: 569.1054 - val_loss: 636.2684\n",
      "Epoch 1973/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 569.1273 - val_loss: 636.6312\n",
      "Epoch 1974/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.0971 - val_loss: 637.1144\n",
      "Epoch 1975/2000\n",
      "357/357 [==============================] - 0s 94us/step - loss: 569.3101 - val_loss: 635.6766\n",
      "Epoch 1976/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.0989 - val_loss: 635.9075\n",
      "Epoch 1977/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.0945 - val_loss: 636.4870\n",
      "Epoch 1978/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 569.0582 - val_loss: 636.3668\n",
      "Epoch 1979/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.1153 - val_loss: 636.6163\n",
      "Epoch 1980/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.0968 - val_loss: 636.8058\n",
      "Epoch 1981/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.0638 - val_loss: 636.3081\n",
      "Epoch 1982/2000\n",
      "357/357 [==============================] - 0s 91us/step - loss: 569.1649 - val_loss: 636.0662\n",
      "Epoch 1983/2000\n",
      "357/357 [==============================] - 0s 87us/step - loss: 569.1085 - val_loss: 636.8556\n",
      "Epoch 1984/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.1519 - val_loss: 636.8281\n",
      "Epoch 1985/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 569.3055 - val_loss: 637.4836\n",
      "Epoch 1986/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.2329 - val_loss: 637.2133\n",
      "Epoch 1987/2000\n",
      "357/357 [==============================] - 0s 90us/step - loss: 569.0804 - val_loss: 636.6114\n",
      "Epoch 1988/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 569.1122 - val_loss: 636.4757\n",
      "Epoch 1989/2000\n",
      "357/357 [==============================] - 0s 93us/step - loss: 569.0935 - val_loss: 636.7731\n",
      "Epoch 1990/2000\n",
      "357/357 [==============================] - 0s 98us/step - loss: 569.2909 - val_loss: 636.9456\n",
      "Epoch 1991/2000\n",
      "357/357 [==============================] - 0s 81us/step - loss: 568.8469 - val_loss: 635.6075\n",
      "Epoch 1992/2000\n",
      "357/357 [==============================] - 0s 78us/step - loss: 568.9687 - val_loss: 634.5776\n",
      "Epoch 1993/2000\n",
      "357/357 [==============================] - 0s 82us/step - loss: 569.2799 - val_loss: 634.3172\n",
      "Epoch 1994/2000\n",
      "357/357 [==============================] - 0s 83us/step - loss: 569.4912 - val_loss: 635.3581\n",
      "Epoch 1995/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.5360 - val_loss: 636.6815\n",
      "Epoch 1996/2000\n",
      "357/357 [==============================] - 0s 88us/step - loss: 569.0085 - val_loss: 636.0478\n",
      "Epoch 1997/2000\n",
      "357/357 [==============================] - 0s 96us/step - loss: 569.3093 - val_loss: 635.3630\n",
      "Epoch 1998/2000\n",
      "357/357 [==============================] - 0s 106us/step - loss: 569.0939 - val_loss: 636.1216\n",
      "Epoch 1999/2000\n",
      "357/357 [==============================] - 0s 85us/step - loss: 569.1581 - val_loss: 636.4851\n",
      "Epoch 2000/2000\n",
      "357/357 [==============================] - 0s 92us/step - loss: 569.1980 - val_loss: 635.4114\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_x, \n",
    "                 y=train_y, \n",
    "                 batch_size=32,\n",
    "                 validation_data=[test_x, test_y],\n",
    "#                  steps_per_epoch=100,\n",
    "#                  validation_steps=50,\n",
    "                 callbacks = [lh, reduce_lr],\n",
    "                 epochs=2000,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logs['loss'])\n",
    "plt.plot(logs['val_loss'])\n",
    "plt.ylim([0, 1e3])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_x = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_y, pred_test_x)\n",
    "plt.ylabel('prediction')\n",
    "plt.xlabel('ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
