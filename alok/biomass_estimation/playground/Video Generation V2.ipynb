{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime as dt\n",
    "import json, os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from multiprocessing import Pool\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "THUMBNAIL_WIDTH = 512\n",
    "PIXEL_COUNT_WIDTH = 4096\n",
    "PIXEL_COUNT_HEIGHT = 3000\n",
    "X_PADDING_FULLRES = 190\n",
    "Y_PADDING_FULLRES = 140\n",
    "X_PADDING = X_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_WIDTH)\n",
    "Y_PADDING = Y_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_HEIGHT)\n",
    "ROOT_DIR = '/root/data/s3'\n",
    "OUTPUT_BASE_DIR = 'imr_jan_27'\n",
    "INBOUND_BUCKET = 'aquabyte-frames-resized-inbound'\n",
    "VIDEO_BUCKET = 'aquabyte-images-adhoc'\n",
    "UPLOAD_TO_S3 = False\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "\n",
    "def _get_bucket_key(url):\n",
    "    parsed_url = urlparse(url, allow_fragments=False)\n",
    "    if parsed_url.netloc.startswith('s3'):\n",
    "        url_components = parsed_url.path.lstrip('/').split('/')\n",
    "        bucket, key = url_components[0], os.path.join(*url_components[1:])\n",
    "    else:\n",
    "        bucket = parsed_url.netloc.split('.')[0]\n",
    "        key = parsed_url.path.lstrip('/')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def process_s3_key_dir(s3_key_dir):\n",
    "    try:\n",
    "        left_f = s3_access_utils.download_from_s3(INBOUND_BUCKET, os.path.join(s3_key_dir,\n",
    "            'left_frame.resize_512_512.jpg'))\n",
    "        right_f = s3_access_utils.download_from_s3(INBOUND_BUCKET, os.path.join(s3_key_dir,\n",
    "            'right_frame.resize_512_512.jpg'))\n",
    "        crop_metadata_f = s3_access_utils.download_from_s3(INBOUND_BUCKET, os.path.join(s3_key_dir,\n",
    "            'crops.json'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "\n",
    "    # open images and metadata files\n",
    "    left_im = Image.open(left_f)\n",
    "    right_im = Image.open(right_f)\n",
    "    crop_metadata = json.load(open(crop_metadata_f))\n",
    "    try:\n",
    "        depth = crop_metadata['capture']['sensors'].get('aquabyte_depth_meters')\n",
    "    except:\n",
    "        depth = 'Depth not found'\n",
    "\n",
    "    # draw boxes on images\n",
    "    left_draw = ImageDraw.Draw(left_im)\n",
    "    right_draw = ImageDraw.Draw(right_im)\n",
    "    anns = crop_metadata['annotations']\n",
    "    if anns:\n",
    "        for ann in anns:\n",
    "            c1 = max(ann['bbox'][0] - X_PADDING, 0)\n",
    "            c2 = max(ann['bbox'][1] - Y_PADDING, 0)\n",
    "            c3 = min(ann['bbox'][0] + ann['bbox'][2] + X_PADDING, THUMBNAIL_WIDTH)\n",
    "            c4 = min(ann['bbox'][1] + ann['bbox'][3] + Y_PADDING, THUMBNAIL_WIDTH)\n",
    "            if ann['image_id'] == 1:\n",
    "                left_draw.rectangle([(c1, c2), (c3, c4)])\n",
    "            elif ann['image_id'] == 2:\n",
    "                right_draw.rectangle([(c1, c2), (c3, c4)])\n",
    "\n",
    "    # stitch images\n",
    "    result = Image.new('RGB', (2*THUMBNAIL_WIDTH, THUMBNAIL_WIDTH))\n",
    "    result.paste(im=left_im, box=(0, 0))\n",
    "    result.paste(im=right_im, box=(THUMBNAIL_WIDTH, 0))\n",
    "\n",
    "    # write timestamp on stitched image\n",
    "    result_draw = ImageDraw.Draw(result)\n",
    "    ts = [c for c in left_f.split('/') if c.startswith('at=')][0]\n",
    "    display_ts = 'UTC Time: {}'.format(ts.replace('at=', ''))\n",
    "    display_depth = 'Depth: {}m'.format(depth)\n",
    "    result_draw.text((0, 0), display_ts, (255, 255, 255))\n",
    "    result_draw.text((0, 10), display_depth, (255, 255, 255))\n",
    "\n",
    "    output_f = left_f.replace(ROOT_DIR, OUTPUT_BASE_DIR).replace('left_', 'stereo_')\n",
    "    if not os.path.exists(os.path.dirname(output_f)):\n",
    "        os.makedirs(os.path.dirname(output_f))\n",
    "    result.save(output_f)\n",
    "\n",
    "def generate_video(pen_id, date):\n",
    "    query = \"\"\"\n",
    "        SELECT captured_at, left_crop_url\n",
    "        FROM prod.crop_annotation ca\n",
    "        WHERE ca.pen_id={} AND ca.service_id = 2 AND ca.captured_at > '{}'\n",
    "        LIMIT 1;\n",
    "    \"\"\".format(pen_id, date)\n",
    "    print(query)\n",
    "    print('Extracting s3 keys...')\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "    print(df)\n",
    "    image_url = df.left_crop_url.iloc[0]\n",
    "    print(image_url)\n",
    "    bucket, key = _get_bucket_key(image_url)\n",
    "\n",
    "    s3_folder = os.path.join(key[:key.index('date')], 'date={}'.format(date))\n",
    "    print(s3_folder)\n",
    "    generator = s3_access_utils.get_matching_s3_keys(INBOUND_BUCKET, s3_folder, suffixes=['capture.json'])\n",
    "    keys = [key for key in generator]\n",
    "    s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))\n",
    "    print('S3 keys extraction complete!')\n",
    "\n",
    "    print('Generating frames...')\n",
    "    # generate frames\n",
    "    pool = Pool(20)\n",
    "    pool.map(process_s3_key_dir, s3_key_dirs)\n",
    "    print('Frame generation complete!')\n",
    "\n",
    "    print('Generating video...')\n",
    "    # generate video\n",
    "    image_fs = sorted(filter(lambda path: 'stereo' in path, glob.glob(os.path.join(OUTPUT_BASE_DIR, '**', '*.jpg'), recursive=True)))\n",
    "    print('\\n\\nIMAGE FILES')\n",
    "    print(image_fs)\n",
    "    im = cv2.imread(image_fs[0])\n",
    "    height, width, layers = im.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "    video_f = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}_date_{}_videofull.avi'.format(str(pen_id), date))\n",
    "    video = cv2.VideoWriter(video_f, fourcc, 4, (width, height), True)\n",
    "    for idx, image_f in enumerate(image_fs):\n",
    "        if idx % 1000 == 0:\n",
    "            print(idx)\n",
    "        im = cv2.imread(image_f, cv2.IMREAD_COLOR)\n",
    "        video.write(im)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    print('Video generation complete!')\n",
    "\n",
    "    if UPLOAD_TO_S3:\n",
    "        # upload to s3\n",
    "        video_key = os.path.join('videos', str(pen_id), os.path.basename(video_f))\n",
    "        s3_access_utils.s3_client.upload_file(video_f, VIDEO_BUCKET, video_key)\n",
    "\n",
    "\n",
    "# def main(pen_ids, days_back=60):\n",
    "#     date = dt.datetime.now() - dt.timedelta(days_back)\n",
    "#     date = date.strftime('%Y-%m-%d')\n",
    "def main(pen_ids, date):\n",
    "    print(f'Building video for days since {date} for {pen_ids}')\n",
    "\n",
    "    for pen_id in pen_ids:\n",
    "        print(f'Generating pen:{pen_id} video starting at {date}')\n",
    "        generate_video(pen_id, date)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main([61], '2020-01-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file(video_f, VIDEO_BUCKET, video_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_f = '/root/alok/repos/cv_research/alok/biomass_estimation/playground/imr_jan_27/pen_id_61_date_2020-01-27_videofull.avi'\n",
    "s3_access_utils.s3_client.upload_file(video_f, VIDEO_BUCKET, 'videos/61/pen_id_61_date_2020-01-27_videofull.avi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
