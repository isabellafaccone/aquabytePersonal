{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from filter_optimization.filter_optimization_task import generate_sampling_filters, extract_biomass_data, \\\n",
    "    NoDataException, SamplingFilter, generate_pm_base, PopulationMetricsBase, generate_filter_mask, get_dates_in_range, \\\n",
    "    find_optimal_filter, gen_pm_base\n",
    "from population_metrics.population_metrics_base import generate_pm_base, PopulationMetricsBase\n",
    "from population_metrics.confidence_metrics import compute_biomass_kpi, generate_distribution_consistency\n",
    "from population_metrics.raw_metrics import get_raw_sample_size\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "from population_metrics.population_metrics_base import PopulationMetricsBase, ValidationError\n",
    "from population_metrics.growth_rate import generate_regression_input, compute_growth_rate\n",
    "from population_metrics.raw_metrics import get_raw_weight_values, get_raw_sample_size\n",
    "from research.utils.datetime_utils import add_days, get_dates_in_range\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This module contains helper functions for computing daily level features representing confidence metrics.\n",
    "Namely, it covers trend stability, distribution consistency, and overall biomass KPI. Ask Alok for more \n",
    "context behind how these are mathematically formulated.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _not_none_mean(x):\n",
    "    return np.mean([i for i in x if i is not None])\n",
    "\n",
    "\n",
    "def compute_distribution_consistency(raw_weights: List[float], historical_weights: List[float]) -> float:\n",
    "    \"\"\"Computes distribution consistency using qq-plot approach for two arbitrary lists.\"\"\"\n",
    "    raw_weights = np.array(raw_weights)\n",
    "    historical_weights = np.array(historical_weights)\n",
    "    mean_adjustment = _not_none_mean(raw_weights) - _not_none_mean(historical_weights)\n",
    "\n",
    "    # compute qq-plot based metric\n",
    "    x = np.percentile(historical_weights + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(raw_weights, list(range(100)))\n",
    "    distribution_consistency = 1.0 - 10.0 * (np.mean(np.abs(y[1:99] - x[1:99]) ** 2) ** 0.5 / 10000.0)\n",
    "    return distribution_consistency\n",
    "\n",
    "\n",
    "def get_raw_and_historical_weights(pm_base: PopulationMetricsBase, date: str, window: int) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Gets list of raw weights for input date and historical weights of the provided window size\n",
    "    (not including input date).\n",
    "    \"\"\"\n",
    "\n",
    "    # get today's weights\n",
    "    raw_weights = []\n",
    "    three_days_ago = add_days(date, -3)\n",
    "    dates = get_dates_in_range(three_days_ago, date)\n",
    "    for curr_date in dates:\n",
    "        weights = get_raw_weight_values(pm_base, curr_date)\n",
    "        raw_weights.extend(weights)\n",
    "        \n",
    "    # get past weights\n",
    "    seven_days_ago, four_days_ago = add_days(date, -window), add_days(date, -4)\n",
    "    dates = get_dates_in_range(seven_days_ago, four_days_ago)\n",
    "    historical_weights = []\n",
    "    for curr_date in dates:\n",
    "        weights = get_raw_weight_values(pm_base, curr_date)\n",
    "        historical_weights.extend(weights)\n",
    "\n",
    "    if not raw_weights or not historical_weights:\n",
    "        raise ValidationError('Insufficient data to compute distribution consistency!')\n",
    "\n",
    "    return raw_weights, historical_weights\n",
    "\n",
    "\n",
    "def generate_distribution_consistency(pm_base: PopulationMetricsBase, date: str, window: int = 7) \\\n",
    "                                      -> Union[float, None]:\n",
    "    \"\"\"\n",
    "    Generates distribution consistency, which represents how consistent a given day's raw weight distribution\n",
    "    is to previous days' distribution not including latest date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_weights, historical_weights = get_raw_and_historical_weights(pm_base, date, window)\n",
    "    except ValidationError as err:\n",
    "        print(str(err))\n",
    "        return None\n",
    "    dc = compute_distribution_consistency(raw_weights, historical_weights)\n",
    "    return dc\n",
    "\n",
    "def compute_biomass_kpi(pm_base: PopulationMetricsBase, date: str) -> Union[float, None]:\n",
    "    \"\"\"\n",
    "    Computes biomass KPI for given PopulationMetricsBase instance and date.\n",
    "    \"\"\"\n",
    "    raw_sample_size = get_raw_sample_size(pm_base, date)\n",
    "    distribution_consistency = generate_distribution_consistency(pm_base, date)\n",
    "    if not raw_sample_size or not distribution_consistency:\n",
    "        return None\n",
    "    biomass_kpi = np.log(raw_sample_size * distribution_consistency**20) / np.log(500 * 0.9**20)\n",
    "    return biomass_kpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id = 125\n",
    "start_date = '2020-08-10'\n",
    "end_date = '2020-08-29'\n",
    "akpd_score_cutoff = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_biomass_data(pen_id, start_date, end_date, akpd_score_cutoff)\n",
    "start_hours = [0]\n",
    "end_hours = [24]\n",
    "kf_cutoffs = np.arange(1.0, 1.5, 0.005)\n",
    "sampling_filters = generate_sampling_filters(start_hours, end_hours, kf_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics_for_pm_base(pm_base: PopulationMetricsBase, dates: List[str]) -> float:\n",
    "    \"\"\"Generates mean biomass KPI given a PopulationMetricsBase instance and dates to consider.\"\"\"\n",
    "\n",
    "    kpis, sample_sizes = [], []\n",
    "    for date in dates:\n",
    "        sample_size = get_raw_sample_size(pm_base, date)\n",
    "        biomass_kpi = compute_biomass_kpi(pm_base, date)\n",
    "        sample_sizes.append(sample_size)\n",
    "        kpis.append(biomass_kpi)\n",
    "\n",
    "    # compute sample-size weighted kpi and final smart average\n",
    "    kpis = np.array([k if k else np.nan for k in kpis])\n",
    "    sample_sizes = np.array([s if s else np.nan for s in sample_sizes])\n",
    "    mean_kpi = np.nansum(kpis * sample_sizes) / np.nansum(sample_sizes)\n",
    "    return mean_kpi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_filter(df: pd.DataFrame, sampling_filters: List[SamplingFilter]) -> SamplingFilter:\n",
    "    \"\"\"Finds optimal filter given data-frame of raw biomass computations and different sampling filters. \"\"\"\n",
    "\n",
    "    analysis_data = defaultdict(list)\n",
    "    for sampling_filter in sampling_filters:\n",
    "        print('Start hour: {}, End hour: {}, KF cutoff: {}'.format(\n",
    "            sampling_filter.start_hour, sampling_filter.end_hour, sampling_filter.kf_cutoff\n",
    "        ))\n",
    "        pm_base = gen_pm_base(df, sampling_filter)\n",
    "\n",
    "        if pm_base:\n",
    "            unique_dates = sorted(df.date.unique().tolist())\n",
    "            dates = get_dates_in_range(unique_dates[0], unique_dates[-1])\n",
    "            mean_kpi = generate_metrics_for_pm_base(pm_base, dates)\n",
    "        else:\n",
    "            mean_kpi = None\n",
    "\n",
    "        # add to data\n",
    "        analysis_data['mean_kpi'].append(mean_kpi)\n",
    "        analysis_data['start_hour'].append(sampling_filter.start_hour)\n",
    "        analysis_data['end_hour'].append(sampling_filter.end_hour)\n",
    "        analysis_data['kf_cutoff'].append(sampling_filter.kf_cutoff)\n",
    "        analysis_data['akpd_score_cutoff'].append(sampling_filter.akpd_score_cutoff)\n",
    "\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    best_sampling_filter_params = analysis_df.sort_values('mean_kpi', ascending=False).iloc[0]\n",
    "\n",
    "    best_sampling_filter = SamplingFilter(\n",
    "        start_hour=float(best_sampling_filter_params.start_hour),\n",
    "        end_hour=float(best_sampling_filter_params.end_hour),\n",
    "        kf_cutoff=float(best_sampling_filter_params.kf_cutoff),\n",
    "        akpd_score_cutoff=float(best_sampling_filter_params.akpd_score_cutoff)\n",
    "    )\n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df, best_sampling_filter = find_optimal_filter(df, sampling_filters)\n",
    "analysis_df = find_optimal_filter(df, sampling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(analysis_df.kf_cutoff, analysis_df.mean_kpi)\n",
    "plt.xlabel('KF Cutoff')\n",
    "plt.ylabel('Sample size weighted KPI')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(analysis_df.kf_cutoff, analysis_df.mean_kpi)\n",
    "plt.xlabel('KF Cutoff')\n",
    "plt.ylabel('Sample size weighted KPI')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
