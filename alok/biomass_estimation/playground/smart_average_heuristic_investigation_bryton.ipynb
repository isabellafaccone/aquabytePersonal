{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "from report_generation.report_generator import generate_ts_data, SamplingFilter\n",
    "from research.utils.datetime_utils import add_days\n",
    "from report_generation.report_generator import gen_pm_base\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, ValidationError\n",
    "from filter_optimization.filter_optimization_task import _add_date_hour_columns\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_names = [\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13',\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17',\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24',\n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26',\n",
    "    'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02',\n",
    "    'silda_pen_id_86_2020-06-19_2020-07-19',\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30',\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06',\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/root/data/alok/biomass_estimation/playground'\n",
    "batch_name = 'test'\n",
    "dfs, gt_metadatas = {}, {}\n",
    "for cohort_name in cohort_names:\n",
    "    s3_dir = os.path.join(\n",
    "        'https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/alok/production_datasets',\n",
    "        cohort_name\n",
    "    )\n",
    "\n",
    "    ground_truth_metadata_url = os.path.join(s3_dir, 'ground_truth_metadata.json')\n",
    "    ground_truth_key_base = os.path.join(batch_name, cohort_name, 'ground_truth_metadata.json')\n",
    "    ground_truth_f = os.path.join(ROOT_DIR, ground_truth_key_base)\n",
    "    s3.download_from_url(ground_truth_metadata_url, custom_location=ground_truth_f)\n",
    "    gt_metadata = json.load(open(ground_truth_f))\n",
    "    gt_metadatas[cohort_name] = gt_metadata\n",
    "    \n",
    "    data_url = os.path.join(s3_dir, 'annotation_dataset.csv')\n",
    "    data_f, _, _= s3.download_from_url(data_url)\n",
    "    df = pd.read_csv(data_f)\n",
    "    df = _add_date_hour_columns(df)\n",
    "    dfs[cohort_name] = df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate average weight accuracy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding):\n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    date = add_days(last_feeding_date, days_post_feeding)\n",
    "    weights, _ = generate_smart_individual_values(pm_base, date, max_day_diff, True, apply_growth_rate, 0.9)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def generate_average_weight_accuracy(weights, gt_metadata, loss_factor):\n",
    "    avg_weight_prediction = np.mean(weights)\n",
    "    gutted_weight_prediction = avg_weight_prediction * (1.0 - loss_factor)\n",
    "    gt_weight = gt_metadata['gutted_average_weight']\n",
    "    avg_weight_err = (gutted_weight_prediction - gt_weight) / gt_weight\n",
    "    return avg_weight_err\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [6, 7, 8]\n",
    "end_hours = [13, 14, 15]\n",
    "apply_growth_rate_list = [False, True]\n",
    "max_day_diff_list = [1, 2, 3]\n",
    "days_post_feeding_list = [0, 1, 2, 3]\n",
    "max_final_days_post_feeding = 5\n",
    "loss_factors = [0.16, 0.17]\n",
    "\n",
    "cohort_name_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "apply_growth_rate_col = []\n",
    "max_day_diff_col = []\n",
    "days_post_feeding_col = []\n",
    "final_days_post_feeding_col = []\n",
    "loss_factor_col = []\n",
    "avg_weight_error_col = []\n",
    "\n",
    "for cohort_name in sorted(list(dfs.keys())):\n",
    "    print(cohort_name)\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "    for start_hour in start_hours:\n",
    "        for end_hour in end_hours:\n",
    "            for final_days_post_feeding in days_post_feeding_list:\n",
    "                sampling_filter = SamplingFilter(\n",
    "                    start_hour=start_hour,\n",
    "                    end_hour=end_hour,\n",
    "                    kf_cutoff=0.0,\n",
    "                    akpd_score_cutoff=0.95\n",
    "                )\n",
    "                df = dfs[cohort_name]\n",
    "                final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "                tdf = df[df.date <= final_date_post_feeding]\n",
    "                pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                for apply_growth_rate in apply_growth_rate_list:\n",
    "                    for max_day_diff in max_day_diff_list:\n",
    "                        for days_post_feeding in range(0, final_days_post_feeding + 1):\n",
    "                            for loss_factor in loss_factors:\n",
    "                                try:\n",
    "                                    weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "                                except ValidationError as err:\n",
    "                                    continue\n",
    "                                avg_weight_err = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                                \n",
    "                                cohort_name_col.append(cohort_name)\n",
    "                                start_hour_col.append(start_hour)\n",
    "                                end_hour_col.append(end_hour)\n",
    "                                apply_growth_rate_col.append(apply_growth_rate)\n",
    "                                max_day_diff_col.append(max_day_diff)\n",
    "                                days_post_feeding_col.append(days_post_feeding)\n",
    "                                final_days_post_feeding_col.append(final_days_post_feeding)\n",
    "                                loss_factor_col.append(loss_factor)\n",
    "                                avg_weight_error_col.append(avg_weight_err)\n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame({\n",
    "    'cohort_name': cohort_name_col,\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'apply_growth_rate': apply_growth_rate_col,\n",
    "    'max_day_diff': max_day_diff_col,\n",
    "    'days_post_feeding': days_post_feeding_col,\n",
    "    'final_days_post_feeding': final_days_post_feeding_col,\n",
    "    'loss_factor': loss_factor_col,\n",
    "    'avg_weight_error': avg_weight_error_col\n",
    "})\n",
    "\n",
    "tdf['avg_weight_error_abs'] = tdf.avg_weight_error.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name in cohort_names:\n",
    "    mask = tdf.cohort_name == cohort_name\n",
    "    print(tdf[mask].sort_values('avg_weight_error_abs', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['vikane_pen_id_60_2020-08-05_2020-08-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.cohort_name == 'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02') & (tdf.days_post_feeding == 0) & (tdf.final_days_post_feeding == 1)\n",
    "tdf[mask].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "apply_growth_rate_col = []\n",
    "max_day_diff_col = []\n",
    "days_post_feeding_col = []\n",
    "final_days_post_feeding_col = []\n",
    "loss_factor_col = []\n",
    "std_avg_weight_error_col = []\n",
    "abs_avg_weight_error_col = []\n",
    "mean_avg_weight_error_col = []\n",
    "\n",
    "for start_hour in start_hours:\n",
    "    for end_hour in end_hours:\n",
    "        for apply_growth_rate in apply_growth_rate_list:\n",
    "            for max_day_diff in max_day_diff_list:\n",
    "                for days_post_feeding in days_post_feeding_list:\n",
    "                    for final_days_post_feeding in days_post_feeding_list:\n",
    "                        for loss_factor in loss_factors:\n",
    "                            mask = (tdf.start_hour_col == start_hour) & \\\n",
    "                            (tdf.end_hour_col == end_hour) & \\\n",
    "                            (tdf.apply_growth_rate == apply_growth_rate) & \\\n",
    "                            (tdf.max_day_diff == max_day_diff) & \\\n",
    "                            (tdf.days_post_feeding == days_post_feeding) & \\\n",
    "                            (tdf.final_days_post_feeding == final_days_post_feeding) & \\\n",
    "                            (tdf.loss_factor == loss_factor)\n",
    "                            \n",
    "                            start_hour_col.append(start_hour)\n",
    "                            end_hour_col.append(end_hour)\n",
    "                            apply_growth_rate_col.append(apply_growth_rate)\n",
    "                            max_day_diff_col.append(max_day_diff)\n",
    "                            days_post_feeding_col.append(days_post_feeding)\n",
    "                            final_days_post_feeding_col.append(final_days_post_feeding)\n",
    "                            loss_factor_col.append(loss_factor)\n",
    "                            std_avg_weight_error_col.append(tdf[mask].avg_weight_error.std())\n",
    "                            abs_avg_weight_error_col.append(tdf[mask].avg_weight_error_abs.mean())\n",
    "                            mean_avg_weight_error_col.append(tdf[mask].avg_weight_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame({\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'apply_growth_rate': apply_growth_rate_col,\n",
    "    'max_day_diff': max_day_diff_col,\n",
    "    'days_post_feeding': days_post_feeding_col,\n",
    "    'final_days_post_feeding': final_days_post_feeding_col,\n",
    "    'loss_factor': loss_factor_col,\n",
    "    'abs_avg_weight_error': abs_avg_weight_error_col,\n",
    "    'std_avg_weight_error': std_avg_weight_error_col,\n",
    "    'mean_avg_weight_error': mean_avg_weight_error_col,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (rdf.loss_factor == 0.16)\n",
    "rdf[mask].sort_values('abs_avg_weight_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_names = [\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13',\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17',\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24',\n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26',\n",
    "    'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02',\n",
    "    'silda_pen_id_86_2020-06-19_2020-07-19',\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30',\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06',\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('/root/data/alok/biomass_estimation/playground/smart_average_param_grid_search_bryton.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'seglberget_pen_id_66_2020-05-13_2020-06-13')].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [6, 7, 8]\n",
    "end_hours = [13, 14, 15]\n",
    "apply_growth_rate_list = [False, True]\n",
    "max_day_diff_list = [1, 2, 3]\n",
    "days_post_feeding_list = [0, 1, 2, 3]\n",
    "max_final_days_post_feeding = 5\n",
    "loss_factors = [0.16, 0.17]\n",
    "\n",
    "for start_hour in start_hours:\n",
    "    avgs = []\n",
    "    avg_accuracies = []\n",
    "    \n",
    "    for cohort_name in cohort_names:\n",
    "    \n",
    "        a = tdf[((tdf.cohort_name == cohort_name) & (tdf.days_post_feeding == 1) & (tdf.max_day_diff == 3) & (tdf.apply_growth_rate == True))].sort_values('avg_weight_error_abs')\n",
    "\n",
    "        try:\n",
    "            vals = a.index[(a.start_hour_col == start_hour)]\n",
    "            ranks = [1 - a.index.get_loc(val) / len(a) for val in vals ]\n",
    "            accuracies = [ a.avg_weight_error_abs[val] for val in vals ]\n",
    "            avg = np.mean(ranks[1:50])\n",
    "            avg_accuracy = np.mean(accuracies[1:50])\n",
    "            \n",
    "            if np.isnan(avg):\n",
    "                continue\n",
    "                \n",
    "            avgs.append(avg)\n",
    "            avg_accuracies.append(avg_accuracy)\n",
    "            \n",
    "            print(max_day_diff, avg, avg_accuracy)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "    print('total', start_hour, np.mean(avgs), np.mean(avg_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'bolaks_pen_id_88_2020-02-10_2020-03-10')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'langoy_pen_id_108_2020-05-07_2020-05-17')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'tittelsnes_pen_id_37_2020-05-23_2020-06-24')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'aplavika_pen_id_95_2020-06-26_2020-07-26')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'silda_pen_id_86_2020-06-19_2020-07-19')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'vikane_pen_id_60_2020-08-05_2020-08-30')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'eldviktaren_pen_id_164_2020-09-06_2020-10-06')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'habranden_pen_id_100_2020-08-10_2020-08-31')].sort_values('avg_weight_error_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Vikane average weight and distribution error - explore basic parameters\n",
    "\n",
    "ground_truth_metadata = json.load(open(ground_truth_f))\n",
    "day_after_feeding_stop = add_days(ground_truth_metadata['last_feeding_date'], 1)\n",
    "start_date, end_date = add_days(day_after_feeding_stop, -2), add_days(day_after_feeding_stop, -1)\n",
    "tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=7,\n",
    "    end_hour=15,\n",
    "    akpd_score_cutoff=0.95,\n",
    "    kf_cutoff=0.0\n",
    ")\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "weights, _ = generate_smart_individual_values(pm_base, day_after_feeding_stop, 3, True, True, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "movikodden_gt = {'pen_id': 167,\n",
    "  'gutted_weight_distribution': {'0.0-1.0': 0.0,\n",
    "   '1.0-2.0': 1.53,\n",
    "   '2.0-3.0': 10.67,\n",
    "   '3.0-4.0': 19.87,\n",
    "   '4.0-5.0': 42.36,\n",
    "   '5.0-6.0': 22.28,\n",
    "   '6.0-7.0': 3.15,\n",
    "   '7.0-8.0': 0.13,\n",
    "   '8.0-9.0': 0.00,\n",
    "   '9.0-10.0': 0.00},\n",
    "  'gutted_avg_weight_distribution': {'0.0-1.0': 0.0,\n",
    "   '1.0-2.0': 1675.00,\n",
    "   '2.0-3.0': 2522.77,\n",
    "   '3.0-4.0': 3438.65,\n",
    "   '4.0-5.0': 4560.83,\n",
    "   '5.0-6.0': 5268.86,\n",
    "   '6.0-7.0': 6206.66,\n",
    "   '7.0-8.0': 7205.90,\n",
    "   '8.0-9.0': 0.00,\n",
    "   '9.0-10.0': 0.00},  \n",
    "  'gutted_unit_weight_distribution': {'0.0-1.0': 0.0,\n",
    "   '1.0-2.0': 0.60,\n",
    "   '2.0-3.0': 6.28,\n",
    "   '3.0-4.0': 15.93,\n",
    "   '4.0-5.0': 45.05,\n",
    "   '5.0-6.0': 27.37,\n",
    "   '6.0-7.0': 4.56,\n",
    "   '7.0-8.0': 0.23,\n",
    "   '8.0-9.0': 0.00,\n",
    "   '9.0-10.0': 0.00},              \n",
    "  'gutted_average_weight': 4289.31,\n",
    "  'expected_loss_factor': 18,\n",
    "  'last_feeding_date': '2020-10-23',\n",
    "  'undeployment_date': '2020-10-28',\n",
    "  'harvest_date': '2020-10-29',\n",
    "  'slaughter_date': '2020-10-30'}\n",
    "\n",
    "pen_id = movikodden_gt['pen_id']\n",
    "df_start_date = '2020-10-18'\n",
    "df_end_date = '2020-10-26'\n",
    "\n",
    "start_hour = 6\n",
    "end_hour = 15\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.95\n",
    ")\n",
    "\n",
    "df = extract_biomass_data(pen_id, df_start_date, df_end_date, 0.95)\n",
    "\n",
    "final_date_post_feeding = add_days(movikodden_gt['last_feeding_date'], 3)\n",
    "tdf = df[df.date <= final_date_post_feeding]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "days_post_feeding = 1\n",
    "final_days_post_feeding = 3\n",
    "\n",
    "loss_factor = 0.18\n",
    "\n",
    "try:\n",
    "    weights = generate_raw_individual_values(pm_base, movikodden_gt, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "except ValidationError as err:\n",
    "    pass\n",
    "avg_weight_err = generate_average_weight_accuracy(weights, movikodden_gt, loss_factor)\n",
    "\n",
    "print(avg_weight_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "\n",
    "buckets = [1000, 2000, 3000, 4000, 5000, 6000, 7000]\n",
    "\n",
    "avg_weight = movikodden_gt['gutted_average_weight']\n",
    "\n",
    "bucket_translation = {\n",
    "    1000: '1.0-2.0',\n",
    "    2000: '2.0-3.0',\n",
    "    3000: '3.0-4.0',\n",
    "    4000: '4.0-5.0',\n",
    "    5000: '5.0-6.0',\n",
    "    6000: '6.0-7.0',\n",
    "    7000: '7.0-8.0',\n",
    "    8000: '8.0-9.0',\n",
    "    9000: '9.0-10.0',\n",
    "}\n",
    "\n",
    "loss_factors = np.arange(0.10, 0.2, 0.001)\n",
    "avg_weight_accuracy = []\n",
    "dist_accuracy = []\n",
    "dist_accuracy_median = []\n",
    "dist_accuracy_new = []\n",
    "dist_accuracy_new_median = []\n",
    "\n",
    "bucket_accuracies = []\n",
    "percentile_accuracies = {}\n",
    "\n",
    "gt_pcts = []\n",
    "gt_pcts_new = []\n",
    "\n",
    "gt_unit_pcts = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    bucket_accuracies.append([])\n",
    "    \n",
    "    gt_pct = movikodden_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    gt_pct_new = new_buckets[buckets[i]]\n",
    "    \n",
    "    gt_unit_pct = movikodden_gt['gutted_unit_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    \n",
    "    gt_pcts.append(gt_pct)\n",
    "    gt_pcts_new.append(gt_pct_new)\n",
    "    \n",
    "    gt_unit_pcts.append(gt_unit_pct)\n",
    "\n",
    "gt_pcts = np.array(gt_pcts)\n",
    "gt_unit_pcts = np.array(gt_unit_pcts)\n",
    "    \n",
    "for i in range(len(buckets) - 1):\n",
    "    bucket_accuracies.append([])\n",
    "    \n",
    "for i in np.arange(50, 90, 10):\n",
    "    percentile_accuracies[i] = []\n",
    "    \n",
    "pct_diff = []\n",
    "pct_diff2 = []\n",
    "pct_unit_diff = []\n",
    "pct_unit_diff2 = []\n",
    "pct_plots = []\n",
    "    \n",
    "for loss_factor in loss_factors:\n",
    "    weights_adj = weights * (1.0 - loss_factor)\n",
    "    weights_adj.sort()\n",
    "    weights_sum = np.cumsum(weights_adj) / np.sum(weights_adj) * 100\n",
    "\n",
    "    dist_accuracies = []\n",
    "    dist_accuracies_new = []\n",
    "    \n",
    "    pcts = []\n",
    "    avg_weights = []\n",
    "    percentiles = []\n",
    "    percentiles2 = []\n",
    "    \n",
    "    for i in range(len(buckets) - 1):\n",
    "        mask1 = (weights_adj >= buckets[i]) & (weights_adj < buckets[i + 1])\n",
    "\n",
    "        gt_pct = gt_pcts[i]\n",
    "        gt_pct_new = gt_pcts_new[i]\n",
    "        \n",
    "        pct = sum(mask1) * 100 / len(mask1)\n",
    "        pcts.append(pct)\n",
    "        \n",
    "        avg_weights.append(np.mean(weights_adj[mask1]))\n",
    "        \n",
    "        dist_accuracies.append(np.abs(pct - gt_pct))\n",
    "        dist_accuracies_new.append(np.abs(pct - gt_pct_new))\n",
    "        \n",
    "        bucket_accuracies[i].append(np.abs(pct - gt_pct))\n",
    "        \n",
    "        equivalent_gt_percentile = np.percentile(weights_adj, np.sum(gt_pcts[0:i]))\n",
    "        percentiles.append(equivalent_gt_percentile)\n",
    "        \n",
    "        # try this....\n",
    "        equivalent_gt_unit_percentile = stats.percentileofscore(weights_sum, np.sum(gt_unit_pcts[0:i]))\n",
    "        equivalent_gt_unit_percentile_weight = np.percentile(weights_adj, equivalent_gt_unit_percentile)\n",
    "        percentiles2.append(equivalent_gt_unit_percentile_weight)\n",
    "        \n",
    "        \n",
    "    pcts = np.array(pcts)\n",
    "    avg_weights = np.array(avg_weights)\n",
    "    percentiles = np.array(percentiles)\n",
    "    percentiles2 = np.array(percentiles2)\n",
    "    unit_pcts = pcts * avg_weights / np.sum(pcts * avg_weights) * 100\n",
    "    \n",
    "#     model = sm.OLS(np.log(np.cumsum(pcts)), np.log(np.cumsum(gt_pcts)))\n",
    "#     results = model.fit()\n",
    "#     pct_diff.append(np.mean(np.abs(np.cumsum(pcts) - np.cumsum(gt_pcts))))\n",
    "# #     pct_diff.append(results.rsquared)\n",
    "#     model = sm.OLS(logit(np.cumsum(pcts / 101)), logit(np.cumsum(gt_pcts / 101)))\n",
    "#     results = model.fit()\n",
    "#     pct_diff2.append(np.mean(np.abs(logit(np.cumsum(pcts / 101)) - logit(np.cumsum(gt_pcts / 101)))))\n",
    "# #     pct_diff2.append(results.rsquared)\n",
    "#     pct_diff.append(np.median(np.abs(percentiles2 - buckets[:-1]) / buckets[:-1]))\n",
    "#     pct_diff2.append(np.mean(np.abs(percentiles2 - buckets[:-1]) / buckets[:-1]))\n",
    "    pct_plots.append(percentiles2)\n",
    "    \n",
    "    model = sm.OLS(percentiles - buckets[:-1], buckets[:-1])\n",
    "#     model = sm.OLS((percentiles - buckets[:-1]) / buckets[:-1], buckets[:-1])\n",
    "    results = model.fit()\n",
    "    pct_diff.append(results.rsquared)\n",
    "    \n",
    "    model = sm.OLS((percentiles2 - buckets[:-1]), buckets[:-1])\n",
    "    results = model.fit()\n",
    "    pct_unit_diff.append(results.rsquared)\n",
    "    \n",
    "    \n",
    "#     model = sm.OLS(np.cumsum(unit_pcts), np.cumsum(gt_unit_pcts))\n",
    "#     model = sm.OLS(logit(np.cumsum(unit_pcts / 101)), logit(np.cumsum(gt_unit_pcts / 101)))\n",
    "    X = buckets[:-1]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(percentiles2 - buckets[:-1], X)\n",
    "#     model = sm.OLS((percentiles2 - buckets[:-1]) / buckets[:-1], X)\n",
    "    results = model.fit()\n",
    "    pct_unit_diff2.append(results.rsquared)\n",
    "    \n",
    "    model = sm.OLS((percentiles - buckets[:-1]), X)\n",
    "    results = model.fit()\n",
    "    pct_diff2.append(results.rsquared)\n",
    "        \n",
    "    avg_weight_accuracy.append(np.abs(np.mean(weights_adj) - avg_weight) / avg_weight * 100)\n",
    "    dist_accuracy.append(np.mean(dist_accuracies))\n",
    "    dist_accuracy_median.append(np.percentile(dist_accuracies, 50))\n",
    "    dist_accuracy_new.append(np.mean(dist_accuracies_new))\n",
    "    dist_accuracy_new_median.append(np.percentile(dist_accuracies_new, 50))\n",
    "    \n",
    "    for i in np.arange(50, 90, 10):\n",
    "        percentile_accuracies[i].append(np.percentile(dist_accuracies, i))\n",
    "#         print('%i - %i: %0.2f, %0.2f vs %0.2f' % (buckets[i], buckets[i + 1], pct - gt_pct, pct, gt_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(buckets[:-1], percentiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(unit_pcts), np.cumsum(gt_unit_pcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_factor1 = 0.18\n",
    "loss_factor2 = 0.139\n",
    "\n",
    "weights_adj1 = weights * (1.0 - loss_factor1)\n",
    "weights_adj2 = weights * (1.0 - loss_factor2)\n",
    "\n",
    "x_buckets = np.array(buckets[1:])\n",
    "\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "gt_pcts = []\n",
    "\n",
    "mean_weights1 = []\n",
    "mean_weights2 = []\n",
    "gt_weights = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask1 = (weights_adj1 >= buckets[i]) & (weights_adj1 < buckets[i + 1])\n",
    "    mask2 = (weights_adj2 >= buckets[i]) & (weights_adj2 < buckets[i + 1])\n",
    "\n",
    "#     gt_pct = movikodden_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "#     gt_pct_new = new_buckets[buckets[i]]\n",
    "\n",
    "    if np.sum(mask1) > 0:\n",
    "        pct1 = sum(mask1) * 100 / len(mask1)\n",
    "        avg_weight1 = np.mean(weights_adj1[mask1])\n",
    "    else:\n",
    "        pct1 = 0\n",
    "        avg_weight1 = 0\n",
    "        \n",
    "    if np.sum(mask2) > 0:\n",
    "        pct2 = sum(mask2) * 100 / len(mask2)\n",
    "        avg_weight2 = np.mean(weights_adj2[mask2])\n",
    "    else:\n",
    "        pct2 = 0\n",
    "        avg_weight2 = 0\n",
    "\n",
    "    gt_pct = movikodden_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    gt_weight = movikodden_gt['gutted_avg_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    \n",
    "    mean_weights1.append(avg_weight1)\n",
    "    mean_weights2.append(avg_weight2)\n",
    "    gt_weights.append(gt_weight)\n",
    "\n",
    "    pcts1.append(pct1)\n",
    "    pcts2.append(pct2)\n",
    "    gt_pcts.append(gt_pct)\n",
    "\n",
    "pcts1 = np.array(pcts1)\n",
    "pcts2 = np.array(pcts2)\n",
    "mean_weights1 = np.array(mean_weights1)\n",
    "mean_weights2 = np.array(mean_weights2)\n",
    "gt_weights = np.array(gt_weights)\n",
    "gt_pcts = np.array(gt_pcts)\n",
    "\n",
    "adj_pcts1 = pcts1 * mean_weights1\n",
    "adj_pcts2 = pcts2 * mean_weights2\n",
    "\n",
    "adj_pcts1 = adj_pcts1 / np.sum(adj_pcts1) * 100\n",
    "adj_pcts2 = adj_pcts2 / np.sum(adj_pcts2) * 100\n",
    "    \n",
    "errors1 = pcts1 - gt_pcts\n",
    "errors2 = pcts2 - gt_pcts\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.bar(x_buckets - 200, mean_weights1, color = 'red', width = 200, label = '18%')\n",
    "# plt.bar(x_buckets + 200, mean_weights2, color = 'blue', width = 200, label = '15.5%')\n",
    "# plt.bar(x_buckets, gt_weights, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts1, color = 'red', width = 200, label = '%0.2f%%' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets + 200, pcts2, color = 'blue', width = 200, label = '%0.2f%%' % (loss_factor2 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts1, color = 'red', width = 200, label = '%0.2f%%' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts2, color = 'blue', width = 200, label ='%0.2f%%' % (loss_factor2 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets + 200, pcts1 - gt_pcts, color = 'red', width = 200, label = '%0.2f%% error' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets, pcts2 - gt_pcts, color = 'blue', width = 200, label = '%0.2f%% error' % (loss_factor2 * 100, ))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, loss_factor in enumerate(loss_factors):\n",
    "    plt.plot(buckets[:-1], pct_plots[i], marker = 'o')\n",
    "#     plt.plot(np.cumsum(gt_pcts), np.cumsum(pct_plots[i]), marker = 'o', label = loss_factor)\n",
    "\n",
    "# plt.plot(np.cumsum(gt_pcts), np.cumsum(gt_pcts), color = 'green', linewidth = 4)\n",
    "plt.plot(buckets[:-1], buckets[:-1], color = 'green', linewidth = 4)\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "plt.plot(logit(np.cumsum(gt_pcts / 100)), logit(np.cumsum(gt_pcts / 100)), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(gt_pcts / 100), np.cumsum(gt_pcts / 100), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "# ax.plot(loss_factors, dist_accuracy, color = 'red', linewidth = 4)\n",
    "# ax.plot(loss_factors, dist_accuracy_median, color = 'red', linestyle = 'dotted')\n",
    "ax.plot(loss_factors, pct_diff, color = 'red', linestyle = 'dotted')\n",
    "ax.plot(loss_factors, pct_diff2, color = 'red', linestyle = 'dotted')\n",
    "ax.plot(loss_factors, pct_unit_diff, color = 'blue', linewidth = 4)\n",
    "ax.plot(loss_factors, pct_unit_diff2, color = 'blue', linewidth = 4)\n",
    "# ax.plot(loss_factors, dist_accuracy_new, color = 'blue')\n",
    "# ax.plot(loss_factors, dist_accuracy_new_median, color = 'blue', linestyle = 'dotted')\n",
    "\n",
    "# for percentile, bucket in percentile_accuracies.items():\n",
    "#     ax.plot(loss_factors, bucket, label = percentile)\n",
    "\n",
    "ax2.plot(loss_factors, -1 * np.array(avg_weight_accuracy), color = 'green')\n",
    "ax.set_xlabel('Loss factor')\n",
    "ax.set_ylabel('Avg dist accuracy')\n",
    "ax.set_title('Avg dist accuracy vs loss factor')\n",
    "\n",
    "# ax.legend()\n",
    "\n",
    "# print(loss_factors[np.argmin(avg_weight_accuracy)])\n",
    "print(loss_factors[np.argmin(pct_diff)])\n",
    "print(loss_factors[np.argmin(pct_diff2)])\n",
    "print(loss_factors[np.argmin(pct_unit_diff)])\n",
    "print(loss_factors[np.argmin(pct_unit_diff2)])\n",
    "# print(loss_factors[np.argmax(pct_diff)])\n",
    "# print(loss_factors[np.argmax(pct_diff2)])\n",
    "# print(loss_factors[np.argmax(pct_unit_diff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tittelsnes_gt = {'pen_id': 37,\n",
    "#   'gutted_weight_distribution': {'0.0-1.0': 0.0,\n",
    "#    '1.0-2.0': 1.74,\n",
    "#    '2.0-3.0': 17.11,\n",
    "#    '3.0-4.0': 32.85,\n",
    "#    '4.0-5.0': 27.77,\n",
    "#    '5.0-6.0': 14.59,\n",
    "#    '6.0-7.0': 4.77,\n",
    "#    '7.0-8.0': 1.04,\n",
    "#    '8.0-9.0': 0.13,\n",
    "#    '9.0-10.0': 0.01},\n",
    "'gutted_weight_distribution': {'0.0-1.0': 0.0,\n",
    "   '1.0-2.0': 3.65,\n",
    "   '2.0-3.0': 21.14,\n",
    "   '3.0-4.0': 32.32,\n",
    "   '4.0-5.0': 25.38,\n",
    "   '5.0-6.0': 12.54,\n",
    "   '6.0-7.0': 3.99,\n",
    "   '7.0-8.0': 0.87,\n",
    "   '8.0-9.0': 0.10,\n",
    "   '9.0-10.0': 0.01},\n",
    "  'gutted_unit_weight_distribution': {'0.0-1.0': 0.0,\n",
    "   '1.0-2.0': 1.58,\n",
    "   '2.0-3.0': 14.16,\n",
    "   '3.0-4.0': 29.00,\n",
    "   '4.0-5.0': 29.25,\n",
    "   '5.0-6.0': 17.50,\n",
    "   '6.0-7.0': 6.59,\n",
    "   '7.0-8.0': 1.66,\n",
    "   '8.0-9.0': 0.23,\n",
    "   '9.0-10.0': 0.02},    \n",
    "  'gutted_average_weight': 3900,\n",
    "  'expected_loss_factor': 16.5,\n",
    "  'last_feeding_date': '2020-06-17',\n",
    "  'undeployment_date': '2020-06-22',\n",
    "  'harvest_date': '2020-06-23',\n",
    "  'slaughter_date': '2020-06-24'}\n",
    "\n",
    "start_hour = 7\n",
    "end_hour = 17\n",
    "cohort_name = 'tittelsnes_pen_id_37_2020-05-23_2020-06-24'\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.95\n",
    ")\n",
    "\n",
    "df = dfs[cohort_name]\n",
    "\n",
    "final_date_post_feeding = add_days(tittelsnes_gt['last_feeding_date'], 3)\n",
    "tdf = df[df.date <= final_date_post_feeding]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "days_post_feeding = 1\n",
    "final_days_post_feeding = 3\n",
    "\n",
    "loss_factor = 0.16\n",
    "\n",
    "try:\n",
    "    weights = generate_raw_individual_values(pm_base, tittelsnes_gt, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "except ValidationError as err:\n",
    "    pass\n",
    "avg_weight_err = generate_average_weight_accuracy(weights, tittelsnes_gt, loss_factor)\n",
    "\n",
    "print(avg_weight_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import logit\n",
    "\n",
    "buckets = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "\n",
    "avg_weight = tittelsnes_gt['gutted_average_weight']\n",
    "\n",
    "bucket_translation = {\n",
    "    1000: '1.0-2.0',\n",
    "    2000: '2.0-3.0',\n",
    "    3000: '3.0-4.0',\n",
    "    4000: '4.0-5.0',\n",
    "    5000: '5.0-6.0',\n",
    "    6000: '6.0-7.0',\n",
    "    7000: '7.0-8.0',\n",
    "    8000: '8.0-9.0',\n",
    "    9000: '9.0-10.0',\n",
    "}\n",
    "\n",
    "new_buckets = {\n",
    "    1000: 3.65,\n",
    "    2000: 21.14,\n",
    "    3000: 32.32,\n",
    "    4000: 25.38,\n",
    "    5000: 12.54,\n",
    "    6000: 3.99,\n",
    "    7000: 0.87,\n",
    "    8000: 0.10,\n",
    "    9000: 0.01\n",
    "}\n",
    "\n",
    "avg_weight_accuracy = []\n",
    "dist_accuracy = []\n",
    "dist_accuracy_median = []\n",
    "dist_accuracy_new = []\n",
    "dist_accuracy_new_median = []\n",
    "\n",
    "bucket_accuracies = []\n",
    "percentile_accuracies = []\n",
    "\n",
    "gt_pcts = []\n",
    "gt_pcts_new = []\n",
    "gt_unit_pcts = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    bucket_accuracies.append([])\n",
    "    \n",
    "    gt_pct = tittelsnes_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    gt_pct_new = new_buckets[buckets[i]]\n",
    "    \n",
    "    gt_unit_pct = tittelsnes_gt['gutted_unit_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    \n",
    "    gt_pcts.append(gt_pct)\n",
    "    gt_pcts_new.append(gt_pct_new)\n",
    "    \n",
    "    gt_unit_pcts.append(gt_unit_pct)\n",
    "\n",
    "gt_pcts = np.array(gt_pcts)\n",
    "gt_unit_pcts = np.array(gt_unit_pcts)\n",
    "    \n",
    "for i in np.arange(50, 90, 10):\n",
    "    percentile_accuracies.append([])\n",
    "    \n",
    "pct_diff = []\n",
    "pct_diff2 = []\n",
    "\n",
    "loss_factors = np.arange(0.12, 0.18, 0.001)\n",
    "# loss_factors = np.arange(0.14, 0.16, 0.001)\n",
    "pct_plots = []\n",
    "percentile_plots = []\n",
    "    \n",
    "for loss_factor in loss_factors:\n",
    "    weights_adj = weights * (1.0 - loss_factor)\n",
    "    weights_adj.sort()\n",
    "    weights_sum = np.cumsum(weights_adj) / np.sum(weights_adj) * 100\n",
    "\n",
    "    pcts = []\n",
    "    avg_weights = []\n",
    "    \n",
    "    dist_accuracies = []\n",
    "    dist_accuracies_new = []\n",
    "    \n",
    "    percentiles = []\n",
    "    percentiles2 = []\n",
    "    \n",
    "    for i in range(len(buckets) - 1):\n",
    "        mask1 = (weights_adj >= buckets[i]) & (weights_adj < buckets[i + 1])\n",
    "\n",
    "        gt_pct = gt_pcts[i]\n",
    "        gt_pct_new = gt_pcts_new[i]\n",
    "        \n",
    "        pct = sum(mask1) * 100 / len(mask1)\n",
    "                \n",
    "        pcts.append(pct)\n",
    "        \n",
    "        avg_weights.append(np.mean(weights_adj[mask1]))\n",
    "        \n",
    "        dist_accuracies.append(np.abs(pct - gt_pct))\n",
    "        dist_accuracies_new.append(np.abs(pct - gt_pct_new))\n",
    "        \n",
    "        bucket_accuracies[i].append(np.abs(pct - gt_pct))\n",
    "        \n",
    "        equivalent_gt_percentile = np.percentile(weights_adj, np.sum(gt_pcts[0:i]))\n",
    "        percentiles.append(equivalent_gt_percentile)\n",
    "        \n",
    "        # try this....\n",
    "        equivalent_gt_unit_percentile = stats.percentileofscore(weights_sum, np.sum(gt_unit_pcts[0:i]))\n",
    "        equivalent_gt_unit_percentile_weight = np.percentile(weights_adj, equivalent_gt_unit_percentile)\n",
    "        percentiles2.append(equivalent_gt_unit_percentile_weight)\n",
    "        \n",
    "    percentiles = np.array(percentiles)\n",
    "    percentiles2 = np.array(percentiles2)\n",
    "    pcts = np.array(pcts)\n",
    "    avg_weights = np.array(avg_weights)\n",
    "    \n",
    "    unit_pcts = pcts * avg_weights / np.sum(pcts * avg_weights) * 100\n",
    "    \n",
    "#     model = sm.OLS((percentiles - buckets[:-1]) / buckets[:-1], buckets[:-1])\n",
    "    model = sm.OLS((percentiles2) - (buckets[:-1]), (buckets[:-1]))\n",
    "    results = model.fit()\n",
    "    pct_diff.append(results.rsquared)\n",
    "    \n",
    "    X = (buckets[:-1])\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS((percentiles2) - (buckets[:-1]), X)\n",
    "#     model = sm.OLS((percentiles - buckets[:-1]) / buckets[:-1], X)\n",
    "    results = model.fit()\n",
    "    pct_diff2.append(results.rsquared)\n",
    "#     pct_diff.append(np.mean(np.abs(percentiles2 - buckets[:-1]) / buckets[:-1]))\n",
    "#     model = sm.OLS(np.cumsum(gt_pcts), np.cumsum(pcts))\n",
    "#     model = sm.OLS(logit(np.cumsum(gt_unit_pcts / 101)), logit(np.cumsum(unit_pcts / 101)))\n",
    "# #     model = sm.OLS(percentiles, buckets[:-1])\n",
    "#     results = model.fit()\n",
    "#     pct_diff.append(results.rsquared)\n",
    "#     pct_diff2.append(np.mean(np.abs(np.cumsum(gt_pcts) - np.cumsum(pcts))))\n",
    "#     pct_diff.append(results.params[0])\n",
    "#     pct_diff.append(np.mean(np.abs(percentiles - buckets[:-1]) / buckets[:-1]))\n",
    "    \n",
    "#     if np.abs(loss_factor - 0.14) < .001:\n",
    "#         pcts14 = percentiles\n",
    "#     if np.abs(loss_factor - 0.16) < .001:\n",
    "#         pcts155 = percentiles\n",
    "#     elif np.abs(loss_factor - 0.18) < .001:\n",
    "#         pcts18 = percentiles\n",
    "    pct_plots.append(pcts)\n",
    "    percentile_plots.append(percentiles)    \n",
    "    \n",
    "    avg_weight_accuracy.append(np.abs(np.mean(weights_adj) - avg_weight) / avg_weight * 100)\n",
    "    dist_accuracy.append(np.mean(dist_accuracies))\n",
    "    dist_accuracy_median.append(np.percentile(dist_accuracies, 90))\n",
    "    dist_accuracy_new.append(np.mean(dist_accuracies_new))\n",
    "    dist_accuracy_new_median.append(np.percentile(dist_accuracies_new, 90))\n",
    "    \n",
    "    for index, i in enumerate(np.arange(50, 90, 10)):\n",
    "        percentile_accuracies[index].append(np.percentile(dist_accuracies_new, i))\n",
    "#         print('%i - %i: %0.2f, %0.2f vs %0.2f' % (buckets[i], buckets[i + 1], pct - gt_pct, pct, gt_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(percentiles)\n",
    "print(percentiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_factor1 = 0.165\n",
    "loss_factor2 = 0.13\n",
    "\n",
    "weights_adj1 = weights * (1.0 - loss_factor1)\n",
    "weights_adj2 = weights * (1.0 - loss_factor2)\n",
    "\n",
    "x_buckets = np.array(buckets[1:])\n",
    "\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "gt_pcts = []\n",
    "\n",
    "mean_weights1 = []\n",
    "mean_weights2 = []\n",
    "gt_weights = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask1 = (weights_adj1 >= buckets[i]) & (weights_adj1 < buckets[i + 1])\n",
    "    mask2 = (weights_adj2 >= buckets[i]) & (weights_adj2 < buckets[i + 1])\n",
    "\n",
    "#     gt_pct = movikodden_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "#     gt_pct_new = new_buckets[buckets[i]]\n",
    "\n",
    "    if np.sum(mask1) > 0:\n",
    "        pct1 = sum(mask1) * 100 / len(mask1)\n",
    "        avg_weight1 = np.mean(weights_adj1[mask1])\n",
    "    else:\n",
    "        pct1 = 0\n",
    "        avg_weight1 = 0\n",
    "        \n",
    "    if np.sum(mask2) > 0:\n",
    "        pct2 = sum(mask2) * 100 / len(mask2)\n",
    "        avg_weight2 = np.mean(weights_adj2[mask2])\n",
    "    else:\n",
    "        pct2 = 0\n",
    "        avg_weight2 = 0\n",
    "\n",
    "    gt_pct = tittelsnes_gt['gutted_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "#     gt_weight = tittelsnes_gt['gutted_avg_weight_distribution'][bucket_translation[buckets[i]]]\n",
    "    \n",
    "#     mean_weights1.append(avg_weight1)\n",
    "#     mean_weights2.append(avg_weight2)\n",
    "#     gt_weights.append(gt_weight)\n",
    "\n",
    "    pcts1.append(pct1)\n",
    "    pcts2.append(pct2)\n",
    "    gt_pcts.append(gt_pct)\n",
    "\n",
    "pcts1 = np.array(pcts1)\n",
    "pcts2 = np.array(pcts2)\n",
    "# mean_weights1 = np.array(mean_weights1)\n",
    "# mean_weights2 = np.array(mean_weights2)\n",
    "# gt_weights = np.array(gt_weights)\n",
    "gt_pcts = np.array(gt_pcts)\n",
    "\n",
    "# adj_pcts1 = pcts1 * mean_weights1\n",
    "# adj_pcts2 = pcts2 * mean_weights2\n",
    "\n",
    "# adj_pcts1 = adj_pcts1 / np.sum(adj_pcts1) * 100\n",
    "# adj_pcts2 = adj_pcts2 / np.sum(adj_pcts2) * 100\n",
    "    \n",
    "errors1 = pcts1 - gt_pcts\n",
    "errors2 = pcts2 - gt_pcts\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.bar(x_buckets - 200, mean_weights1, color = 'red', width = 200, label = '18%')\n",
    "# plt.bar(x_buckets + 200, mean_weights2, color = 'blue', width = 200, label = '15.5%')\n",
    "# plt.bar(x_buckets, gt_weights, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts1, color = 'red', width = 200, label = '%0.2f%%' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets + 200, pcts2, color = 'blue', width = 200, label = '%0.2f%%' % (loss_factor2 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts1, color = 'red', width = 200, label = '%0.2f%%' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets - 200, pcts2, color = 'blue', width = 200, label ='%0.2f%%' % (loss_factor2 * 100, ))\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x_buckets + 200, pcts1 - gt_pcts, color = 'red', width = 200, label = '%0.2f%% error' % (loss_factor1 * 100, ))\n",
    "plt.bar(x_buckets, pcts2 - gt_pcts, color = 'blue', width = 200, label = '%0.2f%% error' % (loss_factor2 * 100, ))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(logit(np.cumsum(gt_pcts / 101)), logit(np.cumsum(gt_pcts / 101)), 'o')\n",
    "plt.plot(logit(np.cumsum(pcts / 101)), logit(np.cumsum(pcts / 101)), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plt.plot(buckets[:-1], pcts14, color = 'red', marker = 'o', label = '16.5%')\n",
    "# plt.plot(buckets[:-1], pcts18, color = 'red', marker = 'o', label = '16.5%')\n",
    "# plt.plot(buckets[:-1], pcts155, color = 'blue', marker = 'o', label = '15.5%')\n",
    "for i, loss_factor in enumerate(loss_factors):\n",
    "#     plt.plot(np.cumsum(gt_pcts), np.cumsum(pct_plots[i]), marker = 'o', label = loss_factor)\n",
    "    plt.plot(buckets[:-1], percentile_plots[i], marker = 'o', label = loss_factor)\n",
    "    \n",
    "plt.plot(buckets[:-1], buckets[:-1], color = 'green', linewidth = 4)\n",
    "\n",
    "for i, loss_factor in enumerate(loss_factors):\n",
    "    model = sm.OLS(percentile_plots[i], buckets[:-1])\n",
    "    results = model.fit()\n",
    "    print('%0 3f, %0.6f, %0.6f, %0.2f' % (loss_factor, results.rsquared, np.median(np.abs(percentile_plots[i] - buckets[:-1]) / buckets[:-1]), np.median(np.abs(percentile_plots[i] - buckets[:-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plt.plot(buckets[:-1], pcts14, color = 'red', marker = 'o', label = '16.5%')\n",
    "# plt.plot(buckets[:-1], pcts18, color = 'red', marker = 'o', label = '16.5%')\n",
    "# plt.plot(buckets[:-1], pcts155, color = 'blue', marker = 'o', label = '15.5%')\n",
    "for i, loss_factor in enumerate(loss_factors):\n",
    "    plt.plot(logit(np.cumsum(gt_pcts / 101)), logit(np.cumsum(pct_plots[i] / 101)), marker = 'o', label = loss_factor)\n",
    "#     plt.plot(buckets[:-1], percentile_plots[i], marker = 'o', label = loss_factor)\n",
    "\n",
    "plt.plot(logit(np.cumsum(gt_pcts / 101)), logit(np.cumsum(gt_pcts / 101)), color = 'green', linewidth = 4)\n",
    "# plt.plot(buckets[:-1], buckets[:-1], color = 'green', linewidth = 4)\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# plt.plot(buckets[:-1], pcts155 - buckets[:-1])\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "for i, loss_factor in enumerate(loss_factors):\n",
    "#     model = sm.OLS(np.cumsum(pct_plots[i]), np.cumsum(gt_pcts))\n",
    "    model = sm.OLS(logit(np.cumsum(pct_plots[i] / 101)), logit(np.cumsum(gt_pcts / 101)))\n",
    "    results = model.fit()\n",
    "    print('%0.3f, %0.2f, %0.6f, %0.2f' % (loss_factor, results.rsquared, np.mean(np.abs(logit(np.cumsum(pct_plots[i] / 101)) - logit(np.cumsum(gt_pcts / 101)))), np.mean(np.abs(np.cumsum(pct_plots[i]) - np.cumsum(gt_pcts)))))\n",
    "# model = sm.OLS(pcts18, buckets[:-1])\n",
    "# results = model.fit()\n",
    "# print(results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(np.array(x_buckets) + 200, pct_plots[4], color = 'blue', width = 200, label = '15.5%')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 200, label = 'Ground truth')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pct_plots[4]), len(x_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(loss_factors, (pct_diff), color = 'red', linestyle = 'dotted')\n",
    "ax.plot(loss_factors, (pct_diff2), color = 'red')\n",
    "\n",
    "ax.set_xlabel('Loss factor')\n",
    "ax.set_ylabel('Avg dist accuracy')\n",
    "ax.set_title('Avg dist accuracy vs loss factor')\n",
    "\n",
    "# ax2.plot(loss_factors, -1 * np.array(avg_weight_accuracy), color = 'green')\n",
    "\n",
    "# ax.axhline(0)\n",
    "\n",
    "# x_buckets = buckets[:-1]\n",
    "# pcts18 = np.array(pcts18)\n",
    "# pcts155 = np.array(pcts155)\n",
    "\n",
    "# import scipy.stats as stats\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(buckets[:-1], pcts18)\n",
    "# print(slope, np.mean(np.abs(pcts18 - x_buckets)))\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(buckets[:-1], pcts155)\n",
    "# print(slope, np.mean(np.abs(pcts155 - x_buckets)))\n",
    "\n",
    "print(loss_factors[np.argmax(pct_diff)])\n",
    "print(loss_factors[np.argmax(pct_diff2)])\n",
    "print(loss_factors[np.argmin(pct_diff)])\n",
    "print(loss_factors[np.argmin(pct_diff2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(loss_factors, dist_accuracy, color = 'red', linewidth = 4)\n",
    "# ax2.plot(loss_factors, dist_accuracy_median, color = 'red', linestyle = 'dotted')\n",
    "# ax.plot(loss_factors, dist_accuracy_new, color = 'blue')\n",
    "# ax.plot(loss_factors, dist_accuracy_new_median, color = 'blue', linestyle = 'dotted')\n",
    "\n",
    "for bucket in percentile_accuracies:\n",
    "    ax.plot(loss_factors, bucket)\n",
    "\n",
    "# ax2.plot(loss_factors, avg_weight_accuracy, color = 'green')\n",
    "ax.set_xlabel('Loss factor')\n",
    "ax.set_ylabel('Avg dist accuracy')\n",
    "ax.set_title('Avg dist accuracy vs loss factor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(loss_factors, dist_accuracy, color = 'red', linewidth = 4)\n",
    "# ax2.plot(loss_factors, dist_accuracy_median, color = 'red', linestyle = 'dotted')\n",
    "# ax.plot(loss_factors, dist_accuracy_new, color = 'blue')\n",
    "# ax.plot(loss_factors, dist_accuracy_new_median, color = 'blue', linestyle = 'dotted')\n",
    "\n",
    "for bucket in bucket_accuracies:\n",
    "    ax.plot(loss_factors, bucket)\n",
    "\n",
    "# ax2.plot(loss_factors, avg_weight_accuracy, color = 'green')\n",
    "ax.set_xlabel('Loss factor')\n",
    "ax.set_ylabel('Avg dist accuracy')\n",
    "ax.set_title('Avg dist accuracy vs loss factor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
