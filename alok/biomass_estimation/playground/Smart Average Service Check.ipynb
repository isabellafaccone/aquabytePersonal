{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from research.weight_estimation.population_metrics import PopulationMetricsEstimator\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import psycopg2\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SCHEDULE_BY_PEN = {\n",
    "    88: (7, 16),\n",
    "    66: (0, 24),\n",
    "}\n",
    "\n",
    "\n",
    "def get_db_params_from_aws():\n",
    "#     ssm = boto3.client('ssm', region_name='eu-west-1')\n",
    "#     host_param = ssm.get_parameter(Name='/DW_DB_RO_HOST', WithDecryption=True)\n",
    "#     host = host_param['Parameter']['Value']\n",
    "\n",
    "#     user_param = ssm.get_parameter(Name='/DW_DB_RO_USER', WithDecryption=True)\n",
    "#     user = user_param['Parameter']['Value']\n",
    "\n",
    "#     password_param = ssm.get_parameter(Name='/DW_DB_RO_PASSWORD', WithDecryption=True)\n",
    "#     password = password_param['Parameter']['Value']\n",
    "\n",
    "#     dbname_param = ssm.get_parameter(Name='/DW_DB_RO_NAME', WithDecryption=True)\n",
    "#     dbname = dbname_param['Parameter']['Value']\n",
    "\n",
    "    credentials = json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS']))\n",
    "    host, user, password, dbname = credentials['host'], credentials['user'], credentials['password'], \\\n",
    "                                   credentials['database']\n",
    "\n",
    "    print(\"Successfully retrieved DB params.\")\n",
    "    return host, user, password, dbname\n",
    "\n",
    "\n",
    "def get_data_from_db(pen_id, dates_to_include, akpd_score_cutoff=0.99):\n",
    "\n",
    "    # get sampling schedule by pen ID (default to 24 hour sampling if pen not available)\n",
    "    if pen_id in SCHEDULE_BY_PEN.keys():\n",
    "        hour_start, hour_end = SCHEDULE_BY_PEN[pen_id]\n",
    "    else:\n",
    "        hour_start, hour_end = 0, 24\n",
    "\n",
    "    date_list = '({})'.format(\", \".join([f\"'{d}'\" for d in dates_to_include]))\n",
    "    print(\"Connecting to DB...\")\n",
    "\n",
    "    host, user, password, dbname = get_db_params_from_aws()\n",
    "\n",
    "    res = []\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\"dbname=\"+dbname+\" user=\"+user+\" host=\"+host+\" password=\"+password)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        query = \"SELECT\\\n",
    "                to_char(captured_at, 'YYYY-MM-DD') AS date,\\\n",
    "                estimated_weight_g, estimated_length_mm\\\n",
    "                FROM prod.biomass_computations\\\n",
    "                WHERE\\\n",
    "                to_char(captured_at, 'YYYY-MM-DD') IN {0}\\\n",
    "                AND date_part('hour', captured_at) BETWEEN {1} AND {2}\\\n",
    "                AND pen_id = {3}\\\n",
    "                AND group_id IN ('{3}')\\\n",
    "                AND akpd_score > {4}\\\n",
    "                AND estimated_weight_g > 0\\\n",
    "                AND estimated_weight_g != double precision 'NaN'\\\n",
    "                ORDER BY date DESC\".format(date_list, hour_start, hour_end, pen_id, akpd_score_cutoff)\n",
    "        print(query)\n",
    "\n",
    "        # execute statement\n",
    "        cur.execute(query)\n",
    "\n",
    "        # fetch rows\n",
    "        rows = cur.fetchall()\n",
    "        for row in rows:\n",
    "            res.append(row)\n",
    "\n",
    "        cur.close()\n",
    "    except psycopg2.DatabaseError as error:\n",
    "        print(error)\n",
    "        print(\"COULD NOT CONNECT TO DB\")\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "    print(f\"Successfully retrieved from DB: {len(res)} rows\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def process_bcs(biomass_computations):\n",
    "    new_bcs = []\n",
    "    for bc in biomass_computations:\n",
    "        date, weight, length = bc\n",
    "        estimated_k_factor = 1e5 * (weight / (length**3)) if (weight and length) else None\n",
    "        new_bcs.append((date, weight, estimated_k_factor))\n",
    "    new_bcs = sorted(new_bcs, key=lambda x: x[0])\n",
    "    return new_bcs\n",
    "\n",
    "\n",
    "def generate_smart_metrics(data):\n",
    "    pen_id = data['penId']\n",
    "    dates_to_compute = sorted(list(data['datesToCompute']))\n",
    "    dates_to_include = sorted(list(data['datesToInclude']))\n",
    "\n",
    "    resp = {}\n",
    "\n",
    "    # Get data from DB\n",
    "    biomass_computations = get_data_from_db(pen_id, dates_to_include)\n",
    "    if not biomass_computations:\n",
    "        return resp\n",
    "\n",
    "    biomass_computations = process_bcs(biomass_computations)\n",
    "\n",
    "    # If any dates to compute pre-date first available date in dates to include, return None\n",
    "    first_available_date = biomass_computations[0][0]\n",
    "    if not all([first_available_date <= date for date in dates_to_compute]):\n",
    "        print('DATE IS BEFORE CAMERA DATA!')\n",
    "        return resp\n",
    "\n",
    "    pme = PopulationMetricsEstimator(biomass_computations)\n",
    "    \n",
    "    for date in dates_to_compute:\n",
    "        metrics = pme.generate_smart_metrics_on_date(date,\n",
    "                                                     max_day_difference=3,\n",
    "                                                     incorporate_future=True,\n",
    "                                                     apply_growth_rate=True,\n",
    "                                                     bucket_size=1000)\n",
    "        print(np.mean(metrics['adj_weights']))\n",
    "\n",
    "        # smart_sample_size is np.int64. Quick workaround to make json happy.\n",
    "        smart_sample_size = int(metrics['smart_sample_size'])\n",
    "        resp_for_date = dict(\n",
    "            weightMovingAvg=metrics['smart_average_weight'],\n",
    "            weightMovingDist=metrics['smart_distribution'],\n",
    "            movingKFactor=metrics['smart_average_kf'],\n",
    "            dailyGrowthRate=metrics['growth_rate'],\n",
    "            numMovingAvgBatiFish=smart_sample_size\n",
    "        )\n",
    "        resp[date] = resp_for_date\n",
    "\n",
    "    return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"penId\": 37,\n",
    "    \"datesToCompute\": [\n",
    "        \"2020-06-18\",\n",
    "        \"2020-06-19\"\n",
    "    ],\n",
    "    \"datesToInclude\": [\n",
    "        \"2020-06-11\",\n",
    "        \"2020-06-12\",\n",
    "        \"2020-06-13\",\n",
    "        \"2020-06-14\",\n",
    "        \"2020-06-15\",\n",
    "        \"2020-06-16\",\n",
    "        \"2020-06-17\",\n",
    "        \"2020-06-18\",\n",
    "        \"2020-06-19\",\n",
    "        \"2020-06-20\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = generate_smart_metrics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist = {}\n",
    "kf_breakdown = {}\n",
    "count = 0\n",
    "for k in list(sm['2020-06-19']['weightMovingDist'].keys()):\n",
    "    key = '{}-{}'.format(str(k), str(float(k)+1))\n",
    "    w_dist[key] = sm['2020-06-19']['weightMovingDist'][k]['count']\n",
    "    kf_breakdown[key] = sm['2020-06-19']['weightMovingDist'][k]['avgKFactor']\n",
    "    count += sm['2020-06-19']['weightMovingDist'][k]['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist = {k: 100 * float(v) / count for k, v in w_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from research.utils.datetime_utils import day_difference, add_days\n",
    "from research.utils.datetime_utils import get_dates_in_range\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "\n",
    "class PopulationMetricsEstimator(object):\n",
    "    \"\"\"\n",
    "    Population Metrics Estimator is a class to help generate biomass population-level metrics for a given\n",
    "    set of historical biomass computations. The most important ones here are the biomass smart average,\n",
    "    smart distribution, and KPI. See this document for further information:\n",
    "    https://aquabyte.atlassian.net/wiki/spaces/Research/pages/361562423/2020-06-02+New+Smart+Average+and+Smart+Distribution\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    biomass_computations : list\n",
    "        List of tuples, where each tuple contains: date, estimated_weight_g, and estimated_k_factor\n",
    "    bcs_by_date : dict\n",
    "        Dict mapping date with list of corresponding individual biomass computations\n",
    "    unique_dates_nr : list\n",
    "        List of day numbers (with first date in the input data corresponding to zero)\n",
    "    unique_dates: list\n",
    "        List of ordered unique dates\n",
    "    average_weights: list\n",
    "        List of raw daily average weights for each date in unique_dates\n",
    "    sample_sizes: list\n",
    "        List of raw daily sample sizes for each date in unique_dates\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, biomass_computations):\n",
    "        self.biomass_computations = biomass_computations\n",
    "        self.bcs_by_date = defaultdict(list)\n",
    "        self.unique_dates_nr = []\n",
    "        self.unique_dates = []\n",
    "        self.average_weights = []\n",
    "        self.sample_sizes = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepares the biomass computation data.\n",
    "\n",
    "        The input biomass computations in the constructor are an (unordered) list of tuples.\n",
    "        This method sorts them by date. It also generates and stores daily level aggregations\n",
    "        of this data.\n",
    "        \"\"\"\n",
    "        self.biomass_computations = sorted(self.biomass_computations, key=lambda x: x[0])\n",
    "        self.generate_daily_values()\n",
    "\n",
    "    def generate_daily_values(self):\n",
    "        \"\"\"Generates daily level aggregations of biomass computations.\n",
    "\n",
    "        This includes: list of unique dates, list of daily raw average weights,\n",
    "        list of daily sample sizes, and list of day numbers\n",
    "        \"\"\"\n",
    "\n",
    "        weights_for_date = []\n",
    "        curr_date = self.biomass_computations[0][0]\n",
    "        for date, weight, kf in self.biomass_computations:\n",
    "            self.bcs_by_date[date].append((weight, kf))\n",
    "            if date != curr_date:\n",
    "                self.unique_dates.append(curr_date)\n",
    "                self.average_weights.append(np.mean(weights_for_date))\n",
    "                self.sample_sizes.append(len(weights_for_date))\n",
    "                weights_for_date = [weight]\n",
    "                curr_date = date\n",
    "            else:\n",
    "                weights_for_date.append(weight)\n",
    "\n",
    "        self.unique_dates.append(curr_date)\n",
    "        self.average_weights.append(np.mean(weights_for_date))\n",
    "        self.sample_sizes.append(len(weights_for_date))\n",
    "        self.unique_dates_nr = [day_difference(date, self.unique_dates[0]) for date in self.unique_dates]\n",
    "\n",
    "    def generate_raw_daily_metrics_on_date(self, date):\n",
    "        \"\"\"Returns the raw average weight and raw sample size on the given date\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "            Date is in the form 'YYYY-MM-DD'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        average_weight: float, sample_size: int\n",
    "        \"\"\"\n",
    "        if date in self.unique_dates:\n",
    "            idx = self.unique_dates.index(date)\n",
    "            return self.average_weights[idx], self.sample_sizes[idx]\n",
    "        return None, None\n",
    "\n",
    "    def generate_raw_weights_kfs_on_date(self, date):\n",
    "        \"\"\"Returns all individual raw weights and k-factors on date.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "            Date is in the form 'YYYY-MM-DD'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights: list, kfs: list\n",
    "        \"\"\"\n",
    "        if len(self.bcs_by_date[date]) > 0:\n",
    "            weights, kfs = [list(l) for l in list(zip(*self.bcs_by_date[date]))]\n",
    "            return weights, kfs\n",
    "        return [], []\n",
    "\n",
    "    def get_start_end_idx(self, start_date, end_date):\n",
    "        \"\"\"Returns indices of self.unique_dates corresponding to start and end dates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date: str\n",
    "            Start date is in the form 'YYYY-MM-DD'\n",
    "        end_date: str\n",
    "            End date is in the form 'YYYY-MM-DD'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        start_idx: int\n",
    "        end_idx: int\n",
    "        \"\"\"\n",
    "\n",
    "        if start_date > self.unique_dates[-1] or end_date < self.unique_dates[0]:\n",
    "            return -1, -1\n",
    "        start_idx = [idx for idx, date in enumerate(self.unique_dates) if date >= start_date][0]\n",
    "        end_idx = [idx for idx, date in enumerate(self.unique_dates) if date <= end_date][-1] + 1\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def compute_growth_rate(self, date, start_date, end_date, decay=0.1):\n",
    "        \"\"\"Returns growth rate between start and end dates, with more weight applied to days closer to\n",
    "        present date.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "            date is in the form 'YYYY-MM-DD'. This is considered 'present date'.\n",
    "        start_date: str\n",
    "        end_date: str\n",
    "        decay: float\n",
    "            This is the exponential weight decay factor to apply to dates not equal to present date.\n",
    "            For example, a 2 day difference would result in a weighting of exp(-decay * 2).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        growth_rate: float\n",
    "        error_magnitude_pct: float\n",
    "            The error magnitude percentage is a representation of deviation between raw average weight and the\n",
    "            exponential curve fit. It is weighted as described in the decay factor description.\n",
    "        \"\"\"\n",
    "\n",
    "        if not any([date in self.unique_dates for date in get_dates_in_range(start_date, end_date)]):\n",
    "            return None, None\n",
    "        start_idx, end_idx = self.get_start_end_idx(start_date, end_date)\n",
    "        X = np.array([day_difference(d, date) for d in self.unique_dates[start_idx:end_idx]]).reshape(-1, 1)\n",
    "        y = np.log(np.array(self.average_weights[start_idx:end_idx]))\n",
    "        n = np.array(self.sample_sizes[start_idx:end_idx])\n",
    "\n",
    "        if X.shape[0] < 4:\n",
    "            return None, None\n",
    "\n",
    "        sample_weights = np.multiply(n, np.exp(-decay * np.abs(X.squeeze())))\n",
    "        reg = LinearRegression().fit(X, y, sample_weight=sample_weights)\n",
    "        growth_rate = reg.coef_[0]\n",
    "        y_pred = reg.predict(X)\n",
    "\n",
    "        error_magnitude_pct = np.average(((np.exp(y) - np.exp(y_pred)) / np.exp(y_pred))**2,\n",
    "                                         weights=sample_weights)**0.5\n",
    "        return growth_rate, error_magnitude_pct\n",
    "\n",
    "    def compute_local_growth_rate(self, date, incorporate_future, window=7):\n",
    "        \"\"\"Returns local growth rate given date, window size, and whether or not future should be incorporated.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "        incorporate_future: bool\n",
    "        window: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        growth_rate: float\n",
    "        error_magnitude_pct: float\n",
    "            More details on this variable in the doc-string of compute_growth_rate(...)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute local growth rate\n",
    "        day_diffs = np.array([day_difference(d, date) for d in self.unique_dates])\n",
    "        if incorporate_future:\n",
    "            start, end = add_days(date, -window), add_days(date, window // 2)\n",
    "            if not any([date in get_dates_in_range(start, end) for date in self.unique_dates]):\n",
    "                return None, None\n",
    "            end_idx = np.where(day_diffs <= window // 2)[0][-1]\n",
    "            end_date = self.unique_dates[end_idx]\n",
    "            start_date = add_days(end_date, -window)\n",
    "        else:\n",
    "            start_date, end_date = add_days(date, -window), date\n",
    "        growth_rate, error_magnitude_pct = self.compute_growth_rate(date, start_date, end_date)\n",
    "        return growth_rate, error_magnitude_pct\n",
    "\n",
    "    def generate_historical_weights(self, date, window=7):\n",
    "        \"\"\"Generates array of historical individual weights from seven days ago to yesterday.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "        window: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        historical_weights: list\n",
    "        \"\"\"\n",
    "        seven_days_ago, yesterday = add_days(date, -window), add_days(date, -1)\n",
    "        dates = get_dates_in_range(seven_days_ago, yesterday)\n",
    "        historical_weights = []\n",
    "        for curr_date in dates:\n",
    "            weights, _ = self.generate_raw_weights_kfs_on_date(curr_date)\n",
    "            historical_weights.extend(weights)\n",
    "\n",
    "        return historical_weights\n",
    "\n",
    "    def generate_distribution_consistency(self, date, window=7):\n",
    "        \"\"\"Generates distribution consistency score for a given date.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "        window: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distribution_consistency: float\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        raw_weights, _ = self.generate_raw_weights_kfs_on_date(date)\n",
    "        historical_weights = self.generate_historical_weights(date, window=window)\n",
    "        if not raw_weights or not historical_weights:\n",
    "            return None\n",
    "        raw_weights = np.array(raw_weights)\n",
    "        historical_weights = np.array(historical_weights)\n",
    "        mean_adjustment = np.mean(raw_weights) - np.mean(historical_weights)\n",
    "        x = np.percentile(historical_weights + mean_adjustment, list(range(100)))\n",
    "        y = np.percentile(raw_weights, list(range(100)))\n",
    "        distribution_consistency = 1.0 - 10.0 * (np.mean(np.abs(y[1:99] - x[1:99]) ** 2) ** 0.5 / 10000.0)\n",
    "        return distribution_consistency\n",
    "\n",
    "    def generate_smart_metrics_on_date(self, date, max_day_difference=3, bucket_size=100, incorporate_future=True,\n",
    "                                       apply_growth_rate=True):\n",
    "        \"\"\"\n",
    "        Generates smart metrics for a given date.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date: str\n",
    "        max_day_difference: int\n",
    "            This is the maximum number of days beyond which we cannot incorporate information into the smart\n",
    "            average calculation\n",
    "        bucket_size: int\n",
    "            This is the bucket size to use for the smart weight distribution representation\n",
    "        incorporate_future: bool\n",
    "            Should we incorporate future data relative to date or not?\n",
    "        apply_growth_rate: bool\n",
    "            Should we enable growth rate application or not?\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        metrics: dict\n",
    "            Dict contains raw and smart metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # compute metrics on this date\n",
    "        distribution_consistency = self.generate_distribution_consistency(date, window=7)\n",
    "        raw_average_weight, raw_sample_size = self.generate_raw_daily_metrics_on_date(date)\n",
    "        _, raw_kfs = self.generate_raw_weights_kfs_on_date(date)\n",
    "        raw_average_kf = np.mean([kf for kf in raw_kfs if kf is not None])\n",
    "\n",
    "        # compute local growth rate\n",
    "        growth_rate, error_magnitude_pct = self.compute_local_growth_rate(date, incorporate_future)\n",
    "\n",
    "        # compute smart average\n",
    "\n",
    "        look_ahead = max_day_difference if incorporate_future else 0\n",
    "        start_date, end_date = add_days(date, -max_day_difference), add_days(date, look_ahead)\n",
    "        if not any([date in get_dates_in_range(start_date, end_date) for date in self.unique_dates]):\n",
    "            return {}\n",
    "        start_idx, end_idx = self.get_start_end_idx(start_date, end_date)\n",
    "\n",
    "        if growth_rate and apply_growth_rate and raw_sample_size and error_magnitude_pct < 0.02 and \\\n",
    "                abs(growth_rate) < 0.02:\n",
    "            growth_rate_for_smart_avg = growth_rate\n",
    "        else:\n",
    "            growth_rate_for_smart_avg = 0.0\n",
    "\n",
    "        x = np.array([day_difference(d, date) for d in self.unique_dates[start_idx:end_idx]])\n",
    "        y = np.array(self.average_weights[start_idx:end_idx])\n",
    "        n = np.array(self.sample_sizes[start_idx:end_idx])\n",
    "        sample_size = int(np.sum(n))\n",
    "        smart_average = np.sum(np.exp(-x * growth_rate_for_smart_avg) * y * n) / sample_size\n",
    "\n",
    "        # compute smart distribution\n",
    "        adj_weights, kfs = [], []\n",
    "        for date_idx, date in enumerate(self.unique_dates[start_idx:end_idx]):\n",
    "            weights_for_date, kfs_for_date = self.generate_raw_weights_kfs_on_date(date)\n",
    "            adj_weights_for_date = np.array(weights_for_date) * np.exp(-x[date_idx] * growth_rate_for_smart_avg)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            kfs.extend(kfs_for_date)\n",
    "\n",
    "        # perform consistency checks\n",
    "        assert len(adj_weights) == np.sum(n), 'Inconsistent sample sizes!'\n",
    "        assert math.isclose(smart_average, np.mean(adj_weights), rel_tol=1e-5), 'Inconsistent smart average numbers!'\n",
    "\n",
    "        adj_weights, kfs = np.array(adj_weights), np.array(kfs)\n",
    "        smart_distribution = dict()\n",
    "\n",
    "        bucket_size_kg = 1e-3 * bucket_size\n",
    "        buckets = [round(x, 2) for x in np.arange(0.0, 1e-3 * np.max(adj_weights), bucket_size_kg)]\n",
    "        for b in buckets:\n",
    "            low, high = 1e3 * b, 1e3 * (b + bucket_size_kg)\n",
    "            count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "            kfs_for_bucket = [kf if kf else np.nan for kf in kfs[(adj_weights >= low) & (adj_weights < high)]]\n",
    "            mean_kf = np.mean(kfs_for_bucket) if count > 0 else np.nan\n",
    "            smart_distribution[str(b)] = {\n",
    "                'count': count,\n",
    "                'avgKFactor': None if np.isnan(mean_kf) else mean_kf\n",
    "            }\n",
    "\n",
    "        metrics = dict(\n",
    "            raw_average_weight=raw_average_weight,\n",
    "            raw_average_kf=raw_average_kf,\n",
    "            raw_sample_size=raw_sample_size,\n",
    "            smart_average_weight=smart_average,\n",
    "            smart_average_kf=np.mean([kf for kf in kfs if kf is not None]),\n",
    "            smart_distribution=smart_distribution,\n",
    "            smart_sample_size=sample_size,\n",
    "            adj_weights=adj_weights,\n",
    "            kfs=kfs,\n",
    "            growth_rate=growth_rate,\n",
    "            error_magnitude_pct=error_magnitude_pct,\n",
    "            distribution_consistency=distribution_consistency\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM\n",
    "        prod.biomass_computations bc\n",
    "        WHERE bc.pen_id=23\n",
    "        and bc.akpd_score > 0.0\n",
    "        and bc.estimated_weight_g > 0.0;\n",
    "    \"\"\"\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "\n",
    "    df = df.sort_values('captured_at').copy(deep=True)\n",
    "    df.index = pd.to_datetime(df.captured_at)\n",
    "    dates = df.index.date.astype(str)\n",
    "    df['date'] = dates\n",
    "    df['hour'] = df.index.hour\n",
    "\n",
    "    if 'estimated_k_factor' not in df.columns.tolist():\n",
    "        df['estimated_k_factor'] = 0.0\n",
    "\n",
    "    biomass_computations = []\n",
    "    for idx, row in df.iterrows():\n",
    "        biomass_computations.append((row.date, row.estimated_weight_g, row.estimated_k_factor))\n",
    "\n",
    "    pme = PopulationMetricsEstimator(biomass_computations)\n",
    "    start_date, end_date = df.date.iloc[0], df.date.iloc[-1]\n",
    "    dates_in_range = get_dates_in_range(start_date, end_date)\n",
    "\n",
    "    for curr_date in dates_in_range:\n",
    "        # generate raw daily metrics\n",
    "        raw_average_weight, raw_sample_size = pme.generate_raw_daily_metrics_on_date(curr_date)\n",
    "\n",
    "        # generate smart daily metrics\n",
    "        smart_average_weight, _, _, adj_weights, adj_kfs, growth_rate = pme.generate_smart_metrics_on_date(\n",
    "            curr_date,\n",
    "            incorporate_future=True,\n",
    "            apply_growth_rate=True)\n",
    "        print(curr_date, raw_average_weight, raw_sample_size, smart_average_weight, growth_rate)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
