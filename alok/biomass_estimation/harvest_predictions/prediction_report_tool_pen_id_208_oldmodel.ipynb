{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from filter_optimization.filter_optimization_task import NoDataException, SamplingFilter, generate_filter_mask, \\\n",
    "     extract_biomass_data\n",
    "from population_metrics.population_metrics_base import generate_pm_base, PopulationMetricsBase\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.raw_metrics import get_raw_kf_values, generate_raw_average_weight, get_raw_sample_size\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, \\\n",
    "     generate_smart_distribution, generate_smart_avg_kf, get_smart_sample_size, get_smart_growth_rate, \\\n",
    "     generate_smart_standard_deviation\n",
    "from population_metrics.confidence_metrics import generate_trend_stability, generate_distribution_consistency, \\\n",
    "     compute_biomass_kpi, get_raw_and_historical_weights\n",
    "from research.utils.datetime_utils import get_dates_in_range\n",
    "from research.utils.data_access_utils import RDSAccessUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(extract_biomass_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "\n",
    "def gen_pm_base(df: pd.DataFrame, sampling_filter: SamplingFilter) -> PopulationMetricsBase:\n",
    "    \"\"\"\n",
    "    Returns PopulationMetricsBase instance given input biomass computations\n",
    "    data-frame (see README for more details) and SamplingFilter instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = generate_filter_mask(df, sampling_filter)\n",
    "\n",
    "    # get filtered set of biomass computations\n",
    "    biomass_computations = list(zip(df[mask].date.values,\n",
    "                                    df.loc[mask, 'estimated_weight_g'].values,\n",
    "                                    df[mask].estimated_k_factor.values))\n",
    "\n",
    "    # generate population metrics estimator\n",
    "    if not biomass_computations:\n",
    "        raise NoDataException('No data found for given filter!')\n",
    "    return generate_pm_base(biomass_computations)\n",
    "\n",
    "\n",
    "def generate_ts_data(df: pd.DataFrame, sampling_filter: SamplingFilter) -> defaultdict:\n",
    "    \"\"\"\n",
    "    Given input data-frame of biomass computations and SamplingFilter instance,\n",
    "    generates time-series data for different raw metrics, smart metrics, growth rate metrics,\n",
    "    and confidence metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    pm_base = gen_pm_base(df, sampling_filter)\n",
    "    start_date, end_date = pm_base.unique_dates[0], pm_base.unique_dates[-1]\n",
    "    dates = get_dates_in_range(start_date, end_date)\n",
    "    ts_data = defaultdict(list)\n",
    "    ts_data['date'].extend(dates)\n",
    "    for date in dates:\n",
    "\n",
    "        # raw metrics\n",
    "        raw_average_weight = generate_raw_average_weight(pm_base, date)\n",
    "        raw_sample_size = get_raw_sample_size(pm_base, date)\n",
    "\n",
    "        # growth rate metrics\n",
    "        growth_rate = compute_local_growth_rate(pm_base, date)\n",
    "\n",
    "        # confidence metrics\n",
    "        distribution_consistency = generate_distribution_consistency(pm_base, date)\n",
    "        kpi = compute_biomass_kpi(pm_base, date)\n",
    "\n",
    "        # smart metrics\n",
    "        smart_average_weight = generate_smart_avg_weight(pm_base, date)\n",
    "        smart_average_kf = generate_smart_avg_kf(pm_base, date)\n",
    "        smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "        smart_growth_rate = get_smart_growth_rate(pm_base, date)\n",
    "\n",
    "        ts_data['raw_average_weight'].append(raw_average_weight)\n",
    "        ts_data['raw_sample_size'].append(raw_sample_size)\n",
    "        ts_data['growth_rate'].append(growth_rate)\n",
    "        ts_data['distribution_consistency'].append(distribution_consistency)\n",
    "        ts_data['kpi'].append(kpi)\n",
    "        ts_data['smart_average_weight'].append(smart_average_weight)\n",
    "        ts_data['smart_average_kf'].append(smart_average_kf)\n",
    "        ts_data['smart_sample_size'].append(smart_sample_size)\n",
    "        ts_data['smart_growth_rate'].append(smart_growth_rate)\n",
    "\n",
    "    return ts_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import population_metrics\n",
    "print(population_metrics.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: end_date is exclusive. \n",
    "\n",
    "pen_id, start_date, end_date = 208, '2020-11-23', '2020-12-03'\n",
    "sampling_filter = SamplingFilter(start_hour=9, end_hour=13, kf_cutoff=0, akpd_score_cutoff=0.95)\n",
    "df = extract_biomass_data(pen_id, start_date, end_date, sampling_filter.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_filter2 = SamplingFilter(start_hour=0, end_hour=23, kf_cutoff=0, akpd_score_cutoff=0.95)\n",
    "df2 = extract_biomass_data(pen_id, start_date, end_date, sampling_filter2.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df2.plot.hist(y='estimated_weight_g', bins=100, figsize=(20, 5))\n",
    "plt.axvline(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot.scatter(x='captured_at', y='estimated_weight_g', figsize=(20, 5))\n",
    "plt.xlim((df2['captured_at'].min(), df2['captured_at'].max()))\n",
    "plt.axhline(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['captured_at'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, bins = pd.cut(df2['estimated_weight_g'], 100, retbins=True)\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "df2['bins'] = c\n",
    "c.value_counts().sort_index().plot.bar(ax=ax)\n",
    "df2.groupby('bins')['estimated_k_factor'].mean().plot(ax=ax, secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(df, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-02', 3, True, True, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(generate_smart_avg_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-12-02'\n",
    "\n",
    "def get_distribution(weights, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        bucket_count = weights[(weights >= low) & (weights < high)].shape[0]\n",
    "        dist[bucket] = bucket_count\n",
    "        count += bucket_count\n",
    "    \n",
    "    dist = {k: round(100 * v / count, 1) for k, v in dist.items()}\n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_kf_breakdown(weights, kfs, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        mean_kf = kfs[(weights >= low) & (weights < high)].mean()\n",
    "        dist[bucket] = round(mean_kf, 2)\n",
    "    \n",
    "    return dist\n",
    "        \n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "    \n",
    "\n",
    "def generate_info(pm_base, date, loss_factor):\n",
    "    weights, kfs = generate_smart_individual_values(pm_base, date, 1, False, False, 0.9)\n",
    "    vals = weights * 1.0 * (1.0 - loss_factor)\n",
    "    smart_avg = np.mean(vals)\n",
    "    smart_kf = np.mean(kfs)\n",
    "    smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "    smart_std = np.std(vals)\n",
    "    cov = smart_std / smart_avg\n",
    "    weight_dist = get_distribution(vals, np.arange(0, 14000, 1000))\n",
    "    kf_breakdown = get_kf_breakdown(vals, kfs, np.arange(0, 14000, 1000))\n",
    "    \n",
    "    print('Loss Factor: {}%'.format(round(100 * loss_factor)))\n",
    "    print('-----------')\n",
    "    print('Smart Avg Weight: {}g'.format(round(smart_avg)))\n",
    "    print('Smart K Factor: {}'.format(round(smart_kf, 2)))\n",
    "    print('Smart Sample Size: {}'.format(smart_sample_size))\n",
    "    print('Smart Standard Deviation: {}g'.format(round(smart_std)))\n",
    "    print('Coefficient of Variation: {}%'.format(round(100 * cov, 1)))\n",
    "    print('Weight Distribution:')\n",
    "    print(json.dumps(weight_dist, indent=4))\n",
    "    print('KF Breakdown:')\n",
    "    print(json.dumps(kf_breakdown, indent=4))\n",
    "    \n",
    "    return {\n",
    "        'loss_factor': round(100 * loss_factor),\n",
    "        'smart_average_weight': round(smart_avg),\n",
    "        'smart_k_factor': round(smart_kf, 2),\n",
    "        'smart_sample_size': smart_sample_size,\n",
    "        'smart_standard_deviation': smart_std,\n",
    "        'coefficient_of_variation': round(100 * cov, 1),\n",
    "        'weight_distribution': weight_dist,\n",
    "        'kf_breakdown': kf_breakdown\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for loss_factor in [0] + list(np.arange(0.13, 0.19, 0.01)):\n",
    "    output.append(generate_info(pm_base, date, loss_factor))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output, indent=4).replace('NaN', 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains constants representing core & auxiliary fish body parts.\n",
    "\"\"\"\n",
    "\n",
    "UPPER_LIP = 'UPPER_LIP'\n",
    "EYE = 'EYE'\n",
    "PECTORAL_FIN = 'PECTORAL_FIN'\n",
    "DORSAL_FIN = 'DORSAL_FIN'\n",
    "PELVIC_FIN = 'PELVIC_FIN'\n",
    "ADIPOSE_FIN = 'ADIPOSE_FIN'\n",
    "ANAL_FIN = 'ANAL_FIN'\n",
    "TAIL_NOTCH = 'TAIL_NOTCH'\n",
    "UPPER_PRECAUDAL_PIT = 'UPPER_PRECAUDAL_PIT'\n",
    "LOWER_PRECAUDAL_PIT = 'LOWER_PRECAUDAL_PIT'\n",
    "HYPURAL_PLATE = 'HYPURAL_PLATE'\n",
    "\n",
    "core_body_parts = sorted([UPPER_LIP,\n",
    "                          EYE,\n",
    "                          PECTORAL_FIN,\n",
    "                          DORSAL_FIN,\n",
    "                          PELVIC_FIN,\n",
    "                          ADIPOSE_FIN,\n",
    "                          ANAL_FIN,\n",
    "                          TAIL_NOTCH])\n",
    "\n",
    "auxiliary_body_parts = sorted([UPPER_PRECAUDAL_PIT,\n",
    "                               LOWER_PRECAUDAL_PIT,\n",
    "                               HYPURAL_PLATE])\n",
    "\n",
    "all_body_parts = sorted(core_body_parts + auxiliary_body_parts)\n",
    "\n",
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = core_body_parts.index(UPPER_LIP)\n",
    "    tail_notch_idx = core_body_parts.index(TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input\n",
    "\n",
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [''.join([f.split('_')[0]] + [p[0].upper() + p[1:] for p in f.split('_')[1:]]) for f in CameraMetadata._fields]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]\n",
    "row_meta = row['camera_metadata']\n",
    "row_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CameraMetadata(row_meta[fields[0]], row_meta[fields[1]], \n",
    "               row_meta[fields[2][:-1]], row_meta[fields[3]], \n",
    "               row_meta[fields[4]], row_meta[fields[5]], \n",
    "               row_meta[fields[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_estimator = WeightEstimator('models/nn_epoch_798_v2.pb', 'models/kf_predictor_v2.pb')\n",
    "row = df.iloc[0]\n",
    "weight, length, kf = weight_estimator.predict(row['annotation'], cm)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(row):\n",
    "    row_meta = row['camera_metadata']\n",
    "    fields = [''.join([f.split('_')[0]] + [p[0].upper() + p[1:] for p in f.split('_')[1:]]) for f in CameraMetadata._fields]\n",
    "    cm = CameraMetadata(row_meta[fields[0]], row_meta[fields[1]], \n",
    "               row_meta[fields[2][:-1]], row_meta[fields[3]], \n",
    "               row_meta[fields[4]], row_meta[fields[5]], \n",
    "               row_meta[fields[6]])\n",
    "    weight, length, kf = weight_estimator.predict(row['annotation'], cm)\n",
    "    return {'estimated_weight_g': weight, 'estimated_length_mm': length, 'estimated_k_factor': kf}\n",
    "pred(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = df.apply(pred, axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(new_preds.columns, axis=1), new_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(df, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(new_preds.columns, axis=1), new_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(df, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('date')['estimated_weight_g'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, _ = generate_smart_individual_values(pm_base, '2020-12-02', 3, True, True, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mapping = {\n",
    "    0: -1.5,\n",
    "    1000: 1.7,\n",
    "    2000: 1.0,\n",
    "    3000: 1.7,\n",
    "    4000: 0.9,\n",
    "    5000: -0.8,\n",
    "    6000: -1.0,\n",
    "    7000: -1.4,\n",
    "    8000: -1.4,\n",
    "    9000: -5.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights = []\n",
    "for w in weights:\n",
    "    bucket = int(1000 * int(w / 1000))\n",
    "    adj_weight = (1.0 - 0.01 * bias_mapping[bucket]) * w\n",
    "    adj_weights.append(adj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(adj_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(adj_weights) - 6470) / 6570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6409 * 1.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-12-02'\n",
    "\n",
    "def get_distribution(weights, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        bucket_count = weights[(weights >= low) & (weights < high)].shape[0]\n",
    "        dist[bucket] = bucket_count\n",
    "        count += bucket_count\n",
    "    \n",
    "    dist = {k: round(100 * v / count, 1) for k, v in dist.items()}\n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_kf_breakdown(weights, kfs, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        mean_kf = kfs[(weights >= low) & (weights < high)].mean()\n",
    "        dist[bucket] = round(mean_kf, 2)\n",
    "    \n",
    "    return dist\n",
    "        \n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "    \n",
    "\n",
    "def generate_info(pm_base, date, loss_factor):\n",
    "    weights, kfs = generate_smart_individual_values(pm_base, date, 3, True, True, 0.9)\n",
    "    vals = 1.01 * weights * 1.0 * (1.0 - loss_factor)\n",
    "    smart_avg = np.mean(vals)\n",
    "    smart_kf = np.mean(kfs)\n",
    "    smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "    smart_std = np.std(vals)\n",
    "    cov = smart_std / smart_avg\n",
    "    weight_dist = get_distribution(vals, np.arange(0, 10000, 1000))\n",
    "    kf_breakdown = get_kf_breakdown(vals, kfs, np.arange(0, 10000, 1000))\n",
    "    \n",
    "    print('Loss Factor: {}%'.format(round(100 * loss_factor)))\n",
    "    print('-----------')\n",
    "    print('Smart Avg Weight: {}g'.format(round(smart_avg)))\n",
    "    print('Smart K Factor: {}'.format(round(smart_kf, 2)))\n",
    "    print('Smart Sample Size: {}'.format(smart_sample_size))\n",
    "    print('Smart Standard Deviation: {}g'.format(round(smart_std)))\n",
    "    print('Coefficient of Variation: {}%'.format(round(100 * cov, 1)))\n",
    "    print('Weight Distribution:')\n",
    "    print(json.dumps(weight_dist, indent=4))\n",
    "    print('KF Breakdown:')\n",
    "    print(json.dumps(kf_breakdown, indent=4))\n",
    "    \n",
    "    return {\n",
    "        'loss_factor': round(100 * loss_factor),\n",
    "        'smart_average_weight': round(smart_avg),\n",
    "        'smart_k_factor': round(smart_kf, 2),\n",
    "        'smart_sample_size': smart_sample_size,\n",
    "        'smart_standard_deviation': smart_std,\n",
    "        'coefficient_of_variation': round(100 * cov, 1),\n",
    "        'weight_distribution': weight_dist,\n",
    "        'kf_breakdown': kf_breakdown\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for loss_factor in [0] + list(np.arange(0.06, 0.09, 0.01)):\n",
    "    output.append(generate_info(pm_base, date, loss_factor))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output, indent=4).replace('NaN', 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, start_date, end_date = 208, '2020-11-23', '2020-12-03'\n",
    "sampling_filter = SamplingFilter(start_hour=0, end_hour=24, kf_cutoff=1.09, akpd_score_cutoff=0.99)\n",
    "df = extract_biomass_data(pen_id, start_date, end_date, sampling_filter.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = df.apply(pred, axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(new_preds.columns, axis=1), new_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(df, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, _ = generate_smart_individual_values(pm_base, '2020-12-02', 3, True, True, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mapping = {\n",
    "    0: -1.5,\n",
    "    1000: 1.7,\n",
    "    2000: 1.0,\n",
    "    3000: 1.7,\n",
    "    4000: 0.9,\n",
    "    5000: -0.8,\n",
    "    6000: -1.0,\n",
    "    7000: -1.4,\n",
    "    8000: -1.4,\n",
    "    9000: -5.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights = []\n",
    "for w in weights:\n",
    "    bucket = int(1000 * int(w / 1000))\n",
    "    adj_weight = (1.0 - 0.01 * bias_mapping[bucket]) * w\n",
    "    adj_weights.append(adj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(adj_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(adj_weights) - 6470) / 6570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6409 * 1.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-12-02'\n",
    "\n",
    "def get_distribution(weights, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        bucket_count = weights[(weights >= low) & (weights < high)].shape[0]\n",
    "        dist[bucket] = bucket_count\n",
    "        count += bucket_count\n",
    "    \n",
    "    dist = {k: round(100 * v / count, 1) for k, v in dist.items()}\n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_kf_breakdown(weights, kfs, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        mean_kf = kfs[(weights >= low) & (weights < high)].mean()\n",
    "        dist[bucket] = round(mean_kf, 2)\n",
    "    \n",
    "    return dist\n",
    "        \n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "    \n",
    "\n",
    "def generate_info(pm_base, date, loss_factor):\n",
    "    weights, kfs = generate_smart_individual_values(pm_base, date, 3, True, True, 0.9)\n",
    "    vals = weights * 1.0 * (1.0 - loss_factor)\n",
    "    smart_avg = np.mean(vals)\n",
    "    smart_kf = np.mean(kfs)\n",
    "    smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "    smart_std = np.std(vals)\n",
    "    cov = smart_std / smart_avg\n",
    "    weight_dist = get_distribution(vals, np.arange(0, 10000, 1000))\n",
    "    kf_breakdown = get_kf_breakdown(vals, kfs, np.arange(0, 10000, 1000))\n",
    "    \n",
    "    print('Loss Factor: {}%'.format(round(100 * loss_factor)))\n",
    "    print('-----------')\n",
    "    print('Smart Avg Weight: {}g'.format(round(smart_avg)))\n",
    "    print('Smart K Factor: {}'.format(round(smart_kf, 2)))\n",
    "    print('Smart Sample Size: {}'.format(smart_sample_size))\n",
    "    print('Smart Standard Deviation: {}g'.format(round(smart_std)))\n",
    "    print('Coefficient of Variation: {}%'.format(round(100 * cov, 1)))\n",
    "    print('Weight Distribution:')\n",
    "    print(json.dumps(weight_dist, indent=4))\n",
    "    print('KF Breakdown:')\n",
    "    print(json.dumps(kf_breakdown, indent=4))\n",
    "    \n",
    "    return {\n",
    "        'loss_factor': round(100 * loss_factor),\n",
    "        'smart_average_weight': round(smart_avg),\n",
    "        'smart_k_factor': round(smart_kf, 2),\n",
    "        'smart_sample_size': smart_sample_size,\n",
    "        'smart_standard_deviation': smart_std,\n",
    "        'coefficient_of_variation': round(100 * cov, 1),\n",
    "        'weight_distribution': weight_dist,\n",
    "        'kf_breakdown': kf_breakdown\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for loss_factor in [0] + list(np.arange(0.13, 0.19, 0.01)):\n",
    "    output.append(generate_info(pm_base, date, loss_factor))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output, indent=4).replace('NaN', 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_std = generate_smart_standard_deviation(pm_base, date)\n",
    "print(smart_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = smart_std / smart_avg\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, kfs = generate_smart_individual_values(pm_base, date, 3, True, True, 0.9)\n",
    "# weights = weights * 0.9985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_kf(pm_base, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, start_date, end_date = 208, '2020-11-23', '2020-12-03'\n",
    "sampling_filter = SamplingFilter(start_hour=9, end_hour=13, kf_cutoff=0, akpd_score_cutoff=0.95)\n",
    "df = extract_biomass_data(pen_id, start_date, end_date, sampling_filter.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(weights, np.arange(0, 10000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kf_breakdown(weights, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        mean_kf = kfs[(weights >= low) & (weights < high)].mean()\n",
    "        dist[bucket] = round(mean_kf, 2)\n",
    "    \n",
    "    return dist\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_kf_breakdown(weights, np.arange(0, 10000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_distribution(weights, loss_factor, bucket_cutoffs):\n",
    "    adj_weights = weights * (1.0 - loss_factor)\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{low}-{high}'\n",
    "        bucket_count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        dist[bucket] = bucket_count\n",
    "        count += bucket_count\n",
    "    \n",
    "    dist = {k: 100 * v / count for k, v in dist.items()}\n",
    "    return dist\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_cutoffs = np.arange(0, 10000, 1000)\n",
    "loss_factor = 0.16\n",
    "dist_16 = get_adj_distribution(weights, loss_factor, bucket_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adj_distribution(weights, 0.17, bucket_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adj_distribution(weights, 0.1752, bucket_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [6, 7, 8, 11, 7, 6, 11]\n",
    "Y = [0.0526624699, -0.009913795167, 0.01558849764, -0.02291304971, -0.01581060603, -0.001067805761, -0.01236907407]\n",
    "W = [6440.00, 6589.00, 20874.00, 5178.00, 39081, 39081, 39081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X =  np.array(X).reshape(-1, 1)\n",
    "Y = np.array(Y)\n",
    "W = np.array(W)\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False).fit(X, Y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "1 - np.exp(lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(7*lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(lr.intercept_ + 3*lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = -0.00277068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(7*coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
