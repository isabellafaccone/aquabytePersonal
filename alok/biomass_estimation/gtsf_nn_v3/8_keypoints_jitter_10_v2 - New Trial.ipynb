{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import os, random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "from research.weight_estimation.gtsf_data.gtsf_dataset import GTSFDataset, BODY_PARTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-02-01', '2020-03-30', akpd_scorer_url)\n",
    "df = gtsf_dataset.get_prepared_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Construct Point Cloud Data Transform </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_wkps(wkps, cm, base_jitter):\n",
    "    wkps_jittered = []\n",
    "    for idx in range(len(BODY_PARTS)):\n",
    "        x_p_left = wkps[idx, 0] * cm['focalLengthPixel'] / wkps[idx, 1]\n",
    "        y_p_left = wkps[idx, 2] * cm['focalLengthPixel'] / wkps[idx, 1]\n",
    "        disparity = cm['focalLengthPixel'] * cm['baseline'] / wkps[idx, 1]\n",
    "        x_p_left_jitter = np.random.normal(0, base_jitter)\n",
    "        x_p_right_jitter = np.random.normal(0, base_jitter)\n",
    "        disparity_jitter = x_p_left_jitter + x_p_right_jitter\n",
    "        \n",
    "        x_p_left_jittered = x_p_left + x_p_left_jitter\n",
    "        disparity_jittered = disparity + disparity_jitter\n",
    "        depth_jittered = cm['focalLengthPixel'] * cm['baseline'] / disparity_jittered\n",
    "        x_jittered = x_p_left_jittered * depth_jittered / cm['focalLengthPixel']\n",
    "        y_jittered = y_p_left * depth_jittered / cm['focalLengthPixel']\n",
    "        wkp_jittered = [x_jittered, depth_jittered, y_jittered]\n",
    "        wkps_jittered.append(wkp_jittered)\n",
    "    if wkps.shape[0] == len(BODY_PARTS) + 1:\n",
    "        wkps_jittered.append(wkps[len(BODY_PARTS), :].tolist())\n",
    "    wkps_jittered = np.array(wkps_jittered)\n",
    "    return wkps_jittered\n",
    "\n",
    "\n",
    "def get_jittered_keypoints(wkps, cm, base_jitter=5):    \n",
    "    # put at random depth and apply jitter\n",
    "    depth = np.random.uniform(low=0.3, high=2.5)\n",
    "    wkps[:, 1] = wkps[:, 1] - np.median(wkps[:, 1]) + depth\n",
    "    \n",
    "    # apply jitter\n",
    "    jittered_wkps = jitter_wkps(wkps, cm, base_jitter)\n",
    "    return jittered_wkps\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Neural Network </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset\n",
    "    This is the base version of the dataset that is used to map 3D keypoints to a\n",
    "    biomass estimate. The label is the weight, and the input is the 3D workd keypoints\n",
    "    obtained during triangulation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        if self.transform:\n",
    "            input_sample = {\n",
    "                'kp_input': row.centered_keypoint_arr,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "            }\n",
    "            if 'weight' in dict(row).keys():\n",
    "                input_sample['label'] = row.weight\n",
    "            sample = self.transform(input_sample)\n",
    "            return sample\n",
    "\n",
    "        world_keypoints = row.world_keypoints\n",
    "        weight = row.weight\n",
    "\n",
    "        sample = {'kp_input': world_keypoints, 'label': weight, 'stereo_pair_id': row.id}\n",
    "\n",
    "        return sample\n",
    "\n",
    "class NormalizedCentered3D(object):\n",
    "    \n",
    "    def __init__(self, base_jitter):\n",
    "        self.base_jitter = base_jitter\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        keypoint_arr, cm, stereo_pair_id, label = \\\n",
    "            sample['kp_input'], sample['cm'], sample.get('stereo_pair_id'), sample.get('label')\n",
    "    \n",
    "        jittered_wkps = get_jittered_keypoints(keypoint_arr, cm, base_jitter=self.base_jitter)\n",
    "        normalized_label = label * 1e-4\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'kp_input': jittered_wkps,\n",
    "            'label': normalized_label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'cm': cm,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "\n",
    "        return transformed_sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        x, label, stereo_pair_id = \\\n",
    "            sample['kp_input'], sample.get('label'), sample.get('stereo_pair_id')\n",
    "        \n",
    "        if sample.get('single_point_inference'):\n",
    "            x = np.array([x])\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        \n",
    "        kp_input_tensor = torch.from_numpy(x).float()\n",
    "        \n",
    "        tensorized_sample = {\n",
    "            'kp_input': kp_input_tensor\n",
    "        }\n",
    "\n",
    "        if label:\n",
    "            label_tensor = torch.from_numpy(np.array([label])).float() if label else None\n",
    "            tensorized_sample['label'] = label_tensor\n",
    "\n",
    "        if stereo_pair_id:\n",
    "            tensorized_sample['stereo_pair_id'] = stereo_pair_id\n",
    "\n",
    "        \n",
    "        return tensorized_sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define train and test data loaders </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_fish_identifiers = list(df.fish_id.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "date_mask = (df.captured_at < '2019-09-10')\n",
    "train_mask = date_mask & df.fish_id.isin(fish_ids)\n",
    "test_mask = date_mask & ~df.fish_id.isin(fish_ids)\n",
    "\n",
    "train_dataset = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                                  NormalizedCentered3D(10),\n",
    "                                                  ToTensor()\n",
    "                                              ]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=25, shuffle=True, num_workers=1)\n",
    "\n",
    "test_dataset = KeypointsDataset(df[test_mask], transform=transforms.Compose([\n",
    "                                                      NormalizedCentered3D(10),\n",
    "                                                      ToTensor()\n",
    "                                                  ]))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=25, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    new_wkps = data['kp_input']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(new_wkps[0][:, 0], new_wkps[0][:, 2], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'batch_25_jitter_10_lr_1e-4_v2_rot_fix'\n",
    "write_outputs = True\n",
    "\n",
    "# establish output directory where model .pb files will be written\n",
    "if write_outputs:\n",
    "    dt_now = dt.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    output_base = '/root/data/alok/biomass_estimation/results/neural_network'\n",
    "    output_dir = os.path.join(output_base, run_name, dt_now)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "# instantiate neural network\n",
    "network = Network()\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# track train and test losses\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "seed = 0\n",
    "for epoch in range(epochs):\n",
    "    network.train()\n",
    "    np.random.seed(seed)\n",
    "    seed += 1\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print(running_loss / i)\n",
    "            \n",
    "    # run on test set\n",
    "    else:\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            for i, data_batch in enumerate(test_dataloader):\n",
    "                X_batch, y_batch, kpid_batch = \\\n",
    "                    data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "                y_pred = network(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                test_running_loss += loss.item()\n",
    "\n",
    "    train_loss_for_epoch = running_loss / len(train_dataloader)\n",
    "    test_loss_for_epoch = test_running_loss / len(test_dataloader)\n",
    "    train_losses.append(train_loss_for_epoch)\n",
    "    test_losses.append(test_loss_for_epoch)\n",
    "    \n",
    "    # save current state of network\n",
    "    if write_outputs:\n",
    "        f_name = 'nn_epoch_{}.pb'.format(str(epoch).zfill(3))\n",
    "        f_path = os.path.join(output_dir, f_name)\n",
    "        torch.save(network, f_path)\n",
    "    \n",
    "    # print current loss values\n",
    "    print('-'*20)\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Train Loss: {}'.format(train_loss_for_epoch))\n",
    "    print('Test Loss: {}'.format(test_loss_for_epoch))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(range(len(train_losses)), train_losses, color='blue', label='training loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, color='orange', label='validation loss')\n",
    "plt.ylim([0, 0.01])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value (MSE)')\n",
    "plt.title('Loss curves (MSE - Adam optimizer)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_dataset = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                                  NormalizedCentered3D(0),\n",
    "                                                  ToTensor()\n",
    "                                              ]))\n",
    "\n",
    "oos_dataloader = DataLoader(oos_dataset, batch_size=25, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Display Accuracy Numbers with Best Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(test_losses)\n",
    "best_network = torch.load(os.path.join(output_dir, f'nn_epoch_{best_epoch}.pb'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    best_network.eval()\n",
    "    y_preds_train, y_gt_train, y_preds_test, y_gt_test, kpids_train, kpids_test = \\\n",
    "        [], [], [], [], [], []\n",
    "    for i, data_batch in enumerate(oos_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = best_network(X_batch)\n",
    "        y_preds_train.extend(list(y_pred.numpy().flatten()))\n",
    "        y_gt_train.extend(list(y_batch.numpy().flatten()))\n",
    "        kpids_train.extend(list(kpid_batch.numpy().flatten()))\n",
    "    \n",
    "    for i, data_batch in enumerate(test_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = best_network(X_batch)\n",
    "        y_preds_test.extend(list(y_pred.numpy().flatten()))\n",
    "        y_gt_test.extend(list(y_batch.numpy().flatten()))\n",
    "        kpids_test.extend(list(kpid_batch.numpy().flatten()))\n",
    "\n",
    "analysis_df_train = pd.DataFrame({'y_pred': 1e4 * np.array(y_preds_train), \n",
    "                                  'y_gt': 1e4 * np.array(y_gt_train), \n",
    "                                  'kpid': kpids_train})\n",
    "analysis_df_test = pd.DataFrame({'y_pred': 1e4 * np.array(y_preds_test), \n",
    "                                 'y_gt': 1e4 * np.array(y_gt_test), \n",
    "                                 'kpid': kpids_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_err_pct = np.mean(np.abs(((analysis_df_train.y_pred - analysis_df_train.y_gt) / analysis_df_train.y_gt).values))\n",
    "print(f'Mean absolute error percentage (train): {round(100 * mean_absolute_err_pct, 2)}%')\n",
    "mean_absolute_err_pct = np.mean(np.abs(((analysis_df_test.y_pred - analysis_df_test.y_gt) / analysis_df_test.y_gt).values))\n",
    "print(f'Mean absolute error percentage (test): {round(100 * mean_absolute_err_pct, 2)}%')\n",
    "\n",
    "median_absolute_err_pct = np.median(np.abs(((analysis_df_train.y_pred - analysis_df_train.y_gt) / analysis_df_train.y_gt).values))\n",
    "print(f'Median absolute error percentage (train): {round(100 * median_absolute_err_pct, 2)}%')\n",
    "median_absolute_err_pct = np.median(np.abs(((analysis_df_test.y_pred - analysis_df_test.y_gt) / analysis_df_test.y_gt).values))\n",
    "print(f'Median absolute error percentage (test): {round(100 * median_absolute_err_pct, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis_df_train.y_pred.mean() - analysis_df_train.y_gt.mean()) / analysis_df_train.y_gt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Show Error with respect to K-Factor </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['k_factor'] = 1e5 * df.weight / df.data.apply(lambda x: x['lengthMms']**3).astype(float)\n",
    "analysis_df_train['error_raw'] = analysis_df_train.y_pred - analysis_df_train.y_gt\n",
    "analysis_df_train['error_pct'] = analysis_df_train.error_raw / analysis_df_train.y_gt\n",
    "analysis_df_train['abs_error_pct'] = (analysis_df_train.error_raw / analysis_df_train.y_gt).abs()\n",
    "fish_ids, kfs = [], []\n",
    "for idx, row in analysis_df_train.iterrows():\n",
    "    kpid_mask = df.id == row.kpid\n",
    "    fish_id = df[kpid_mask].fish_id.iloc[0]\n",
    "    kf = df[kpid_mask].k_factor.iloc[0]\n",
    "    fish_ids.append(fish_id)\n",
    "    kfs.append(kf)\n",
    "analysis_df_train['fish_id'] = fish_ids\n",
    "analysis_df_train['kf'] = kfs\n",
    "    \n",
    "fish_analysis_data = defaultdict(list)\n",
    "for fish_id in sorted(analysis_df_train.fish_id.unique()):\n",
    "    mask = analysis_df_train.fish_id == fish_id\n",
    "    fish_analysis_data['fish_id'].append(fish_id)\n",
    "    fish_analysis_data['num_stereo_images'].append(analysis_df_train[mask].shape[0])\n",
    "    fish_analysis_data['mean_err_pct'].append(analysis_df_train[mask].error_pct.mean())\n",
    "    fish_analysis_data['std_err_pct'].append(analysis_df_train[mask].error_pct.std())\n",
    "    fish_analysis_data['kf'].append(analysis_df_train[mask].kf.iloc[0])\n",
    "    \n",
    "fish_analysis_df = pd.DataFrame(fish_analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fish_analysis_df[fish_analysis_df.num_stereo_images > 20].kf.values, \n",
    "            fish_analysis_df[fish_analysis_df.num_stereo_images > 20].mean_err_pct.values)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    network.eval()\n",
    "    y_preds_train, y_gt_train, y_preds_test, y_gt_test = [], [], [], []\n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = network(X_batch)\n",
    "        y_preds_train.extend(list(y_pred.numpy().flatten()))\n",
    "        y_gt_train.extend(list(y_batch.numpy().flatten()))\n",
    "    \n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = network(X_batch)\n",
    "        y_preds_test.extend(list(y_pred.numpy().flatten()))\n",
    "        y_gt_test.extend(list(y_batch.numpy().flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_test = 1e4 * np.array(y_preds_test)\n",
    "y_gt_test = 1e4 * np.array(y_gt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(np.abs((y_preds_test - y_gt_test) / y_gt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(1e4*np.array(y_gt_train), 1e4*np.array(y_preds_train), color='blue', alpha=1.0)\n",
    "plt.scatter(1e4*np.array(y_gt_test), 1e4*np.array(y_preds_test), color='red', alpha=0.3)\n",
    "plt.plot([0, 10000], [0, 10000], color='blue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_values = np.arange(0.7, 1.8, 0.1)\n",
    "sample_sizes = []\n",
    "abs_err_list = []\n",
    "mean_err_list = []\n",
    "for idx in range(len(kf_values) - 1):\n",
    "    mask = (analysis_df_train.kf > kf_values[idx]) & (analysis_df_train.kf < kf_values[idx + 1])\n",
    "    sample_sizes.append(mask.sum())\n",
    "    abs_err = np.mean(np.abs((analysis_df_train[mask].y_pred - analysis_df_train[mask].y_gt) / analysis_df_train[mask].y_gt))\n",
    "    abs_err_list.append(abs_err)\n",
    "    mean_err = np.mean((analysis_df_train[mask].y_pred - analysis_df_train[mask].y_gt) / analysis_df_train[mask].y_gt)\n",
    "    mean_err_list.append(mean_err)\n",
    "\n",
    "pd.DataFrame({'sample_size': sample_sizes, 'mean_err': mean_err_list, 'abs_err': abs_err_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_urls = list(df.sort_values('k_factor', ascending=False).left_url.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "image_f, _, _ = s3_access_utils.download_from_url(sorted_urls[8])\n",
    "im = Image.open(image_f)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "image_f, _, _ = s3_access_utils.download_from_url(sorted_urls[-3])\n",
    "im = Image.open(image_f)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_network.state_dict(), '/root/data/alok/biomass_estimation/playground/nn_8_keypoints_jitter_10.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Network()\n",
    "new_model.load_state_dict(torch.load('/root/data/alok/biomass_estimation/playground/nn_8_keypoints_jitter_10.pb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model(torch.from_numpy(np.array([df.keypoint_arr.iloc[0]])).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate weight estimator class\n",
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-03-26T11-58-00/nn_8_keypoints_jitter_10.pb'\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "weight_estimator = WeightEstimator(model_f)\n",
    "\n",
    "# generate sample predictions\n",
    "weights = []\n",
    "for idx, row in df.iterrows():\n",
    "    keypoints, camera_metadata = row.keypoints, row.camera_metadata\n",
    "    weight_prediction = weight_estimator.predict(keypoints, camera_metadata)\n",
    "    weights.append(weight_prediction)\n",
    "    if len(weights) % 1000 == 0:\n",
    "        print(len(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_centered_3d = NormalizedCentered3D(0)\n",
    "to_tensor = ToTensor()\n",
    "preds = []\n",
    "for idx, row in df.iterrows():\n",
    "    input_sample = {\n",
    "        'kp_input': get_keypoint_arr(row.keypoints, row.camera_metadata),\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'label': row.weight,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    kps = normalized_centered_3d.__call__(input_sample)\n",
    "    kps_tensor = to_tensor(kps)\n",
    "    pred = 1e4 * best_network(kps_tensor['kp_input']).item()\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    new_wkps = data['kp_input']\n",
    "    print(data['stereo_pair_id'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wkps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
