{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "from research.weight_estimation.keypoint_utils.body_parts import BodyParts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import copy, deepcopy\n",
    "import json, math, os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "from research.weight_estimation.akpd_utils.akpd_scorer import generate_confidence_score\n",
    "from research.weight_estimation.keypoint_utils.body_parts import BodyParts\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "# get core body parts used in GTSF and weight estimation model\n",
    "BODY_PARTS = BodyParts().get_core_body_parts()\n",
    "\n",
    "\n",
    "# Class for extracting and pre-processing GTSF data\n",
    "class GTSFDataset(object):\n",
    "\n",
    "    def __init__(self, start_date, end_date, akpd_scorer_url, species='salmon', add_template_matching_keypoints=False):\n",
    "        self.s3_access_utils = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "        self.df = self.generate_raw_df(start_date, end_date)\n",
    "        self.prepare_df(akpd_scorer_url, species, add_template_matching_keypoints)\n",
    "\n",
    "    # generate raw GTSF dataframe from database\n",
    "    @staticmethod\n",
    "    def generate_raw_df(start_date, end_date):\n",
    "        rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "        query = \"\"\"\n",
    "            select * from research.fish_metadata a left join keypoint_annotations b\n",
    "            on a.left_url = b.left_image_url \n",
    "            where b.keypoints -> 'leftCrop' is not null\n",
    "            and b.keypoints -> 'rightCrop' is not null\n",
    "            and b.captured_at between '{0}' and '{1}';\n",
    "        \"\"\".format(start_date, end_date)\n",
    "        df = rds_access_utils.extract_from_database(query)\n",
    "        print('Raw dataframe loaded!')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_world_keypoints(row):\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "\n",
    "    # add AKPD score, spatial information, k-factor, and template matching body-keypoints (if applicable)\n",
    "    def prepare_df(self, akpd_scorer_url, species, add_template_matching_keypoints):\n",
    "        # use QA'ed entries, and only use Cogito entries when QA data is unavailable\n",
    "        self.df = self.df[self.df.data.apply(lambda x: x['species'].lower()) == species].copy(deep=True)\n",
    "        qa_df = self.df[self.df.is_qa == True]\n",
    "        cogito_df = self.df[(self.df.is_qa != True) & ~(self.df.left_image_url.isin(qa_df.left_image_url))]\n",
    "        self.df = pd.concat([qa_df, cogito_df], axis=0)\n",
    "        print('Dataset preparation beginning...')\n",
    "\n",
    "        # add 3D spatial information\n",
    "        self.df['world_keypoints'] = self.df.apply(lambda x: self.get_world_keypoints(x), axis=1)\n",
    "#         self.df['median_depth'] = self.df.world_keypoints.apply(lambda x: np.median([wkp[1] for wkp in x.values()]))\n",
    "        print('3D spatial information added!')\n",
    "\n",
    "        # add k-factor\n",
    "        self.df['k_factor'] = 1e5 * self.df.weight / self.df.data.apply(lambda x: x['lengthMms']**3).astype(float)\n",
    "        \n",
    "        # add AKPD scores and convert world keypoints to matrix form\n",
    "        self.add_akpd_scores(akpd_scorer_url)\n",
    "        if add_template_matching_keypoints:\n",
    "            self.add_template_matching_keypoints()\n",
    "\n",
    "    @staticmethod\n",
    "    def in_hull(p, hull):\n",
    "        hull = Delaunay(hull)\n",
    "        return hull.find_simplex(p)>=0\n",
    "\n",
    "    # generate SIFT based template matching keypoints\n",
    "    def add_template_matching_keypoints(self):\n",
    "        print('Adding template matching body keypoints...')\n",
    "\n",
    "        # load data\n",
    "        gen = self.s3_access_utils.get_matching_s3_keys(\n",
    "            'aquabyte-research', \n",
    "            prefix='template-matching/2019-12-05T02:50:57', \n",
    "            suffixes=['.parquet']\n",
    "        )\n",
    "\n",
    "        keys = [key for key in gen]\n",
    "        f = self.s3_access_utils.download_from_s3('aquabyte-research', keys[0])\n",
    "        pdf = pd.read_parquet(f)\n",
    "        pdf['homography'] = pdf.homography_and_matches.apply(lambda x: np.array(x[0].tolist(), dtype=np.float))\n",
    "        pdf['matches'] = pdf.homography_and_matches.apply(lambda x: np.array(x[1].tolist(), dtype=np.int) if len(x) > 1 else None)\n",
    "\n",
    "        # merge with existing dataframe\n",
    "        self.df = pd.merge(self.df, pdf[['left_image_url', 'homography', 'matches']], how='inner', on='left_image_url')\n",
    "\n",
    "        # generate list of modified keypoints\n",
    "        modified_keypoints_list = []\n",
    "        count = 0\n",
    "        for idx, row in self.df.iterrows():\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "            count += 1\n",
    "            X_keypoints = np.array([[item['xFrame'], item['yFrame']] for item in row.keypoints['leftCrop']])\n",
    "            X_body = np.array(row.matches)\n",
    "            is_valid = self.in_hull(X_body[:, :2], X_keypoints)\n",
    "            X_body = X_body[np.where(is_valid)]\n",
    "            \n",
    "            keypoints = deepcopy(row.keypoints)\n",
    "            left_keypoints, right_keypoints = keypoints['leftCrop'], keypoints['rightCrop']\n",
    "            left_item = {'keypointType': 'BODY', 'xFrame': X_body[:, 0], 'yFrame': X_body[:, 1]}\n",
    "            right_item = {'keypointType': 'BODY', 'xFrame': X_body[:, 2], 'yFrame': X_body[:, 3]}\n",
    "            \n",
    "            left_keypoints.append(left_item)\n",
    "            right_keypoints.append(right_item)\n",
    "            modified_keypoints = {'leftCrop': left_keypoints, 'rightCrop': right_keypoints}\n",
    "            modified_keypoints_list.append(modified_keypoints)\n",
    "\n",
    "        # add modified keypoints information to dataframe\n",
    "        self.df['old_keypoints'] = self.df.keypoints\n",
    "        self.df['keypoints'] = modified_keypoints_list\n",
    "        self.df = self.df[self.df.keypoints.apply(lambda x: x['leftCrop'][-1]['xFrame'].shape[0]) > 500]\n",
    "\n",
    "    # generate AKPD scores\n",
    "    def add_akpd_scores(self, akpd_scorer_url):\n",
    "        print('Adding AKPD scores...')\n",
    "        # load neural network weights\n",
    "        akpd_scorer_path, _, _ = self.s3_access_utils.download_from_url(akpd_scorer_url)\n",
    "        akpd_scorer_network = load_model(akpd_scorer_path)\n",
    "\n",
    "        akpd_scores = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            input_sample = {\n",
    "                'keypoints': row.keypoints,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "                'single_point_inference': True\n",
    "            }\n",
    "            akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "            akpd_scores.append(akpd_score)\n",
    "        self.df['akpd_score'] = akpd_scores\n",
    "\n",
    "    # return fully pre-processed GTSF dataset for downstream training\n",
    "    def get_prepared_dataset(self):\n",
    "        return self.df\n",
    "\n",
    "\n",
    "def main():\n",
    "    akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "    gtsf_dataset = GTSFDataset('2019-06-01', '2019-06-10', akpd_scorer_url)\n",
    "    df = gtsf_dataset.get_prepared_dataset()\n",
    "    print(df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = '2019-03-01', '2020-01-01'\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from stereo_frame_pairs;\n",
    "\"\"\".format(start_date, end_date)\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.ground_truth_metadata.apply(lambda x: json.loads(x)['data'].get('species')) == 'trout')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-03-01', '2020-02-10', akpd_scorer_url, species='trout')\n",
    "df = gtsf_dataset.get_prepared_dataset()\n",
    "# df = df[(df.captured_at < '2019-09-20') & (df.median_depth < 1.0) & (df.akpd_score > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.old.weight_estimator_old import NormalizedStabilityTransform, Network\n",
    "from research.weight_estimation.old.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2019-11-08T00-13-09/nn_epoch_798.pb'\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "network = torch.load(model_f)\n",
    "\n",
    "\n",
    "weight_predictions = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    \n",
    "    input_sample = {\n",
    "        'keypoints': row.keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "    \n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    weight_prediction = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    weight_predictions.append(weight_prediction)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.utils.datetime_utils import add_days, get_dates_in_range\n",
    "from research.weight_estimation.population_metrics import PopulationMetricsEstimator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "S3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "RDS = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "OUTPUT_DIR = '/root/data/recommendations'\n",
    "UPLOAD_BUCKET = 'aquabyte-images-adhoc'\n",
    "UPLOAD_KEY_BASE = 'alok/filter_recommendations'\n",
    "\n",
    "\n",
    "class NoDataException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "SamplingFilter = namedtuple('SamplingFilter', 'start_hour end_hour kf_cutoff akpd_score_cutoff')\n",
    "\n",
    "\n",
    "def generate_filter_mask(df, sampling_filter):\n",
    "    \"\"\"Generates boolean mask on data-frame of raw biomass computations corresponding to sampling filter.\"\"\"\n",
    "\n",
    "    if sampling_filter.start_hour < sampling_filter.end_hour:\n",
    "        hour_mask = (df.hour >= sampling_filter.start_hour) & (df.hour <= sampling_filter.end_hour)\n",
    "    else:\n",
    "        hour_mask = (df.hour >= sampling_filter.start_hour) | (df.hour <= sampling_filter.end_hour)\n",
    "    kf_mask = (df.estimated_k_factor >= sampling_filter.kf_cutoff)\n",
    "    akpd_score_mask = (df.akpd_score >= sampling_filter.akpd_score_cutoff)\n",
    "    mask = hour_mask & kf_mask & akpd_score_mask\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_pme(df, sampling_filter):\n",
    "    \"\"\"Generates population metrics estimator give data-frame of raw biomass computations and sampling filter.\n",
    "    Args:\n",
    "        - df: data-frame of raw biomass computations\n",
    "        - sampling_filter: SamplingFilter instance representing filters to apply\n",
    "\n",
    "    Returns:\n",
    "        - PopulationMetricsEstimator instance\n",
    "    \"\"\"\n",
    "    mask = generate_filter_mask(df, sampling_filter)\n",
    "\n",
    "    # get filtered set of biomass computations\n",
    "    biomass_computations = list(zip(df[mask].date.values,\n",
    "                                    df.loc[mask, 'estimated_weight_g'].values,\n",
    "                                    df[mask].estimated_k_factor.values))\n",
    "\n",
    "    # generate population metrics estimator\n",
    "    if not biomass_computations:\n",
    "        raise NoDataException('No data found for given filter!')\n",
    "    return PopulationMetricsEstimator(biomass_computations)\n",
    "\n",
    "\n",
    "def _not_none_mean(x):\n",
    "    \"\"\"Returns mean of all non-None values in list.\"\"\"\n",
    "    return np.mean([i for i in x if i is not None])\n",
    "\n",
    "\n",
    "def generate_metrics_for_pme(pme, dates):\n",
    "    \"\"\"Generates mean biomass KPI given a PopulationMetricsEstimator instance and dates to consider.\"\"\"\n",
    "\n",
    "    kpis = []\n",
    "    for date in dates:\n",
    "        metrics = pme.generate_smart_metrics_on_date(date)\n",
    "        kpis.append(metrics.get('biomass_kpi'))\n",
    "\n",
    "    # compute mean kpi, mean distribution consistency, and final smart average\n",
    "    mean_kpi = _not_none_mean(kpis)\n",
    "    return mean_kpi\n",
    "\n",
    "\n",
    "def find_optimal_filter(df, sampling_filters):\n",
    "    \"\"\"Finds optimal filter given data-frame of raw biomass computations and different sampling filters.\n",
    "    Args:\n",
    "        - df: DataFrame of raw biomass computations from data warehouse\n",
    "        - sampling_filters: list of SamplingFilter instances to iterate over\n",
    "    Returns:\n",
    "        - best_sampling_filter: SamplingFilter instance corresponding to the one that maximizes biomass KPI\n",
    "    \"\"\"\n",
    "    analysis_data = defaultdict(list)\n",
    "    for sampling_filter in sampling_filters:\n",
    "        print('Start hour: {}, End hour: {}, KF cutoff: {}'.format(\n",
    "            sampling_filter.start_hour, sampling_filter.end_hour, sampling_filter.kf_cutoff\n",
    "        ))\n",
    "        try:\n",
    "            pme = generate_pme(df, sampling_filter)\n",
    "            unique_dates = sorted(df.date.unique().tolist())\n",
    "            dates = get_dates_in_range(unique_dates[0], unique_dates[-1])\n",
    "            mean_kpi = generate_metrics_for_pme(pme, dates)\n",
    "        except NoDataException as err:\n",
    "            print(str(err))\n",
    "            mean_kpi = None\n",
    "\n",
    "        # add to data\n",
    "        analysis_data['mean_kpi'].append(mean_kpi)\n",
    "        analysis_data['start_hour'].append(sampling_filter.start_hour)\n",
    "        analysis_data['end_hour'].append(sampling_filter.end_hour)\n",
    "        analysis_data['kf_cutoff'].append(sampling_filter.kf_cutoff)\n",
    "        analysis_data['akpd_score_cutoff'].append(sampling_filter.akpd_score_cutoff)\n",
    "\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    best_row = analysis_df.sort_values('mean_kpi', ascending=False).iloc[0]\n",
    "\n",
    "    best_sampling_filter = SamplingFilter(\n",
    "        start_hour=float(best_row.start_hour),\n",
    "        end_hour=float(best_row.end_hour),\n",
    "        kf_cutoff=float(best_row.kf_cutoff),\n",
    "        akpd_score_cutoff=float(best_row.akpd_score_cutoff)\n",
    "    )\n",
    "    return best_sampling_filter\n",
    "\n",
    "\n",
    "def generate_sampling_filters(start_hours, end_hours, kf_cutoffs, akpd_score_cutoff=0.99):\n",
    "    \"\"\"Generates list of SamplingFilter instances given start hour, end hour, and k-factor values to grid over.\"\"\"\n",
    "    sampling_filters = []\n",
    "    for start_hour in start_hours:\n",
    "        for end_hour in end_hours:\n",
    "            for kf_cutoff in kf_cutoffs:\n",
    "                sampling_filters.append(\n",
    "                    SamplingFilter(\n",
    "                        start_hour=start_hour,\n",
    "                        end_hour=end_hour,\n",
    "                        kf_cutoff=kf_cutoff,\n",
    "                        akpd_score_cutoff=akpd_score_cutoff\n",
    "                    )\n",
    "                )\n",
    "    return sampling_filters\n",
    "\n",
    "\n",
    "def perform_coarse_grid_search(df, max_kf=1.3):\n",
    "    \"\"\"Perform a coarse but broad grid search to determine best sampling filter.\n",
    "    Args:\n",
    "        - df: DataFrame of raw biomass computations from data-warehouse\n",
    "        - max_kf: Maximum k-factor value to go up to during grid search\n",
    "    Returns:\n",
    "        - best_coarse_filter: SamplingFilter instance corresopnding to best coarse-search filter\n",
    "    \"\"\"\n",
    "    min_hour, max_hour = int(df.hour.min()), int(df.hour.max())\n",
    "    start_hours = np.arange(min_hour, max_hour, 1)\n",
    "    end_hours = np.arange(min_hour, max_hour, 1)\n",
    "    min_kf_cutoff = .05 * int(df.estimated_k_factor.min() / .05)\n",
    "    kf_cutoffs = np.arange(min_kf_cutoff, max_kf, 0.05)\n",
    "    sampling_filters = generate_sampling_filters(start_hours, end_hours, kf_cutoffs)\n",
    "    best_coarse_filter = find_optimal_filter(df, sampling_filters)\n",
    "    return best_coarse_filter\n",
    "\n",
    "\n",
    "def perform_fine_grid_search(df, best_coarse_filter):\n",
    "    \"\"\"Perform a fine, local grid search around provided sampling filter to determine best sampling filter.\n",
    "    Args:\n",
    "        - df: DataFrame of raw biomass computations from data-warehouse\n",
    "        - max_kf: Maximum k-factor value to go up to during grid search\n",
    "    Returns:\n",
    "        - best_fine_filter: SamplingFilter instance corresponding to best find-search filter\n",
    "    \"\"\"\n",
    "    lo_start_hr, hi_start_hr = max(best_coarse_filter.start_hour - 1, 0), min(best_coarse_filter.start_hour + 1, 24)\n",
    "    lo_end_hr, hi_end_hr = max(best_coarse_filter.end_hour - 1, 0), min(best_coarse_filter.end_hour + 1, 24)\n",
    "    lo_kf, hi_kf = best_coarse_filter.kf_cutoff - 0.1, best_coarse_filter.kf_cutoff + 0.1\n",
    "    start_hours = np.arange(lo_start_hr, hi_start_hr, 1)\n",
    "    end_hours = np.arange(lo_end_hr, hi_end_hr, 1)\n",
    "    kf_cutoffs = np.arange(lo_kf, hi_kf, 0.005)\n",
    "    sampling_filters = generate_sampling_filters(start_hours, end_hours, kf_cutoffs)\n",
    "    best_fine_filter = find_optimal_filter(df, sampling_filters)\n",
    "    return best_fine_filter\n",
    "\n",
    "\n",
    "def generate_global_optimum_filter(pen_id, start_date, end_date, akpd_score_cutoff=0.99):\n",
    "    \"\"\"Determine best global optimal sampling strategy for given pen_id, start_date, and end_date.\n",
    "        Args:\n",
    "    \"\"\"\n",
    "    df = extract_biomass_data(pen_id, start_date, end_date, akpd_score_cutoff)\n",
    "\n",
    "    print('Performing coarse grid search...')\n",
    "    best_coarse_filter = perform_coarse_grid_search(df)\n",
    "    print(f'Coarse grid search complete with best start hour of {best_coarse_filter.start_hour}, '\n",
    "          f'best end hour of {best_coarse_filter.end_hour}, best kf cutoff of {best_coarse_filter.kf_cutoff}')\n",
    "\n",
    "    print('Perform fine grid search...')\n",
    "    best_fine_filter = perform_fine_grid_search(df, best_coarse_filter)\n",
    "    return best_fine_filter\n",
    "\n",
    "\n",
    "def get_active_pen_ids():\n",
    "    \"\"\"Get all active customer pen IDs.\"\"\"\n",
    "\n",
    "    query = 'SELECT id FROM customer.pens WHERE is_active=TRUE;'\n",
    "    pdf = RDS.extract_from_database(query)\n",
    "    pen_ids = sorted(pdf.id.values.tolist())\n",
    "    return pen_ids\n",
    "\n",
    "\n",
    "def get_historical_date_range(pen_id, curr_date, lookback_days):\n",
    "    \"\"\"Get date range corresponding to past two weeks if pen has data, else raise NoDataException.\"\"\"\n",
    "\n",
    "    # TODO @alok: weight KPI for each date by sample size\n",
    "\n",
    "    today, two_weeks_ago = curr_date, add_days(curr_date, -lookback_days)\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM\n",
    "            (SELECT CAST(captured_at as DATE) as date, COUNT(estimated_weight_g)\n",
    "            FROM prod.biomass_computations\n",
    "            WHERE pen_id={}\n",
    "            AND akpd_score >= 0.99\n",
    "            GROUP BY date\n",
    "            ORDER BY date DESC) AS COUNT_BY_DATE\n",
    "        WHERE date >= '{}'\n",
    "        AND date <= '{}';\n",
    "    \"\"\".format(pen_id, two_weeks_ago, today)\n",
    "    print(query)\n",
    "    tdf = RDS.extract_from_database(query)\n",
    "    if not tdf.shape[0]:\n",
    "        raise NoDataException('No data present was found in last two weeks for this pen!')\n",
    "    return two_weeks_ago, today\n",
    "\n",
    "\n",
    "def _add_date_hour_columns(df):\n",
    "    \"\"\"Adds date and hour columns to DataFrame of biomass computations\"\"\"\n",
    "    df.index = list(range(df.shape[0]))\n",
    "    df = df.sort_values('captured_at').copy(deep=True)\n",
    "    df.index = pd.to_datetime(df.captured_at)\n",
    "    dates = df.index.date.astype(str)\n",
    "    df['date'] = dates\n",
    "    df['hour'] = df.index.hour\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_biomass_data(pen_id, start_date, end_date, akpd_score_cutoff):\n",
    "    \"\"\"Get raw biomass computations for given pen_id, date range, and AKPD score cutoff.\"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM\n",
    "        prod.biomass_computations bc\n",
    "        WHERE bc.pen_id={}\n",
    "        AND bc.akpd_score >= {}\n",
    "        AND bc.captured_at BETWEEN '{}' and '{}'\n",
    "        AND bc.estimated_weight_g > 0.0\n",
    "    \"\"\".format(pen_id, akpd_score_cutoff, start_date, end_date)\n",
    "\n",
    "    df = RDS.extract_from_database(query)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = _add_date_hour_columns(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # pen_ids = get_active_pen_ids()\n",
    "    pen_ids = [86]\n",
    "\n",
    "\n",
    "    curr_date = add_days(dt.datetime.strftime(dt.datetime.now(), '%Y-%m-%d'), -1)\n",
    "    while True:\n",
    "        f = os.path.join(OUTPUT_DIR, 'recommendations_{}.json'.format(curr_date))\n",
    "        today = dt.datetime.strftime(dt.datetime.now(), '%Y-%m-%d')\n",
    "        if curr_date < today:\n",
    "            recommendations = {}\n",
    "            for pen_id in pen_ids:\n",
    "                curr_date = '2020-07-12'\n",
    "                \n",
    "\n",
    "                print('Optimizing filters for Pen ID: {}'.format(pen_id))\n",
    "\n",
    "                # get date range corresponding to last two weeks\n",
    "                try:\n",
    "                    start_date, end_date = get_historical_date_range(pen_id, curr_date, 14)\n",
    "                except NoDataException as err:\n",
    "                    print(str(err))\n",
    "                    continue\n",
    "                \n",
    "                # get best overall start hour, end hour, and k-factor cutoff\n",
    "                best_global_filter = generate_global_optimum_filter(pen_id, start_date, end_date)\n",
    "                recommendations[pen_id] = dict(\n",
    "                    best_start_hr=best_global_filter.start_hour,\n",
    "                    best_end_hr=best_global_filter.end_hour,\n",
    "                    best_kf_cutoff=best_global_filter.kf_cutoff\n",
    "                )\n",
    "\n",
    "                print(f'Best Start Hour: {best_global_filter.start_hour}')\n",
    "                print(f'Best End Hour: {best_global_filter.end_hour}')\n",
    "                print(f'Best KF Cutoff: {best_global_filter.kf_cutoff}')\n",
    "\n",
    "                json.dump(recommendations, open(f, 'w'))\n",
    "                raise\n",
    "\n",
    "            # upload to s3\n",
    "            print('Uploading recommendations for {} to S3...'.format(curr_date))\n",
    "            key = os.path.join(UPLOAD_KEY_BASE, os.path.basename(f))\n",
    "            S3.s3_client.upload_file(f, UPLOAD_BUCKET, key)\n",
    "            curr_date = add_days(curr_date, 1)\n",
    "        else:\n",
    "            print('Now sleeping for one hour...')\n",
    "            time.sleep(3600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_filter = SamplingFilter(start_hour=0, end_hour=24, kf_cutoff=1.17, akpd_score_cutoff=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_biomass_data(86, '2020-07-01', '2020-07-15', 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pme = generate_pme(df, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = pme.generate_smart_metrics_on_date('2020-07-12', bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist = {}\n",
    "kf_breakdown = {}\n",
    "count = 0\n",
    "for k in list(sm['smart_distribution'].keys()):\n",
    "    key = '{}-{}'.format(str(k), str(float(k)+1))\n",
    "    w_dist[key] = sm['smart_distribution'][k]['count']\n",
    "    kf_breakdown[key] = sm['smart_distribution'][k]['avgKFactor']\n",
    "    count += sm['smart_distribution'][k]['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist = {k: 100 * float(v) / count for k, v in w_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
