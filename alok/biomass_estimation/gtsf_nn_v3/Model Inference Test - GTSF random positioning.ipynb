{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.weight_estimation.weight_estimator import WeightEstimator\n",
    "from research.weight_estimation.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from research.gtsf_data.gtsf_dataset import GTSFDataset\n",
    "from research.gtsf_data.body_parts import BodyParts\n",
    "from research.utils.keypoint_transformations import get_keypoint_arr\n",
    "from research.weight_estimation.optics import pixel2world\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-02-01', '2020-03-30', akpd_scorer_url)\n",
    "df = gtsf_dataset.get_prepared_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS = BodyParts().get_core_body_parts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[(df.median_depth < 0.75) & (df.akpd_score > 0.5) & (df.captured_at < '2019-09-27')].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = defaultdict(list)\n",
    "\n",
    "# instantiate weight estimator class\n",
    "model_f = '/root/data/alok/biomass_estimation/playground/nn_epoch_253.pb'\n",
    "weight_estimator = WeightEstimator(model_f)\n",
    "\n",
    "\n",
    "count = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    world_keypoints = row.world_keypoints\n",
    "    cm = row.camera_metadata\n",
    "    weight = row.weight\n",
    "    \n",
    "    for t in range(10):\n",
    "        random_x_addition = np.random.uniform(-0.3, 0.3)\n",
    "        random_y_addition = np.random.uniform(-0.3, 1.5)\n",
    "        random_z_addition = np.random.uniform(-0.3, 0.3)\n",
    "        \n",
    "        new_world_keypoints = {\n",
    "            body_part: np.array([item[0] + random_x_addition, \n",
    "                                 item[1] + random_y_addition, \n",
    "                                 item[2] + random_z_addition])\n",
    "            for body_part, item in world_keypoints.items()}\n",
    "\n",
    "        new_ann_left, new_ann_right = [], []\n",
    "        for body_part in BODY_PARTS:\n",
    "            # add left item\n",
    "            x, y, z = new_world_keypoints[body_part]\n",
    "            x_frame_l = x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] // 2\n",
    "            y_frame_l = -z * cm['focalLengthPixel'] / y + cm['pixelCountHeight'] // 2\n",
    "            item_l = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_l,\n",
    "                'yFrame': y_frame_l\n",
    "            }\n",
    "            new_ann_left.append(item_l)\n",
    "\n",
    "            # add right item\n",
    "            disparity = cm['focalLengthPixel'] * cm['baseline'] / y\n",
    "            x_frame_r = x_frame_l - disparity\n",
    "            y_frame_r = y_frame_l\n",
    "            item_r = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_r,\n",
    "                'yFrame': y_frame_r\n",
    "            }\n",
    "            new_ann_right.append(item_r)\n",
    "\n",
    "        new_ann = {\n",
    "            'leftCrop': new_ann_left,\n",
    "            'rightCrop': new_ann_right\n",
    "        }\n",
    "        new_world_keypoints = pixel2world(new_ann['leftCrop'], new_ann['rightCrop'], cm)\n",
    "        new_median_x = np.median([wkp[0] for wkp in new_world_keypoints.values()])\n",
    "        new_median_y = np.median([wkp[1] for wkp in new_world_keypoints.values()])\n",
    "        new_median_z = np.median([wkp[2] for wkp in new_world_keypoints.values()])\n",
    "\n",
    "        weight_prediction = weight_estimator.predict(new_ann, camera_metadata)\n",
    "        input_sample = {\n",
    "            'keypoints': new_ann,\n",
    "            'cm': cm,\n",
    "            'stereo_pair_id': row.id,\n",
    "            'single_point_inference': True\n",
    "        }\n",
    "        nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "        normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "        tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "        original_weight_prediction = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "        \n",
    "\n",
    "        analysis_data['new_ann_list'].append(new_world_keypoints)\n",
    "        analysis_data['new_median_x'].append(new_median_x)\n",
    "        analysis_data['new_median_y'].append(new_median_y)\n",
    "        analysis_data['new_median_z'].append(new_median_z)\n",
    "        analysis_data['weight'].append(weight)\n",
    "        analysis_data['weight_prediction'].append(weight_prediction)\n",
    "        analysis_data['original_weight_prediction'].append(original_weight_prediction)\n",
    "        analysis_data['error_pct'].append((weight_prediction - weight) / weight)\n",
    "        analysis_data['original_error_pct'].append((original_weight_prediction - weight) / weight)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(-0.4, 0.4, 0.1)\n",
    "for idx in range(len(x_values) - 1):\n",
    "    x_low, x_high = x_values[idx], x_values[idx + 1]\n",
    "    x_mask = (analysis_df.new_median_x > x_low) & (analysis_df.new_median_x < x_high)\n",
    "    y_mask = (analysis_df.new_median_y > 1.8)# & (analysis_df.new_median_y < 1.3)\n",
    "    mask = x_mask * y_mask\n",
    "    error_pct = 100 * (analysis_df[mask].weight_prediction.mean() - analysis_df[mask].weight.mean()) / \\\n",
    "                analysis_df[mask].weight.mean()\n",
    "    print('X Range: {} <-> {}, Deviation: {}%'.format(round(x_low, 2), \n",
    "                                                      round(x_high, 2),\n",
    "                                                      round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = np.arange(-0.4, 0.4, 0.1)\n",
    "for idx in range(len(z_values) - 1):\n",
    "    z_low, z_high = z_values[idx], z_values[idx + 1]\n",
    "    z_mask = (analysis_df.new_median_z > z_low) & (analysis_df.new_median_z < z_high)\n",
    "    y_mask = (analysis_df.new_median_y > 1.8)# & (analysis_df.new_median_y < 1.3)\n",
    "    mask = z_mask * y_mask\n",
    "    error_pct = 100 * (analysis_df[mask].weight_prediction.mean() - analysis_df[mask].weight.mean()) / \\\n",
    "                analysis_df[mask].weight.mean()\n",
    "    print('X Range: {} <-> {}, Deviation: {}%'.format(round(x_low, 2), \n",
    "                                                      round(x_high, 2),\n",
    "                                                      round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(-0.4, 0.4, 0.1)\n",
    "for idx in range(len(x_values) - 1):\n",
    "    x_low, x_high = x_values[idx], x_values[idx + 1]\n",
    "    x_mask = (analysis_df.new_median_x > x_low) & (analysis_df.new_median_x < x_high)\n",
    "    y_mask = (analysis_df.new_median_y > 1.8)# & (analysis_df.new_median_y < 1.3)\n",
    "    mask = x_mask * y_mask\n",
    "    error_pct = 100 * (analysis_df[mask].original_weight_prediction.mean() - analysis_df[mask].weight.mean()) / \\\n",
    "                analysis_df[mask].weight.mean()\n",
    "    print('X Range: {} <-> {}, Deviation: {}%'.format(round(x_low, 2), \n",
    "                                                      round(x_high, 2),\n",
    "                                                      round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = np.arange(-0.4, 0.5, 0.1)\n",
    "for idx in range(len(z_values) - 1):\n",
    "    z_low, z_high = z_values[idx], z_values[idx + 1]\n",
    "    z_mask = (analysis_df.new_median_z > z_low) & (analysis_df.new_median_z < z_high)\n",
    "    y_mask = (analysis_df.new_median_y < 1.0)# & (analysis_df.new_median_y < 1.5)\n",
    "    mask = z_mask * y_mask\n",
    "    error_pct = 100 * (analysis_df[mask].original_weight_prediction.mean() - analysis_df[mask].weight.mean()) / \\\n",
    "                analysis_df[mask].weight.mean()\n",
    "    print('Y Range: {} <-> {}, Deviation: {}%'.format(round(z_low, 2), \n",
    "                                                      round(z_high, 2),\n",
    "                                                      round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "        return pixel2world(row.new_ann['leftCrop'], row.new_ann['rightCrop'], row.camera_metadata)\n",
    "\n",
    "df['new_ann'] = new_ann_list\n",
    "df['new_world_keypoints'] = df.apply(lambda x: get_world_keypoints(x), axis=1)\n",
    "df['new_median_depth'] = df.new_world_keypoints.apply(lambda x: np.median([wkp[1] for wkp in x.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[(df.median_depth < 0.75) & (df.akpd_score > 0.5) & (df.captured_at < '2019-09-27')].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data transforms so that we can run inference with neural network\n",
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "# Get neural network weights from sample training\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2019-11-08T00-13-09/nn_epoch_798.pb'\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "network = torch.load(model_f)\n",
    "\n",
    "weight_predictions, depths = [], []\n",
    "for idx, row in tdf.iterrows():\n",
    "    input_sample = {\n",
    "        'keypoints': row.new_ann,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    weight_prediction = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    weight_predictions.append(weight_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['pred_weight'] = weight_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(tdf.weight.values, tdf.pred_weight.values)\n",
    "plt.plot([0, 10000], [0, 10000], color='red')\n",
    "plt.xlim([0, 10000])\n",
    "plt.ylim([0, 10000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tdf.pred_weight.mean() - tdf.weight.mean()) / tdf.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = np.arange(0.5, 2.4, 0.1)\n",
    "for idx in range(len(depth_values) - 1):\n",
    "    low_depth, high_depth = depth_values[idx], depth_values[idx + 1]\n",
    "    mask = (tdf.new_median_depth > low_depth) & (tdf.new_median_depth < high_depth)\n",
    "    error_pct = 100 * (tdf[mask].pred_weight.mean() - tdf[mask].weight.mean()) / tdf[mask].weight.mean()\n",
    "    print('Depth Range: {}-{}, Deviation: {}%'.format(round(low_depth, 2), \n",
    "                                                     round(high_depth, 2),\n",
    "                                                     round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = np.arange(0.5, 2.4, 0.1)\n",
    "for idx in range(len(depth_values) - 1):\n",
    "    low_depth, high_depth = depth_values[idx], depth_values[idx + 1]\n",
    "    mask = (tdf.new_median_depth > low_depth) & (tdf.new_median_depth < high_depth)\n",
    "    error_pct = 100 * (tdf[mask].pred_weight_new.mean() - tdf[mask].weight.mean()) / tdf[mask].weight.mean()\n",
    "    print('Depth Range: {}-{}, Deviation: {}%'.format(round(low_depth, 2), \n",
    "                                                     round(high_depth, 2),\n",
    "                                                     round(error_pct, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate weight estimator class\n",
    "model_f = '/root/data/alok/biomass_estimation/playground/nn_epoch_253.pb'\n",
    "weight_estimator = WeightEstimator(model_f)\n",
    "\n",
    "# generate sample predictions\n",
    "weights = []\n",
    "for idx, row in tdf.iterrows():\n",
    "    keypoints, camera_metadata = row.new_ann, row.camera_metadata\n",
    "    weight_prediction = weight_estimator.predict(keypoints, camera_metadata)\n",
    "    weights.append(weight_prediction)\n",
    "    if len(weights) % 1000 == 0:\n",
    "        print(len(weights))\n",
    "\n",
    "tdf['pred_weight_new'] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(tdf.weight.values, tdf.pred_weight_new.values)\n",
    "plt.plot([0, 10000], [0, 10000], color='red')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tdf.pred_weight_new.mean() - tdf.weight.mean()) / tdf.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tdf.head(11).iterrows():\n",
    "    input_sample = {\n",
    "        'keypoints': row.keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    \n",
    "    keypoint_arr = get_keypoint_arr(row.keypoints, row.camera_metadata, recover_original_depth=True)\n",
    "    \n",
    "wkps = tensorized_kps['kp_input'].numpy()[0]\n",
    "wkp_1 = (0.5 * 0.1) / wkps[:, 2]\n",
    "x = wkps[:, 0] * wkp_1 / 0.5\n",
    "y = wkps[:, 1] * wkp_1 / 0.5\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(x, y, color='blue')\n",
    "\n",
    "x, y = keypoint_arr[:, 0], keypoint_arr[:, 2]\n",
    "plt.scatter(x, y, color='red')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df.keypoint_arr.iloc[0]\n",
    "arr -= arr.mean(axis=0)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(np.dot(arr.T, arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkps = np.dot(eigen_vectors.T, arr.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_3D_coordinates(wkps):\n",
    "\n",
    "    # translate fish to origin\n",
    "    v = np.mean(wkps[:8], axis=0)\n",
    "    wkps -= v\n",
    "\n",
    "    # perform PCA decomposition and rotate with respect to new axes\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(np.dot(wkps.T, wkps))\n",
    "    wkps = np.dot(eigen_vectors.T, wkps.T).T\n",
    "\n",
    "    return wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_3D_coordinates(df.keypoint_arr.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_rotate_and_translate(wkps, random_x_addition, random_y_addition,\n",
    "                                  random_z_addition, yaw, pitch, roll):\n",
    "\n",
    "    # convert to radians\n",
    "    yaw, pitch, roll = [theta * np.pi / 180.0 for theta in [yaw, pitch, roll]]\n",
    "    \n",
    "    R_yaw = np.array([\n",
    "        [np.cos(yaw), -np.sin(yaw), 0],\n",
    "        [np.sin(yaw), np.cos(yaw), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    R_pitch = np.array([\n",
    "        [np.cos(pitch), 0, np.sin(pitch)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(pitch), 0, np.cos(pitch)]\n",
    "    ])\n",
    "\n",
    "    R_roll = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(roll), -np.sin(roll)],\n",
    "        [0, np.sin(roll), np.cos(roll)]\n",
    "    ])\n",
    "\n",
    "    R = np.dot(R_yaw, (np.dot(R_pitch, R_roll)))\n",
    "    wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    # perform translation\n",
    "    wkps[:, 0] += random_x_addition\n",
    "    wkps[:, 1] += random_y_addition\n",
    "    wkps[:, 2] += random_z_addition\n",
    "    return wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wkps = randomly_rotate_and_translate(wkps, 0.2, 1.0, 0.3, -30, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wkps -= new_wkps.mean(axis=0)\n",
    "_, eigen_vectors = np.linalg.eig(np.dot(new_wkps.T, new_wkps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "x, y = wkps[:, 0], wkps[:, 2]\n",
    "plt.scatter(x, y, color='blue')\n",
    "x, y = new_wkps[:, 0], new_wkps[:, 2]\n",
    "plt.scatter(x, y, color='red')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
