{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "from research.weight_estimation.gtsf_data.gtsf_dataset import GTSFDataset\n",
    "from research.weight_estimation.gtsf_data.gtsf_augmentation import GTSFAugmentation\n",
    "from research.weight_estimation.keypoint_utils.body_parts import BodyParts\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load GTSF Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe loaded!\n",
      "Dataset preparation beginning...\n",
      "3D spatial information added!\n",
      "Adding AKPD scores...\n",
      "Converting world keypoints to matrix form...\n"
     ]
    }
   ],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-03-01', '2020-02-10', akpd_scorer_url)\n",
    "df = gtsf_dataset.get_prepared_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perform Augmentation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Percentage completed: 0.0\n",
      "Percentage completed: 0.68\n",
      "Percentage completed: 1.35\n",
      "Percentage completed: 2.03\n",
      "Percentage completed: 2.7\n",
      "Percentage completed: 3.38\n",
      "Percentage completed: 4.06\n",
      "Percentage completed: 4.73\n",
      "Percentage completed: 5.41\n",
      "Percentage completed: 6.08\n",
      "Percentage completed: 6.76\n",
      "Percentage completed: 7.44\n",
      "Percentage completed: 8.11\n",
      "Percentage completed: 8.79\n",
      "Percentage completed: 9.46\n",
      "Percentage completed: 10.14\n",
      "Percentage completed: 10.82\n",
      "Percentage completed: 11.49\n",
      "Percentage completed: 12.17\n",
      "Percentage completed: 12.84\n",
      "Percentage completed: 13.52\n",
      "Percentage completed: 14.2\n",
      "Percentage completed: 14.87\n",
      "Percentage completed: 15.55\n",
      "Percentage completed: 16.22\n",
      "Percentage completed: 16.9\n",
      "Percentage completed: 17.58\n",
      "Percentage completed: 18.25\n",
      "Percentage completed: 18.93\n",
      "Percentage completed: 19.61\n",
      "Percentage completed: 20.28\n",
      "Percentage completed: 20.96\n",
      "Percentage completed: 21.63\n",
      "Percentage completed: 22.31\n",
      "Percentage completed: 22.99\n",
      "Percentage completed: 23.66\n",
      "Percentage completed: 24.34\n",
      "Percentage completed: 25.01\n",
      "Percentage completed: 25.69\n",
      "Percentage completed: 26.37\n",
      "Percentage completed: 27.04\n",
      "Percentage completed: 27.72\n",
      "Percentage completed: 28.39\n",
      "Percentage completed: 29.07\n",
      "Percentage completed: 29.75\n",
      "Percentage completed: 30.42\n",
      "Percentage completed: 31.1\n",
      "Percentage completed: 31.77\n",
      "Percentage completed: 32.45\n",
      "Percentage completed: 33.13\n",
      "Percentage completed: 33.8\n",
      "Percentage completed: 34.48\n",
      "Percentage completed: 35.15\n",
      "Percentage completed: 35.83\n",
      "Percentage completed: 36.51\n",
      "Percentage completed: 37.18\n",
      "Percentage completed: 37.86\n",
      "Percentage completed: 38.53\n",
      "Percentage completed: 39.21\n",
      "Percentage completed: 39.89\n",
      "Percentage completed: 40.56\n",
      "Percentage completed: 41.24\n",
      "Percentage completed: 41.91\n",
      "Percentage completed: 42.59\n",
      "Percentage completed: 43.27\n",
      "Percentage completed: 43.94\n",
      "Percentage completed: 44.62\n",
      "Percentage completed: 45.29\n",
      "Percentage completed: 45.97\n",
      "Percentage completed: 46.65\n",
      "Percentage completed: 47.32\n",
      "Percentage completed: 48.0\n",
      "Percentage completed: 48.67\n",
      "Percentage completed: 49.35\n",
      "Percentage completed: 50.03\n",
      "Percentage completed: 50.7\n",
      "Percentage completed: 51.38\n",
      "Percentage completed: 52.06\n",
      "Percentage completed: 52.73\n",
      "Percentage completed: 53.41\n",
      "Percentage completed: 54.08\n",
      "Percentage completed: 54.76\n",
      "Percentage completed: 55.44\n",
      "Percentage completed: 56.11\n",
      "Percentage completed: 56.79\n",
      "Percentage completed: 57.46\n",
      "Percentage completed: 58.14\n",
      "Percentage completed: 58.82\n",
      "Percentage completed: 59.49\n",
      "Percentage completed: 60.17\n",
      "Percentage completed: 60.84\n",
      "Percentage completed: 61.52\n",
      "Percentage completed: 62.2\n",
      "Percentage completed: 62.87\n",
      "Percentage completed: 63.55\n",
      "Percentage completed: 64.22\n",
      "Percentage completed: 64.9\n",
      "Percentage completed: 65.58\n",
      "Percentage completed: 66.25\n",
      "Percentage completed: 66.93\n",
      "Percentage completed: 67.6\n",
      "Percentage completed: 68.28\n",
      "Percentage completed: 68.96\n",
      "Percentage completed: 69.63\n",
      "Percentage completed: 70.31\n",
      "Percentage completed: 70.98\n",
      "Percentage completed: 71.66\n",
      "Percentage completed: 72.34\n",
      "Percentage completed: 73.01\n",
      "Percentage completed: 73.69\n",
      "Percentage completed: 74.36\n",
      "Percentage completed: 75.04\n",
      "Percentage completed: 75.72\n",
      "Percentage completed: 76.39\n",
      "Percentage completed: 77.07\n",
      "Percentage completed: 77.74\n",
      "Percentage completed: 78.42\n",
      "Percentage completed: 79.1\n",
      "Percentage completed: 79.77\n",
      "Percentage completed: 80.45\n",
      "Percentage completed: 81.12\n",
      "Percentage completed: 81.8\n",
      "Percentage completed: 82.48\n",
      "Percentage completed: 83.15\n",
      "Percentage completed: 83.83\n",
      "Percentage completed: 84.51\n",
      "Percentage completed: 85.18\n",
      "Percentage completed: 85.86\n",
      "Percentage completed: 86.53\n",
      "Percentage completed: 87.21\n",
      "Percentage completed: 87.89\n",
      "Percentage completed: 88.56\n",
      "Percentage completed: 89.24\n",
      "Percentage completed: 89.91\n",
      "Percentage completed: 90.59\n",
      "Percentage completed: 91.27\n",
      "Percentage completed: 91.94\n",
      "Percentage completed: 92.62\n",
      "Percentage completed: 93.29\n",
      "Percentage completed: 93.97\n",
      "Percentage completed: 94.65\n",
      "Percentage completed: 95.32\n",
      "Percentage completed: 96.0\n",
      "Percentage completed: 96.67\n",
      "Percentage completed: 97.35\n",
      "Percentage completed: 98.03\n",
      "Percentage completed: 98.7\n",
      "Percentage completed: 99.38\n",
      "(147920, 14)\n"
     ]
    }
   ],
   "source": [
    "df = df[(df.captured_at < '2019-09-20') & (df.median_depth < 1.0) & (df.akpd_score > 0.5)]\n",
    "gtsf_augmentation = GTSFAugmentation(df)\n",
    "y_bounds, max_jitter_std, trials = (0.5, 3.0), 10, 10\n",
    "augmented_df = gtsf_augmentation.generate_augmented_dataset(y_bounds, max_jitter_std, trials, random_seed=0)\n",
    "print(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotation'] = df.keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/20200520_gtsf_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left_image_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Train / Test Split </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stabilized_input(augmented_df, mask=None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        X = augmented_df[mask].wkps.values\n",
    "        y = 1e-4 * augmented_df[mask].weight.values\n",
    "    else:\n",
    "        X = augmented_df.wkps.values\n",
    "        y = 1e-4 * augmented_df.weight.values\n",
    "    X = np.concatenate(X).reshape(X.shape[0], 8, 3)\n",
    "    \n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, :, 0] = 0.5 * X[:, :, 0] / X[:, :, 1]\n",
    "    X_new[:, :, 1] = 0.5 * X[:, :, 2] / X[:, :, 1]\n",
    "    X_new[:, :, 2] = 0.05 / X[:, :, 1]\n",
    "    X_new = X_new.reshape(-1, 24)\n",
    "    return X_new, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "fish_ids = augmented_df.fish_id.unique()\n",
    "train_pct, val_pct, test_pct = 0.8, 0.1, 0.1\n",
    "train_cnt, val_cnt, test_cnt = np.random.multinomial(len(fish_ids), [train_pct, val_pct, test_pct])\n",
    "assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "np.random.shuffle(assignments)\n",
    "train_fish_ids = fish_ids[np.where(assignments == 0)]\n",
    "val_fish_ids = fish_ids[np.where(assignments == 1)]\n",
    "test_fish_ids = fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "train_mask = augmented_df.fish_id.isin(train_fish_ids)\n",
    "val_mask = augmented_df.fish_id.isin(val_fish_ids)\n",
    "test_mask = augmented_df.fish_id.isin(test_fish_ids)\n",
    "\n",
    "X_train, y_train = generate_stabilized_input(augmented_df, train_mask)\n",
    "X_val, y_val = generate_stabilized_input(augmented_df, val_mask)\n",
    "X_test, y_test = generate_stabilized_input(augmented_df, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Neural Network in Keras </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(24,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "# x = Dense(64, activation='relu')(inputs)\n",
    "# x = Dense(128, activation='relu')(inputs)\n",
    "x = Dense(256, activation='relu')(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "pred = Dense(1)(x)\n",
    "model = Model(input=inputs, output=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                           min_delta=0,\n",
    "                                           patience=10,\n",
    "                                           verbose=0, \n",
    "                                           mode='auto')]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks, batch_size=64, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_breakdown(df, vals, field):\n",
    "    for idx in range(len(vals) - 1):\n",
    "        mask = (df[field] > vals[idx]) & (df[field] < vals[idx + 1])\n",
    "        error_pct = (df[mask].y_pred.mean() - df[mask].weight.mean()) / (df[mask].weight.mean())\n",
    "        abs_error_pct = np.mean(np.abs((df[mask].y_pred - df[mask].weight) / df[mask].weight))\n",
    "        print('Errors for {} in range {} <-> {}: {}, {}'.format(\n",
    "            field,\n",
    "            round(vals[idx], 2), \n",
    "            round(vals[idx + 1], 2),\n",
    "            round(100*error_pct, 2),\n",
    "            round(100*abs_error_pct, 2)\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get error breakdown by depth on current augmented dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_stabilized_input(augmented_df)\n",
    "augmented_df['y_pred'] = 1e4 * model.predict(X).squeeze().astype(float)\n",
    "generate_error_breakdown(augmented_df, np.arange(0, 3.1, 0.1), 'mean_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs((augmented_df.y_pred - augmented_df.weight) / augmented_df.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bounds, jitter, trials = (0.5, 3.0), 20, 5\n",
    "augmented_df_2 = gtsf_augmentation.generate_augmented_dataset(y_bounds, jitter, trials, random_seed=0)\n",
    "X_oos, y_oos = generate_stabilized_input(augmented_df_2)\n",
    "augmented_df_2['y_pred'] = 1e4 * model.predict(X_oos).squeeze().astype(float)\n",
    "generate_error_breakdown(augmented_df_2, np.arange(0, 2.3, 0.1), 'mean_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = ((augmented_df.y_pred - augmented_df.weight) / augmented_df.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(errs.values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/playground/model_keras_reduced_jitter.h5'\n",
    "model.save(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "s3_access_utils.s3_client.upload_file(f, 'aquabyte-models', 'playground/20200520_model_keras_reduced_jitter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply old model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.captured_at < '2019-09-20') & (df.median_depth < 1.0) & (df.akpd_score > 0.5)]\n",
    "gtsf_augmentation = GTSFAugmentation(df)\n",
    "y_bounds, jitter, trials = (0.7, 1.0), 0, 1\n",
    "augmented_df = gtsf_augmentation.generate_augmented_dataset(y_bounds, jitter, trials, random_seed=0)\n",
    "print(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.old.weight_estimator_old import NormalizedStabilityTransform, Network\n",
    "from research.weight_estimation.old.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2019-11-08T00-13-09/nn_epoch_798.pb'\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "network = torch.load(model_f)\n",
    "\n",
    "\n",
    "weight_predictions = []\n",
    "count = 0\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    \n",
    "    input_sample = {\n",
    "        'keypoints': row.ann,\n",
    "        'cm': row.cm,\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "    \n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    weight_prediction = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    weight_predictions.append(weight_prediction)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['old_y_pred'] = weight_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs((augmented_df.old_y_pred - augmented_df.weight) / augmented_df.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_error_breakdown(augmented_df, np.arange(0, 1.0, 0.05), 'mean_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = augmented_df.original_wkps.iloc[2]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X[:, 0], X[:, 2])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "X_o = augmented_df.original_wkps.iloc[idx]\n",
    "X = augmented_df.centered_wkps.iloc[idx]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X_o[:, 0], X_o[:, 2], color='blue')\n",
    "plt.scatter(X[:, 0], X[:, 2], color='red')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
