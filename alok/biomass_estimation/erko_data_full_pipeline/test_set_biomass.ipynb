{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load annotations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import skimage.io\n",
    "\n",
    "LIB_DIRECTORY = '/root/alok/repos/cv_research/lib/'\n",
    "sys.path.insert(0, os.path.join(LIB_DIRECTORY, 'maskrcnn'))\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "import mrcnn.model as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "from mrcnn.model import log\n",
    "import mcoco.coco as coco\n",
    "# import mextra.utils as extra_utils\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "LIB_DIRECTORY = '/root/alok/repos/cv_research/lib/'\n",
    "sys.path.insert(0, LIB_DIRECTORY)\n",
    "\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "BASE_DIR = '/root/data/models/erko/mask_rcnn_instance_segmentation'\n",
    "DATA_DIR = '/root/data/erko/'\n",
    "WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"logs\", \"body_part_segmentation_20181031_21H02\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '/root/alok/data/images/annotation_file_test_set.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_output = json.load(open(annotation_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualize the Results </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = COCO(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annotations(image_id):    \n",
    "    image_data = coco.loadImgs([image_id])[0]\n",
    "    image_file_path = image_data['local_path']\n",
    "    annotation_ids = coco.getAnnIds(imgIds=[image_id])\n",
    "    annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "    # load and display instance annotations\n",
    "    image = skimage.io.imread(image_file_path)\n",
    "    f, ax = plt.subplots(1, figsize=(20, 20))\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    coco.showAnns(annotations)\n",
    "    \n",
    "    # display bounding boxes\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        rectangle = Rectangle((bbox[1], bbox[0]), bbox[3]-bbox[1], bbox[2]-bbox[0], edgecolor='w', facecolor=None, fill=False, linestyle='--', linewidth=2)\n",
    "        ax.add_patch(rectangle)\n",
    "#         category_id = ann['category_id']\n",
    "        category_id = ann['id']\n",
    "        ax.text(bbox[1], bbox[0] - 10, category_id, fontsize=16, color='w')\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotations(527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotations(2222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Determine left-right matches (bipartite graph approach) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = coco.getImgIds()\n",
    "images = coco.loadImgs(image_ids)\n",
    "stereo_frame_pairs = {}\n",
    "for image in images:\n",
    "    local_path = image['local_path']\n",
    "    sfp_id = int(local_path.split('/')[-3])\n",
    "    side = 'left' if 'left' in local_path else 'right'\n",
    "    if sfp_id not in list(stereo_frame_pairs.keys()):\n",
    "        stereo_frame_pairs[sfp_id] = {}\n",
    "    stereo_frame_pairs[sfp_id][side] = image['id']\n",
    "    \n",
    "stereo_frame_pairs = list(stereo_frame_pairs.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fish_detections(image_id, full_fish_category_id=1):\n",
    "    \n",
    "    # get all annotations corresponding to fish detections\n",
    "    \n",
    "    fish_detections = []\n",
    "    annotation_ids = coco.getAnnIds(imgId=[image_id])\n",
    "    annotations = coco.loadAnns(annotation_ids)\n",
    "    full_fish_annotations = [ann for ann in annotations if ann['category_id'] == full_fish_category_id]\n",
    "    for full_fish_annotation in full_fish_annotations:\n",
    "        fish_detection = {\n",
    "            'full_fish_annotation': full_fish_annotation, \n",
    "            'body_part_annotations': []\n",
    "        }\n",
    "        fish_detection['bounding_box'] = transform_coco_bbox(full_fish_annotation['bbox'])\n",
    "        fish_detections.append(fish_detection)\n",
    "        \n",
    "    # for each fish detection, append\n",
    "    body_part_annotations = [ann for ann in annotations if ann['category_id'] != FULL_FISH_CATEGORY_ID]\n",
    "    for body_part_annotation in body_part_annotations:\n",
    "        body_part_centroid = get_centroid_from_coco_bbox(body_part_annotation['bbox'])\n",
    "        body_part_matched_to_fish_detection = False\n",
    "        for fish_detection in fish_detections:\n",
    "            body_part_inside_detection = \\\n",
    "                determine_if_body_part_falls_inside_detection(body_part_centroid, fish_detection['bounding_box'])\n",
    "            if body_part_inside_detection:\n",
    "                fish_detection['body_part_annotations'].append(body_part_annotation)\n",
    "                body_part_matched_to_fish_detection = True\n",
    "        if not body_part_matched_to_fish_detection:\n",
    "            unmatched_body_parts.append(body_part_annotation)\n",
    "    fish_detections.extend(fish_detections_in_image)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_right_matching(left_fish_detections, right_fish_detections):\n",
    "\n",
    "    # let's use x1, x2 to match bboxes\n",
    "    left_corners = []\n",
    "    for ann in left_fish_detections:\n",
    "        bbox = ann['bbox']\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        left_corners.append(corner)\n",
    "\n",
    "    right_corners = []\n",
    "    for ann in right_fish_detections:\n",
    "        bbox = ann['bbox']\n",
    "        # centroid = [(bbox[3] - bbox[1])/2.0, (bbox[2] - bbox[0])/2.0]\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        right_corners.append(corner)\n",
    "\n",
    "    # euclidean distance in (x1, x2) space\n",
    "    cost_matrix = euclidean_distances(left_corners, right_corners)\n",
    "    \n",
    "    # hungarian algorithm to minimize weights in bipartite graph\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    matched_fish_detections = []\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        matched_fish_detections.append((left_fish_detections[r], right_fish_detections[c]))\n",
    "    \n",
    "    return matched_fish_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_body_parts(annotations, categories, whole_fish=\"salmon\"):\n",
    "    \"\"\" take the predictions on a single frame and associate body parts with whole salmon using a unique identifier\"\"\"\n",
    "    whole_fish_id = [cat for cat in categories if cat['name'] == whole_fish][0]['id']\n",
    "    fish = []\n",
    "    body_parts = []\n",
    "    fish_counter = 0\n",
    "\n",
    "    # first we separate whole salmon from body parts and give a unique identifier\n",
    "    for annotation in annotations:\n",
    "        if annotation['category_id'] == whole_fish_id:\n",
    "            annotation['fish_id'] = fish_counter\n",
    "            fish.append(annotation)\n",
    "            fish_counter += 1\n",
    "        else:\n",
    "            body_parts.append(annotation)\n",
    "\n",
    "    # second we match body parts with whole salmon\n",
    "    # @TODO (Thomas) this is not great because it does not take edges cases into account.\n",
    "    for part in body_parts:\n",
    "        bbox = part['bbox']\n",
    "        part_centroid = [np.mean([bbox[0], bbox[2]]), np.mean([bbox[1], bbox[3]])]\n",
    "        for f in fish:\n",
    "            bbox = f['bbox']\n",
    "            if bbox[0] < part_centroid[0] < bbox[2] and bbox[1] < part_centroid[1] < bbox[3]:\n",
    "                part['fish_id'] = f['fish_id']\n",
    "                break\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "FOCAL_LENGTH = 0.0107\n",
    "BASELINE = 0.135\n",
    "PIXEL_SIZE_M = 3.45 * 1e-6\n",
    "FOCAL_LENGTH_PIXEL = FOCAL_LENGTH / PIXEL_SIZE_M\n",
    "IMAGE_SENSOR_WIDTH = 0.01412\n",
    "IMAGE_SENSOR_HEIGHT = 0.01412\n",
    "RANDOM_STATE = 170\n",
    "DENSITY = 1e6  # g per cubic meter\n",
    "\n",
    "LENGTH_FACTOR = 44.0  # where is this coming from? DATA\n",
    "COST_THRESHOLD = 100.0  # another magic number\n",
    "\n",
    "MODEL_PATH = '/root/data/models/biomass/model.pickle'\n",
    "COMPONENTS_PATH = '/root/data/models/biomass/components.npy'\n",
    "IQRS_PATH = '/root/data/models/biomass/iqrs.pkl'\n",
    "\n",
    "CATEGORIES = [\n",
    "    {'id': 1, 'name': 'salmon'},\n",
    "    {'id': 2, 'name': 'Head'},\n",
    "    {'id': 3, 'name': 'Caudal Fin'},\n",
    "    {'id': 4, 'name': 'Dorsal Fin'},\n",
    "    {'id': 5, 'name': 'Adipose Fin'},\n",
    "    {'id': 6, 'name': 'Anal Fin'},\n",
    "    {'id': 7, 'name': 'Pelvic Fin'},\n",
    "    {'id': 8, 'name': 'Pectoral Fin'},\n",
    "    {'id': 9, 'name': 'Eye'}\n",
    "]\n",
    "\n",
    "MINIMAL_BODY_PART_LIST = [2, 4, 6]\n",
    "\n",
    "\n",
    "def convert_to_world_point(x, y, d):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    # TODO (@Thomas) this is hard coded and this bad....\n",
    "    image_center_x = 3000 / 2.0  # depth_map.shape[1] / 2.0\n",
    "    image_center_y = 4096 / 2.0  # depth_map.shape[0] / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (IMAGE_SENSOR_WIDTH / 3000)\n",
    "    sensor_z = px_z * (IMAGE_SENSOR_HEIGHT / 4096)\n",
    "\n",
    "    # d = depth_map[y, x]\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / FOCAL_LENGTH\n",
    "    world_z = (world_y * sensor_z) / FOCAL_LENGTH\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "\n",
    "def depth_from_disp(disp):\n",
    "    depth = FOCAL_LENGTH_PIXEL*BASELINE / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def match_body_parts(annotations, categories, whole_fish=\"salmon\"):\n",
    "    \"\"\" take the predictions on a single frame and associate body parts with whole salmon using a unique identifier\"\"\"\n",
    "    whole_fish_id = [cat for cat in categories if cat['name'] == whole_fish][0]['id']\n",
    "    fish = []\n",
    "    body_parts = []\n",
    "    fish_counter = 0\n",
    "\n",
    "    # first we separate whole salmon from body parts and give a unique identifier\n",
    "    for annotation in annotations:\n",
    "        if annotation['category_id'] == whole_fish_id:\n",
    "            annotation['fish_id'] = fish_counter\n",
    "            fish.append(annotation)\n",
    "            fish_counter += 1\n",
    "        else:\n",
    "            body_parts.append(annotation)\n",
    "\n",
    "    # second we match body parts with whole salmon\n",
    "    # @TODO (Thomas) this is not great because it does not take edges cases into account.\n",
    "    for part in body_parts:\n",
    "        bbox = part['bbox']\n",
    "        part_centroid = [np.mean([bbox[0], bbox[2]]), np.mean([bbox[1], bbox[3]])]\n",
    "        for f in fish:\n",
    "            bbox = f['bbox']\n",
    "            if bbox[0] < part_centroid[0] < bbox[2] and bbox[1] < part_centroid[1] < bbox[3]:\n",
    "                part['fish_id'] = f['fish_id']\n",
    "                break\n",
    "\n",
    "    matched_body_pars = [ann for ann in annotations if 'fish_id' in ann.keys()]\n",
    "    return matched_body_pars\n",
    "\n",
    "\n",
    "def left_right_matching(left_annotations, right_annotations, categories, whole_fish=\"salmon\"):\n",
    "    \"\"\"match the bboxes. Return a list of matched bboxes\"\"\"\n",
    "    # get the salmon category id\n",
    "    whole_fish_id = [cat for cat in categories if cat['name'] == whole_fish][0]['id']\n",
    "\n",
    "    # let's use x1, x2 to match bboxes\n",
    "    left_centroids = []\n",
    "    left_ids = []\n",
    "    for ann in left_annotations:\n",
    "        if ann['category_id'] != whole_fish_id:\n",
    "            continue\n",
    "        bbox = ann['bbox']\n",
    "        # centroid = [(bbox[3] - bbox[1])/2.0, (bbox[2] - bbox[0])/2.0]\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        left_ids.append(ann['fish_id'])\n",
    "        left_centroids.append(corner)\n",
    "\n",
    "    right_centroids = []\n",
    "    right_ids = []\n",
    "    for ann in right_annotations:\n",
    "        if ann['category_id'] != whole_fish_id:\n",
    "            continue\n",
    "        bbox = ann['bbox']\n",
    "        # centroid = [(bbox[3] - bbox[1])/2.0, (bbox[2] - bbox[0])/2.0]\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        right_ids.append(ann['fish_id'])\n",
    "        right_centroids.append(corner)\n",
    "\n",
    "    # euclidean distance in (x1, x2) space\n",
    "    cost_matrix = euclidean_distances(left_centroids, right_centroids)\n",
    "    \n",
    "    # hungarian algorithm to minimize weights in bipartite graph\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    matched_annotations = []\n",
    "    for (r, c) in zip(row_ind, col_ind):\n",
    "        if cost_matrix[r, c] < COST_THRESHOLD:\n",
    "            left_matched_parts = [l for l in left_annotations if l['fish_id'] == left_ids[r] and l['category_id'] != whole_fish_id]\n",
    "            right_matched_parts = [r for r in right_annotations if r['fish_id'] == right_ids[c] and r['category_id'] != whole_fish_id]\n",
    "            matched_annotations.append([left_matched_parts, right_matched_parts])\n",
    "\n",
    "    return matched_annotations\n",
    "\n",
    "def weight_estimator(left_matched_parts, right_matched_parts, categories, model, components, iqrs):\n",
    "    left_category_ids = [ann['category_id'] for ann in left_matched_parts]\n",
    "    right_category_ids = [ann['category_id'] for ann in right_matched_parts]\n",
    "    \n",
    "    left_valid = all([body_part in left_category_ids for body_part in MINIMAL_BODY_PART_LIST])\n",
    "    right_valid = all([body_part in right_category_ids for body_part in MINIMAL_BODY_PART_LIST])\n",
    "    \n",
    "    if left_valid and right_valid:\n",
    "        \"\"\"take left and right body parts and calculates the weights\"\"\"\n",
    "        # first check that all the parts are here\n",
    "        left_parts_ids = list(set([ann['category_id'] for ann in left_matched_parts]))\n",
    "        right_parts_ids = list(set([ann['category_id'] for ann in right_matched_parts]))\n",
    "        \n",
    "        # create observation\n",
    "        category_ids = [str(cat['id']) for cat in categories if cat['name'] != 'salmon']\n",
    "        distances = [c + k for (i, c) in enumerate(category_ids) for k in category_ids[i + 1:]]\n",
    "        dataset = {}\n",
    "        for d in distances:\n",
    "            dataset[d] = []\n",
    "\n",
    "        # calculate disparities & world coordinates\n",
    "        world_coordinates = {}\n",
    "        for cat in categories:\n",
    "            if cat['name'] == 'salmon' or cat['id'] not in MINIMAL_BODY_PART_LIST:\n",
    "                continue\n",
    "            # calculate left centroid\n",
    "            \n",
    "            left_part = [part for part in left_matched_parts if part['category_id'] == cat['id']][0]\n",
    "            seg = left_part['segmentation'][0]\n",
    "            poly = np.array(seg).reshape((int(len(seg) / 2), 2))\n",
    "            p = [(r[0], r[1]) for r in poly]\n",
    "            left_mask = Image.new('L', (4096, 3000), 0)\n",
    "            ImageDraw.Draw(left_mask).polygon(p, outline=1, fill=1)\n",
    "            left_mask = np.array(left_mask)\n",
    "            x, y = np.nonzero(left_mask)\n",
    "            left_centroid = [np.mean(x), np.mean(y)]\n",
    "\n",
    "            # calculate right centroid\n",
    "            right_part = [part for part in right_matched_parts if part['category_id'] == cat['id']][0]\n",
    "            seg = right_part['segmentation'][0]\n",
    "            poly = np.array(seg).reshape((int(len(seg) / 2), 2))\n",
    "            p = [(r[0], r[1]) for r in poly]\n",
    "            right_mask = Image.new('L', (4096, 3000), 0)\n",
    "            ImageDraw.Draw(right_mask).polygon(p, outline=1, fill=1)\n",
    "            right_mask = np.array(right_mask)\n",
    "            x, y = np.nonzero(right_mask)\n",
    "            right_centroid = [np.mean(x), np.mean(y)]\n",
    "\n",
    "            # calculate disparity\n",
    "            disparities = np.abs(left_centroid[1] - right_centroid[1])\n",
    "            depth = depth_from_disp(disparities)\n",
    "            world_coordinates[str(cat['id'])] = convert_to_world_point(left_centroid[0], left_centroid[1], depth)\n",
    "\n",
    "        # now calculate the pairwise distances\n",
    "        for pair in dataset.keys():\n",
    "            cat0, cat1 = pair[0], pair[1]\n",
    "            dist = np.linalg.norm(world_coordinates[cat0] - world_coordinates[cat1])\n",
    "            dataset[pair].append(dist)\n",
    "        \n",
    "        # TODO (@Thomas) probably no reasons to use pandas here\n",
    "        df = pd.DataFrame(dataset)\n",
    "        for col in df.columns.tolist():\n",
    "            df[col] = df[col] / iqrs[col]\n",
    "        \n",
    "        pidx = np.indices((df.shape[1], df.shape[1])).reshape(2, -1)\n",
    "        lcol = pd.MultiIndex.from_product([df.columns, df.columns],  names=[df.columns.name, df.columns.name])\n",
    "        X = pd.DataFrame(df.values[:, pidx[0]] * df.values[:, pidx[1]],  columns=lcol)\n",
    "        \n",
    "\n",
    "        # load pca components\n",
    "        newX = np.dot(X, components.T)\n",
    "\n",
    "        weight = model.predict(newX)\n",
    "        return weight\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = coco.getImgIds()\n",
    "images = coco.loadImgs(image_ids)\n",
    "stereo_frame_pairs = {}\n",
    "for image in images:\n",
    "    local_path = image['local_path']\n",
    "    sfp_id = int(local_path.split('/')[-3])\n",
    "    side = 'left' if 'left' in local_path else 'right'\n",
    "    if sfp_id not in list(stereo_frame_pairs.keys()):\n",
    "        stereo_frame_pairs[sfp_id] = {}\n",
    "    stereo_frame_pairs[sfp_id][side] = image['id']\n",
    "    \n",
    "stereo_frame_pairs = list(stereo_frame_pairs.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.load(MODEL_PATH)\n",
    "components = np.load(COMPONENTS_PATH)\n",
    "iqrs = pickle.load(open(IQRS_PATH, 'rb'))\n",
    "weights = []\n",
    "for i, stereo_frame_pair in enumerate(stereo_frame_pairs):\n",
    "    left_image_id = stereo_frame_pair['left']\n",
    "    left_image = coco.loadImgs([left_image_id])\n",
    "    left_annotation_ids = coco.getAnnIds(imgIds=[left_image_id])\n",
    "    left_annotations = coco.loadAnns(left_annotation_ids)\n",
    "    \n",
    "    right_image_id = stereo_frame_pair['right']\n",
    "    right_image = coco.loadImgs([left_image_id])\n",
    "    right_annotation_ids = coco.getAnnIds(imgIds=[right_image_id])\n",
    "    right_annotations = coco.loadAnns(right_annotation_ids)\n",
    "    categories = CATEGORIES\n",
    "    filtered_categories = [kv for kv in categories if kv['id'] in MINIMAL_BODY_PART_LIST]\n",
    "    \n",
    "    left_annotations = match_body_parts(left_annotations, categories)\n",
    "    right_annotations = match_body_parts(right_annotations, categories)\n",
    "    if left_annotations and right_annotations:\n",
    "        matched_annotations = left_right_matching(left_annotations, right_annotations, categories)\n",
    "        for pair in matched_annotations:\n",
    "            if pair[0] and pair[1]:\n",
    "                weight = weight_estimator(pair[0], pair[1], filtered_categories, model, components, iqrs)\n",
    "                if weight and 1000 < weight < 6000:\n",
    "                    weights.append(weight)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(float(i) / len(stereo_frame_pairs))\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.squeeze(weights))\n",
    "plt.xlabel('Biomass (grams)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.squeeze(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
