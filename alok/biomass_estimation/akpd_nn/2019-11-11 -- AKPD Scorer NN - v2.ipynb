{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.visualize import Visualizer, _normalize_world_keypoints\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from aquabyte.data_loader import BODY_PARTS, KeypointsDataset, NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.akpd import AKPD\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from keypoint_annotations\n",
    "    where keypoints is not null\n",
    "    and keypoints -> 'leftCrop' is not null\n",
    "    and keypoints -> 'rightCrop' is not null\n",
    "    limit 10000;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create data transforms </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeCentered2D(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the 2D left and right keypoints such that:\n",
    "        (1) The center of the left image 2D keypoints is located at the center of the left image\n",
    "            (i.e. 2D translation)\n",
    "        (2) The left image keypoints are possibly flipped such that the upper-lip x-coordinate \n",
    "            is greater than the tail-notch coordinate. This is done to reduce the total number of \n",
    "            spatial orientations the network must learn from -> reduces the training size\n",
    "        (3) The left image keypoints are then rotated such that upper-lip is located on the x-axis.\n",
    "            As in (2), this is done to reduce the total number of spatial orientations the network \n",
    "            must learn from -> reduces the training size\n",
    "        (4) Rescale all left image keypoints by some random number between 'lo' and 'hi' args\n",
    "        (5) Apply Gaussian random noise \"jitter\" to each keypoint to mimic annotation error\n",
    "        (5) For all transformations above, the right image keypoint coordinates are accordingly\n",
    "            transformed such that the original disparity values are preserved for all keypoints\n",
    "            (or adjusted during rescaling event)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def flip_center_kps(self, left_kps, right_kps):\n",
    "\n",
    "        x_min_l = min([kp[0] for kp in left_kps.values()])\n",
    "        x_max_l = max([kp[0] for kp in left_kps.values()])\n",
    "        x_mid_l = np.mean([x_min_l, x_max_l])\n",
    "\n",
    "        y_min_l = min([kp[1] for kp in left_kps.values()])\n",
    "        y_max_l = max([kp[1] for kp in left_kps.values()])\n",
    "        y_mid_l = np.mean([y_min_l, y_max_l])\n",
    "\n",
    "        x_min_r = min([kp[0] for kp in right_kps.values()])\n",
    "        x_max_r = max([kp[0] for kp in right_kps.values()])\n",
    "        x_mid_r = np.mean([x_min_r, x_max_r])\n",
    "\n",
    "        y_min_r = min([kp[1] for kp in right_kps.values()])\n",
    "        y_max_r = max([kp[1] for kp in right_kps.values()])\n",
    "        y_mid_r = np.mean([y_min_r, y_max_r])\n",
    "\n",
    "        fc_left_kps, fc_right_kps = {}, {}\n",
    "        flip_factor = 1 if left_kps['UPPER_LIP'][0] > left_kps['TAIL_NOTCH'][0] else -1\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            if flip_factor > 0:\n",
    "                fc_left_kp = np.array([left_kp[0] - x_mid_l, left_kp[1] - y_mid_l])\n",
    "                fc_right_kp = np.array([right_kp[0] - x_mid_l, right_kp[1] - y_mid_l])\n",
    "            else:\n",
    "                fc_right_kp = np.array([x_mid_r - left_kp[0], left_kp[1] - y_mid_r])\n",
    "                fc_left_kp = np.array([x_mid_r - right_kp[0], right_kp[1] - y_mid_r])\n",
    "            fc_left_kps[bp] = fc_left_kp\n",
    "            fc_right_kps[bp] = fc_right_kp\n",
    "\n",
    "        return fc_left_kps, fc_right_kps\n",
    "\n",
    "\n",
    "    def _rotate_cc(self, p, theta):\n",
    "        R = np.array([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "\n",
    "        rotated_kp = np.dot(R, p)\n",
    "        return rotated_kp\n",
    "\n",
    "\n",
    "    def rotate_kps(self, left_kps, right_kps):\n",
    "        upper_lip_x, upper_lip_y = left_kps['UPPER_LIP']\n",
    "        theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "        r_left_kps, r_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            rotated_kp = self._rotate_cc(left_kps[bp], -theta)\n",
    "            r_left_kps[bp] = rotated_kp\n",
    "            disp = abs(left_kps[bp][0] - right_kps[bp][0])\n",
    "            r_right_kps[bp] = np.array([rotated_kp[0] - disp, rotated_kp[1]])\n",
    "\n",
    "        return r_left_kps, r_right_kps\n",
    "\n",
    "\n",
    "    def translate_kps(self, left_kps, right_kps, factor):\n",
    "        t_left_kps, t_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            t_left_kps[bp] = factor * np.array(left_kps[bp])\n",
    "            t_right_kps[bp] = factor * np.array(right_kps[bp])\n",
    "\n",
    "        return t_left_kps, t_right_kps\n",
    "\n",
    "\n",
    "    def jitter_kps(self, left_kps, right_kps, jitter):\n",
    "        j_left_kps, j_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            j_left_kps[bp] = np.array([left_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                       left_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "            j_right_kps[bp] = np.array([right_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                        right_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "\n",
    "        return j_left_kps, j_right_kps\n",
    "\n",
    "\n",
    "\n",
    "    def modify_kps(self, left_kps, right_kps, factor, jitter, cm):\n",
    "        fc_left_kps, fc_right_kps = self.flip_center_kps(left_kps, right_kps)\n",
    "#         r_left_kps, r_right_kps = self.rotate_kps(fc_left_kps, fc_right_kps)\n",
    "        t_left_kps, t_right_kps = self.translate_kps(fc_left_kps, fc_right_kps, factor)\n",
    "        j_left_kps, j_right_kps  = self.jitter_kps(t_left_kps, t_right_kps, jitter)\n",
    "        j_left_kps_list, j_right_kps_list = [], []\n",
    "        for bp in BODY_PARTS:\n",
    "            l_item = {\n",
    "                'keypointType': bp,\n",
    "#                 'xFrame': j_left_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "#                 'yFrame': j_left_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                'xFrame': j_left_kps[bp][0],\n",
    "                'yFrame': j_left_kps[bp][1]\n",
    "            }\n",
    "\n",
    "            r_item = {\n",
    "                'keypointType': bp,\n",
    "#                 'xFrame': j_right_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "#                 'yFrame': j_right_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                'xFrame': j_right_kps[bp][0],\n",
    "                'yFrame': j_right_kps[bp][1]\n",
    "            }\n",
    "\n",
    "            j_left_kps_list.append(l_item)\n",
    "            j_right_kps_list.append(r_item)\n",
    "\n",
    "        modified_kps = {\n",
    "            'leftCrop': j_left_kps_list,\n",
    "            'rightCrop': j_right_kps_list\n",
    "        }\n",
    "\n",
    "        return modified_kps\n",
    "\n",
    "    \n",
    "    def __init__(self, lo=None, hi=None, jitter=0.0):\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "        self.jitter = jitter\n",
    "    \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        keypoints, cm, stereo_pair_id, label = \\\n",
    "            sample['keypoints'], sample['cm'], sample.get('stereo_pair_id'), sample.get('label')\n",
    "        left_keypoints_list = keypoints['leftCrop']\n",
    "        right_keypoints_list = keypoints['rightCrop']\n",
    "        left_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints_list}\n",
    "        right_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints_list}\n",
    "        \n",
    "        factor = 1.0 \n",
    "        if self.lo and self.hi:\n",
    "            factor = np.random.uniform(low=self.lo, high=self.hi)\n",
    "            \n",
    "        jitter = np.random.uniform(high=self.jitter)\n",
    "        \n",
    "        modified_kps = self.modify_kps(left_kps, right_kps, factor, jitter, cm)\n",
    "        left_kp_input = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in modified_kps['leftCrop']}\n",
    "        right_kp_input = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in modified_kps['rightCrop']}\n",
    "        kp_input = {}\n",
    "        for bp in BODY_PARTS:\n",
    "            kp_input[bp] = [\n",
    "                left_kp_input[bp][0] / 4096.0, \n",
    "                left_kp_input[bp][1] / 4096.0, \n",
    "                right_kp_input[bp][0] / 4096.0, \n",
    "                right_kp_input[bp][1] / 4096.0\n",
    "            ]\n",
    "        print(kp_input)\n",
    "        \n",
    "        sample = {\n",
    "            'kp_input': kp_input,\n",
    "            'label': label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'cm': cm,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        kp_input, label, stereo_pair_id = \\\n",
    "            sample['kp_input'], sample.get('label'), sample.get('stereo_pair_id')\n",
    "        \n",
    "        x = []\n",
    "        for bp in BODY_PARTS:\n",
    "            kp_data = kp_input[bp]\n",
    "            x.append(kp_data)\n",
    "        if sample.get('single_point_inference'):\n",
    "            x = np.array([x])\n",
    "        else:\n",
    "            x = np.array([x])\n",
    "        \n",
    "        kp_input_tensor = torch.from_numpy(x).float()\n",
    "        label_tensor = torch.from_numpy(np.array([label])).float() if label else None\n",
    "        \n",
    "        tensorized_sample = {\n",
    "            'kp_input': kp_input_tensor,\n",
    "            'stereo_pair_id': stereo_pair_id\n",
    "        }\n",
    "        return tensorized_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create \"bad\" Data Transforms </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.camera_metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPerturbation(object):\n",
    "        \n",
    "    def __init__(self, p_perturbation, min_magnitude, max_magnitude):\n",
    "        self.p_perturbation = p_perturbation\n",
    "        self.min_magnitude = min_magnitude\n",
    "        self.max_magnitude = max_magnitude\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        keypoints = sample['keypoints']\n",
    "        left_keypoints, right_keypoints = keypoints['leftCrop'], keypoints['rightCrop']\n",
    "        perturbed_left_keypoints = []\n",
    "        \n",
    "        # pick body parts to perturb (at least one)\n",
    "        indices = []\n",
    "        while len(indices) == 0:\n",
    "            indices = [x for x in range(len(BODY_PARTS)) if (random.random() < self.p_perturbation)]\n",
    "            \n",
    "        # apply perturbation\n",
    "        perturbed_left_keypoints, perturbed_right_keypoints = [], []\n",
    "        for idx, _ in enumerate(left_keypoints):\n",
    "            left_item, right_item = left_keypoints[idx], right_keypoints[idx]\n",
    "            left_perturbation_x, right_perturbation_x, left_perturbation_y, right_perturbation_y = \\\n",
    "                0.0, 0.0, 0.0, 0.0\n",
    "            if idx in indices:\n",
    "                case = np.random.choice([0, 1, 2], 1).item()\n",
    "                if case == 0:\n",
    "                    left_perturbation_x = np.random.normal(0, np.random.uniform(low=self.min_magnitude, high=self.max_magnitude))\n",
    "                    right_perturbation_x = np.random.normal(0, np.random.uniform(low=self.min_magnitude, high=self.max_magnitude))\n",
    "                    left_perturbation_y = np.random.normal(0, np.random.uniform(low=self.min_magnitude, high=self.max_magnitude))\n",
    "                    right_perturbation_y = np.random.normal(0, np.random.uniform(low=self.min_magnitude, high=self.max_magnitude))\n",
    "                elif case == 1:\n",
    "                    x_magnitude = np.random.uniform(low=self.min_magnitude, high=self.max_magnitude)\n",
    "                    y_magnitude = np.random.uniform(low=self.min_magnitude, high=self.max_magnitude)\n",
    "                    left_perturbation_x = np.random.normal(0, x_magnitude)\n",
    "                    right_perturbation_x = np.random.normal(0, abs(x_magnitude + np.random.normal(0, 20)))\n",
    "                    left_perturbation_y = np.random.normal(0, y_magnitude)\n",
    "                    right_perturbation_y = np.random.normal(0, abs(y_magnitude + np.random.normal(0, 20)))\n",
    "                else:\n",
    "                    k = list(range(len(BODY_PARTS)))\n",
    "                    k.remove(idx)\n",
    "                    random_idx = np.random.choice(k, 1).item()\n",
    "                    left_perturbation_x = left_keypoints[random_idx]['xFrame'] - left_item['xFrame'] + np.random.normal(0, 20)\n",
    "                    left_perturbation_y = left_keypoints[random_idx]['yFrame'] - left_item['yFrame'] + np.random.normal(0, 20)\n",
    "                    right_perturbation_x = right_keypoints[random_idx]['xFrame'] - right_item['xFrame'] + np.random.normal(0, 20)\n",
    "                    right_perturbation_y = right_keypoints[random_idx]['yFrame'] - right_item['yFrame'] + np.random.normal(0, 20)\n",
    "                \n",
    "            perturbed_left_item = {\n",
    "                'keypointType': left_item['keypointType'],\n",
    "                'xFrame': left_item['xFrame'] + left_perturbation_x,\n",
    "                'yFrame': left_item['yFrame'] + left_perturbation_y\n",
    "            }\n",
    "            \n",
    "            perturbed_right_item = {\n",
    "                'keypointType': right_item['keypointType'],\n",
    "                'xFrame': right_item['xFrame'] + right_perturbation_x,\n",
    "                'yFrame': right_item['yFrame'] + right_perturbation_y\n",
    "            }\n",
    "            \n",
    "            perturbed_left_keypoints.append(perturbed_left_item)\n",
    "            perturbed_right_keypoints.append(perturbed_right_item)\n",
    "        \n",
    "        perturbed_keypoints = {\n",
    "            'leftCrop': perturbed_left_keypoints,\n",
    "            'rightCrop': perturbed_right_keypoints\n",
    "        }\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'keypoints': perturbed_keypoints,\n",
    "            'cm': sample['cm'],\n",
    "            'stereo_pair_id': sample.get('stereo_pair_id')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Establish train / test datasets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.8\n",
    "train_mask = df.index <= (train_pct * df.shape[0])\n",
    "val_mask = df.index > (train_pct * df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dataset_train = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                              NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                              ToTensor()\n",
    "                                          ]))\n",
    "\n",
    "good_dataloader_train = DataLoader(good_dataset_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "good_dataset_val = KeypointsDataset(df[val_mask], transform=transforms.Compose([\n",
    "                                              NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                              ToTensor()\n",
    "                                          ]))\n",
    "\n",
    "good_dataloader_val = DataLoader(good_dataset_val, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dataset_train = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                              KeypointPerturbation(0.2, 30, 500),\n",
    "                                              NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                              ToTensor()\n",
    "                                          ]))\n",
    "\n",
    "bad_dataloader_train = DataLoader(bad_dataset_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "bad_dataset_val = KeypointsDataset(df[val_mask], transform=transforms.Compose([\n",
    "                                              KeypointPerturbation(0.2, 30, 500),\n",
    "                                              NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                              ToTensor()\n",
    "                                          ]))\n",
    "\n",
    "bad_dataloader_val = DataLoader(bad_dataset_val, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodBadKeypointsDataset(Dataset):\n",
    "    \"\"\"Good / Bad Keypoints dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, good_dataloader, bad_dataloader, transform=None):\n",
    "        self.good_dataloader = good_dataloader\n",
    "        self.bad_dataloader = bad_dataloader\n",
    "        self.process()\n",
    "        \n",
    "    def process(self):\n",
    "        self.X = []\n",
    "        self.labels = []\n",
    "        count = 0\n",
    "        for X_batch in self.good_dataloader:\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "            count += 1\n",
    "            x = X_batch['kp_input'].numpy().squeeze()\n",
    "            self.X.append(x)\n",
    "            self.labels.append(1)\n",
    "        for X_batch in self.bad_dataloader:\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "            count += 1\n",
    "            x = X_batch['kp_input'].numpy().squeeze()\n",
    "            self.X.append(x)\n",
    "            self.labels.append(0)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        return x, torch.from_numpy(np.array([y])).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = GoodBadKeypointsDataset(good_dataloader_train, bad_dataloader_train)\n",
    "dataset_val = GoodBadKeypointsDataset(good_dataloader_val, bad_dataloader_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=25, shuffle=True, num_workers=1)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=25, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AKPDScorerNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = AKPDScorerNetwork()\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = data_batch\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(running_loss / i)\n",
    "    else:\n",
    "        # print validation loss\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            y_pred_list, y_list = [], []\n",
    "            for i, data_batch in enumerate(dataloader_val):\n",
    "                X_batch, y_batch = data_batch\n",
    "                y_pred = network(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                y_pred_list.extend((y_pred.numpy().squeeze() > 0.5).astype(int).tolist())\n",
    "                y_list.extend(y_batch.numpy().squeeze().tolist())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "\n",
    "        val_loss_for_epoch = val_loss / len(dataloader_val)\n",
    "        y_pred_list, y_list = np.array(y_pred_list), np.array(y_list)\n",
    "        val_accuracy_for_epoch = sum(y_pred_list == y_list) / len(y_list)\n",
    "        \n",
    "\n",
    "    loss_for_epoch = running_loss / len(dataloader_train)\n",
    "    \n",
    "    print('-'*20)\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Train Loss: {}'.format(loss_for_epoch))\n",
    "    print('Validation Loss: {}'.format(val_loss_for_epoch))\n",
    "    print('Validation Accuracy: {}'.format(val_accuracy_for_epoch))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network, '/root/data/alok/biomass_estimation/playground/akpd_scorer_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test on Real Examples </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, side='both', overlay_keypoints=True, show_labels=False):\n",
    "    assert side == 'left' or side == 'right' or side == 'both', \\\n",
    "        'Invalid side value: {}'.format(side)\n",
    "\n",
    "    if side == 'left' or side == 'right':\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        image_f = left_image_f if side == 'left' else right_image_f\n",
    "        keypoints = left_keypoints if side == 'left' else right_keypoints\n",
    "        image = plt.imread(image_f)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in keypoints.items():\n",
    "                ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    ax.annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "        left_image = plt.imread(left_image_f)\n",
    "        right_image = plt.imread(right_image_f)\n",
    "        axes[0].imshow(left_image)\n",
    "        axes[1].imshow(right_image)\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in left_keypoints.items():\n",
    "                axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "            for bp, kp in right_keypoints.items():\n",
    "                axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "# query = \"\"\"\n",
    "#     SELECT * FROM prod.crop_annotation ca\n",
    "#     INNER JOIN prod.annotation_state pas on pas.id=ca.annotation_state_id\n",
    "#     WHERE ca.service_id = (SELECT ID FROM prod.service where name='LATI')\n",
    "#     AND ca.left_crop_url is not null\n",
    "#     AND ca.right_crop_url is not null\n",
    "#     AND ca.pen_id = 64\n",
    "#     AND (ca.annotation_state_id=6 OR ca.annotation_state_id=7)\n",
    "#     AND ca.captured_at > '2019-09-01'\n",
    "#     LIMIT 40;\n",
    "# \"\"\"\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "        select * from keypoint_annotations \n",
    "        where pen_id=61 and captured_at >= '2019-09-13' and captured_at <= '2019-09-20'\n",
    "        and keypoints -> 'leftCrop' is not null\n",
    "        and keypoints -> 'rightCrop' is not null\n",
    "        order by captured_at\n",
    "        limit 2;\n",
    "    \"\"\"\n",
    "tdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "aws_credentials = json.load(open(os.environ['AWS_CREDENTIALS']))\n",
    "akpd = AKPD(aws_credentials)\n",
    "\n",
    "keypoints = []\n",
    "for idx, row in tdf.iterrows():\n",
    "    left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "    left_crop_metadata, right_crop_metadata = row.left_crop_metadata, row.right_crop_metadata\n",
    "    left_image_f, _, _ = s3_access_utils.download_from_url(left_crop_url)\n",
    "    right_image_f, _, _ = s3_access_utils.download_from_url(right_crop_url)\n",
    "    akpd_keypoints = akpd.predict_keypoints(left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata)\n",
    "    keypoints.append(akpd_keypoints[0])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['keypoints'] = keypoints\n",
    "tdf.id = tdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = KeypointsDataset(tdf, transform=transforms.Compose([\n",
    "                                              NormalizeCentered2D(),\n",
    "                                              ToTensor()\n",
    "                                          ]))\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=5, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_list, spid_list = [], []\n",
    "    for i, data_batch in enumerate(dataloader_test):\n",
    "        X_batch, spid_batch = data_batch['kp_input'], data_batch['stereo_pair_id']\n",
    "        y_pred = network(X_batch)\n",
    "        y_pred_list.extend((y_pred.numpy().squeeze() > 0.5).astype(int).tolist())\n",
    "        spid_list.extend(spid_batch.numpy().squeeze().tolist())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "X = X_batch[0].numpy().squeeze()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(X[:, 2], X[:, 3])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(y_pred_list) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "akpd_keypoints = tdf.keypoints.iloc[idx]\n",
    "left_crop_url, right_crop_url = tdf.left_crop_url.iloc[idx], tdf.right_crop_url.iloc[idx]\n",
    "left_crop_metadata, right_crop_metadata = tdf.left_crop_metadata.iloc[idx], tdf.right_crop_metadata.iloc[idx]\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(left_crop_url)\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(right_crop_url)\n",
    "left_keypoints = {item['keypointType']: np.array([item['xCrop'], item['yCrop']]) for item in akpd_keypoints['leftCrop']}\n",
    "right_keypoints = {item['keypointType']: np.array([item['xCrop'], item['yCrop']]) for item in akpd_keypoints['rightCrop']}\n",
    "display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, X_batch in enumerate(bad_dataloader_train):\n",
    "    if i == 2:\n",
    "        print(X_batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "X = X_batch['kp_input'].numpy().squeeze()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(X[:, 2], X[:, 3])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
