{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.utils import CameraMetadata, get_ann_from_keypoint_arrs, get_left_right_keypoint_arrs, \\\n",
    "    normalize_left_right_keypoint_arrs, convert_to_world_point_arr\n",
    "from weight_estimation.body_parts import core_body_parts\n",
    "from weight_estimation.weight_estimator import WeightEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from keypoint_annotations\n",
    "    where keypoints is not null\n",
    "    and keypoints -> 'leftCrop' is not null\n",
    "    and keypoints -> 'rightCrop' is not null\n",
    "    limit 10000;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add synthetic data </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Add weight field </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_model_f = '/root/data/s3/aquabyte-models/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb'\n",
    "# kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "# weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "# weights = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     camera_metadata = row.camera_metadata\n",
    "    \n",
    "#     camera_metadata_obj = CameraMetadata(\n",
    "#         focal_length=camera_metadata['focalLength'],\n",
    "#         focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "#         baseline_m=camera_metadata['baseline'],\n",
    "#         pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "#         pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "#         image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "#         image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "#     )\n",
    "    \n",
    "#     weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "#     weights.append(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for idx, row in df.iterrows():\n",
    "    camera_metadata = row.camera_metadata\n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    ann = row.keypoints\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "    \n",
    "    world_keypoints = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "    length = np.linalg.norm(world_keypoints[6] - world_keypoints[7])\n",
    "    lengths.append(length)\n",
    "\n",
    "df['length'] = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simulate Larger Fish </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_larger_fish(world_keypoints, pct_inflation):\n",
    "    modified_world_keypoints = {}\n",
    "    for body_part in core_body_parts:\n",
    "        kps = world_keypoints[body_part]\n",
    "        modified_kps = (1.0 + pct_inflation) * kps\n",
    "        modified_world_keypoints[body_part] = modified_kps\n",
    "        \n",
    "    return modified_world_keypoints\n",
    "    \n",
    "\n",
    "def get_ann_from_world_keypoints(world_keypoints, cm):\n",
    "    ann = {'leftCrop': [], 'rightCrop': []}\n",
    "    for body_part in core_body_parts:\n",
    "        x, y, z = world_keypoints[body_part]\n",
    "        px_x = round(x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] / 2.0)\n",
    "        px_y = round(cm['pixelCountHeight'] / 2.0 - z * cm['focalLengthPixel'] / y)\n",
    "        disparity = round(cm['focalLengthPixel'] * cm['baseline'] / y)\n",
    "        \n",
    "        left_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x,\n",
    "            'yFrame': px_y\n",
    "        }\n",
    "        \n",
    "        right_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x - disparity,\n",
    "            'yFrame': px_y + random.uniform(0, 10)\n",
    "        }\n",
    "        \n",
    "        ann['leftCrop'].append(left_item)\n",
    "        ann['rightCrop'].append(right_item)\n",
    "    return ann\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_anns, cms = [], []\n",
    "\n",
    "mask = df.length > np.percentile(df.length, 90)\n",
    "for idx, row in df[mask].iterrows():\n",
    "    camera_metadata = row.camera_metadata\n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    ann = row.keypoints\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "    \n",
    "    world_keypoints = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "    world_keypoints_dict = {}\n",
    "    for idx, body_part in enumerate(core_body_parts):\n",
    "        world_keypoints_dict[body_part] = world_keypoints[idx]\n",
    "        \n",
    "    modified_world_keypoints = simulate_larger_fish(world_keypoints_dict, random.uniform(0, 0.25))\n",
    "    modified_ann = get_ann_from_world_keypoints(modified_world_keypoints, camera_metadata)\n",
    "    modified_anns.append(modified_ann)\n",
    "    cms.append(camera_metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anns = list(df.keypoints.values) + modified_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Construct \"good\" and \"bad\" class </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_COUNT_WIDTH = 4096\n",
    "\n",
    "\n",
    "def convert_to_akpd_nn_input(ann):\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X = np.hstack([X_left_norm, X_right_norm]) / PIXEL_COUNT_WIDTH\n",
    "    return X\n",
    "\n",
    "\n",
    "def perturb_ann(ann, p_perturbation=0.2, min_magnitude=30, max_magnitude=200):\n",
    "    \n",
    "    left_keypoints, right_keypoints = ann['leftCrop'], ann['rightCrop']\n",
    "    perturbed_left_keypoints = []\n",
    "    \n",
    "    # pick body parts to perturb (at least one)\n",
    "    indices = []\n",
    "    while len(indices) == 0:\n",
    "        indices = [x for x in range(len(core_body_parts)) if (random.random() < p_perturbation)]\n",
    "    \n",
    "    # apply perturbation\n",
    "    perturbed_left_keypoints, perturbed_right_keypoints = [], []\n",
    "    for idx, _ in enumerate(left_keypoints):\n",
    "        left_item, right_item = left_keypoints[idx], right_keypoints[idx]\n",
    "        left_perturbation_x, right_perturbation_x, left_perturbation_y, right_perturbation_y = \\\n",
    "            0.0, 0.0, 0.0, 0.0\n",
    "        if idx in indices:\n",
    "            case = np.random.choice([0, 1, 2], 1).item()\n",
    "            if case == 0:\n",
    "                left_perturbation_x = np.random.normal(0, np.random.uniform(low=min_magnitude, high=max_magnitude))\n",
    "                right_perturbation_x = np.random.normal(0, np.random.uniform(low=min_magnitude, high=max_magnitude))\n",
    "                left_perturbation_y = np.random.normal(0, np.random.uniform(low=min_magnitude, high=max_magnitude))\n",
    "                right_perturbation_y = np.random.normal(0, np.random.uniform(low=min_magnitude, high=max_magnitude))\n",
    "            elif case == 1:\n",
    "                x_magnitude = np.random.uniform(low=min_magnitude, high=max_magnitude)\n",
    "                y_magnitude = np.random.uniform(low=min_magnitude, high=max_magnitude)\n",
    "                left_perturbation_x = np.random.normal(0, x_magnitude)\n",
    "                right_perturbation_x = np.random.normal(0, abs(x_magnitude + np.random.normal(0, 20)))\n",
    "                left_perturbation_y = np.random.normal(0, y_magnitude)\n",
    "                right_perturbation_y = np.random.normal(0, abs(y_magnitude + np.random.normal(0, 20)))\n",
    "            else:\n",
    "                k = list(range(len(core_body_parts)))\n",
    "                k.remove(idx)\n",
    "                random_idx = np.random.choice(k, 1).item()\n",
    "                left_perturbation_x = left_keypoints[random_idx]['xFrame'] - left_item['xFrame'] + np.random.normal(0, 20)\n",
    "                left_perturbation_y = left_keypoints[random_idx]['yFrame'] - left_item['yFrame'] + np.random.normal(0, 20)\n",
    "                right_perturbation_x = right_keypoints[random_idx]['xFrame'] - right_item['xFrame'] + np.random.normal(0, 20)\n",
    "                right_perturbation_y = right_keypoints[random_idx]['yFrame'] - right_item['yFrame'] + np.random.normal(0, 20)\n",
    "\n",
    "        perturbed_left_item = {\n",
    "            'keypointType': left_item['keypointType'],\n",
    "            'xFrame': left_item['xFrame'] + left_perturbation_x,\n",
    "            'yFrame': left_item['yFrame'] + left_perturbation_y\n",
    "        }\n",
    "\n",
    "        perturbed_right_item = {\n",
    "            'keypointType': right_item['keypointType'],\n",
    "            'xFrame': right_item['xFrame'] + right_perturbation_x,\n",
    "            'yFrame': right_item['yFrame'] + right_perturbation_y\n",
    "        }\n",
    "        \n",
    "        perturbed_left_keypoints.append(perturbed_left_item)\n",
    "        perturbed_right_keypoints.append(perturbed_right_item)\n",
    "\n",
    "    perturbed_keypoints = {\n",
    "        'leftCrop': perturbed_left_keypoints,\n",
    "        'rightCrop': perturbed_right_keypoints\n",
    "    }\n",
    "    \n",
    "    return perturbed_keypoints\n",
    "        \n",
    "\n",
    "\n",
    "X_good_arr, X_bad_arr = [], []\n",
    "count = 0\n",
    "for ann in all_anns:\n",
    "    \n",
    "    # construct \"good\" class\n",
    "    \n",
    "    X_good = convert_to_akpd_nn_input(ann)\n",
    "    X_good_arr.append(X_good)\n",
    "    \n",
    "    # construct \"bad\" class\n",
    "    ann_bad = perturb_ann(ann)\n",
    "    X_bad = convert_to_akpd_nn_input(ann_bad)\n",
    "    X_bad_arr.append(X_bad)\n",
    "    \n",
    "\n",
    "X_good_arr = np.array(X_good_arr)\n",
    "X_bad_arr = np.array(X_bad_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create train / val split </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.8\n",
    "\n",
    "X_good_train = X_good_arr[:int(len(X_good_arr) * train_pct)]\n",
    "X_good_val = X_good_arr[int(len(X_good_arr) * train_pct):]\n",
    "X_bad_train = X_bad_arr[:int(len(X_bad_arr) * train_pct)]\n",
    "X_bad_val = X_bad_arr[int(len(X_bad_arr) * train_pct):]\n",
    "\n",
    "X_train = np.vstack([X_good_train, X_bad_train])\n",
    "y_train = np.array([1] * len(X_good_train) + [0] * len(X_bad_train))\n",
    "shuffle_idx = np.array(range(len(X_train)))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "X_train = X_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]\n",
    "\n",
    "X_val = np.vstack([X_good_val, X_bad_val])\n",
    "y_val = np.array([1] * len(X_good_val) + [0] * len(X_bad_val))\n",
    "shuffle_idx = np.array(range(len(X_val)))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "X_val = X_val[shuffle_idx]\n",
    "y_val = y_val[shuffle_idx]\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_val = X_val.reshape(len(X_val), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(32,))\n",
    "\n",
    "x = Dense(256, activation='relu')(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=100,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test on Real Examples </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, side='both', overlay_keypoints=True, show_labels=False):\n",
    "    assert side == 'left' or side == 'right' or side == 'both', \\\n",
    "        'Invalid side value: {}'.format(side)\n",
    "\n",
    "    if side == 'left' or side == 'right':\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        image_f = left_image_f if side == 'left' else right_image_f\n",
    "        keypoints = left_keypoints if side == 'left' else right_keypoints\n",
    "        image = plt.imread(image_f)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in keypoints.items():\n",
    "                ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    ax.annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "        left_image = plt.imread(left_image_f)\n",
    "        right_image = plt.imread(right_image_f)\n",
    "        axes[0].imshow(left_image)\n",
    "        axes[1].imshow(right_image)\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in left_keypoints.items():\n",
    "                axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "            for bp, kp in right_keypoints.items():\n",
    "                axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                if show_labels:\n",
    "                    axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "        select * from prod.biomass_computations\n",
    "        where pen_id=144 and captured_at >= '2021-01-08' and captured_at <= '2021-01-12';\n",
    "    \"\"\"\n",
    "tdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scores = []\n",
    "count = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    \n",
    "    ann = row.annotation\n",
    "    X = convert_to_akpd_nn_input(ann)\n",
    "    score = model.predict(np.array(X.reshape(-1, 32)))[0][0]\n",
    "    akpd_scores.append(score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[np.array(akpd_scores) > 0.01].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.akpd_score > 0.01].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "left_image_url = tdf.left_crop_url.iloc[idx]\n",
    "right_image_url = tdf.left_crop_url.iloc[idx]\n",
    "left_image_f, _, _ = s3.download_from_url(left_image_url)\n",
    "right_image_f, _, _ = s3.download_from_url(right_image_url)\n",
    "\n",
    "ann = tdf.annotation.iloc[idx]\n",
    "left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "\n",
    "display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
