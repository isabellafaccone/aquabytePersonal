{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.optics import euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Instantiate data extraction tools </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prod research SQL credentaials\n",
    "prod_research_sql_credentials = json.load(open(os.environ[\"PROD_RESEARCH_SQL_CREDENTIALS\"]))\n",
    "rds_access_utils = RDSAccessUtils(prod_research_sql_credentials)\n",
    "\n",
    "\n",
    "sql_query = '''\n",
    "select * from keypoint_annotations\n",
    "where pen_id = 48\n",
    "and keypoints is not NULL\n",
    "and is_qa=false;\n",
    "'''\n",
    "\n",
    "original_df = rds_access_utils.extract_from_database(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "research_rds_access_utils = RDSAccessUtils(research_sql_credentials)\n",
    "sql_engine = research_rds_access_utils.sql_engine\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Helper functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from sqlalchemy import create_engine, MetaData, Table, exc, exists, select, literal\n",
    "import pickle\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**0.5\n",
    "\n",
    "\n",
    "def convert_to_world_point(x, y, d, parameters):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    # get relevant parameters\n",
    "    pixel_count_height = parameters[\"pixelCountHeight\"]\n",
    "    pixel_count_width = parameters[\"pixelCountWidth\"]\n",
    "    sensor_width = parameters[\"imageSensorWidth\"]\n",
    "    sensor_height = parameters[\"imageSensorHeight\"]\n",
    "    focal_length = parameters[\"focalLength\"]\n",
    "\n",
    "    image_center_x = pixel_count_height / 2.0\n",
    "    image_center_y = pixel_count_width / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (sensor_height / pixel_count_height)\n",
    "    sensor_z = px_z * (sensor_width / pixel_count_width)\n",
    "\n",
    "    # now move to world coordinates\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / focal_length\n",
    "    world_z = (world_y * sensor_z) / focal_length\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "\n",
    "def depth_from_disp(disp, parameters):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    focal_length_pixel = parameters[\"focalLengthPixel\"]\n",
    "\n",
    "    baseline = parameters[\"baseline\"]\n",
    "    depth = focal_length_pixel * baseline / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def pixel2world(left_crop, right_crop, parameters):\n",
    "    \"\"\"2D pixel coordinates to 3D world coordinates\"\"\"\n",
    "\n",
    "    # first create a dic with crop keypoints\n",
    "    image_coordinates = {\"leftCrop\": {},\n",
    "                         \"rightCrop\": {}}\n",
    "    for keypoint in left_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        image_coordinates[\"leftCrop\"][name] = [keypoint[\"xFrame\"], keypoint[\"yFrame\"]]\n",
    "    for keypoint in right_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        image_coordinates[\"rightCrop\"][name] = [keypoint[\"xFrame\"], keypoint[\"yFrame\"]]\n",
    "\n",
    "    # then loop through the right crop keypoints and calculate the world coordinates\n",
    "    world_coordinates = {}\n",
    "    for keypoint in left_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        disparity = image_coordinates[\"leftCrop\"][name][0] - image_coordinates[\"rightCrop\"][name][0]\n",
    "        depth = depth_from_disp(disparity, parameters)\n",
    "        world_point = convert_to_world_point(image_coordinates[\"leftCrop\"][name][1],\n",
    "                                             image_coordinates[\"leftCrop\"][name][0],\n",
    "                                             depth,\n",
    "                                             parameters)\n",
    "        world_coordinates[name] = world_point\n",
    "    return world_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "capture_f = '/root/data/temp/capture.json'\n",
    "\n",
    "\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "k = 0\n",
    "\n",
    "session.rollback()\n",
    "for idx, row in original_df.iterrows():\n",
    "    if k % 10 == 0:\n",
    "        print(k)\n",
    "    k += 1\n",
    "    \n",
    "    # get gtsf_fish_identifier, ground truth metadata, and weight for this row\n",
    "    left_crop_url = row.left_image_url\n",
    "    right_crop_url = row.right_image_url\n",
    "\n",
    "    left_crop_key = left_crop_url.replace('https://s3-eu-west-1.amazonaws.com/aquabyte-crops/', '')\n",
    "    right_crop_key = right_crop_url.replace('https://s3-eu-west-1.amazonaws.com/aquabyte-crops/', '')\n",
    "    crop_key_dir = os.path.dirname(left_crop_key)\n",
    "    capture_key = os.path.join(crop_key_dir, 'capture.json')\n",
    "    left_image_key = os.path.join(crop_key_dir, 'left_frame.jpg')\n",
    "    right_image_key = os.path.join(crop_key_dir, 'right_frame.jpg')\n",
    "    image_bucket = 'aquabyte-frames-resized-inbound'\n",
    "    s3_access_utils.download_from_s3(image_bucket, capture_key, capture_f)\n",
    "    capture_info = json.load(open(capture_f))\n",
    "    \n",
    "    gtsf_fish_identifier = capture_info['gtsf_fish_identifier']\n",
    "    gtsf_data_collection = session.query(GtsfDataCollection).filter(GtsfDataCollection.gtsf_fish_identifier == gtsf_fish_identifier).all()[0]\n",
    "    ground_truth_metadata = json.loads(gtsf_data_collection.ground_truth_metadata)\n",
    "    weight, length, kfactor = None, None, None\n",
    "    if 'data' in ground_truth_metadata.keys():\n",
    "        keys = ground_truth_metadata['data'].keys()\n",
    "        if 'weight' in keys or 'weightKgs' in keys:\n",
    "            weightKey = 'weight' if 'weight' in keys else 'weightKgs'\n",
    "            lengthKey = 'length' if 'length' in keys else 'lengthMms'\n",
    "            weight = ground_truth_metadata['data'][weightKey]\n",
    "            length = ground_truth_metadata['data'][lengthKey]\n",
    "            kfactor = (weight / length**3) * 1e5\n",
    "    if not weight:\n",
    "        print('No weight recorded for GTSF fish identifier: {}'.format(gtsf_fish_identifier))\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # get left, right, and world keypoints\n",
    "    keypoints = row.keypoints\n",
    "    if 'leftCrop' not in keypoints or 'rightCrop' not in keypoints:\n",
    "        continue\n",
    "        \n",
    "    keypoint_world_coordinates = pixel2world(keypoints['leftCrop'], keypoints['rightCrop'], row.camera_metadata)\n",
    "    \n",
    "    # write row to dataframe\n",
    "    df_row = {}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(keypoint_world_coordinates[body_parts[i]], \n",
    "                                   keypoint_world_coordinates[body_parts[j]])\n",
    "            df_row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    df_row['weight'] = weight\n",
    "    df_row['length'] = length\n",
    "    df_row['kfactor'] = kfactor\n",
    "    df_row['captured_at'] = row.captured_at\n",
    "    df_row['gtsf_fish_identifier'] = gtsf_fish_identifier\n",
    "    df_row['keypoints']= row.keypoints\n",
    "    df_row['keypoint_world_coordinates'] = keypoint_world_coordinates\n",
    "    df_row['left_image_key'] = left_image_key\n",
    "    df_row['right_image_key'] = right_image_key\n",
    "    df_row['left_crop_key'] = left_crop_key\n",
    "    df_row['right_crop_key'] = right_crop_key\n",
    "    df_row['image_bucket'] = 'aquabyte-frames-resized-inbound'\n",
    "    df_row['crop_bucket'] = 'aquabyte-crops'\n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.gtsf_fish_identifier == '190301010003']['weight'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.captured_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "])\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        df[interaction_column] = df[col1] * df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            df[interaction_column] = df[col1] * df[col2] * df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mask = generate_train_mask(df, train_frac=1.0)\n",
    "mask = mask & (df.epoch.isin(features_df.epoch))\n",
    "columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "X_train = df.loc[mask, columns].values\n",
    "print(X_train.sum())\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "# idx = 4\n",
    "print(idx)\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "print(reg.score(X_test_transformed, y_test))\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts_subset   \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save model to disk </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('/root/data/models/biomass/20190722_bati_post_axiom_calibration.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.plot_predictions_vs_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cross validation study </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "biomass_error_pcts = []\n",
    "for i in range(N):\n",
    "    print(i)\n",
    "    mask = generate_train_mask(df, train_frac=0.8)\n",
    "    mask = mask & (df.epoch.isin(features_df.epoch))\n",
    "    columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "    X_train = df.loc[mask, columns].values\n",
    "    y_train = df.loc[mask, 'weight'].values\n",
    "    X_test = df.loc[~mask, columns].values\n",
    "    y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "    pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "    pca.fit(X_train_normalized)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "    idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "\n",
    "    pca = PCA(n_components=idx+1)\n",
    "    pca.fit(X_train_normalized)\n",
    "    X_train_transformed = pca.transform(X_train_normalized)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "\n",
    "    y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "    df['prediction'] = y_pred\n",
    "    df['error'] = df.prediction - df.weight\n",
    "    df['error_pct'] = df.error / df.weight\n",
    "    df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "    model = {\n",
    "        'mean': scaler.mean_,\n",
    "        'std': scaler.scale_,\n",
    "        'PCA_components': pca.components_,\n",
    "        'reg_coef': reg.coef_,\n",
    "        'reg_intercept': reg.intercept_,\n",
    "        'body_parts': body_parts   \n",
    "    }\n",
    "    \n",
    "    amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)\n",
    "    accuracy_metrics = amg.generate_train_test_accuracy_metrics()\n",
    "    biomass_error_pct = accuracy_metrics['test']['biomass_error_pct']\n",
    "    biomass_error_pcts.append(biomass_error_pct)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = sorted(list([abs(x) for x in biomass_error_pcts]))\n",
    "p = 1.0 * np.arange(len(data_sorted)) / (len(data_sorted) - 1)\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(p, data_sorted)\n",
    "ax1.set_xlabel('p')\n",
    "ax1.set_ylabel('OOS error percentage')\n",
    "plt.axvline(x=0.95, linestyle='--', color='red', label='p = 0.95')\n",
    "plt.title('CDF of OOS errors (sample size = 250)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.ix[(df.epoch.isin(features_df.epoch)), columns + ['weight']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = features_df.ix[(features_df.epoch.isin(df.epoch)) & (features_df.pen_id == 48), columns + ['weight']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.optics import euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[(df.gtsf_fish_identifier == '190607010041_bolaks-mjanes') & (df.prediction < 7000)].prediction)\n",
    "plt.axvline(5571, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df[df.gtsf_fish_identifier == '190607010041_bolaks-mjanes'].prediction.mean()) - 5571)/5571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.prediction - df.weight).std() / df.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get all epochs for this dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "for idx, row in original_df.iterrows():\n",
    "    captured_at = str(row.captured_at)\n",
    "    unix_epoch = dt.datetime.utcfromtimestamp(0)\n",
    "    try:\n",
    "        pattern = '%Y-%m-%d %H:%M:%S.%f+00:00'\n",
    "        timestamp = dt.datetime.strptime(captured_at, pattern)\n",
    "    except ValueError as e:\n",
    "        pattern = '%Y-%m-%d %H:%M:%S+00:00'\n",
    "        timestamp = dt.datetime.strptime(captured_at, pattern)\n",
    "        \n",
    "    epoch = int((timestamp - unix_epoch).total_seconds() * 1000.0)\n",
    "    epochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_hdf('/root/data/temp/features_df.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.captured_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nanoepoch'] = df.captured_at.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['nanoepoch'] = features_df.captured_at.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.captured_at.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.epoch.isin(features_df.epoch)) & (df.epoch.isin(original_df[original_df.is_qa==False].epoch))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[features_df.pen_id==48].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = (df.epoch.isin(features_df.epoch))\n",
    "for epoch in df[m].epoch.unique():\n",
    "    if df[df.epoch==epoch].shape[0] > 1:\n",
    "        continue\n",
    "    \n",
    "    a = df.ix[df.epoch==epoch, 'weight'].values.sum()\n",
    "    b = features_df.ix[features_df.epoch==epoch, 'weight'].values.sum()\n",
    "    print(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.epoch.isin(features_df.epoch)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
