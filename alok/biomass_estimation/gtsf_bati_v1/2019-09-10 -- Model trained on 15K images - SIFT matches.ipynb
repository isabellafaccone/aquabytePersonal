{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from wpca import WPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from aquabyte.visualize import Visualizer, _normalize_world_keypoints\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "from scipy.stats import norm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Extract base data from database </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from research.fish_metadata a left join keypoint_annotations b\n",
    "    on a.left_url = b.left_image_url \n",
    "    where b.keypoints is not null and b.is_qa = false;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Append world kepyoints to the data </h1>\n",
    "<h3> Ideally, this data should already live directly in the database </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get the features dataframe from the base data with all pairwise distances </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = defaultdict(list)\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'HYPURAL_PLATE',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "])\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    world_keypoints = row.world_keypoints\n",
    "    if world_keypoints:\n",
    "        for i in range(len(body_parts)-1):\n",
    "            for j in range(i+1, len(body_parts)):\n",
    "                if body_parts[i] in body_parts_subset and body_parts[j] in body_parts_subset:\n",
    "                    d = euclidean_distance(world_keypoints[body_parts[i]], \n",
    "                                           world_keypoints[body_parts[j]])\n",
    "                    features_data['{}-{}'.format(i, j)].append(d)\n",
    "\n",
    "            \n",
    "        features_data['world_keypoints'].append(row.world_keypoints)\n",
    "        features_data['weight'].append(row.weight)\n",
    "        features_data['captured_at'].append(row.captured_at)\n",
    "        features_data['gtsf_fish_identifier'].append(row.fish_id)\n",
    "        features_data['pen_id'].append(row.pen_id)\n",
    "        features_data['keypoint_annotation_id'].append(row.id)\n",
    "        features_data['kf'].append(1e5 * row.weight / row['data']['lengthMms']**3) \n",
    "        features_data['length'].append(row['data']['lengthMms'] * 1e-3)\n",
    "        features_data['width'].append(row['data']['widthMms'] * 1e-3 if 'widthMms' in row['data'] else None)\n",
    "#         features_data['breadth'].append(row['data']['breadthMms'] * 1e-3 if 'breadthMms' in row['data'] else None)\n",
    "\n",
    "features_df = pd.DataFrame(features_data)\n",
    "\n",
    "# get rid of bad keypoint annotation ids\n",
    "\n",
    "blacklisted_keypoint_annotation_ids = [\n",
    "    606484, \n",
    "    635806, \n",
    "    637801, \n",
    "    508773, \n",
    "    640493, \n",
    "    639409, \n",
    "    648536, \n",
    "    507003,\n",
    "    706002,\n",
    "    507000,\n",
    "    709298,\n",
    "    714073,\n",
    "    719239\n",
    "]\n",
    "\n",
    "# blacklist_mask = features_df['8-9'] > 1.0\n",
    "blacklist_mask = features_df['2-6'] > 1.0\n",
    "for kp_id in blacklisted_keypoint_annotation_ids:\n",
    "    if blacklist_mask is None:\n",
    "        blacklist_mask = features_df.keypoint_annotation_id == kp_id\n",
    "    else:\n",
    "        blacklist_mask = blacklist_mask | (features_df.keypoint_annotation_id == kp_id)\n",
    "features_df = features_df[~blacklist_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        features_df[interaction_column] = features_df[col1] * features_df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            features_df[interaction_column] = features_df[col1] * features_df[col2] * features_df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Weight each datapoint based on the number of stereo images captured for that fish </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute best fit plane </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute and visualize SIFT features </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT based correction - functions\n",
    "\n",
    "def enhance(image, clip_limit=5):\n",
    "    # convert image to LAB color model\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # split the image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(image_lab)\n",
    "\n",
    "    # apply CLAHE to lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
    "    merged_channels = cv2.merge((cl, a_channel, b_channel))\n",
    "\n",
    "    # convert image from LAB color model back to RGB color model\n",
    "    final_image = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
    "    return final_image \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_matches(img1, kp1, img2, kp2, matches, matchesMask, color=None, drawFeatures=True): \n",
    "    if len(img1.shape) == 3:\n",
    "        new_shape = (max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], img1.shape[2])\n",
    "    elif len(img1.shape) == 2:\n",
    "        new_shape = (max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1])\n",
    "    new_img = np.zeros(new_shape, type(img1.flat[0]))  \n",
    "    # Place images onto the new image.\n",
    "    new_img[0:img1.shape[0],0:img1.shape[1]] = img1\n",
    "    new_img[0:img2.shape[0],img1.shape[1]:img1.shape[1]+img2.shape[1]] = img2\n",
    "    \n",
    "    if drawFeatures==False:\n",
    "        return new_img\n",
    "\n",
    "    # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n",
    "    r = 15\n",
    "    thickness = 3\n",
    "    if color:\n",
    "        c = color\n",
    "    i=0\n",
    "    for m in matches:\n",
    "        i=i+1\n",
    "        # Generate random color for RGB/BGR and grayscale images as needed.\n",
    "        if not color: \n",
    "            c = np.random.randint(0,256,3) if len(img1.shape) == 3 else np.random.randint(0,256)\n",
    "            c = tuple([int(x) for x in c])        \n",
    "        if matchesMask[i-1]==0: \n",
    "            continue\n",
    "        end1 = tuple(np.round(kp1[m.queryIdx].pt).astype(int))\n",
    "        end2 = tuple(np.round(kp2[m.trainIdx].pt).astype(int) + np.array([img1.shape[1], 0]))\n",
    "        cv2.line(new_img, end1, end2, c, thickness, )\n",
    "        cv2.circle(new_img, end1, r, c, thickness)\n",
    "        cv2.circle(new_img, end2, r, c, thickness)\n",
    "    return new_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def _normalize_world_keypoints(wkps, rotate=True):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['HYPURAL_PLATE'] for bp in body_parts}\n",
    "\n",
    "    if not rotate:\n",
    "        return translated_wkps\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = _generate_rotation_matrix(u_base, v)\n",
    "    print(R.shape, translated_wkps['BODY'].shape)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp].T) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['ADIPOSE_FIN'] - np.array([norm_wkps_intermediate['ADIPOSE_FIN'][0], 0, 0])\n",
    "    R = _generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform reflecton if necessary\n",
    "    if norm_wkps['PECTORAL_FIN'][1] > 0:\n",
    "        norm_wkps = {bp: np.array([\n",
    "            norm_wkps[bp][0],\n",
    "            -norm_wkps[bp][1],\n",
    "            norm_wkps[bp][2]\n",
    "        ]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "\n",
    "\n",
    "def draw_matches_3D(img1, kp1, img2, kp2, matches, matchesMask): \n",
    "    \n",
    "\n",
    "    # Draw lines between matches.  Make sure to offset kp coords in second image appropriately.\n",
    "    i=0\n",
    "    wkps = []\n",
    "    for m in matches:\n",
    "        # Generate random color for RGB/BGR and grayscale images as needed.\n",
    "        \n",
    "        if matchesMask[i] == 1:\n",
    "            p1 = tuple(np.round(kp1[m.queryIdx].pt).astype(int))\n",
    "            p2 = tuple(np.round(kp2[m.trainIdx].pt).astype(int))\n",
    "            p1_x_frame = p1[0] + df.left_crop_metadata.iloc[idx]['x_coord']\n",
    "            p1_y_frame = p1[1] + df.left_crop_metadata.iloc[idx]['y_coord']\n",
    "            p2_x_frame = p2[0] + df.right_crop_metadata.iloc[idx]['x_coord']\n",
    "            params = df.camera_metadata.iloc[idx]\n",
    "            disp = abs(p1_x_frame - p2_x_frame)\n",
    "            depth = depth_from_disp(disp, params)\n",
    "            wkp = convert_to_world_point(p1_y_frame, p1_x_frame, depth, params)\n",
    "            wkps.append(wkp)\n",
    "        i += 1\n",
    "        \n",
    "    return wkps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "idx = 0\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(df.left_image_url.iloc[idx])\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(df.right_image_url.iloc[idx])\n",
    "imageL = cv2.imread(left_image_f)\n",
    "imageR = cv2.imread(right_image_f)\n",
    "\n",
    "MIN_MATCH_COUNT = 10\n",
    "GOOD_PERC = 0.7\n",
    "\n",
    "sift = cv2.KAZE_create()\n",
    "img1 = enhance(imageL)\n",
    "img2 = enhance(imageR)\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < GOOD_PERC*n.distance:\n",
    "        good.append(m)\n",
    "if len(good)>=MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,7.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "else:\n",
    "    print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "    matchesMask = None\n",
    "    \n",
    "img3 = draw_matches(img1,kp1,img2,kp2,good,matchesMask,None,False)\n",
    "img3o = draw_matches(img1,kp1,img2,kp2,good,matchesMask,None,True)\n",
    "alpha = 0.7  # Transparency factor.\n",
    "img3 = cv2.addWeighted(img3o, alpha, img3, 1 - alpha, 0)\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(20, 10))\n",
    "ax.imshow(img3)\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_wkps = draw_matches_3D(img1, kp1, img2, kp2, good, matchesMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkps = df.world_keypoints.iloc[idx]\n",
    "wkps['BODY'] = np.array(body_wkps)\n",
    "norm_wkps = _normalize_world_keypoints(wkps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "body_parts = [k for k in norm_wkps.keys() if k != 'BODY']\n",
    "xs = [norm_wkps[bp][0] for bp in body_parts]\n",
    "ys = [norm_wkps[bp][1] for bp in body_parts]\n",
    "zs = [norm_wkps[bp][2] for bp in body_parts]\n",
    "xs.extend(list(norm_wkps['BODY'][0]))\n",
    "ys.extend(list(norm_wkps['BODY'][1]))\n",
    "zs.extend(list(norm_wkps['BODY'][2]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim3d(0, max(xs))\n",
    "ax.set_ylim3d(-0.3, 0.3)\n",
    "ax.set_zlim3d(-0.3, 0.3)\n",
    "ax.scatter(xs, ys, zs, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.data.apply(lambda x: x.get('breadthMms')) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def generate_oos_score(features_df, mask, train_size, num_eigenvectors):\n",
    "    np.random.seed(0)\n",
    "    columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "    X_train = features_df.loc[mask, columns].values\n",
    "    y_train = features_df.loc[mask, 'weight'].values\n",
    "    w_train = features_df.loc[mask, 'w'].values\n",
    "    X_test = features_df.loc[~mask, columns].values\n",
    "    y_test = features_df.loc[~mask, 'weight'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "    pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "    pca.fit(X_train_normalized)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "    idx = num_eigenvectors\n",
    "\n",
    "    pca = PCA(n_components=idx+1)\n",
    "    pca.fit(X_train_normalized)\n",
    "    X_train_transformed = pca.transform(X_train_normalized)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train_transformed, y_train, sample_weight=w_train)\n",
    "    score = reg.score(X_test_transformed, y_test)\n",
    "\n",
    "    y_pred = reg.predict(pca.transform(scaler.transform(features_df[columns].values)))\n",
    "    features_df['prediction'] = y_pred\n",
    "    features_df['error'] = features_df.prediction - features_df.weight\n",
    "    features_df['error_pct'] = features_df.error / features_df.weight\n",
    "    features_df['abs_error_pct'] = features_df.error_pct.abs()\n",
    "\n",
    "    model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts_subset   \n",
    "    }\n",
    "    \n",
    "\n",
    "    return mask, model, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot one instance of train / test where the training set consists of 2000 fish </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(fea.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num eigenvectors = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tdf = features_df.copy(deep=True)\n",
    "\n",
    "weights = []\n",
    "i = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    count = tdf[tdf.gtsf_fish_identifier == row.gtsf_fish_identifier].shape[0]\n",
    "    if count > 1:\n",
    "        weights.append(1.0 / count ** 0.5)\n",
    "    else:\n",
    "        weights.append(1)\n",
    "        \n",
    "tdf['w'] = weights\n",
    "\n",
    "gtsf_fish_identifiers = list(tdf.gtsf_fish_identifier.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "mask = tdf.gtsf_fish_identifier.isin(fish_ids)\n",
    "mask, model, score = generate_oos_score(tdf, mask, 2000, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator()\n",
    "amg.set_data(mask, tdf.prediction.values, tdf.weight.values, w=tdf.w.values)\n",
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator()\n",
    "m = tdf.max_error_m < 0.04\n",
    "# amg.set_data(mask, tdf.prediction.values, tdf.weight.values, w=tdf.w.values)\n",
    "amg.set_data(m & mask, tdf.prediction.values, tdf.weight.values, test_mask=~m & mask, w=tdf.w.values)\n",
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num eigenvectors = 20\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tdf = features_df[(features_df.horizontal_angle.abs() < 20) & (features_df.vertical_angle.abs() < 20) & (features_df.rms_error_m < 0.1)]\n",
    "\n",
    "weights = []\n",
    "i = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    count = tdf[tdf.gtsf_fish_identifier == row.gtsf_fish_identifier].shape[0]\n",
    "    if count > 1:\n",
    "        weights.append(1.0 / count ** 0.5)\n",
    "#         weights.append(1.0 / count)\n",
    "    else:\n",
    "        weights.append(1)\n",
    "        \n",
    "tdf['w'] = weights\n",
    "\n",
    "\n",
    "gtsf_fish_identifiers = list(tdf.gtsf_fish_identifier.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "mask = tdf.gtsf_fish_identifier.isin(fish_ids)\n",
    "mask, model, score = generate_oos_score(tdf, mask, 2000, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator()\n",
    "amg.set_data(mask, tdf.prediction.values, tdf.weight.values, w=tdf.w.values)\n",
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator()\n",
    "amg.set_data(mask, tdf.prediction.values, tdf.weight.values, w=tdf.w.values)\n",
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('/root/data/alok/biomass_estimation/playground/model_lateral_only.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.average(tdf[mask].prediction.values, weights=tdf[mask].w.values) - np.average(tdf[mask].weight.values, weights=tdf[mask].w.values)) / np.average(tdf[mask].weight.values, weights=tdf[mask].w.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tdf.prediction.values - tdf.weight.values\n",
    "ground_truth = tdf.weight.values\n",
    "w = tdf.w.values\n",
    "np.sqrt(np.average((error / ground_truth)**2, weights=w) - np.average(error / ground_truth, weights=w)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 713939].left_image_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 714830].iloc[0].left_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['keypoint_annotation_id', 'gtsf_fish_identifier', 'rms_error_m', 'weight', 'width']\n",
    "features_df.ix[features_df.rms_error_m > 0.02, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualize Individual Cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "credentials = json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS']))\n",
    "rds_access_utils = RDSAccessUtils(credentials)\n",
    "v = Visualizer(s3_access_utils, rds_access_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for idx, row in features_df.sample(100).sort_values('rms_error_m', ascending=False).iterrows():\n",
    "    v.load_data(row.keypoint_annotation_id)\n",
    "    v.display_crops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "v.display_3d_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(features_df.rms_error_m, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('seismic')\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "mask = features_df.rms_error_m < 0.5\n",
    "sc = ax.scatter(features_df[mask].sort_values('rms_error_m').weight.values, \n",
    "            features_df[mask].sort_values('rms_error_m').prediction.values,\n",
    "            c=features_df[mask].sort_values('rms_error_m').rms_error_m.values,\n",
    "            cmap=cm)\n",
    "plt.colorbar(sc)\n",
    "plt.plot([0, 10000], [0, 10000])\n",
    "# plt.xlim([0, 10000])\n",
    "# plt.ylim([0, 10000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[mask].sort_values('error_pct', ascending=False).keypoint_annotation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_depth(wkps):\n",
    "    if wkps:\n",
    "        mean = np.mean(np.array([wkp[1] for wkp in wkps.values()]))\n",
    "        if mean == np.inf:\n",
    "            return None\n",
    "        return mean\n",
    "    return None\n",
    "\n",
    "features_df['centroid_depth'] = features_df.world_keypoints.apply(lambda x: centroid_depth(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(features_df[(features_df.centroid_depth > 0) & (features_df.centroid_depth < 2.0)].centroid_depth, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(features_df.centroid_depth, features_df.rms_error_m)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.rms_error_m.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left_image_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
