{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-05,2019-06-12).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-12,2019-06-19).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-19,2019-06-26).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-26,2019-07-03).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-07-03,2019-07-04).csv')\n",
    "])\n",
    "\n",
    "df = df.sort_values('captured_at')\n",
    "df['estimated_weight_g'] = df.weight\n",
    "df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n",
    "hour_mask = (df.hour > 7) & (df.hour < 16)\n",
    "df = df[hour_mask].copy(deep=True)\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(df.index.date.astype(str)))):\n",
    "    records[date].extend(df[date].weight.values.tolist())\n",
    "    \n",
    "records_json = json.dumps(records)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How do AKPD keypoints compare to precise manual keypoints? </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM keypoint_annotations\n",
    "WHERE pen_id=5\n",
    "AND captured_at BETWEEN '2019-06-05' AND '2019-07-02'\n",
    "AND keypoints is not null\n",
    "AND keypoints -> 'leftCrop' is not null\n",
    "AND keypoints -> 'rightCrop' is not null\n",
    "AND is_qa = FALSE;\n",
    "\"\"\"\n",
    "\n",
    "mdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_intersection = sorted(list(set(mdf.left_image_url).intersection(df.left_crop_url)))\n",
    "tdf = df[df.left_crop_url.isin(url_intersection)].sort_values('left_crop_url')\n",
    "tdf['manual_keypoints'] = mdf[mdf.left_image_url.isin(url_intersection)].sort_values('left_image_url').keypoints.values\n",
    "tdf['camera_metadata'] = mdf[mdf.left_image_url.isin(url_intersection)].sort_values('left_image_url').camera_metadata.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS = [\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'TAIL_NOTCH',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'ADIPOSE_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'ANAL_FIN'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disparity(x, body_part):\n",
    "    if type(x) == str:\n",
    "        x = json.loads(x)\n",
    "    left_kps = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in x['leftCrop']}\n",
    "    right_kps = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in x['rightCrop']}\n",
    "    disp = abs(left_kps[body_part][0] - right_kps[body_part][0])\n",
    "    return disp\n",
    "\n",
    "diffs_dict = {}\n",
    "diffs_dict['percentile'] = ['5th', '25th', '50th', '75th', '95th', '99th']\n",
    "for body_part in BODY_PARTS:\n",
    "    x = tdf.annotation.apply(lambda x: generate_disparity(x, body_part))\n",
    "    y = tdf.manual_keypoints.apply(lambda x: generate_disparity(x, body_part))\n",
    "    diffs = x.values - y.values\n",
    "    diffs_dict[body_part] = np.percentile(abs(diffs), [5, 25, 50, 75, 95, 99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(diffs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How do AKPD weights compare to precise manual weights? </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "# initialize data transforms so that we can run inference with biomass neural network\n",
    "normalize_centered_2D_transform_biomass = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "\n",
    "# load neural network weights\n",
    "biomass_network = torch.load('/root/data/alok/biomass_estimation/results/neural_network/2019-11-08T00:13:09/nn_epoch_798.pb')\n",
    "akpd_scorer_network = load_model('/root/data/alok/biomass_estimation/playground/akpd_scorer_model_TF.h5') # make this better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_prediction(row_id, keypoints, cm):\n",
    "    \n",
    "    # run biomass estimation\n",
    "    input_sample = {\n",
    "        'keypoints': keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform_biomass.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    weight_prediction = biomass_network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    \n",
    "    return weight_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_predictions = []\n",
    "\n",
    "args = []\n",
    "count = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    cm = row.camera_metadata\n",
    "    keypoints = row.manual_keypoints\n",
    "    row_id = idx\n",
    "    weight_prediction = generate_weight_prediction(row_id, keypoints, cm)\n",
    "    weight_predictions.append(weight_prediction)\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['manual_weight'] = weight_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.weight - tdf.manual_weight, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_metrics(x, y):\n",
    "    mean_error_pct = np.mean((x-y)/y)\n",
    "    mean_abs_error_pct = np.mean(np.abs((x-y)/y))\n",
    "    percentiles = [5, 25, 50, 75, 95]\n",
    "    percentiles_abs_error_pct = np.percentile(np.abs((x-y)/y), percentiles)\n",
    "    print('Mean error percentage: {}'.format(mean_error_pct))\n",
    "    print('Mean absolute error percentage: {}'.format(mean_abs_error_pct))\n",
    "    for p, val in zip(percentiles, percentiles_abs_error_pct):\n",
    "        print('{}th percentile absolute error percentage: {}'.format(p, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy_metrics(tdf.weight.values, tdf.manual_weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How do AKPD trendlines compare to Manual trendlines? </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute daily growth rate via fitting an exponential curve,\n",
    "# weighting each day by its sample size\n",
    "def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "    x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    y = np.log(tdf.values)\n",
    "    reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "    growth_rate = reg.coef_[0]\n",
    "    trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "    return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# of today's distribution against distribution in the remainder of the window\n",
    "def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "    mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "    distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "    return distribution_confidence\n",
    "\n",
    "\n",
    "# NOTE: we need to think more carefully about this to understand how distribution \n",
    "# confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "    return 5000\n",
    "    \n",
    "# Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# or until the minimum sample size is achieved\n",
    "def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = sorted(list(tdf.index.date.astype(str)))\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(3, window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# generate date range given current date and window size. If future data\n",
    "# is available relative to current date, windows where the current date\n",
    "# is centered are preferred\n",
    "def compute_date_range(historical_dates, date, window_size_d):\n",
    "    FMT = '%Y-%m-%d'\n",
    "    max_num_days = 0\n",
    "    start_date, end_date = None, None\n",
    "    for i in range(window_size_d // 2 + 1):\n",
    "        lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "                            dt.timedelta(days=i)).strftime(FMT)\n",
    "        upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "        num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "                    (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "        if num_days >= max_num_days:\n",
    "            start_date, end_date = lower_bound_date, upper_bound_date\n",
    "            max_num_days = num_days\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    dts, vals = [], []\n",
    "    for iter_date in records:\n",
    "        for val in records[iter_date]:\n",
    "            dts.append(iter_date)\n",
    "            vals.append(val)\n",
    "\n",
    "    df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    # get raw statistics\n",
    "    raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['estimated_weight_g'] = tdf.weight\n",
    "tdf.index = pd.to_datetime(tdf.captured_at)\n",
    "tdf['hour'] = tdf.index.hour\n",
    "hour_mask = (tdf.hour > 7) & (tdf.hour < 16)\n",
    "kdf = tdf[hour_mask].copy(deep=True)\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(kdf.index.date.astype(str)))):\n",
    "    records[date].extend(kdf[date].estimated_weight_g.values.tolist())\n",
    "    \n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(kdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = kdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['estimated_weight_g'] = tdf.manual_weight\n",
    "tdf.index = pd.to_datetime(tdf.captured_at)\n",
    "tdf['hour'] = tdf.index.hour\n",
    "hour_mask = (tdf.hour > 7) & (tdf.hour < 16)\n",
    "kdf = tdf[hour_mask].copy(deep=True)\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(kdf.index.date.astype(str)))):\n",
    "    records[date].extend(kdf[date].estimated_weight_g.values.tolist())\n",
    "    \n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(kdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = kdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_weight_g'] = df.weight\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n",
    "hour_mask = (df.hour > 7) & (df.hour < 16)\n",
    "kdf = df[hour_mask].copy(deep=True)\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(kdf.index.date.astype(str)))):\n",
    "    records[date].extend(kdf[date].estimated_weight_g.values.tolist())\n",
    "    \n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(kdf.index.date.astype(str))))\n",
    "raw_avg_weights_2, raw_sample_sizes_2, growth_rates_2, trend_scores_2, smart_averages_2, distribution_confidences_2 = \\\n",
    "    [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates_2.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores_2.append(metadata['trend_score'])\n",
    "    raw_avg_weights_2.append(raw_avg_weight)\n",
    "    raw_sample_sizes_2.append(raw_sample_size)\n",
    "    smart_averages_2.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences_2.append(metadata['distribution_confidence'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = kdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights_2, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages_2, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages_2), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages_2), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes_2, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates_2)\n",
    "axes[3].plot(x_values, trend_scores_2)\n",
    "axes[4].plot(x_values, distribution_confidences_2)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(x_values[:-1], raw_sample_sizes, label='Manual Daily Sample Size')\n",
    "plt.plot(x_values[:-1], raw_sample_sizes_2[:-1], label='AKPD Daily Sample Size')\n",
    "# plt.plot(x_values[:-1], 1.02 * np.array(smart_averages_2[:-1]), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "# plt.plot(x_values[:-1], 0.98 * np.array(smart_averages_2[:-1]), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "plt.title('AKPD vs. Manual on Blom Kjeppevikholmen')\n",
    "# plt.plot(x_values[:-1], smart_averages, label='Manual Raw Average Weights')\n",
    "# plt.plot(x_values[:-1], smart_averages_2[:-1], label='AKPD Raw Average Weights')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "x = (4.73*np.exp(-7*growth_rates[i])*17651 + \\\n",
    " 4.78*np.exp(-8*growth_rates[i])*13524 + \\\n",
    " 4.85*np.exp(-9*growth_rates[i])*3960 + \\\n",
    " 5.36*np.exp(-29*growth_rates[i])*15259 + \\\n",
    " 5.52*np.exp(-35*growth_rates[i])*23111) / \\\n",
    " (17651+13524+3960+15259+23111)\n",
    "\n",
    "print((smart_averages[-1]-x*1e3)/(x*1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "x = (4.73*np.exp(-6*growth_rates_2[i])*17651 + \\\n",
    " 4.78*np.exp(-7*growth_rates_2[i])*13524 + \\\n",
    " 4.85*np.exp(-8*growth_rates_2[i])*3960 + \\\n",
    " 5.36*np.exp(-28*growth_rates_2[i])*15259 + \\\n",
    " 5.52*np.exp(-34*growth_rates_2[i])*23111) / \\\n",
    " (17651+13524+3960+15259+23111)\n",
    "\n",
    "print((smart_averages_2[-1]-x*1e3)/(x*1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
