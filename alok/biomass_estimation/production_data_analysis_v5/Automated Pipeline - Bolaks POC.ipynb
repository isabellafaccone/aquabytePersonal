{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from PIL import Image\n",
    "\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # compute daily growth rate via fitting an exponential curve,\n",
    "# # weighting each day by its sample size\n",
    "# def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "#     x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "#                  dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "#                  for k in tdf.index.date.astype(str)]\n",
    "#     X = np.array(x_values).reshape(-1, 1)\n",
    "#     y = np.log(tdf.values)\n",
    "#     reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "#     growth_rate = reg.coef_[0]\n",
    "#     trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "#     return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# # compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# # of today's distribution against distribution in the remainder of the window\n",
    "# def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "#     mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "#     x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "#     y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "#     distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "#     return distribution_confidence\n",
    "\n",
    "\n",
    "# # NOTE: we need to think more carefully about this to understand how distribution \n",
    "# # confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "# def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "#     return 5000\n",
    "    \n",
    "# # Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# # or until the minimum sample size is achieved\n",
    "# def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "#                           trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "#     dates = sorted(list(tdf.index.date.astype(str)))\n",
    "#     if len(dates) == 1:\n",
    "#         growth_rate = 0.0\n",
    "#     minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "#     x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "#                  dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "#                  for k in tdf.index.date.astype(str)]\n",
    "#     X = np.array(x_values).reshape(-1, 1)\n",
    "#     Y = tdf.values\n",
    "#     N = rdf.values\n",
    "    \n",
    "#     for i in range(window_size_d):\n",
    "#         if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "#             break\n",
    "#     N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "#     smart_average = 0.0\n",
    "#     sample_size = 0.0\n",
    "#     adj_weights = []\n",
    "#     total_days = 0\n",
    "#     for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "#         smart_average += np.exp(x * growth_rate) * y * n\n",
    "#         sample_size += n\n",
    "#         if n > 0:\n",
    "#             adj_weights_for_date = \\\n",
    "#                 list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "#             adj_weights.extend(adj_weights_for_date)\n",
    "#             total_days += 1\n",
    "        \n",
    "#     smart_average /= sample_size\n",
    "    \n",
    "#     adj_weights = np.array(adj_weights)\n",
    "#     distribution = {}\n",
    "#     buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "#     for b in buckets:\n",
    "#         low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "#         count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "#         distribution[b] = count / sample_size\n",
    "    \n",
    "#     output = {\n",
    "#         'weightMovingAvg': float(smart_average),\n",
    "#         'weightMovingDist': distribution,\n",
    "#         'numMovingAvgBatiFish': sample_size,\n",
    "#         'numMovingAvgLookbackDays': total_days,\n",
    "#         'dailyGrowthRate': growth_rate\n",
    "#     }\n",
    "    \n",
    "#     return output, adj_weights\n",
    "\n",
    "\n",
    "# # generate date range given current date and window size. If future data\n",
    "# # is available relative to current date, windows where the current date\n",
    "# # is centered are preferred\n",
    "# def compute_date_range(historical_dates, date, window_size_d):\n",
    "#     FMT = '%Y-%m-%d'\n",
    "#     max_num_days = 0\n",
    "#     start_date, end_date = None, None\n",
    "#     for i in range(window_size_d // 2 + 1):\n",
    "#         lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "#                             dt.timedelta(days=i)).strftime(FMT)\n",
    "#         upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "#         num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "#                     (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "#         if num_days >= max_num_days:\n",
    "#             start_date, end_date = lower_bound_date, upper_bound_date\n",
    "#             max_num_days = num_days\n",
    "#     return start_date, end_date\n",
    "\n",
    "\n",
    "# def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "#     records = json.loads(records_json)\n",
    "    \n",
    "#     dts, vals = [], []\n",
    "#     for iter_date in records:\n",
    "#         for val in records[iter_date]:\n",
    "#             dts.append(iter_date)\n",
    "#             vals.append(val)\n",
    "\n",
    "#     df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "#     # get raw statistics\n",
    "#     raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "#     raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "#     # compute relevant date range\n",
    "#     historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "#     start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "#     rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "#     tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "#     tdf = tdf[rdf > 0].copy(deep=True)\n",
    "#     rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "#     growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "#     if start_date < end_date:\n",
    "#         growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "#         distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "#     smart_average, adj_weights = compute_smart_average(df, tdf, rdf, date, \n",
    "#                                           distribution_confidence, growth_rate, \n",
    "#                                           trend_score, window_size_d)\n",
    "#     metadata = {\n",
    "#         'trend_score': trend_score,\n",
    "#         'distribution_confidence': distribution_confidence\n",
    "#     }\n",
    "\n",
    "#     return raw_avg_weight, raw_sample_size, smart_average, metadata, adj_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframe\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "pen_id, group_id = 88, '88'\n",
    "query = \"\"\"\n",
    "    SELECT * FROM\n",
    "    prod.biomass_computations bc\n",
    "    WHERE bc.pen_id={0}\n",
    "    AND bc.group_id='{1}';\n",
    "\"\"\".format(pen_id, group_id)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=88) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at between '2020-02-10' and '2020-03-10'\n",
    "      AND prod.biomass_computations.akpd_score > 0.9) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at between '2020-02-10' and '2020-03-10'\n",
    "    AND x.pen_id = 88\n",
    "    AND x.group_id = '88';\n",
    "\"\"\"\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "df = df.sort_values('captured_at')\n",
    "df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for idx, row in df[df.akpd_score > 0.95].iterrows():\n",
    "    ann_c = row.annotation\n",
    "    ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "    ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "    for bp in BODY_PARTS:\n",
    "        diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "        diffs.append(diff)\n",
    "print(np.mean(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "    \n",
    "df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'bolaks_pen_id_88_2020-02-10_2020-03-10.csv'\n",
    "f = os.path.join('/root/data/alok/biomass_estimation/playground', f_name)\n",
    "df.to_csv(f)\n",
    "s3_access_utils.s3_client.upload_file(f, 'aquabyte-images-adhoc', 'alok/production_datasets/{}'.format(f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/bolaks_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file('/root/data/alok/biomass_estimation/playground/bolaks_data_full.csv', 'aquabyte-images-adhoc', 'bolaks_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Y-Coordinate Deviation Diagnosis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', \n",
    "                                      'aquabyte-images-adhoc', \n",
    "                                      'bolaks_data_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-02-11'\n",
    "FMT = '%Y-%m-%d'\n",
    "dates = [dt.datetime.strftime(dt.datetime.strptime(start_date, FMT) + dt.timedelta(days=i), FMT) for i in range(27)]\n",
    "for i in range(len(dates) - 1):\n",
    "    start_date, end_date = dates[i], dates[i+1]\n",
    "    diffs = []\n",
    "    for idx, row in df[(df.captured_at > start_date) & (df.captured_at < end_date) & (df.akpd_score > 0.95)].iterrows():\n",
    "        ann_c = row.annotation\n",
    "        ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "        ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "        for bp in BODY_PARTS:\n",
    "            diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "            diffs.append(diff)\n",
    "    print('Mean y-coordinate difference on {}: {}'.format(start_date, np.mean(diffs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.left_crop_url == 'https://aquabyte-crops.s3.eu-west-1.amazonaws.com/environment=production/site-id=56/pen-id=88/date=2020-02-11/hour=15/at=2020-02-11T15:24:13.622513000Z/left_frame_crop_1316_1714_3316_2715.jpg'\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(df[mask].left_crop_url.iloc[0].iloc[0])\n",
    "left_image = Image.open(left_image_f)\n",
    "print(np.array(left_image).shape)\n",
    "print(row.left_crop_metadata)\n",
    "\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(df[mask].right_crop_url)\n",
    "right_image = Image.open(right_image_f)\n",
    "print(np.array(right_image).shape)\n",
    "print(row.right_crop_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM keypoint_annotations\n",
    "WHERE pen_id=88\n",
    "AND keypoints is not null\n",
    "AND keypoints -> 'leftCrop' is not null\n",
    "AND keypoints -> 'rightCrop' is not null\n",
    "AND is_qa = FALSE;\n",
    "\"\"\"\n",
    "\n",
    "mdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for idx, row in mdf.iterrows():\n",
    "    ann_c = row.keypoints\n",
    "    ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "    ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "    for bp in BODY_PARTS:\n",
    "        diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "        diffs.append(diff)\n",
    "print(np.median(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(diffs, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/data/alok/biomass_estimation/playground/bolaks_raw_data.csv', 'w') as f:\n",
    "    for idx in range(len(adj_weights) - 1):\n",
    "        f.write('{},\\n'.format(adj_weights[idx]))\n",
    "    f.write(str(adj_weights[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_avg_weight, raw_sample_size, smart_average, metadata, adj_weights = compute_metrics('2020-03-06', records_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mask = (df.hour > 7) & (df.hour < 16)\n",
    "# low_kf_mask = df.k_factor < 1.0\n",
    "tdf = df[day_mask]\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "    records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = tdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_average['weightMovingAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(df[(df.captured_at > '2020-03-01') & (df.captured_at < '2020-03-06') & (day_mask)].left_crop_url.values[:, 0])\n",
    "base_keys = []\n",
    "for url in urls:\n",
    "    base_key = os.path.dirname(url[url.index('environment'):])\n",
    "    if base_key not in base_keys:\n",
    "        base_keys.append(base_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "out_f = '/root/data/alok/biomass_estimation/playground/bolaks_data.csv'\n",
    "with open(out_f, 'w', newline='\\n') as f:\n",
    "    for line in base_keys[:-1]:\n",
    "        f.write(line + ',\\n')\n",
    "    f.write(base_keys[-1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates, trend_scores, raw_avg_weights, raw_sample_sizes, smart_averages = \\\n",
    "defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "for lo in [6, 7, 8]:\n",
    "    for hi in [14, 15, 16, 17]:\n",
    "        key = '{}-{}'.format(lo, hi)\n",
    "        print(key)\n",
    "        day_mask = (df.hour > lo) & (df.hour < hi)\n",
    "        tdf = df[day_mask]\n",
    "\n",
    "        # get daily averages and sample sizes\n",
    "\n",
    "        records = defaultdict(list)\n",
    "        for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "            records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "        records_json = json.dumps(records)\n",
    "\n",
    "        dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "        for date in dates:\n",
    "            raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "            growth_rates[key].append(smart_average['dailyGrowthRate'])\n",
    "            trend_scores[key].append(metadata['trend_score'])\n",
    "            raw_avg_weights[key].append(raw_avg_weight)\n",
    "            raw_sample_sizes[key].append(raw_sample_size)\n",
    "            smart_averages[key].append(smart_average['weightMovingAvg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for lo in [6, 7, 8]:\n",
    "    for hi in [14, 15, 16, 17]:\n",
    "        key = '{}-{}'.format(lo, hi)\n",
    "        fig.add_trace(go.Scatter(x=x_values[1:], y=smart_averages[key][1:],\n",
    "                            mode='lines',\n",
    "                            name=key))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame(smart_averages, index=x_values).round(2)\n",
    "mdf['max_variation_pct'] = (mdf.max(axis=1) - mdf.min(axis=1)) / mdf.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axes[0].plot(x_values[1:], smart_averages, label='Smart Avg.')\n",
    "axes[0].set_title('Growth trend for fish with KF > 1')\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].plot(x_values, smart_averages_2, label='Smart Avg. 2')\n",
    "axes[1].set_title('Growth trend for fish with KF < 1')\n",
    "axes[1].grid()\n",
    "# axes[1].set_ylim([1400, 1700])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(smart_averages[-1] / smart_averages[0]) / len(smart_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(smart_averages_2[-1] / smart_averages_2[0]) / len(smart_averages_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mask = (df.hour > 7) & (df.hour < 16)\n",
    "low_kf_mask = df.k_factor < 1.0\n",
    "tdf = df[~day_mask & low_kf_mask]\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "    records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages_2, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages_2.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = tdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "hour_mask = (df.hour > 7) & (df.hour < 19)\n",
    "plt.hist(df[~hour_mask & (df.captured_at > '2020-02-16') & (df.captured_at < '2020-02-17')].estimated_weight_g, bins=20)\n",
    "plt.title('Aggregate Weight Distribution (24 hour data)')\n",
    "plt.xlabel('Weight Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df.estimated_weight_g, bins=50)\n",
    "plt.title('Aggregate Weight Distribution (Daylight hours only)')\n",
    "plt.xlabel('Weight Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "N = 2\n",
    "for idx in range(len(dates)-N-1):\n",
    "    start_date, end_date, date = dates[idx], dates[idx+N], dates[idx+N+1]\n",
    "    mean_adjustment = df[date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date].estimated_weight_g, list(range(100)))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    axes[0].scatter(x[1:99], y[1:99])\n",
    "    axes[0].plot([x[1], x[99]], [x[1], x[99]], color='red')\n",
    "    axes[0].set_title(date)\n",
    "    axes[0].grid()\n",
    "    \n",
    "    lower_bound = int(df[start_date:date].estimated_weight_g.min() * 0.8)\n",
    "    upper_bound = int(df[start_date:date].estimated_weight_g.max() * 1.2)\n",
    "    \n",
    "    axes[1].hist(df[start_date:end_date].estimated_weight_g, bins=list(np.arange(lower_bound, upper_bound, 100)), color='blue', alpha=0.5, density=True)\n",
    "    axes[1].hist(df[date].estimated_weight_g, bins=list(np.arange(lower_bound, upper_bound, 100)), color='red', alpha=0.5, density=True)\n",
    "    axes[1].set_title(date)\n",
    "    axes[1].grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "cols = 3\n",
    "rows = len(dates) // 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "fig.tight_layout(pad=3.0)\n",
    "for idx, date in enumerate(dates):\n",
    "    if idx == 8:\n",
    "        continue\n",
    "    tdf = df[date][df[date].estimated_weight_g > 3000].groupby('hour')['estimated_weight_g'].agg(lambda x: x.shape[0])\n",
    "    \n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].plot(tdf.index, tdf.values)\n",
    "    axes[row, col].set_xlabel('Hour')\n",
    "    axes[row, col].set_ylabel('Avg. Weight')\n",
    "    axes[row, col].set_title(date)\n",
    "    \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add in Length / K-Factor Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset\n",
    "    This is the base version of the dataset that is used to map 3D keypoints to a\n",
    "    biomass estimate. The label is the weight, and the input is the 3D workd keypoints\n",
    "    obtained during triangulation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        if self.transform:\n",
    "            input_sample = {\n",
    "                'keypoints': row.keypoints,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "            }\n",
    "            if 'length' in dict(row).keys():\n",
    "                input_sample['label'] = row.length\n",
    "            sample = self.transform(input_sample)\n",
    "            return sample\n",
    "\n",
    "        world_keypoints = row.world_keypoints\n",
    "        length = row.length\n",
    "\n",
    "        sample = {'kp_input': world_keypoints, 'label': length, 'stereo_pair_id': row.id}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class NormalizedStabilityTransform(object):\n",
    "    \"\"\"\n",
    "        Transforms world keypoints into a more stable coordinate system - this will lead to better\n",
    "        training / convergene\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        modified_kps, label, stereo_pair_id, cm = \\\n",
    "            sample['modified_kps'], sample['label'], sample['stereo_pair_id'], sample['cm']\n",
    "        modified_wkps = pixel2world(modified_kps['leftCrop'], modified_kps['rightCrop'], cm)\n",
    "        stabilized_coordinates = {}\n",
    "        for bp in BODY_PARTS:\n",
    "            wkp = modified_wkps[bp]\n",
    "            stabilized_kp_info = [0.5 * wkp[0]/wkp[1], 0.5 * wkp[2]/wkp[1], 0.5 * 0.1/wkp[1]]\n",
    "            stabilized_coordinates[bp] = stabilized_kp_info\n",
    "            \n",
    "        normalized_label = label\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'kp_input': stabilized_coordinates,\n",
    "            'label': normalized_label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.load('/root/data/alok/biomass_estimation/playground/length_predictor.pb')\n",
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_length(row_id, akpd_keypoints, cm):\n",
    "\n",
    "    # run length estimation\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    length_prediction = network(tensorized_kps['kp_input']).item() * 1e-3\n",
    "    \n",
    "    return length_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "args = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    cm = row.camera_metadata\n",
    "    akpd_keypoints = row.annotation\n",
    "    row_id = idx\n",
    "    length = generate_length(row_id, akpd_keypoints, cm)\n",
    "    lengths.append(length)\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = lengths\n",
    "df['k_factor'] = 1e5 * df.estimated_weight_g / (1e3 * df.length)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.k_factor, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k_factor > 2.5].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.estimated_weight_g, df.k_factor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.arange(500, 9000, 500)\n",
    "mean_kfs = []\n",
    "for idx in range(len(weights) - 1):\n",
    "    lo, hi = weights[idx], weights[idx + 1]\n",
    "    mask = (df.estimated_weight_g > lo) & (df.estimated_weight_g < hi)\n",
    "    mean_kf = df[mask].k_factor.mean()\n",
    "    mean_kfs.append(mean_kf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "x_pos = np.arange(len(weights[:-1]))\n",
    "plt.bar(x_pos, mean_kfs, align='edge', width=0.9)\n",
    "plt.xticks(x_pos, weights[:-1])\n",
    "plt.title('K-Factor vs. Weight')\n",
    "plt.xlabel('Predicted weight (g)')\n",
    "plt.ylabel('Predicted K-factor')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.k_factor < np.percentile(df.k_factor, 98)].copy(deep=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "start_date, end_date = '2020-02-19', '2020-02-20'\n",
    "plt.scatter(tdf[start_date:end_date].index, \n",
    "            tdf[start_date:end_date].estimated_weight_g,\n",
    "            c=tdf[start_date:end_date].k_factor, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Estimated Weight (g)')\n",
    "plt.title('Weight Predictions vs. Time, color-coded by K-Factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
