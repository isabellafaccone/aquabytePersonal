{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "# from aquabyte.akpd import AKPD\n",
    "# from aquabyte.template_matching import find_matches_and_homography\n",
    "# from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "# from aquabyte.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# from aquabyte.akpd_scorer import generate_confidence_score\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframe\n",
    "s3_access_utils = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "pen_id = 5\n",
    "group_id = str(pen_id)\n",
    "start_date = '2019-06-18'\n",
    "end_date = '2019-07-02'\n",
    "name = 'kjeppevikholmen'\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id={pen_id}) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, estimated_k_factor, estimated_length_mm, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at >= '{start_date}' and prod.biomass_computations.captured_at <= '{end_date}'\n",
    "      AND prod.biomass_computations.akpd_score > 0.0) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at >= '{start_date}' and x.captured_at <= '{end_date}'\n",
    "    AND x.pen_id = {pen_id}\n",
    "    AND x.group_id = '{pen_id}';\n",
    "\"\"\"\n",
    "\n",
    "# query = f\"\"\"\n",
    "# SELECT captured_at, left_crop_url, estimated_weight_g, estimated_k_factor, akpd_score, annotation, camera_metadata FROM prod.biomass_computations\n",
    "# WHERE prod.biomass_computations.captured_at >= '{start_date}' and prod.biomass_computations.captured_at <= '{end_date}'\n",
    "# AND prod.biomass_computations.akpd_score > 0.99\n",
    "# \"\"\"\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "df = df.sort_values('captured_at')\n",
    "df = df[df.akpd_score > 0.0].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'/root/data/alok/biomass_estimation/playground/{name}_pen_id_{pen_id}_{start_date}_{end_date}.csv')\n",
    "f = f'/root/data/alok/biomass_estimation/playground/{name}_pen_id_{pen_id}_{start_date}_{end_date}.csv'\n",
    "s3_access_utils.s3_client.upload_file(f, 'aquabyte-images-adhoc', f'alok/production_datasets/{name}_pen_id_{pen_id}_{start_date}_{end_date}/annotation_dataset.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframe\n",
    "s3_access_utils = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "pen_id = 114\n",
    "group_id = str(pen_id)\n",
    "start_date = '2020-10-12'\n",
    "end_date = '2020-10-13'\n",
    "name = 'movikodden'\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "    \n",
    "    SELECT captured_at, left_crop_url, estimated_weight_g, estimated_k_factor, estimated_length_mm, akpd_score FROM prod.biomass_computations\n",
    "    WHERE prod.biomass_computations.captured_at >= '{start_date}' and prod.biomass_computations.captured_at <= '{end_date}'\n",
    "    AND prod.biomass_computations.akpd_score > 0.9\n",
    "    AND pen_id=114\n",
    "\"\"\".format(start_date, end_date)\n",
    "\n",
    "# query = f\"\"\"\n",
    "# SELECT captured_at, left_crop_url, estimated_weight_g, estimated_k_factor, akpd_score, annotation, camera_metadata FROM prod.biomass_computations\n",
    "# WHERE prod.biomass_computations.captured_at >= '{start_date}' and prod.biomass_computations.captured_at <= '{end_date}'\n",
    "# AND prod.biomass_computations.akpd_score > 0.99\n",
    "# \"\"\"\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "df = df.sort_values('captured_at')\n",
    "df = df[df.akpd_score > 0.99].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kf'] = 1e5 * df.estimated_weight_g.values / (df.estimated_length_mm.values ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.estimated_length_mm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e5 * df.estimated_weight_g.mean() / (df.estimated_length_mm.mean() ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    ann = row.annotation\n",
    "    ann_left_dict = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['leftCrop']}\n",
    "    ann_right_dict = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['rightCrop']}\n",
    "    for bp in BODY_PARTS:\n",
    "        diffs.append(ann_left_dict[bp][1] - ann_right_dict[bp][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.camera_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id = 5\n",
    "group_id = str(pen_id)\n",
    "start_date = '2019-06-05'\n",
    "end_date = '2020-07-02'\n",
    "name = 'kjeppevikholmen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(np.log(1 - df.akpd_score))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for idx, row in df[df.akpd_score > 0.95].iterrows():\n",
    "    ann_c = row.annotation\n",
    "    ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "    ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "    for bp in BODY_PARTS:\n",
    "        diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "        diffs.append(diff)\n",
    "print(np.mean(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "    \n",
    "df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'bolaks_pen_id_88_2020-02-10_2020-03-10.csv'\n",
    "f = os.path.join('/root/data/alok/biomass_estimation/playground', f_name)\n",
    "df.to_csv(f)\n",
    "s3_access_utils.s3_client.upload_file(f, 'aquabyte-images-adhoc', 'alok/production_datasets/{}'.format(f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/bolaks_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file('/root/data/alok/biomass_estimation/playground/bolaks_data_full.csv', 'aquabyte-images-adhoc', 'bolaks_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Y-Coordinate Deviation Diagnosis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', \n",
    "                                      'aquabyte-images-adhoc', \n",
    "                                      'bolaks_data_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-02-11'\n",
    "FMT = '%Y-%m-%d'\n",
    "dates = [dt.datetime.strftime(dt.datetime.strptime(start_date, FMT) + dt.timedelta(days=i), FMT) for i in range(27)]\n",
    "for i in range(len(dates) - 1):\n",
    "    start_date, end_date = dates[i], dates[i+1]\n",
    "    diffs = []\n",
    "    for idx, row in df[(df.captured_at > start_date) & (df.captured_at < end_date) & (df.akpd_score > 0.95)].iterrows():\n",
    "        ann_c = row.annotation\n",
    "        ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "        ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "        for bp in BODY_PARTS:\n",
    "            diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "            diffs.append(diff)\n",
    "    print('Mean y-coordinate difference on {}: {}'.format(start_date, np.mean(diffs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.left_crop_url == 'https://aquabyte-crops.s3.eu-west-1.amazonaws.com/environment=production/site-id=56/pen-id=88/date=2020-02-11/hour=15/at=2020-02-11T15:24:13.622513000Z/left_frame_crop_1316_1714_3316_2715.jpg'\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(df[mask].left_crop_url.iloc[0].iloc[0])\n",
    "left_image = Image.open(left_image_f)\n",
    "print(np.array(left_image).shape)\n",
    "print(row.left_crop_metadata)\n",
    "\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(df[mask].right_crop_url)\n",
    "right_image = Image.open(right_image_f)\n",
    "print(np.array(right_image).shape)\n",
    "print(row.right_crop_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM keypoint_annotations\n",
    "WHERE pen_id=88\n",
    "AND keypoints is not null\n",
    "AND keypoints -> 'leftCrop' is not null\n",
    "AND keypoints -> 'rightCrop' is not null\n",
    "AND is_qa = FALSE;\n",
    "\"\"\"\n",
    "\n",
    "mdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for idx, row in mdf.iterrows():\n",
    "    ann_c = row.keypoints\n",
    "    ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "    ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "    for bp in BODY_PARTS:\n",
    "        diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "        diffs.append(diff)\n",
    "print(np.median(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(diffs, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/data/alok/biomass_estimation/playground/bolaks_raw_data.csv', 'w') as f:\n",
    "    for idx in range(len(adj_weights) - 1):\n",
    "        f.write('{},\\n'.format(adj_weights[idx]))\n",
    "    f.write(str(adj_weights[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_avg_weight, raw_sample_size, smart_average, metadata, adj_weights = compute_metrics('2020-03-06', records_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mask = (df.hour > 7) & (df.hour < 16)\n",
    "# low_kf_mask = df.k_factor < 1.0\n",
    "tdf = df[day_mask]\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "    records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = tdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_average['weightMovingAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(df[(df.captured_at > '2020-03-01') & (df.captured_at < '2020-03-06') & (day_mask)].left_crop_url.values[:, 0])\n",
    "base_keys = []\n",
    "for url in urls:\n",
    "    base_key = os.path.dirname(url[url.index('environment'):])\n",
    "    if base_key not in base_keys:\n",
    "        base_keys.append(base_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "out_f = '/root/data/alok/biomass_estimation/playground/bolaks_data.csv'\n",
    "with open(out_f, 'w', newline='\\n') as f:\n",
    "    for line in base_keys[:-1]:\n",
    "        f.write(line + ',\\n')\n",
    "    f.write(base_keys[-1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates, trend_scores, raw_avg_weights, raw_sample_sizes, smart_averages = \\\n",
    "defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "for lo in [6, 7, 8]:\n",
    "    for hi in [14, 15, 16, 17]:\n",
    "        key = '{}-{}'.format(lo, hi)\n",
    "        print(key)\n",
    "        day_mask = (df.hour > lo) & (df.hour < hi)\n",
    "        tdf = df[day_mask]\n",
    "\n",
    "        # get daily averages and sample sizes\n",
    "\n",
    "        records = defaultdict(list)\n",
    "        for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "            records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "        records_json = json.dumps(records)\n",
    "\n",
    "        dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "        for date in dates:\n",
    "            raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "            growth_rates[key].append(smart_average['dailyGrowthRate'])\n",
    "            trend_scores[key].append(metadata['trend_score'])\n",
    "            raw_avg_weights[key].append(raw_avg_weight)\n",
    "            raw_sample_sizes[key].append(raw_sample_size)\n",
    "            smart_averages[key].append(smart_average['weightMovingAvg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for lo in [6, 7, 8]:\n",
    "    for hi in [14, 15, 16, 17]:\n",
    "        key = '{}-{}'.format(lo, hi)\n",
    "        fig.add_trace(go.Scatter(x=x_values[1:], y=smart_averages[key][1:],\n",
    "                            mode='lines',\n",
    "                            name=key))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.DataFrame(smart_averages, index=x_values).round(2)\n",
    "mdf['max_variation_pct'] = (mdf.max(axis=1) - mdf.min(axis=1)) / mdf.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axes[0].plot(x_values[1:], smart_averages, label='Smart Avg.')\n",
    "axes[0].set_title('Growth trend for fish with KF > 1')\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].plot(x_values, smart_averages_2, label='Smart Avg. 2')\n",
    "axes[1].set_title('Growth trend for fish with KF < 1')\n",
    "axes[1].grid()\n",
    "# axes[1].set_ylim([1400, 1700])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(smart_averages[-1] / smart_averages[0]) / len(smart_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(smart_averages_2[-1] / smart_averages_2[0]) / len(smart_averages_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mask = (df.hour > 7) & (df.hour < 16)\n",
    "low_kf_mask = df.k_factor < 1.0\n",
    "tdf = df[~day_mask & low_kf_mask]\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(tdf.index.date.astype(str)))):\n",
    "    records[date].extend(tdf[date].estimated_weight_g.values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)\n",
    "\n",
    "dates = sorted(list(set(tdf.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages_2, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages_2.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = tdf.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "hour_mask = (df.hour > 7) & (df.hour < 19)\n",
    "plt.hist(df[~hour_mask & (df.captured_at > '2020-02-16') & (df.captured_at < '2020-02-17')].estimated_weight_g, bins=20)\n",
    "plt.title('Aggregate Weight Distribution (24 hour data)')\n",
    "plt.xlabel('Weight Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df.estimated_weight_g, bins=50)\n",
    "plt.title('Aggregate Weight Distribution (Daylight hours only)')\n",
    "plt.xlabel('Weight Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "N = 2\n",
    "for idx in range(len(dates)-N-1):\n",
    "    start_date, end_date, date = dates[idx], dates[idx+N], dates[idx+N+1]\n",
    "    mean_adjustment = df[date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date].estimated_weight_g, list(range(100)))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    axes[0].scatter(x[1:99], y[1:99])\n",
    "    axes[0].plot([x[1], x[99]], [x[1], x[99]], color='red')\n",
    "    axes[0].set_title(date)\n",
    "    axes[0].grid()\n",
    "    \n",
    "    lower_bound = int(df[start_date:date].estimated_weight_g.min() * 0.8)\n",
    "    upper_bound = int(df[start_date:date].estimated_weight_g.max() * 1.2)\n",
    "    \n",
    "    axes[1].hist(df[start_date:end_date].estimated_weight_g, bins=list(np.arange(lower_bound, upper_bound, 100)), color='blue', alpha=0.5, density=True)\n",
    "    axes[1].hist(df[date].estimated_weight_g, bins=list(np.arange(lower_bound, upper_bound, 100)), color='red', alpha=0.5, density=True)\n",
    "    axes[1].set_title(date)\n",
    "    axes[1].grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "cols = 3\n",
    "rows = len(dates) // 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "fig.tight_layout(pad=3.0)\n",
    "for idx, date in enumerate(dates):\n",
    "    if idx == 8:\n",
    "        continue\n",
    "    tdf = df[date][df[date].estimated_weight_g > 3000].groupby('hour')['estimated_weight_g'].agg(lambda x: x.shape[0])\n",
    "    \n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].plot(tdf.index, tdf.values)\n",
    "    axes[row, col].set_xlabel('Hour')\n",
    "    axes[row, col].set_ylabel('Avg. Weight')\n",
    "    axes[row, col].set_title(date)\n",
    "    \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset_url = 'https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/alok/production_datasets/kjeppevikholmen_pen_5_2019-06-05_2019-07-03/annotation_dataset.csv'\n",
    "annotation_dataset_f, _, _ = s3_access_utils.download_from_url(annotation_dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(annotation_dataset_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add in Length / K-Factor Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset\n",
    "    This is the base version of the dataset that is used to map 3D keypoints to a\n",
    "    biomass estimate. The label is the weight, and the input is the 3D workd keypoints\n",
    "    obtained during triangulation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        if self.transform:\n",
    "            input_sample = {\n",
    "                'keypoints': row.keypoints,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "            }\n",
    "            if 'length' in dict(row).keys():\n",
    "                input_sample['label'] = row.length\n",
    "            sample = self.transform(input_sample)\n",
    "            return sample\n",
    "\n",
    "        world_keypoints = row.world_keypoints\n",
    "        length = row.length\n",
    "\n",
    "        sample = {'kp_input': world_keypoints, 'label': length, 'stereo_pair_id': row.id}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class NormalizedStabilityTransform(object):\n",
    "    \"\"\"\n",
    "        Transforms world keypoints into a more stable coordinate system - this will lead to better\n",
    "        training / convergene\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        modified_kps, label, stereo_pair_id, cm = \\\n",
    "            sample['modified_kps'], sample['label'], sample['stereo_pair_id'], sample['cm']\n",
    "        modified_wkps = pixel2world(modified_kps['leftCrop'], modified_kps['rightCrop'], cm)\n",
    "        stabilized_coordinates = {}\n",
    "        for bp in BODY_PARTS:\n",
    "            wkp = modified_wkps[bp]\n",
    "            stabilized_kp_info = [0.5 * wkp[0]/wkp[1], 0.5 * wkp[2]/wkp[1], 0.5 * 0.1/wkp[1]]\n",
    "            stabilized_coordinates[bp] = stabilized_kp_info\n",
    "            \n",
    "        normalized_label = label\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'kp_input': stabilized_coordinates,\n",
    "            'label': normalized_label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample\n",
    "    \n",
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "class NormalizeCentered2D(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the 2D left and right keypoints such that:\n",
    "        (1) The center of the left image 2D keypoints is located at the center of the left image\n",
    "            (i.e. 2D translation)\n",
    "        (2) The left image keypoints are possibly flipped such that the upper-lip x-coordinate \n",
    "            is greater than the tail-notch coordinate. This is done to reduce the total number of \n",
    "            spatial orientations the network must learn from -> reduces the training size\n",
    "        (3) The left image keypoints are then rotated such that upper-lip is located on the x-axis.\n",
    "            As in (2), this is done to reduce the total number of spatial orientations the network \n",
    "            must learn from -> reduces the training size\n",
    "        (4) Rescale all left image keypoints by some random number between 'lo' and 'hi' args\n",
    "        (5) Apply Gaussian random noise \"jitter\" to each keypoint to mimic annotation error\n",
    "        (5) For all transformations above, the right image keypoint coordinates are accordingly\n",
    "            transformed such that the original disparity values are preserved for all keypoints\n",
    "            (or adjusted during rescaling event)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def flip_center_kps(self, left_kps, right_kps):\n",
    "\n",
    "        x_min_l = min([kp[0] for kp in left_kps.values()])\n",
    "        x_max_l = max([kp[0] for kp in left_kps.values()])\n",
    "        x_mid_l = np.mean([x_min_l, x_max_l])\n",
    "\n",
    "        y_min_l = min([kp[1] for kp in left_kps.values()])\n",
    "        y_max_l = max([kp[1] for kp in left_kps.values()])\n",
    "        y_mid_l = np.mean([y_min_l, y_max_l])\n",
    "\n",
    "        x_min_r = min([kp[0] for kp in right_kps.values()])\n",
    "        x_max_r = max([kp[0] for kp in right_kps.values()])\n",
    "        x_mid_r = np.mean([x_min_r, x_max_r])\n",
    "\n",
    "        y_min_r = min([kp[1] for kp in right_kps.values()])\n",
    "        y_max_r = max([kp[1] for kp in right_kps.values()])\n",
    "        y_mid_r = np.mean([y_min_r, y_max_r])\n",
    "\n",
    "        fc_left_kps, fc_right_kps = {}, {}\n",
    "        flip_factor = 1 if left_kps['UPPER_LIP'][0] > left_kps['TAIL_NOTCH'][0] else -1\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            if flip_factor > 0:\n",
    "                fc_left_kp = np.array([left_kp[0] - x_mid_l, left_kp[1] - y_mid_l])\n",
    "                fc_right_kp = np.array([right_kp[0] - x_mid_l, right_kp[1] - y_mid_l])\n",
    "            else:\n",
    "                fc_right_kp = np.array([x_mid_r - left_kp[0], left_kp[1] - y_mid_r])\n",
    "                fc_left_kp = np.array([x_mid_r - right_kp[0], right_kp[1] - y_mid_r])\n",
    "            fc_left_kps[bp] = fc_left_kp\n",
    "            fc_right_kps[bp] = fc_right_kp\n",
    "\n",
    "        return fc_left_kps, fc_right_kps\n",
    "\n",
    "\n",
    "    def _rotate_cc(self, p, theta):\n",
    "        R = np.array([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "\n",
    "        rotated_kp = np.dot(R, p)\n",
    "        return rotated_kp\n",
    "\n",
    "\n",
    "    def rotate_kps(self, left_kps, right_kps):\n",
    "        upper_lip_x, upper_lip_y = left_kps['UPPER_LIP']\n",
    "        theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "        r_left_kps, r_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            rotated_kp = self._rotate_cc(left_kps[bp], -theta)\n",
    "            r_left_kps[bp] = rotated_kp\n",
    "            disp = abs(left_kps[bp][0] - right_kps[bp][0])\n",
    "            r_right_kps[bp] = np.array([rotated_kp[0] - disp, rotated_kp[1]])\n",
    "\n",
    "        return r_left_kps, r_right_kps\n",
    "\n",
    "\n",
    "    def scale_kps(self, left_kps, right_kps, factor):\n",
    "        s_left_kps, s_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            s_left_kps[bp] = factor * np.array(left_kps[bp])\n",
    "            s_right_kps[bp] = factor * np.array(right_kps[bp])\n",
    "\n",
    "        return s_left_kps, s_right_kps\n",
    "\n",
    "\n",
    "    def jitter_kps(self, left_kps, right_kps, jitter):\n",
    "        j_left_kps, j_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            j_left_kps[bp] = np.array([left_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                       left_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "            j_right_kps[bp] = np.array([right_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                        right_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "\n",
    "        return j_left_kps, j_right_kps\n",
    "\n",
    "\n",
    "\n",
    "    def modify_kps(self, left_kps, right_kps, factor, jitter, cm, rotate=True, center=False):\n",
    "        fc_left_kps, fc_right_kps = self.flip_center_kps(left_kps, right_kps)\n",
    "        if rotate:\n",
    "            r_left_kps, r_right_kps = self.rotate_kps(fc_left_kps, fc_right_kps)\n",
    "            s_left_kps, s_right_kps = self.scale_kps(r_left_kps, r_right_kps, factor)\n",
    "        else:\n",
    "            s_left_kps, s_right_kps = self.scale_kps(fc_left_kps, fc_right_kps, factor)\n",
    "        j_left_kps, j_right_kps  = self.jitter_kps(s_left_kps, s_right_kps, jitter)\n",
    "        j_left_kps_list, j_right_kps_list = [], []\n",
    "        if not center:\n",
    "            for bp in BODY_PARTS:\n",
    "                l_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_left_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_left_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "\n",
    "                r_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_right_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_right_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "\n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "        else:\n",
    "            for bp in BODY_PARTS:\n",
    "                l_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_left_kps[bp][0],\n",
    "                    'yFrame': j_left_kps[bp][1]\n",
    "                }\n",
    "\n",
    "                r_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_right_kps[bp][0],\n",
    "                    'yFrame': j_right_kps[bp][1]\n",
    "                }\n",
    "\n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "\n",
    "\n",
    "        modified_kps = {\n",
    "            'leftCrop': j_left_kps_list,\n",
    "            'rightCrop': j_right_kps_list\n",
    "        }\n",
    "\n",
    "        return modified_kps\n",
    "\n",
    "    \n",
    "    def __init__(self, lo=None, hi=None, jitter=0.0, rotate=True, center=False):\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "        self.jitter = jitter\n",
    "        self.rotate = rotate\n",
    "        self.center = center\n",
    "    \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        keypoints, cm, stereo_pair_id, label = \\\n",
    "            sample['keypoints'], sample['cm'], sample.get('stereo_pair_id'), sample.get('label')\n",
    "        left_keypoints_list = keypoints['leftCrop']\n",
    "        right_keypoints_list = keypoints['rightCrop']\n",
    "        left_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints_list}\n",
    "        right_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints_list}\n",
    "        \n",
    "        factor = 1.0 \n",
    "        if self.lo and self.hi:\n",
    "            factor = np.random.uniform(low=self.lo, high=self.hi)\n",
    "            \n",
    "        jitter = np.random.uniform(high=self.jitter)\n",
    "        \n",
    "        modified_kps = self.modify_kps(left_kps, right_kps, factor, jitter, cm, \n",
    "            rotate=self.rotate, center=self.center)\n",
    "\n",
    "        kp_input = {}\n",
    "        for idx, _ in enumerate(modified_kps['leftCrop']):\n",
    "            left_item, right_item = modified_kps['leftCrop'][idx], modified_kps['rightCrop'][idx]\n",
    "            bp = left_item['keypointType']\n",
    "            kp_input[bp] = [left_item['xFrame'], left_item['yFrame'], right_item['xFrame'], right_item['yFrame']]\n",
    "\n",
    "\n",
    "        transformed_sample = {\n",
    "            'kp_input': kp_input,\n",
    "            'modified_kps': modified_kps,\n",
    "            'label': label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'cm': cm,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample\n",
    "        \n",
    "\n",
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        kp_input, label, stereo_pair_id = \\\n",
    "            sample['kp_input'], sample.get('label'), sample.get('stereo_pair_id')\n",
    "        \n",
    "        x = []\n",
    "        for bp in BODY_PARTS:\n",
    "            kp_data = kp_input[bp]\n",
    "            x.append(kp_data)\n",
    "        if sample.get('single_point_inference'):\n",
    "            x = np.array([x])\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        \n",
    "        kp_input_tensor = torch.from_numpy(x).float()\n",
    "        \n",
    "        tensorized_sample = {\n",
    "            'kp_input': kp_input_tensor\n",
    "        }\n",
    "\n",
    "        if label:\n",
    "            label_tensor = torch.from_numpy(np.array([label])).float() if label else None\n",
    "            tensorized_sample['label'] = label_tensor\n",
    "\n",
    "        if stereo_pair_id:\n",
    "            tensorized_sample['stereo_pair_id'] = stereo_pair_id\n",
    "\n",
    "\n",
    "        \n",
    "        return tensorized_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/trained_models/2020-06-05T100500/kf_predictor.pb'\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "\n",
    "network = torch.load(model_f)\n",
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kf(row_id, akpd_keypoints, cm):\n",
    "\n",
    "    # run length estimation\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    kf_prediction = network(tensorized_kps['kp_input']).item()\n",
    "    \n",
    "    return kf_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfs = []\n",
    "args = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    cm = row.camera_metadata\n",
    "    akpd_keypoints = row.annotation\n",
    "    row_id = idx\n",
    "    kf = generate_kf(row_id, akpd_keypoints, cm)\n",
    "    kfs.append(kf)\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_k_factor'] = kfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS = sorted([\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'EYE',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'TAIL_NOTCH',\n",
    "    'UPPER_LIP'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import pixel2world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.captured_at > '2020-08-22') & (df.estimated_weight_g < 1120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid()\n",
    "plt.hist(df[mask].estimated_weight_g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
