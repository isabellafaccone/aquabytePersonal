{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/data_dump_1.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-06-from-2019-10-25-to-2019-11-01.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-07-from-2019-11-01-to-2019-11-08.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-08-from-2019-11-08-to-2019-11-15.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-09-from-2019-11-15-to-2019-11-22.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-10-from-2019-11-22-to-2019-11-29.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-11-from-2019-11-29-to-2019-12-06.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-12-from-2019-12-06-to-2019-12-13.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-13-from-2019-12-13-to-2019-12-20.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-14-from-2019-12-20-to-2019-12-27.csv')\n",
    "])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "      SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=88\n",
    "      LIMIT 1;\n",
    "\"\"\"\n",
    "\n",
    "tdf = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add in Length / K-Factor Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset\n",
    "    This is the base version of the dataset that is used to map 3D keypoints to a\n",
    "    biomass estimate. The label is the weight, and the input is the 3D workd keypoints\n",
    "    obtained during triangulation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        if self.transform:\n",
    "            input_sample = {\n",
    "                'keypoints': row.keypoints,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "            }\n",
    "            if 'length' in dict(row).keys():\n",
    "                input_sample['label'] = row.length\n",
    "            sample = self.transform(input_sample)\n",
    "            return sample\n",
    "\n",
    "        world_keypoints = row.world_keypoints\n",
    "        length = row.length\n",
    "\n",
    "        sample = {'kp_input': world_keypoints, 'label': length, 'stereo_pair_id': row.id}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class NormalizedStabilityTransform(object):\n",
    "    \"\"\"\n",
    "        Transforms world keypoints into a more stable coordinate system - this will lead to better\n",
    "        training / convergene\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        modified_kps, label, stereo_pair_id, cm = \\\n",
    "            sample['modified_kps'], sample['label'], sample['stereo_pair_id'], sample['cm']\n",
    "        modified_wkps = pixel2world(modified_kps['leftCrop'], modified_kps['rightCrop'], cm)\n",
    "        stabilized_coordinates = {}\n",
    "        for bp in BODY_PARTS:\n",
    "            wkp = modified_wkps[bp]\n",
    "            stabilized_kp_info = [0.5 * wkp[0]/wkp[1], 0.5 * wkp[2]/wkp[1], 0.5 * 0.1/wkp[1]]\n",
    "            stabilized_coordinates[bp] = stabilized_kp_info\n",
    "            \n",
    "        normalized_label = label\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'kp_input': stabilized_coordinates,\n",
    "            'label': normalized_label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.load('/root/data/alok/biomass_estimation/playground/kf_predictor.pb')\n",
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kf(row_id, akpd_keypoints, cm):\n",
    "\n",
    "    # run length estimation\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    kf_prediction = network(tensorized_kps['kp_input']).item()\n",
    "    \n",
    "    return kf_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfs = []\n",
    "args = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    cm = tdf.camera_metadata.iloc[0]\n",
    "    akpd_keypoints = json.loads(row.annotation)\n",
    "    row_id = idx\n",
    "    kf = generate_kf(row_id, akpd_keypoints, cm)\n",
    "    kfs.append(kf)\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_k_factor'] = kfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kf'] = kfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(df.estimated_k_factor, bins=20)\n",
    "plt.title('K Factor Distribution')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k_factor > 2.5].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.estimated_weight_g, df.k_factor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.arange(500, 9000, 500)\n",
    "mean_kfs = []\n",
    "for idx in range(len(weights) - 1):\n",
    "    lo, hi = weights[idx], weights[idx + 1]\n",
    "    mask = (df.estimated_weight_g > lo) & (df.estimated_weight_g < hi)\n",
    "    mean_kf = df[mask].k_factor.mean()\n",
    "    mean_kfs.append(mean_kf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "x_pos = np.arange(len(weights[:-1]))\n",
    "plt.bar(x_pos, mean_kfs, align='edge', width=0.9)\n",
    "plt.xticks(x_pos, weights[:-1])\n",
    "plt.title('K-Factor vs. Weight')\n",
    "plt.xlabel('Predicted weight (g)')\n",
    "plt.ylabel('Predicted K-factor')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.estimated_k_factor < np.percentile(df.estimated_k_factor, 100)].copy(deep=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "start_date, end_date = '2019-10-10', '2019-10-20'\n",
    "plt.scatter(tdf[start_date:end_date].index, \n",
    "            tdf[start_date:end_date].estimated_weight_g,\n",
    "            c=tdf[start_date:end_date].estimated_k_factor, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Estimated Weight (g)')\n",
    "plt.title('Weight Predictions vs. Time, color-coded by K-Factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_weight_g'] = df.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['captured_at', 'estimated_weight_g', 'estimated_k_factor', 'akpd_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.akpd_score > 0.0].to_csv('/root/data/alok/biomass_estimation/playground/imr_data_kf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
