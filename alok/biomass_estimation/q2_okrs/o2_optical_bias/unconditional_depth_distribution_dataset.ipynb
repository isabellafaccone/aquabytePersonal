{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table\n",
    "\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')\n",
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials'\n",
    "rds = RDSAccessUtils(json.load(open(os.environ['PLALI_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from plali.plali_annotations x\n",
    "    inner join \n",
    "    ( select a.id as plali_image_id, a.images, a.metadata, b.id as workflow_id, b.name from plali.plali_images a\n",
    "    inner join plali.plali_workflows b\n",
    "    on a.workflow_id = b.id ) y\n",
    "    on x.plali_image_id = y.plali_image_id\n",
    "    where workflow_id = '00000000-0000-0000-0000-000000000112';\n",
    "\"\"\"\n",
    "\n",
    "df = rds.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_spec_name'] = df.metadata.apply(lambda x: x.get('data_spec_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('data_spec_name').apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_count(ann):\n",
    "    if 'skipReasons' in ann:\n",
    "        return 0\n",
    "    detections = ann['annotations']\n",
    "    full_count = len([d for d in detections if d['label'] == 'FULL'])\n",
    "    return full_count\n",
    "\n",
    "def get_full_and_partial_count(ann):\n",
    "    if 'skipReasons' in ann:\n",
    "        return 0\n",
    "    detections = ann['annotations']\n",
    "    return len(detections)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_count'] = df.annotation.apply(lambda x: get_full_count(x))\n",
    "df['full_and_partial_count'] = df.annotation.apply(lambda x: get_full_and_partial_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_specs = sorted(df.data_spec_name.unique().tolist())\n",
    "for data_spec_name in data_specs:\n",
    "    mask = df.data_spec_name == data_spec_name\n",
    "    print(df[mask].full_count.sum(), df[mask].full_and_partial_count.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/high_recall_fish_annotation_v1_2258.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/root/data/alok/biomass_estimation/playground/high_recall_fish_annotation_images'\n",
    "count = 0\n",
    "image_fs = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_s3_url = row.images[0]\n",
    "    image_s3_url_components = image_s3_url.replace('s3://', '').split('/')\n",
    "    bucket, key = image_s3_url_components[0], os.path.join(*image_s3_url_components[1:])\n",
    "    image_f = os.path.join(output_dir, 'image_{}.jpg'.format(count))\n",
    "    s3.download_from_s3(bucket, key, image_f)\n",
    "    image_fs.append(image_f)\n",
    "    \n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "        \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path_on_quad'] = image_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values_to_insert = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     id_str = str(uuid.uuid4())\n",
    "#     images = {row.images[0].replace('left', 'right')}\n",
    "#     metadata = row.metadata\n",
    "#     priority = 1.0\n",
    "#     values = {\n",
    "#         'id': id_str,\n",
    "#         'workflow_id': '00000000-0000-0000-0000-000000000112',\n",
    "#         'images': images,\n",
    "#         'metadata': metadata,\n",
    "#         'priority': priority\n",
    "#     }\n",
    "#     values_to_insert.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10\n",
    "# count = 0\n",
    "# for chunk in chunker(values_to_insert, n):\n",
    "#     insert_into_plali(chunk, engine, sql_metadata)\n",
    "    \n",
    "#     count += 1\n",
    "#     print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Upload sample images for stereo crop annotations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials'\n",
    "\n",
    "def establish_plali_connection():\n",
    "    rds = RDSAccessUtils(json.load(open(os.environ['PLALI_SQL_CREDENTIALS'])))\n",
    "    engine = rds.sql_engine\n",
    "    sql_metadata = MetaData()\n",
    "    sql_metadata.reflect(bind=engine)\n",
    "    return engine, sql_metadata\n",
    "\n",
    "def insert_into_plali(values_to_insert, engine, sql_metadata):\n",
    "    table = sql_metadata.tables['plali_images']\n",
    "    conn = engine.connect()\n",
    "    trans = conn.begin()\n",
    "    conn.execute(table.insert(), values_to_insert)\n",
    "    trans.commit()\n",
    "    \n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ann_count'] = df.annotation.apply(lambda x: len(x.get('annotations')) if x.get('annotations') else 0)\n",
    "mask = df.ann_count < 6\n",
    "\n",
    "image_urls = []\n",
    "left_image_s3_urls = [x[0] for x in df[mask].sort_values('ann_count', ascending=False).head(3).images.tolist()]\n",
    "for left_image_s3_url in left_image_s3_urls:\n",
    "    right_image_s3_url = left_image_s3_url.replace('left', 'right')\n",
    "    image_urls.append(left_image_s3_url)\n",
    "    image_urls.append(right_image_s3_url)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine, sql_metadata = establish_plali_connection()\n",
    "values_to_insert = []\n",
    "for image_url in image_urls:\n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    images = {image_url}\n",
    "    metadata = {'name': 'small_test'}\n",
    "    priority = 1.0\n",
    "\n",
    "    values = {\n",
    "        'id': uuid_str,\n",
    "        'workflow_id': '00000000-0000-0000-0000-000000000048',\n",
    "        'images': images,\n",
    "        'metadata': metadata,\n",
    "        'priority': priority\n",
    "    }\n",
    "\n",
    "    values_to_insert.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "count = 0\n",
    "for chunk in chunker(values_to_insert, n):\n",
    "    insert_into_plali(chunk, engine, sql_metadata)\n",
    "    \n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Extract pairs and run through Hungarian Matcher </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def hungarian_matcher(left_ids, left_bottom_top_edge_locations, right_ids, right_bottom_top_edge_locations):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    Returns\n",
    "        a list of left and right id pair. If either id is None, it is an unmatched item.\n",
    "    \"\"\"\n",
    "    # match the bboxes. Return a list of matched bboxes\n",
    "    COST_THRESHOLD = 100.0\n",
    "\n",
    "    pairs = []\n",
    "    if left_ids and right_ids:\n",
    "        # pairwise euclidean distance matrix\n",
    "        cost_matrix = cdist(left_bottom_top_edge_locations, right_bottom_top_edge_locations, metric='euclidean')\n",
    "\n",
    "        # hungarian algorithm to minimize weights in bipartite graph\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        # move matched items from left_ids/right_ids to pairs\n",
    "        for (r, c) in zip(row_ind, col_ind):\n",
    "            if cost_matrix[r, c] < COST_THRESHOLD:\n",
    "                pairs.append((left_ids[r], right_ids[c]))\n",
    "                left_ids[r] = None\n",
    "                right_ids[c] = None\n",
    "\n",
    "    # unmatched singles\n",
    "    lefts = [(key, None) for key in left_ids if key]\n",
    "    rights = [(None, key) for key in right_ids if key]\n",
    "\n",
    "    logging.info(\"hungarian_matcher left={}, right={} -> matched={}, left={}, right={}\".format(\n",
    "        len(left_ids), len(right_ids), len(pairs), len(lefts), len(rights)))\n",
    "\n",
    "    # merge all into pairs as final result\n",
    "    pairs.extend(lefts)\n",
    "    pairs.extend(rights)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select * from plali.plali_annotations x\n",
    "    inner join \n",
    "    ( select a.id as plali_image_id, a.images, a.metadata, b.id as workflow_id, b.name from plali.plali_images a\n",
    "    inner join plali.plali_workflows b\n",
    "    on a.workflow_id = b.id ) y\n",
    "    on x.plali_image_id = y.plali_image_id\n",
    "    where workflow_id = '00000000-0000-0000-0000-000000000112';\n",
    "\"\"\"\n",
    "\n",
    "df = rds.extract_from_database(query)\n",
    "df['ann_count'] = df.annotation.apply(lambda x: len(x['annotations']) if 'annotations' in x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crop pairs\n",
    "\n",
    "crop_pair_dict = defaultdict(dict)\n",
    "mask = df.ann_count > 0\n",
    "for idx, row in df[mask].iterrows():\n",
    "    image_url = row.images[0]\n",
    "    url_components = image_url.split('/')\n",
    "    ts = [x for x in url_components if x.startswith('at=')][0]\n",
    "    side = url_components[-1].split('_')[0]\n",
    "    crop_pair_dict[ts]['{}_url'.format(side)] = image_url\n",
    "    crop_pair_dict[ts]['{}_ann'.format(side)] = row.annotation\n",
    "    \n",
    "new_dict = {}\n",
    "for k, v in crop_pair_dict.items():\n",
    "    if len(v) == 4:\n",
    "        new_dict[k] = v\n",
    "\n",
    "crop_pair_dict = new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Hungarian Matcher\n",
    "crop_pair_objs = []\n",
    "for key, crop_pair_obj in crop_pair_dict.items():\n",
    "    left_ann = crop_pair_obj['left_ann']\n",
    "    right_ann = crop_pair_obj['right_ann']\n",
    "    \n",
    "    left_ids, left_bottom_top_edge_locations = [], []\n",
    "    for idx, crop in enumerate(left_ann['annotations']):\n",
    "        left_ids.append(idx)\n",
    "        left_bottom_top_edge_location = [crop['yCrop'], crop['yCrop'] + crop['height']]\n",
    "        left_bottom_top_edge_locations.append(left_bottom_top_edge_location)\n",
    " \n",
    "    right_ids, right_bottom_top_edge_locations = [], []\n",
    "    for idx, crop in enumerate(right_ann['annotations']):\n",
    "        right_ids.append(idx)\n",
    "        right_bottom_top_edge_location = [crop['yCrop'], crop['yCrop'] + crop['height']]\n",
    "        right_bottom_top_edge_locations.append(right_bottom_top_edge_location)\n",
    "        \n",
    "    id_pairs = hungarian_matcher(left_ids, left_bottom_top_edge_locations, right_ids, right_bottom_top_edge_locations)\n",
    "    crop_pair_obj_new = dict(crop_pair_obj)\n",
    "    crop_pair_obj_new['id_pairs'] = id_pairs\n",
    "    crop_pair_objs.append(crop_pair_obj_new)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Upload upscaled unrectified crops to s3 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3_url(s3_url):\n",
    "    s3_url_components = s3_url.replace('s3://', '').split('/')\n",
    "    bucket, key = s3_url_components[0], os.path.join(*s3_url_components[1:])\n",
    "    f = s3.download_from_s3(bucket, key)\n",
    "    return f\n",
    "\n",
    "def get_bbox(ann):\n",
    "    c1 = ann['xCrop']\n",
    "    c2 = ann['yCrop']\n",
    "    c3 = ann['xCrop'] + ann['width']\n",
    "    c4 = ann['yCrop'] + ann['height']\n",
    "    bbox = [c1, c2, c3, c4]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def resize_bbox(bbox, original_dims, new_dims):\n",
    "    resized_bbox = [\n",
    "        int(bbox[0] * float(new_dims[0] / original_dims[0])),\n",
    "        int(bbox[1] * float(new_dims[1] / original_dims[1])),\n",
    "        int(bbox[2] * float(new_dims[0] / original_dims[0])),\n",
    "        int(bbox[3] * float(new_dims[1] / original_dims[1]))\n",
    "    ]\n",
    "    \n",
    "    return resized_bbox\n",
    "\n",
    "\n",
    "def generate_crop_metadata(bbox):\n",
    "    crop_metadata = {\n",
    "        'x_coord': bbox[0],\n",
    "        'y_coord': bbox[1],\n",
    "        'width': bbox[2] - bbox[0],\n",
    "        'height': bbox[3] - bbox[1]\n",
    "    }\n",
    "    \n",
    "    return crop_metadata\n",
    "\n",
    "\n",
    "def produce_crop(image, bbox):\n",
    "    crop = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    return crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_RES_WIDTH = 4096\n",
    "FULL_RES_HEIGHT = 3000\n",
    "THUMBNAIL_WIDTH = 512\n",
    "THUMBNAIL_HEIGHT = 512\n",
    "\n",
    "left_crop_urls, right_crop_urls, metadatas = [], [], []\n",
    "\n",
    "destination_bucket = 'aquabyte-images-adhoc'\n",
    "\n",
    "for idx, crop_pair_obj in enumerate(crop_pair_objs):\n",
    "    print(idx)\n",
    "    left_s3_url = crop_pair_obj['left_url']\n",
    "    right_s3_url = crop_pair_obj['right_url']\n",
    "    left_ann = crop_pair_obj['left_ann']\n",
    "    right_ann = crop_pair_obj['right_ann']\n",
    "    left_image_f = download_from_s3_url(left_s3_url)\n",
    "    right_image_f = download_from_s3_url(right_s3_url)\n",
    "    left_image = cv2.imread(left_image_f, cv2.COLOR_BGR2GRAY)\n",
    "    right_image = cv2.imread(right_image_f, cv2.COLOR_BGR2GRAY)\n",
    "    left_image = cv2.resize(left_image, (FULL_RES_WIDTH, FULL_RES_HEIGHT))\n",
    "    right_image = cv2.resize(right_image, (FULL_RES_WIDTH, FULL_RES_HEIGHT))\n",
    "    \n",
    "    for left_id, right_id in crop_pair_obj['id_pairs']:\n",
    "        if not left_id or not right_id:\n",
    "            continue\n",
    "        left_bbox = get_bbox(left_ann['annotations'][left_id])\n",
    "        right_bbox = get_bbox(right_ann['annotations'][right_id])\n",
    "        left_bbox = resize_bbox(left_bbox, (THUMBNAIL_WIDTH, THUMBNAIL_HEIGHT), (FULL_RES_WIDTH, FULL_RES_HEIGHT))\n",
    "        right_bbox = resize_bbox(right_bbox, (THUMBNAIL_WIDTH, THUMBNAIL_HEIGHT), (FULL_RES_WIDTH, FULL_RES_HEIGHT))\n",
    "        left_crop = produce_crop(left_image, left_bbox)\n",
    "        right_crop = produce_crop(right_image, right_bbox)\n",
    "        \n",
    "        left_crop_f_name = 'left_frame_crop_{}_{}_{}_{}.jpg'.format(*left_bbox)\n",
    "        right_crop_f_name = 'right_frame_crop_{}_{}_{}_{}.jpg'.format(*right_bbox)\n",
    "        left_crop_f = os.path.join(os.path.dirname(left_image_f), left_crop_f_name)\n",
    "        right_crop_f = os.path.join(os.path.dirname(right_image_f), right_crop_f_name)\n",
    "        \n",
    "        cv2.imwrite(left_crop_f, left_crop)\n",
    "        cv2.imwrite(right_crop_f, right_crop)\n",
    "        \n",
    "        left_crop_key = left_crop_f[left_crop_f.index('environment='):]\n",
    "        right_crop_key = right_crop_f[right_crop_f.index('environment='):]\n",
    "        s3.s3_client.upload_file(left_crop_f, destination_bucket, left_crop_key)\n",
    "        s3.s3_client.upload_file(right_crop_f, destination_bucket, right_crop_key)\n",
    "        \n",
    "        left_crop_s3_url = os.path.join('s3://', destination_bucket, left_crop_key)\n",
    "        right_crop_s3_url = os.path.join('s3://', destination_bucket, right_crop_key)\n",
    "        \n",
    "        left_crop_metadata = generate_crop_metadata(left_bbox)\n",
    "        right_crop_metadata = generate_crop_metadata(right_bbox)\n",
    "        metadata = {\n",
    "            'name': 'keypoint_small_test_v1',\n",
    "            'left_crop_metadata': left_crop_metadata,\n",
    "            'right_crop_metadata': right_crop_metadata\n",
    "        }\n",
    "        \n",
    "        left_crop_urls.append(left_crop_s3_url)\n",
    "        right_crop_urls.append(right_crop_s3_url)\n",
    "        metadatas.append(metadata)\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Upload to PLALI for key-point annotation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_insert = []\n",
    "\n",
    "for left_crop_url, right_crop_url, metadata in zip(left_crop_urls, right_crop_urls, metadatas):\n",
    "    \n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    images = {left_crop_url, right_crop_url}\n",
    "    priority = random.random()\n",
    "\n",
    "    values = {\n",
    "        'id': uuid_str,\n",
    "        'workflow_id': '00000000-0000-0000-0000-000000000117',\n",
    "        'images': images,\n",
    "        'metadata': metadata,\n",
    "        'priority': priority\n",
    "    }\n",
    "\n",
    "    values_to_insert.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "count = 0\n",
    "for chunk in chunker(values_to_insert, n):\n",
    "    insert_into_plali(chunk, engine, sql_metadata)\n",
    "    \n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load key-point annotations and parse into standard form </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationFormatError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def add_anns(annotated_df):\n",
    "    anns = []\n",
    "    for idx, row in annotated_df.iterrows():\n",
    "        metadata = row.metadata\n",
    "        try:\n",
    "            raw_ann = row.annotation\n",
    "            if 'skipReasons' in raw_ann:\n",
    "                raise AnnotationFormatError\n",
    "\n",
    "            ann = {'leftCrop': [], 'rightCrop': []}\n",
    "\n",
    "            for side in ['leftCrop', 'rightCrop']:\n",
    "                for raw_item in row.annotation[side]['annotation']['annotations']:\n",
    "                    if 'xCrop' not in raw_item or 'yCrop' not in raw_item:\n",
    "                        raise AnnotationFormatError\n",
    "                    item = {\n",
    "                        'xCrop': raw_item['xCrop'],\n",
    "                        'yCrop': raw_item['yCrop'],\n",
    "                        'xFrame': raw_item['xCrop'] + metadata['{}_crop_metadata'.format(side.replace('Crop', ''))]['x_coord'],\n",
    "                        'yFrame': raw_item['yCrop'] + metadata['{}_crop_metadata'.format(side.replace('Crop', ''))]['y_coord'],\n",
    "                        'keypointType': raw_item['category']\n",
    "                    }\n",
    "\n",
    "                    ann[side].append(item)\n",
    "\n",
    "            anns.append(ann)\n",
    "\n",
    "        except AnnotationFormatError as err:\n",
    "            anns.append(None)\n",
    "\n",
    "    annotated_df['ann'] = anns\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = RDSAccessUtils(json.load(open(os.environ['PLALI_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from plali.plali_annotations x\n",
    "    inner join \n",
    "    ( select a.id as plali_image_id, a.images, a.metadata, b.id as workflow_id, b.name from plali.plali_images a\n",
    "    inner join plali.plali_workflows b\n",
    "    on a.workflow_id = b.id ) y\n",
    "    on x.plali_image_id = y.plali_image_id\n",
    "    where workflow_id = '00000000-0000-0000-0000-000000000117';\n",
    "\"\"\"\n",
    "\n",
    "df = rds.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_anns(df)\n",
    "df['pen_id'] = df.images.apply(lambda x: x[0].split('/')[5].replace('pen-id=', '')).astype(int)\n",
    "df['date'] = df.images.apply(lambda x: x[0].split('/')[6].replace('date=', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.images.apply(lambda x: os.path.join(*x[0].split('/')[4:7])).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Rectify key-points </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id_to_stereo = {\n",
    "    56: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40052278_R40052264/2020-05-25T07:21:30.968604000Z_L40052278_R40052264_stereo-parameters.json',\n",
    "    4: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40052270_R40052370/2020-06-03T10:45:56.119147000Z_L40052270_R40052370_stereo-parameters.json',\n",
    "    100: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40034708_R40034561/2020-01-31T00:00:00Z_L40034708_R40034561_stereo-parameters.json',\n",
    "    173: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40032706_R40032703/2020-11-10T09:50:27.538021000Z_L40032706_R40032703_stereo-parameters.json',\n",
    "    86: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40034368_R40034367/2019-12-03T00:00:00Z_L40034368_R40034367_stereo-parameters.json',\n",
    "    95: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40013179_R40048967/2020-04-10T00:00:00Z_L40013179_R40048967_stereo-parameters.json',\n",
    "    144: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40054807_R40054861/2020-07-28T11:20:43.978607000Z_L40054807_R40054861_stereo-parameters.json',\n",
    "    194: 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40049577_R40049578/2020-12-10T11:40:13.694113000Z_L40049577_R40049578_stereo-parameters.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 4096\n",
    "IMAGE_HEIGHT = 3000\n",
    "\n",
    "\n",
    "def get_camera_parameters(params: Dict) -> Dict:\n",
    "    \"\"\"Return individual camera parameters from JSON stereo parameters contents.\"\"\"\n",
    "\n",
    "    camera_matrix_1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    camera_matrix_2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    dist_coeffs_1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                    params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                    [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    dist_coeffs_1 = np.array(dist_coeffs_1)\n",
    "\n",
    "    dist_coeffs_2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                    params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                    [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    dist_coeffs_2 = np.array(dist_coeffs_2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(camera_matrix_1, dist_coeffs_1,\n",
    "                                                               camera_matrix_2,\n",
    "                                                               dist_coeffs_2, image_size, R, T,\n",
    "                                                               None,\n",
    "                                                               None,\n",
    "                                                               None, None, None,\n",
    "                                                               cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "    left_maps = cv2.initUndistortRectifyMap(camera_matrix_1, dist_coeffs_1, R1, P1, image_size,\n",
    "                                            cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(camera_matrix_2, dist_coeffs_2, R2, P2, image_size,\n",
    "                                             cv2.CV_16SC2)\n",
    "\n",
    "    params = {\n",
    "        'left_maps': left_maps,\n",
    "        'right_maps': right_maps,\n",
    "        'camera_matrix_1': camera_matrix_1,\n",
    "        'dist_coeffs_1': dist_coeffs_1,\n",
    "        'R1': R1,\n",
    "        'P1': P1,\n",
    "        'camera_matrix_2': camera_matrix_2,\n",
    "        'dist_coeffs_2': dist_coeffs_2,\n",
    "        'R2': R2,\n",
    "        'P2': P2\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def rectify(ann: Dict, params: Dict) -> Dict:\n",
    "    \"\"\"Rectify ann with params.\"\"\"\n",
    "\n",
    "    camera_matrix_1 = params['camera_matrix_1']\n",
    "    dist_coeffs_1 = params['dist_coeffs_1']\n",
    "    R1 = params['R1']\n",
    "    P1 = params['P1']\n",
    "\n",
    "    camera_matrix_2 = params['camera_matrix_2']\n",
    "    dist_coeffs_2 = params['dist_coeffs_2']\n",
    "    R2 = params['R2']\n",
    "    P2 = params['P2']\n",
    "\n",
    "    ann_r = {'leftCrop': [], 'rightCrop': []}\n",
    "    for side in ['leftCrop', 'rightCrop']:\n",
    "        for item in ann[side]:\n",
    "            bp = item['keypointType']\n",
    "            x = item['xFrame']\n",
    "            y = item['yFrame']\n",
    "            if side == 'leftCrop':\n",
    "                x_new, y_new = \\\n",
    "                    cv2.undistortPoints(\n",
    "                        np.array([[x, y]]).astype(float),\n",
    "                        camera_matrix_1,\n",
    "                        dist_coeffs_1,\n",
    "                        R=R1,\n",
    "                        P=P1)[0][0]\n",
    "            elif side == 'rightCrop':\n",
    "                x_new, y_new = \\\n",
    "                    cv2.undistortPoints(\n",
    "                        np.array([[x, y]]).astype(float),\n",
    "                        camera_matrix_2,\n",
    "                        dist_coeffs_2,\n",
    "                        R=R2,\n",
    "                        P=P2)[0][0]\n",
    "            else:\n",
    "                raise Exception('Invalid side!')\n",
    "\n",
    "            ann_r[side].append({\n",
    "                'keypointType': bp,\n",
    "                'xFrame': x_new,\n",
    "                'yFrame': y_new,\n",
    "            })\n",
    "\n",
    "    return ann_r\n",
    "\n",
    "def get_camera_metadata(stereo_parameters):\n",
    "    \n",
    "    camera_metadata = {\n",
    "        'focalLengthPixel': stereo_parameters['CameraParameters1']['FocalLength'][0],\n",
    "        'baseline': abs(stereo_parameters['TranslationOfCamera2'][0] / 1e3),\n",
    "        'focalLength': stereo_parameters['CameraParameters1']['FocalLength'][0] * 3.45e-6,\n",
    "        'pixelCountWidth': 4096,\n",
    "        'pixelCountHeight': 3000,\n",
    "        'imageSensorWidth': 0.01412,\n",
    "        'imageSensorHeight': 0.01035\n",
    "    }\n",
    "\n",
    "    return camera_metadata\n",
    "\n",
    "\n",
    "stereo_params_cache = {}\n",
    "def rectify_df(df, pen_id_to_stereo):\n",
    "    ann_rs, cms = [], []\n",
    "    \n",
    "    count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        ann = row.ann\n",
    "        pen_id = row.pen_id\n",
    "        if pen_id in stereo_params_cache:\n",
    "            stereo_params = stereo_params_cache[pen_id]\n",
    "        else:\n",
    "            stereo_params_f, _, _ = s3.download_from_url(pen_id_to_stereo[pen_id])\n",
    "            stereo_params = json.load(open(stereo_params_f))\n",
    "            stereo_params_cache[pen_id] = stereo_params\n",
    "        \n",
    "        params = get_camera_parameters(stereo_params)\n",
    "        camera_metadata = get_camera_metadata(stereo_params)\n",
    "        cms.append(camera_metadata)\n",
    "        \n",
    "        if ann is None or ann.get('leftCrop') is None or ann.get('rightCrop') is None:\n",
    "            ann_rs.append(None)\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            # rectify with new params\n",
    "            ann_r = rectify(ann, params)\n",
    "            ann_rs.append(ann_r)\n",
    "        \n",
    "        \n",
    "    \n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "\n",
    "    df['ann_r'] = ann_rs\n",
    "    df['cm'] = cms\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectify_df(df, pen_id_to_stereo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get depth values </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import pixel2world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_is_valid(ann):\n",
    "    if ann is None:\n",
    "        return False\n",
    "    if not ann.get('leftCrop') or not ann.get('rightCrop'):\n",
    "        return False\n",
    "    if len(ann['leftCrop']) < 11 or len(ann['rightCrop']) < 11:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "depths = []\n",
    "valid_body_parts_list = []\n",
    "throw_out_count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    ann, ann_r = row.ann, row.ann_r\n",
    "    cm = row.cm\n",
    "    if not ann_is_valid(ann_r):\n",
    "        depths.append(None)\n",
    "        valid_body_parts_list.append([])\n",
    "        continue\n",
    "        \n",
    "\n",
    "    world_keypoints = pixel2world(ann_r['leftCrop'], ann_r['rightCrop'], cm)\n",
    "    \n",
    "    invalid_body_parts = set()\n",
    "    for item in ann['leftCrop']:\n",
    "        if item['xCrop'] < 50 and item['yCrop'] < 100:\n",
    "            invalid_body_parts.add(item['keypointType'])\n",
    "    for item in ann['rightCrop']:\n",
    "        if item['xCrop'] < 50 and item['yCrop'] < 100:\n",
    "            invalid_body_parts.add(item['keypointType'])\n",
    "    \n",
    "    point_depths, valid_body_parts = [], []\n",
    "    for body_part, coordinates in world_keypoints.items():\n",
    "        if body_part not in invalid_body_parts:\n",
    "            point_depths.append(coordinates[1])\n",
    "            valid_body_parts.append(body_part)\n",
    "            \n",
    "    left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['rightCrop']}\n",
    "    \n",
    "    max_y_dev = 0\n",
    "    for bp in valid_body_parts:\n",
    "        y_deviation = abs(left_keypoints[bp][1] - right_keypoints[bp][1])\n",
    "        if y_deviation > max_y_dev:\n",
    "            max_y_dev = y_deviation\n",
    "    \n",
    "    if max_y_dev > 50:\n",
    "        depths.append(None)\n",
    "        valid_body_parts_list.append([])\n",
    "        throw_out_count += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    depth = np.median(point_depths)\n",
    "    depths.append(depth)\n",
    "    valid_body_parts_list.append(valid_body_parts)\n",
    "    \n",
    "df['depth'] = depths\n",
    "df['valid_body_parts'] = valid_body_parts_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throw_out_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('pen_id').apply(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "mask = (df.depth > 0) & (df.depth < 2) & (df.valid_body_parts.apply(lambda x: len(x)) == 11)\n",
    "mask_2 = (df.depth > 0) & (df.depth < 2)\n",
    "plt.hist(df[mask_2].depth, bins=100, color='red', alpha = 0.5, label='full and partial')\n",
    "plt.hist(df[mask].depth, bins=100, color='blue', label='full')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bucket_cutoffs = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "normalized_sample_sizes = []\n",
    "for low_depth, high_depth in zip(depth_bucket_cutoffs, depth_bucket_cutoffs[1:]):\n",
    "    mask = (df.depth > low_depth) & (df.depth < high_depth)\n",
    "    sample_size = df[mask].shape[0]\n",
    "    field_factor = low_depth ** 2\n",
    "    normalized_sample_size = sample_size / field_factor\n",
    "    normalized_sample_sizes.append(normalized_sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depth_bucket_cutoffs[:-1], normalized_sample_sizes)\n",
    "plt.xlabel('Distance from camera')\n",
    "plt.ylabel('Sample size noramlized by field area')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].shape[0] / (df[mask].valid_body_parts.apply(lambda x: len(x)) == 11).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rds = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "# query = \"\"\"\n",
    "#     select * from prod.biomass_computations\n",
    "#     where pen_id=194\n",
    "#     and captured_at between '2021-01-01' and '2021-01-06'\n",
    "#     and akpd_score > 0.9;\n",
    "# \"\"\"\n",
    "# tdf = rds.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for idx, row in tdf.iterrows():\n",
    "    ann = row.annotation\n",
    "    cm = row.camera_metadata\n",
    "    world_keypoints = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depths.append(np.median([x[1] for x in world_keypoints.values()]))\n",
    "    \n",
    "tdf['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.depth, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "mask = (df.depth < 0.5)\n",
    "for idx, row in df[mask].iterrows():\n",
    "    ann_r = row.ann_r\n",
    "    \n",
    "    if not ann_is_valid(ann_r):\n",
    "        depths.append(None)\n",
    "        valid_body_parts_list.append([])\n",
    "        continue\n",
    "    \n",
    "    left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['rightCrop']}\n",
    "    \n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    max_y_dev = 0\n",
    "    for bp in valid_body_parts:\n",
    "        y_deviation = abs(left_keypoints[bp][1] - right_keypoints[bp][1])\n",
    "        if y_deviation > max_y_dev:\n",
    "            max_y_dev = y_deviation\n",
    "    \n",
    "    if max_y_dev > 50:\n",
    "        continue\n",
    "        \n",
    "    print(row.depth)\n",
    "    left_crop_f = download_from_s3_url(row.images[0])\n",
    "    right_crop_f = download_from_s3_url(row.images[1])\n",
    "    \n",
    "    plt.imshow(cv2.imread(left_crop_f))\n",
    "    plt.show()\n",
    "    plt.imshow(cv2.imread(right_crop_f))\n",
    "    plt.show()\n",
    "\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "df.images.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_f = download_from_s3_url(df.images.iloc[idx][0])\n",
    "right_image_f = download_from_s3_url(df.images.iloc[idx][1])\n",
    "if 'right' in left_image_f:\n",
    "    x = left_image_f\n",
    "    left_image_f = right_image_f\n",
    "    right_image_f = x\n",
    "    \n",
    "\n",
    "left_image = cv2.cvtColor(cv2.imread(left_image_f), cv2.COLOR_BGR2RGB)\n",
    "right_image = cv2.cvtColor(cv2.imread(right_image_f), cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail_url(crop_url):\n",
    "    side = 'left' if 'left' in crop_url else 'right'\n",
    "    thumbnail_url = crop_url.replace('aquabyte-images-adhoc', 'aquabyte-frames-resized-inbound')\n",
    "    thumbnail_url = os.path.join(os.path.dirname(thumbnail_url), '{}_frame.resize_512_512.jpg'.format(side))\n",
    "    return thumbnail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_thumbnail_url = get_thumbnail_url(df.images.iloc[0][0])\n",
    "right_thumbnail_url = get_thumbnail_url(df.images.iloc[0][1])\n",
    "left_thumbnail_f = download_from_s3_url(left_thumbnail_url)\n",
    "right_thumbnail_f = download_from_s3_url(right_thumbnail_url)\n",
    "left_thumbnail = cv2.resize(cv2.cvtColor(cv2.imread(left_thumbnail_f), cv2.COLOR_BGR2RGB), (4096, 3000))\n",
    "right_thumbnail = cv2.resize(cv2.cvtColor(cv2.imread(right_thumbnail_f), cv2.COLOR_BGR2RGB), (4096, 3000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(left_thumbnail)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    ann = row.ann\n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['rightCrop']}\n",
    "    for bp, kp in left_keypoints.items():\n",
    "        if bp in valid_body_parts:\n",
    "            ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(right_thumbnail)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    ann = row.ann\n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['rightCrop']}\n",
    "    for bp, kp in right_keypoints.items():\n",
    "        if bp in valid_body_parts:\n",
    "            ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(left_image)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    if idx != 4:\n",
    "        continue\n",
    "    ann = row.ann\n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "    for bp, kp in left_keypoints.items():\n",
    "        if bp in valid_body_parts:\n",
    "            ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(right_image)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    if idx != 4:\n",
    "        continue\n",
    "    ann = row.ann\n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "    for bp, kp in right_keypoints.items():\n",
    "        if bp in valid_body_parts:\n",
    "            ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ann.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.annotation.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine y-coordinate deviation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coordinate_deviations = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann_r = row.ann_r\n",
    "    left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_r['rightCrop']}\n",
    "    valid_body_parts = row.valid_body_parts\n",
    "    for body_part in valid_body_parts:\n",
    "        y1 = left_keypoints[body_part][1]\n",
    "        y2 = right_keypoints[body_part][1]\n",
    "        y_coordinate_deviations.append(y1 - y2)\n",
    "        print(body_part, y1-y2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
