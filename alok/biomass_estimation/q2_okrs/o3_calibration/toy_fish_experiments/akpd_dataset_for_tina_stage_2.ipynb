{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../q1_o2kr2_dataset_annotations/')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "import uuid\n",
    "from construct_fish_detection_dataset_o2kr2 import establish_plali_connection, insert_into_plali\n",
    "from rectification import rectify\n",
    "\n",
    "from weight_estimation.weight_estimator import WeightEstimator, CameraMetadata\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load the dataset and arrange into pairs </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = RDSAccessUtils(json.load(open(os.environ['PLALI_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from plali.plali_annotations x\n",
    "    inner join \n",
    "    ( select a.id as plali_image_id, a.images, a.metadata, b.id as workflow_id, b.name from plali.plali_images a\n",
    "    inner join plali.plali_workflows b\n",
    "    on a.workflow_id = b.id ) y\n",
    "    on x.plali_image_id = y.plali_image_id\n",
    "    where workflow_id = '00000000-0000-0000-0000-000000000047';\n",
    "\"\"\"\n",
    "\n",
    "annotated_df = rds.extract_from_database(query)\n",
    "annotated_df['image'] = annotated_df['images'].apply(lambda x: x[0])\n",
    "annotated_df['annotation_count'] = annotated_df.annotation.apply(lambda x: len(x['annotations']) if x.get('annotations') else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame_urls, right_frame_urls, left_anns, right_anns = [], [], [], []\n",
    "\n",
    "count = 0\n",
    "mask = annotated_df.annotation_count > 0\n",
    "for idx, row in annotated_df.iterrows():\n",
    "    \n",
    "    if 'right_frame' in row.image:\n",
    "        continue\n",
    "        \n",
    "    # get left and right image URLs and annotations\n",
    "        \n",
    "    left_frame_s3_url = row.image\n",
    "    left_ann = row.annotation\n",
    "    \n",
    "    right_frame_s3_url = left_frame_s3_url.replace('left', 'right')\n",
    "    right_frame_mask = annotated_df.image == right_frame_s3_url\n",
    "    \n",
    "    if right_frame_mask.sum() == 1:\n",
    "        right_ann = annotated_df[right_frame_mask].annotation.iloc[0]\n",
    "\n",
    "        left_frame_urls.append(left_frame_s3_url)\n",
    "        right_frame_urls.append(right_frame_s3_url)\n",
    "        left_anns.append(left_ann)\n",
    "        right_anns.append(right_ann)\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "paired_df = pd.DataFrame({\n",
    "    'left_frame_url': left_frame_urls,\n",
    "    'right_frame_url': right_frame_urls,\n",
    "    'left_ann': left_anns,\n",
    "    'right_ann': right_anns\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Check some of the bounding boxes </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_url):\n",
    "    image_s3_url = image_url\n",
    "    url_components = image_s3_url.replace('s3://', '').split('/')\n",
    "    bucket = url_components[0]\n",
    "    key = os.path.join(*url_components[1:])\n",
    "    image_f = s3.download_from_s3(bucket, key)\n",
    "    return image_f\n",
    "\n",
    "\n",
    "def plot_stereo_image(left_image_f, right_image_f, left_ann, right_ann):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    \n",
    "    # show left image\n",
    "    left_im = cv2.imread(left_image_f)\n",
    "    left_im = cv2.cvtColor(left_im, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(left_im)\n",
    "    \n",
    "    # plot left rectangle\n",
    "    x, y, w, h = left_ann['xCrop'], left_ann['yCrop'], left_ann['width'], left_ann['height']\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1,edgecolor='r',facecolor='none')\n",
    "    axes[0].add_patch(rect)\n",
    "    \n",
    "    # show right image\n",
    "    right_im = cv2.imread(right_image_f)\n",
    "    right_im = cv2.cvtColor(right_im, cv2.COLOR_BGR2RGB)\n",
    "    axes[1].imshow(right_im)\n",
    "    \n",
    "    # plot right rectangle\n",
    "    x, y, w, h = right_ann['xCrop'], right_ann['yCrop'], right_ann['width'], right_ann['height']\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1,edgecolor='r',facecolor='none')\n",
    "    axes[1].add_patch(rect)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# filter only on images in which full fish was found\n",
    "for idx, row in paired_df.head(10).iterrows():\n",
    "    \n",
    "    # download left image\n",
    "    left_image_f = download_image(row.left_frame_url)\n",
    "    right_image_f = download_image(row.right_frame_url)\n",
    "    left_ann = row.left_ann['annotations'][0]\n",
    "    right_ann = row.right_ann['annotations'][0]\n",
    "    \n",
    "    # plot image\n",
    "    plot_stereo_image(left_image_f, right_image_f, left_ann, right_ann)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Rectify frames </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3_url(s3_url):\n",
    "    url_components = s3_url.replace('s3://', '').split('/')\n",
    "    bucket = url_components[0]\n",
    "    key = os.path.join(*url_components[1:])\n",
    "    f = s3.download_from_s3(bucket, key)\n",
    "    return f, bucket, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_rectified_s3_urls, right_image_rectified_s3_urls = [], []\n",
    "\n",
    "stereo_parameters_url = 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40013181_R40012414/2021-02-10T10_35_05.569443000Z_L40013181_R40012414_stereo-parameters.json'\n",
    "count = 0\n",
    "for idx, row in paired_df.iterrows():\n",
    "    \n",
    "    # get unrectified full resolution frames\n",
    "    left_frame_s3_url, right_frame_s3_url = row.left_frame_url, row.right_frame_url\n",
    "    left_full_res_frame_s3_url = left_frame_s3_url.replace('.resize_512_512.jpg', '.jpg')\n",
    "    right_full_res_frame_s3_url = right_frame_s3_url.replace('.resize_512_512.jpg', '.jpg')\n",
    "    left_full_res_frame_f, _, left_full_res_frame_key = download_from_s3_url(left_full_res_frame_s3_url)\n",
    "    right_full_res_frame_f, _, right_full_res_frame_key = download_from_s3_url(right_full_res_frame_s3_url)\n",
    "    stereo_parameters_f, _, _ = s3.download_from_url(stereo_parameters_url)\n",
    "    \n",
    "    # rectify into full resolution stereo frame pair and save to disk\n",
    "    left_image_rectified, right_image_rectified = rectify(left_full_res_frame_f, right_full_res_frame_f, stereo_parameters_f)\n",
    "    left_image_rectified_f = os.path.join(os.path.dirname(left_full_res_frame_f), 'left_frame.rectified.jpg')\n",
    "    right_image_rectified_f = os.path.join(os.path.dirname(right_full_res_frame_f), 'right_frame.rectified.jpg')\n",
    "    cv2.imwrite(left_image_rectified_f, left_image_rectified)\n",
    "    cv2.imwrite(right_image_rectified_f, right_image_rectified)\n",
    "    \n",
    "    # upload rectified stereo frame pairs to s3\n",
    "    left_rectified_full_res_frame_key = left_full_res_frame_key.replace('.jpg', '.rectified.jpg')\n",
    "    right_rectified_full_res_frame_key = right_full_res_frame_key.replace('.jpg', '.rectified.jpg')\n",
    "    s3.s3_client.upload_file(left_image_rectified_f, 'aquabyte-images-raw', left_rectified_full_res_frame_key)\n",
    "    s3.s3_client.upload_file(right_image_rectified_f, 'aquabyte-images-raw', right_rectified_full_res_frame_key)\n",
    "    \n",
    "    # append to url lists\n",
    "    left_image_rectified_s3_url = os.path.join('s3://', 'aquabyte-images-raw', left_rectified_full_res_frame_key)\n",
    "    right_image_rectified_s3_url = os.path.join('s3://', 'aquabyte-images-raw', right_rectified_full_res_frame_key)\n",
    "    left_image_rectified_s3_urls.append(left_image_rectified_s3_url)\n",
    "    right_image_rectified_s3_urls.append(right_image_rectified_s3_url)\n",
    "    \n",
    "    print(count)\n",
    "    count += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('/root/data/alok/biomass_estimation/playground/im1.jpg', left_image_rectified)\n",
    "cv2.imwrite('/root/data/alok/biomass_estimation/playground/im2.jpg', right_image_rectified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Upload the paired data for key-point annotation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_into_plali_records(image_url_pairs, workflow_id):\n",
    "\n",
    "    values_to_insert = []\n",
    "    for idx, image_url_pair in enumerate(image_url_pairs):\n",
    "        id = str(uuid.uuid4())\n",
    "        images = set(image_url_pair)\n",
    "        metadata = {}\n",
    "        priority = float(idx) / len(image_url_pairs)\n",
    "\n",
    "        values = {\n",
    "            'id': id,\n",
    "            'workflow_id': workflow_id,\n",
    "            'images': images,\n",
    "            'metadata': metadata,\n",
    "            'priority': priority\n",
    "        }\n",
    "\n",
    "        values_to_insert.append(values)\n",
    "\n",
    "    return values_to_insert\n",
    "\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url_pairs = list(zip(left_image_rectified_s3_urls, right_image_rectified_s3_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_ID = '00000000-0000-0000-0000-000000000053'    \n",
    "values_to_insert = process_into_plali_records(image_url_pairs, WORKFLOW_ID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials.json'\n",
    "engine, sql_metadata = establish_plali_connection()\n",
    "\n",
    "n = 10\n",
    "count = 0\n",
    "for chunk in chunker(values_to_insert, n):\n",
    "    insert_into_plali(chunk, engine, sql_metadata)\n",
    "    \n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Analyze annotations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load key-point annotations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ann_into_url(ann):\n",
    "    url = ann['leftCrop']['imageUrl']\n",
    "    url = url.replace('%3D', '=').replace('%3A', ':').split('?')[0]\n",
    "    return url\n",
    "\n",
    "def parse_url_into_ts(url):\n",
    "    url_components = url.split('/')\n",
    "    ts = [component for component in url_components if 'at=' in component][0]\n",
    "    return ts\n",
    "    \n",
    "\n",
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials'\n",
    "rds = RDSAccessUtils(json.load(open(os.environ['PLALI_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from plali.plali_annotations x\n",
    "    inner join \n",
    "    ( select a.id as plali_image_id, a.images, a.metadata, b.id as workflow_id, b.name from plali.plali_images a\n",
    "    inner join plali.plali_workflows b\n",
    "    on a.workflow_id = b.id ) y\n",
    "    on x.plali_image_id = y.plali_image_id\n",
    "    where workflow_id = '00000000-0000-0000-0000-000000000053';\n",
    "\"\"\"\n",
    "\n",
    "annotated_df = rds.extract_from_database(query)\n",
    "annotated_df = annotated_df[annotated_df.annotation.apply(lambda x: True if 'leftCrop' in x else False)]\n",
    "annotated_df['left_crop_url'] = annotated_df.annotation.apply(lambda ann: parse_ann_into_url(ann))\n",
    "annotated_df['timestamp'] = annotated_df.left_crop_url.apply(lambda url: parse_url_into_ts(url))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load PIT tag scans </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_tag_df = pd.read_csv('/root/data/alok/biomass_estimation/playground/imr_pit_tag_data.csv')\n",
    "pit_tag_df.Link = pit_tag_df.Link.apply(lambda x: x.replace('%3D', '=').replace('%3A', ':'))\n",
    "mask = pit_tag_df.PITtag.apply(lambda x: x.startswith('04') if type(x) == str else False)\n",
    "pit_tag_df = pit_tag_df[mask]\n",
    "pit_tag_df['timestamp'] = pit_tag_df.Link.apply(lambda url: parse_url_into_ts(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(annotated_df, pit_tag_df, on='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/data/alok/biomass_estimation/playground/langoy_cage_8.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['a', 'b', 'c', 'd', 'e', 'weight', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in measurement_df[~measurement_df.S_w.isnull()].S_w.values:\n",
    "    if 'e' in val:\n",
    "        continue\n",
    "    print(int(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load weight measurements </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_df = pd.read_csv('/root/data/alok/biomass_estimation/playground/fish_measurement_data.csv')\n",
    "full_df = pd.merge(measurement_df, merged_df, on='PITtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predict weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationFormatError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "anns = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    try:\n",
    "        raw_ann = row.annotation\n",
    "        if 'skipReasons' in raw_ann:\n",
    "            raise AnnotationFormatError\n",
    "        \n",
    "        ann = {'leftCrop': [], 'rightCrop': []}\n",
    "        for side in ['leftCrop', 'rightCrop']:\n",
    "            for raw_item in row.annotation[side]['annotation']['annotations']:\n",
    "                if 'xCrop' not in raw_item or 'yCrop' not in raw_item:\n",
    "                    raise AnnotationFormatError\n",
    "                \n",
    "                item = {\n",
    "                    'xFrame': raw_item['xCrop'],\n",
    "                    'yFrame': raw_item['yCrop'],\n",
    "                    'keypointType': raw_item['category']\n",
    "                }\n",
    "                \n",
    "                ann[side].append(item)\n",
    "        \n",
    "\n",
    "        if any([len(ann[side]) != 11 for side in ['leftCrop', 'rightCrop']]):\n",
    "            raise AnnotationFormatError\n",
    "        \n",
    "        left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['leftCrop']}\n",
    "        flip = left_keypoints['DORSAL_FIN'][1] > left_keypoints['PELVIC_FIN'][1]\n",
    "        adj_ann = {'leftCrop': [], 'rightCrop': []}\n",
    "        for side in ['leftCrop', 'rightCrop']:\n",
    "            for item in ann[side]:\n",
    "                adj_item = {\n",
    "                    'xFrame': item['xFrame'],\n",
    "                    'yFrame': 3000 - item['yFrame'] if flip else item['yFrame'],\n",
    "                    'keypointType': item['keypointType']\n",
    "                }\n",
    "\n",
    "                adj_ann[side].append(adj_item)\n",
    "        \n",
    "        \n",
    "        anns.append(adj_ann)\n",
    "        \n",
    "    except AnnotationFormatError as err:\n",
    "        anns.append(None)\n",
    "    \n",
    "    \n",
    "full_df['ann'] = anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in full_df.iterrows():\n",
    "    ann = row.ann\n",
    "    if ann is not None:\n",
    "        left_mean_x = np.mean([item['xFrame'] for item in ann['leftCrop']])\n",
    "        right_mean_x = np.mean([item['xFrame'] for item in ann['rightCrop']])\n",
    "        print(left_mean_x - right_mean_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_parameters_url = 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40013181_R40012414/2021-02-10T10_35_05.569443000Z_L40013181_R40012414_stereo-parameters.json'\n",
    "stereo_parameters_f, _, _ = s3.download_from_url(stereo_parameters_url)\n",
    "\n",
    "stereo_params = json.load(open(stereo_parameters_f))\n",
    "camera_metadata = {\n",
    "    'focalLengthPixel': stereo_params['CameraParameters1']['FocalLength'][0],\n",
    "    'baseline': abs(stereo_params['TranslationOfCamera2'][0] / 1e3),\n",
    "    'focalLength': stereo_params['CameraParameters1']['FocalLength'][0] * 3.45e-6,\n",
    "    'pixelCountWidth': 4096,\n",
    "    'pixelCountHeight': 3000,\n",
    "    'imageSensorWidth': 0.01412,\n",
    "    'imageSensorHeight': 0.01035\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/trained_models/2020-08-08T000000/kf_predictor_v2.pb')\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "pred_weights = []\n",
    "pred_lengths = []\n",
    "pred_kfs = []\n",
    "\n",
    "count = 0\n",
    "for idx, row in full_df.iterrows():\n",
    "    ann = row.ann\n",
    "    if ann is not None:\n",
    "        left_keypoints = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann['leftCrop']}\n",
    "    \n",
    "    if ann is not None:\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        weight, length, kf = weight_estimator.predict(ann, cm)\n",
    "        pred_weights.append(weight)\n",
    "        pred_lengths.append(length)\n",
    "        pred_kfs.append(kf)\n",
    "    else:\n",
    "        pred_weights.append(None)\n",
    "        pred_lengths.append(None)\n",
    "        pred_kfs.append(None)\n",
    "    \n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['pred_weight'] = pred_weights\n",
    "full_df['pred_length'] = pred_lengths\n",
    "full_df['pred_kf'] = pred_kfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~full_df.S_w.isnull() & ~full_df.pred_weight.isnull() & (full_df.S_w.str.contains('e') == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_df[mask].pred_weight.values.mean() - full_df[mask].S_w.astype(float).values.mean()) / full_df[mask].S_w.astype(float).values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(full_df[mask].S_w.astype(float).values, full_df[mask].pred_weight.values)\n",
    "plt.plot([0, 10000], [0, 10000], color='red')\n",
    "plt.xlabel('Ground truth weight (g)')\n",
    "plt.ylabel('Predicted weight (g)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load missing for annotation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_mask = ~(pd.merge(measurement_df, pit_tag_df).timestamp.isin(full_df.timestamp))\n",
    "left_links = pd.merge(measurement_df, pit_tag_df)[missed_mask].Link.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_left_links = []\n",
    "for a, b in annotated_df.images.tolist():\n",
    "    if 'left' in a:\n",
    "        annotated_left_links.append(a)\n",
    "    elif 'left' in b:\n",
    "        annotated_left_links.append(b)\n",
    "    else:\n",
    "        print('error')\n",
    "    \n",
    "\n",
    "\n",
    "image_pairs_to_upload = []\n",
    "for l in left_links:\n",
    "    left_s3_url = os.path.join('s3://', 'aquabyte-images-raw', l[l.index('env'):])\n",
    "    right_s3_url = left_s3_url.replace('left', 'right')\n",
    "    s3_url_pair = (left_s3_url, right_s3_url)\n",
    "    image_pairs_to_upload.append(s3_url_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_rectified_s3_urls, right_image_rectified_s3_urls = [], []\n",
    "\n",
    "stereo_parameters_url = 'https://aquabyte-stereo-parameters.s3-eu-west-1.amazonaws.com/L40013181_R40012414/2021-02-10T10_35_05.569443000Z_L40013181_R40012414_stereo-parameters.json'\n",
    "count = 0\n",
    "for left_frame_s3_url, right_frame_s3_url in image_pairs_to_upload[40:]:\n",
    "    try:\n",
    "    \n",
    "        # get unrectified full resolution frames\n",
    "        left_full_res_frame_s3_url = left_frame_s3_url.replace('.resize_512_512.jpg', '.jpg')\n",
    "        right_full_res_frame_s3_url = right_frame_s3_url.replace('.resize_512_512.jpg', '.jpg')\n",
    "        left_full_res_frame_f, _, left_full_res_frame_key = download_from_s3_url(left_full_res_frame_s3_url)\n",
    "        right_full_res_frame_f, _, right_full_res_frame_key = download_from_s3_url(right_full_res_frame_s3_url)\n",
    "        stereo_parameters_f, _, _ = s3.download_from_url(stereo_parameters_url)\n",
    "\n",
    "        # rectify into full resolution stereo frame pair and save to disk\n",
    "        left_image_rectified, right_image_rectified = rectify(left_full_res_frame_f, right_full_res_frame_f, stereo_parameters_f)\n",
    "        left_image_rectified_f = os.path.join(os.path.dirname(left_full_res_frame_f), 'left_frame.rectified.jpg')\n",
    "        right_image_rectified_f = os.path.join(os.path.dirname(right_full_res_frame_f), 'right_frame.rectified.jpg')\n",
    "        cv2.imwrite(left_image_rectified_f, left_image_rectified)\n",
    "        cv2.imwrite(right_image_rectified_f, right_image_rectified)\n",
    "\n",
    "        # upload rectified stereo frame pairs to s3\n",
    "        left_rectified_full_res_frame_key = left_full_res_frame_key.replace('.jpg', '.rectified.jpg')\n",
    "        right_rectified_full_res_frame_key = right_full_res_frame_key.replace('.jpg', '.rectified.jpg')\n",
    "        s3.s3_client.upload_file(left_image_rectified_f, 'aquabyte-images-raw', left_rectified_full_res_frame_key)\n",
    "        s3.s3_client.upload_file(right_image_rectified_f, 'aquabyte-images-raw', right_rectified_full_res_frame_key)\n",
    "\n",
    "        # append to url lists\n",
    "        left_image_rectified_s3_url = os.path.join('s3://', 'aquabyte-images-raw', left_rectified_full_res_frame_key)\n",
    "        right_image_rectified_s3_url = os.path.join('s3://', 'aquabyte-images-raw', right_rectified_full_res_frame_key)\n",
    "        left_image_rectified_s3_urls.append(left_image_rectified_s3_url)\n",
    "        right_image_rectified_s3_urls.append(right_image_rectified_s3_url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(count)\n",
    "    count += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url_pairs = list(zip(left_image_rectified_s3_urls, right_image_rectified_s3_urls))\n",
    "WORKFLOW_ID = '00000000-0000-0000-0000-000000000053'    \n",
    "values_to_insert = process_into_plali_records(image_url_pairs, WORKFLOW_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PLALI_SQL_CREDENTIALS'] = '/run/secrets/plali_sql_credentials.json'\n",
    "engine, sql_metadata = establish_plali_connection()\n",
    "\n",
    "n = 10\n",
    "count = 0\n",
    "for chunk in chunker(values_to_insert, n):\n",
    "    insert_into_plali(chunk, engine, sql_metadata)\n",
    "    \n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[mask].to_csv('/root/data/alok/biomass_estimation/playground/imr_data_with_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[mask].to_csv('/root/data/alok/biomass_estimation/playground/imr_data_with_predictions_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
