{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.body_parts import core_body_parts\n",
    "from weight_estimation.utils import convert_to_world_point_arr, get_left_right_keypoint_arrs, CameraMetadata\n",
    "\n",
    "from weight_estimation.weight_estimator import WeightEstimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load Datasets </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load Kjeppevikholmen AKPD annotations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-05,2019-06-12).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-12,2019-06-19).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-19,2019-06-26).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-26,2019-07-03).csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-07-03,2019-07-04).csv')\n",
    "])\n",
    "\n",
    "df = df.sort_values('captured_at')\n",
    "df['estimated_weight_g'] = df.weight\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load Kjeppevikholmen Manual Annotations & Merge </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "rds = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM keypoint_annotations\n",
    "    WHERE pen_id=5\n",
    "    AND captured_at BETWEEN '2019-06-05' AND '2019-07-02'\n",
    "    AND is_qa = FALSE;\n",
    "\"\"\"\n",
    "\n",
    "mdf = rds.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_intersection = sorted(list(set(mdf.left_image_url).intersection(df.left_crop_url)))\n",
    "tdf = df[df.left_crop_url.isin(url_intersection)].sort_values('left_crop_url')\n",
    "tdf['manual_keypoints'] = mdf[mdf.left_image_url.isin(url_intersection)].sort_values('left_image_url').keypoints.values\n",
    "tdf['camera_metadata'] = mdf[mdf.left_image_url.isin(url_intersection)].sort_values('left_image_url').camera_metadata.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute Jitter Column </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts, depths, weights, lengths, akpd_scores, diffs_l_x, diffs_r_x, diffs_l_y, diffs_r_y = \\\n",
    "    [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for idx, row in tdf.iterrows():\n",
    "    \n",
    "    if row.akpd_score < 0.01:\n",
    "        continue\n",
    "    \n",
    "    manual_keypoints = row.manual_keypoints\n",
    "    akpd_keypoints = json.loads(row.annotation)\n",
    "    weight = row.estimated_weight_g\n",
    "    akpd_score = row.akpd_score\n",
    "    \n",
    "    # compute depth from manual keypoints\n",
    "    \n",
    "    cm = row.camera_metadata\n",
    "    camera_metadata = CameraMetadata(\n",
    "        baseline_m=cm['baseline'],\n",
    "        focal_length=cm['focalLength'],\n",
    "        focal_length_pixel=cm['focalLengthPixel'],\n",
    "        pixel_count_width=cm['pixelCountWidth'],\n",
    "        pixel_count_height=cm['pixelCountHeight'],\n",
    "        image_sensor_width=cm['imageSensorWidth'],\n",
    "        image_sensor_height=cm['imageSensorHeight'],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    left_kps, right_kps = get_left_right_keypoint_arrs(manual_keypoints)\n",
    "    wkps = convert_to_world_point_arr(left_kps, right_kps, camera_metadata)\n",
    "    depth = np.median(wkps[:, 1])\n",
    "    length_2d_left = np.linalg.norm(left_kps[core_body_parts.index('UPPER_LIP')] - left_kps[core_body_parts.index('TAIL_NOTCH')])\n",
    "    length_2d_right = np.linalg.norm(right_kps[core_body_parts.index('UPPER_LIP')] - right_kps[core_body_parts.index('TAIL_NOTCH')])\n",
    "    mean_length_2d = 0.5 * (length_2d_left + length_2d_right)\n",
    "    \n",
    "    ann_dict_left_kps_m = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in manual_keypoints['leftCrop']}\n",
    "    ann_dict_right_kps_m = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in manual_keypoints['rightCrop']}\n",
    "    ann_dict_left_kps_a = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in akpd_keypoints['leftCrop']}\n",
    "    ann_dict_right_kps_a = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in akpd_keypoints['rightCrop']}\n",
    "    for body_part in core_body_parts:\n",
    "        depths.append(depth)\n",
    "        weights.append(weight)\n",
    "        lengths.append(mean_length_2d)\n",
    "        akpd_scores.append(akpd_score)\n",
    "        body_parts.append(body_part)\n",
    "        diff_l_x = ann_dict_left_kps_m[body_part][0] - ann_dict_left_kps_a[body_part][0]\n",
    "        diff_r_x = ann_dict_right_kps_m[body_part][0] - ann_dict_right_kps_a[body_part][0]\n",
    "        diff_l_y = ann_dict_left_kps_m[body_part][1] - ann_dict_left_kps_a[body_part][1]\n",
    "        diff_r_y = ann_dict_right_kps_m[body_part][1] - ann_dict_right_kps_a[body_part][1]\n",
    "        diffs_l_x.append(diff_l_x)\n",
    "        diffs_r_x.append(diff_r_x)\n",
    "        diffs_l_y.append(diff_l_y)\n",
    "        diffs_r_y.append(diff_r_y)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame({\n",
    "    'body_part': body_parts,\n",
    "    'depth': depths,\n",
    "    'weight': weights,\n",
    "    'length_2d': lengths,\n",
    "    'akpd_score': akpd_scores,\n",
    "    'diff_l_x': diffs_l_x, \n",
    "    'diff_r_x': diffs_r_x,\n",
    "    'diff_l_y': diffs_l_y,\n",
    "    'diff_r_y': diffs_r_y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualizations </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Visualize x-axis diff standard deviation (i.e. jitter) broken down by depth bucket </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_regression_line(buckets, stds):\n",
    "    lr = LinearRegression()\n",
    "    X = np.array(range(len(buckets))).reshape(-1, 1)\n",
    "    y = np.array(stds)\n",
    "    reg = lr.fit(X, y)\n",
    "    return float(reg.coef_), float(reg.intercept_)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for idx, body_part in enumerate(core_body_parts):\n",
    "    depth_buckets = np.arange(0.5, 2.2, 0.1)\n",
    "    buckets, stds = [], []\n",
    "    for low_depth, high_depth in zip(depth_buckets, depth_buckets[1:]):\n",
    "        bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "        mask = (rdf.depth >= low_depth) & (rdf.depth <= high_depth) & (rdf.body_part == body_part) & \\\n",
    "               (rdf.diff_l_x.abs() < 50)\n",
    "        std = rdf[mask].diff_l_x.std()\n",
    "\n",
    "        buckets.append(bucket)\n",
    "        stds.append(std)\n",
    "    \n",
    "    # plot empirical standard deviations broken down by depth bucket\n",
    "    row, col = idx // 4, idx % 4\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(stds, label='empirical jitter std values')\n",
    "    ax.set_xticks(range(len(buckets)))\n",
    "    ax.set_xticklabels(buckets, rotation=90)\n",
    "    ax.grid()\n",
    "    ax.set_title(body_part)\n",
    "    ax.set_xlabel('Depth bucket (m)')\n",
    "    ax.set_ylabel('Jitter standard deviation')\n",
    "    \n",
    "    # compute & plot regression line\n",
    "    m, b = compute_regression_line(buckets, stds)\n",
    "    x_values = np.array(range(len(buckets)))\n",
    "    y_values = m * x_values + b\n",
    "    ax.plot(x_values, y_values, linestyle='dashed', color='red', \n",
    "            label='Regression line: std = {}x + {}'.format(round(m, 2), round(b, 2)))\n",
    "    ax.legend()\n",
    "    \n",
    "\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for body_part in core_body_parts:\n",
    "    depth_buckets = np.arange(0.5, 2.2, 0.1)\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 10))\n",
    "    \n",
    "    idx = 0\n",
    "    print('QQ plots by depth bucket for body part: {}'.format(body_part))\n",
    "    for low_depth, high_depth in zip(depth_buckets, depth_buckets[1:]):\n",
    "        mask = (rdf.depth >= low_depth) & (rdf.depth <= high_depth) & (rdf.body_part == body_part)\n",
    "        vals = rdf[mask].diff_l_x.values\n",
    "        \n",
    "        row, col = idx // 4, idx % 4\n",
    "        ax = axes[row, col]\n",
    "        stats.probplot(vals, dist='norm', plot=ax)\n",
    "        ax.set_title('Depth bucket (m): {}-{}'.format(round(low_depth, 2), round(high_depth, 2)))\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Examine Jitter standard deviation as a function of weight </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for idx, body_part in enumerate(core_body_parts):\n",
    "    weight_buckets = np.arange(1000, 10000, 1000)\n",
    "    buckets, stds = [], []\n",
    "    for low_weight, high_weight in zip(weight_buckets, weight_buckets[1:]):\n",
    "        bucket = '{}-{}'.format(round(low_weight, 2), round(high_weight, 2))\n",
    "        mask = (rdf.weight >= low_weight) & (rdf.weight <= high_weight) & (rdf.body_part == body_part) & \\\n",
    "               (rdf.diff_l_x.abs() < 50)\n",
    "        std = rdf[mask].diff_l_x.std()\n",
    "\n",
    "        buckets.append(bucket)\n",
    "        stds.append(std)\n",
    "    \n",
    "    # plot empirical standard deviations broken down by depth bucket\n",
    "    row, col = idx // 4, idx % 4\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(stds, label='empirical jitter std values')\n",
    "    ax.set_xticks(range(len(buckets)))\n",
    "    ax.set_xticklabels(buckets, rotation=90)\n",
    "    ax.grid()\n",
    "    ax.set_title(body_part)\n",
    "    ax.set_xlabel('Weight bucket (g)')\n",
    "    ax.set_ylabel('Jitter standard deviation')\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Examine AKPD score as a function of weight </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "weight_buckets = np.arange(0, 9000, 1000)\n",
    "buckets, akpd_scores = [], []\n",
    "for low_weight, high_weight in zip(weight_buckets, weight_buckets[1:]):\n",
    "    bucket = '{}-{}'.format(round(low_weight, 2), round(high_weight, 2))\n",
    "    mask = (rdf.weight >= low_weight) & (rdf.weight <= high_weight) & (rdf.body_part == body_part) & \\\n",
    "           (rdf.diff_l_x.abs() < 50)\n",
    "    mean_akpd_score = rdf[mask].akpd_score.mean()\n",
    "\n",
    "    buckets.append(bucket)\n",
    "    akpd_scores.append(mean_akpd_score)\n",
    "    \n",
    "ax.plot(akpd_scores, label='per-bucket mean akpd score')\n",
    "ax.set_xticks(range(len(buckets)))\n",
    "ax.set_xticklabels(buckets, rotation=90)\n",
    "ax.grid()\n",
    "ax.set_title('AKPD score vs. weight bucket')\n",
    "ax.set_xlabel('Weight bucket (g)')\n",
    "ax.set_ylabel('AKPD score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Examine AKPD score versus 2D fish length </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "length_cutoffs = np.arange(800, 4000, 200)\n",
    "buckets, akpd_scores = [], []\n",
    "for low_length, high_length in zip(length_cutoffs, length_cutoffs[1:]):\n",
    "    bucket = '{}-{}'.format(round(low_length, 2), round(high_length, 2))\n",
    "    mask = (rdf.length_2d >= low_length) & (rdf.length_2d <= high_length) & (rdf.body_part == body_part) & \\\n",
    "           (rdf.diff_l_x.abs() < 50)\n",
    "    mean_akpd_score = rdf[mask].akpd_score.mean()\n",
    "\n",
    "    buckets.append(bucket)\n",
    "    akpd_scores.append(mean_akpd_score)\n",
    "    \n",
    "ax.plot(akpd_scores, label='per-bucket mean akpd score')\n",
    "ax.set_xticks(range(len(buckets)))\n",
    "ax.set_xticklabels(buckets, rotation=90)\n",
    "ax.grid()\n",
    "ax.set_title('AKPD score vs. 2D length bucket')\n",
    "ax.set_xlabel('2D length bucket (pixels)')\n",
    "ax.set_ylabel('AKPD score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compare average weight between manual pipeline and automatic pipeline </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.akpd_utils.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_akpd_score(akpd_scorer_network, keypoints: Dict, camera_metadata: Dict) -> float:\n",
    "    input_sample = {\n",
    "        'keypoints': keypoints,\n",
    "        'cm': camera_metadata,\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "    return akpd_score\n",
    "\n",
    "\n",
    "\n",
    "weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "\n",
    "weights_manual_pipeline, weights_auto, manual_akpd_scores = [], [], []\n",
    "count = 0\n",
    "for idx, row in tdf.iterrows():\n",
    "    \n",
    "    camera_metadata = row.camera_metadata\n",
    "    \n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    # auto weight calculation\n",
    "    weight_auto, length, kf = weight_estimator.predict(json.loads(row.annotation), cm)\n",
    "    weights_auto.append(weight_auto)\n",
    "    \n",
    "    # manual weight calculation\n",
    "    ann = row.manual_keypoints\n",
    "    if not ann or (not ann.get('leftCrop') or not ann.get('rightCrop')):\n",
    "        weights_manual_pipeline.append(None)\n",
    "        manual_akpd_scores.append(None)\n",
    "        continue\n",
    "        \n",
    "    # manual AKPD score\n",
    "    manual_akpd_score = compute_akpd_score(akpd_scorer_network, row.manual_keypoints, camera_metadata)\n",
    "    manual_akpd_scores.append(manual_akpd_score)\n",
    "    \n",
    "    weight, length, kf = weight_estimator.predict(ann, cm)\n",
    "    weights_manual_pipeline.append(weight)\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "\n",
    "tdf['weight_manual'] = weights_manual_pipeline\n",
    "tdf['weight_auto'] = weights_auto\n",
    "tdf['manual_akpd_score'] = manual_akpd_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.manual_akpd_score > 0.01].weight_manual.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.akpd_score > 0.01].weight_auto.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cutoffs = np.arange(0, 10000, 1000)\n",
    "avg_weight_auto_list, avg_weight_manual_list, manual_sample_sizes, auto_sample_sizes = [], [], [], []\n",
    "for low_weight, high_weight in zip(weight_cutoffs, weight_cutoffs[1:]):\n",
    "    mask = (tdf.weight_manual > low_weight) & (tdf.weight_manual < high_weight)\n",
    "    avg_weight_auto = tdf[mask & (tdf.akpd_score > 0.01)].weight_auto.mean()\n",
    "    avg_weight_manual = tdf[mask & (tdf.manual_akpd_score > 0.01)].weight_manual.mean()\n",
    "    manual_sample_size = tdf[mask & (tdf.manual_akpd_score > 0.01)].shape[0]\n",
    "    auto_sample_size = tdf[mask & (tdf.akpd_score > 0.01)].shape[0]\n",
    "    avg_weight_auto_list.append(avg_weight_auto)\n",
    "    avg_weight_manual_list.append(avg_weight_manual)\n",
    "    manual_sample_sizes.append(manual_sample_size)\n",
    "    auto_sample_sizes.append(auto_sample_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(avg_weight_auto_list, avg_weight_manual_list, manual_sample_sizes, auto_sample_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.manual_akpd_score, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set(tdf[tdf.manual_akpd_score > 0.01].left_crop_url).intersection(set(tdf[tdf.akpd_score > 0.01].left_crop_url))\n",
    "len(intersection) / tdf[tdf.akpd_score > 0.01].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.manual_akpd_score > 0.01) & (tdf.akpd_score <= 0.01)\n",
    "tdf[mask].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.akpd_score > 0.01].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
