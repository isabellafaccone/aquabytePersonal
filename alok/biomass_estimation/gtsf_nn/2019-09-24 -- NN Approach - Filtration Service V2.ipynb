{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from wpca import WPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, convert_to_world_point\n",
    "from aquabyte.visualize import Visualizer, _normalize_world_keypoints\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Extract base data from database </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from research.fish_metadata a left join keypoint_annotations b\n",
    "    on a.left_url = b.left_image_url \n",
    "    where b.keypoints is not null and b.is_qa = false;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Append world kepyoints to the data </h1>\n",
    "<h3> Ideally, this data should already live directly in the database </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")\n",
    "\n",
    "df = df[~df.world_keypoints.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add weight prediction to data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('/root/alok/repos/cv_algorithms/biomass-production/src/model.pkl', 'rb'))\n",
    "\n",
    "# helper function from cv_algorithms\n",
    "def coord2biomass(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "\n",
    "    interaction_values_quadratic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values_quadratic.append(dist1 * dist2)\n",
    "\n",
    "    interaction_values_cubic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            for k in range(j, len(pairwise_distances)):\n",
    "                dist1 = pairwise_distances[i]\n",
    "                dist2 = pairwise_distances[j]\n",
    "                dist3 = pairwise_distances[k]\n",
    "                interaction_values_cubic.append(dist1 * dist2 * dist3)\n",
    "\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values_quadratic + interaction_values_cubic)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create \"good\" class of data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.5\n",
    "fish_ids = list(df.fish_id.unique())\n",
    "random.shuffle(fish_ids)\n",
    "N = len(fish_ids)\n",
    "train_fish_ids = fish_ids[:int(train_pct * N)]\n",
    "test_fish_ids = fish_ids[int(train_pct * N):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = sorted(list(df.world_keypoints.iloc[0].keys()))\n",
    "def generate_X_y(df):\n",
    "    X, y = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        world_keypoints = row.world_keypoints\n",
    "        if world_keypoints:\n",
    "            norm_wkps = _normalize_world_keypoints(world_keypoints)\n",
    "            keypoints_list = []\n",
    "            for bp in body_parts:\n",
    "                keypoints_list.append(norm_wkps[bp])\n",
    "            if np.isnan(np.array(keypoints_list)).sum() == 0:    \n",
    "                X.append(keypoints_list)\n",
    "                y.append(1)\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "    \n",
    "\n",
    "# create X, y set corresponding to just \"good\" labels for both the train and test set\n",
    "train_mask = df.fish_id.isin(train_fish_ids)\n",
    "X_train_good, y_train_good = generate_X_y(df[train_mask])\n",
    "X_test_good, y_test_good = generate_X_y(df[~train_mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create \"bad\" class of data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bad_X_y(df):\n",
    "    np.random.seed(0)\n",
    "    X, y = [], []\n",
    "    pct_error_threshold_gt = 0.5\n",
    "    pct_error_threshold_original = 0.2\n",
    "    row_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        gt_weight = row.weight\n",
    "        # random keypoint jitters\n",
    "        keypoints = row.keypoints\n",
    "        original_weight = coord2biomass(row.world_keypoints, model)\n",
    "        for jitter in [10, 20, 50, 100]:\n",
    "            for num_bad_locations in [1, 2, 3]:\n",
    "                jittered_keypoints = {'leftCrop': [], 'rightCrop': []}\n",
    "                jittered_locations = list(np.random.choice(body_parts, num_bad_locations))\n",
    "                for key in ['leftCrop', 'rightCrop']:\n",
    "                    for item in keypoints[key]:\n",
    "                        jittered_item = copy(item)\n",
    "                        if jittered_item['keypointType'] in jittered_locations:\n",
    "                            j = np.random.normal(0, jitter)\n",
    "                            jittered_item['xFrame'] += j\n",
    "                        jittered_keypoints[key].append(jittered_item)\n",
    "\n",
    "                jittered_world_keypoints = pixel2world(jittered_keypoints['leftCrop'],\n",
    "                                                       jittered_keypoints['rightCrop'],\n",
    "                                                       row.camera_metadata)\n",
    "                estimated_weight = coord2biomass(jittered_world_keypoints, model)\n",
    "                pct_error_original = (original_weight - gt_weight)/gt_weight\n",
    "                pct_error_estimated = (estimated_weight - gt_weight)/gt_weight\n",
    "\n",
    "                # update X and y\n",
    "                if (abs(pct_error_estimated) > 0.4) or (abs(pct_error_estimated) - abs(pct_error_original) > 0.2):\n",
    "                    norm_wkps = _normalize_world_keypoints(jittered_world_keypoints)\n",
    "                    keypoints_list = []\n",
    "                    for bp in body_parts:\n",
    "                        keypoints_list.append(norm_wkps[bp])\n",
    "                    if np.isnan(np.array(keypoints_list)).sum() == 0:\n",
    "                        X.append(keypoints_list)\n",
    "                        y.append(0)\n",
    "\n",
    "        if row_count % 100 == 0:\n",
    "            print(row_count)\n",
    "        row_count += 1\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bad, y_train_bad = generate_bad_X_y(df[train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bad, y_test_bad = generate_bad_X_y(df[~train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creat PyTorch Dataloader from balanced training set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, labels, transform=None):\n",
    "        self.X = X\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(np.array([y])).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "train_size_good = X_train_good.shape[0]\n",
    "random_choices = np.random.choice(list(range(X_train_bad.shape[0])), train_size_good)\n",
    "X_train = np.vstack([X_train_good, X_train_bad[random_choices]])\n",
    "y_train = np.hstack([y_train_good, y_train_bad[random_choices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KeypointsDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_size_good = X_test_good.shape[0]\n",
    "random_choices = np.random.choice(list(range(X_test_bad.shape[0])), test_size_good)\n",
    "X_test = np.vstack([X_test_good, X_test_bad[random_choices]])\n",
    "y_test = np.hstack([y_test_good, y_test_bad[random_choices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = KeypointsDataset(X_test, y_test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=25, shuffle=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KeypointsDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=True, num_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "epochs = 500\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = data_batch\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    # compute validation loss\n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        with torch.no_grad():\n",
    "            all_equals = []\n",
    "            for i, data_batch in enumerate(val_dataloader):\n",
    "                X_batch, y_batch = data_batch\n",
    "                y_pred = network(X_batch)\n",
    "                equals = y_pred.round() == y_batch.view(y_pred.shape)\n",
    "                all_equals.extend(list(equals))\n",
    "        accuracy = np.mean(np.array(all_equals))\n",
    "    \n",
    "    loss_for_epoch = running_loss / len(dataloader)\n",
    "    print('Loss for epoch {}: {}'.format(epoch, loss_for_epoch))\n",
    "    print('Validation accuracy for epoch {}: {}'.format(epoch, accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(), '/root/data/alok/biomass_estimation/playground/filter_nn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
