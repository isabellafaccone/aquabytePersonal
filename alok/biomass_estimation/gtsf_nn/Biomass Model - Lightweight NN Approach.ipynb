{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from wpca import WPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from aquabyte.visualize import Visualizer\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import copy\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from research.fish_metadata a left join keypoint_annotations b\n",
    "    on a.left_url = b.left_image_url \n",
    "    where b.keypoints is not null and b.is_qa = false;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")\n",
    "\n",
    "df = df[~df.world_keypoints.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_fish_identifiers = list(df.fish_id.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "train_mask = df.fish_id.isin(fish_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, n_factors=1, jitters=[0], oos=False):\n",
    "    X_row, labels_row = [], []\n",
    "    left_keypoints_list = row.keypoints.get('leftCrop')\n",
    "    right_keypoints_list = row.keypoints.get('rightCrop')\n",
    "    world_keypoints = row.world_keypoints\n",
    "    cm = row.camera_metadata\n",
    "    \n",
    "    if left_keypoints_list and right_keypoints_list:\n",
    "        left_keypoints = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints_list}\n",
    "        right_keypoints = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints_list}\n",
    "\n",
    "        adj_left_keypoints, adj_right_keypoints = copy(left_keypoints), copy(right_keypoints)\n",
    "        if adj_left_keypoints['UPPER_LIP'][0] < adj_left_keypoints['TAIL_NOTCH'][0]:\n",
    "            # perform flip\n",
    "            for bp in body_parts:\n",
    "                adj_left_keypoints_bp = copy(left_keypoints[bp])\n",
    "                adj_right_keypoints_bp = copy(right_keypoints[bp])\n",
    "                adj_left_keypoints_bp_0 = 4096 - adj_right_keypoints_bp[0]\n",
    "                adj_left_keypoints_bp_1 = adj_right_keypoints_bp[1]\n",
    "                adj_right_keypoints_bp_0 = 4096 - adj_left_keypoints_bp[0]\n",
    "                adj_right_keypoints_bp_1 = adj_left_keypoints_bp[1]\n",
    "                adj_left_keypoints[bp] = np.array([adj_left_keypoints_bp_0, adj_left_keypoints_bp_1])\n",
    "                adj_right_keypoints[bp] = np.array([adj_right_keypoints_bp_0, adj_right_keypoints_bp_1])\n",
    "\n",
    "        # translate such that tail notch is at origin\n",
    "\n",
    "        left_tail_notch_coords = copy(adj_left_keypoints['TAIL_NOTCH'])\n",
    "        for bp in body_parts:\n",
    "            adj_left_keypoints[bp] -= left_tail_notch_coords\n",
    "            adj_right_keypoints[bp] -= left_tail_notch_coords\n",
    "        \n",
    "        for n in range(n_factors):\n",
    "            if n == 1:\n",
    "                factor = 1.0\n",
    "            else:\n",
    "                factor = np.random.uniform(low=0.5, high=1.5)\n",
    "            for jitter in jitters:\n",
    "                trials = 3 if jitter > 0 else 1\n",
    "                for t in range(trials):\n",
    "                    data_point = []\n",
    "                    for bp in body_parts:\n",
    "                        x, y = factor * adj_left_keypoints[bp][0], factor * adj_left_keypoints[bp][1]\n",
    "                        x_right = factor * adj_right_keypoints[bp][0]\n",
    "                        x += np.random.normal(0, jitter)\n",
    "                        x_right += np.random.normal(0, jitter)\n",
    "                        d = abs(x - x_right)\n",
    "                        if d == 0:\n",
    "                            break\n",
    "                        depth = depth_from_disp(d, cm)\n",
    "                        wkp = convert_to_world_point(y, x, depth, cm)\n",
    "                        data_point.append([wkp[0] / depth, wkp[2] / depth, 0.1 / depth])\n",
    "                        print(wkp)\n",
    "                    if d == 0:\n",
    "                        continue\n",
    "                    X_row.append(data_point)\n",
    "                    if not oos:\n",
    "                        labels_row.append(row.weight)\n",
    "\n",
    "    return X_row, labels_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tdf.head(20).iterrows():\n",
    "    process_row(row, oos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X, labels = [], []\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "row_count = 0\n",
    "for idx, row in df[train_mask].iterrows():\n",
    "    \n",
    "    X_row, labels_row = process_row(row, n_factors=5, jitters=[0, 10, 20])\n",
    "    X.extend(X_row)\n",
    "    labels.extend(labels_row)\n",
    "    \n",
    "    if row_count % 1000 == 0:\n",
    "        print('Percentage complete: {}'.format(row_count / df[train_mask].shape[0]))\n",
    "    row_count += 1\n",
    "    \n",
    "X, labels = np.array(X) / 2.0, np.array(labels) / 10000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 718521].left_image_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 718521].right_image_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, labels, transform=None):\n",
    "        self.X = X\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(np.array([y])).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = KeypointsDataset(X, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "epochs = 2000\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = data_batch\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(running_loss / i)\n",
    "    \n",
    "    loss_for_epoch = running_loss / len(dataloader)\n",
    "    print('Loss for epoch {}: {}'.format(epoch, loss_for_epoch))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, labels_t = [], []\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "for idx, row in df[~train_mask].iterrows():\n",
    "    X_row, labels_row = process_row(row, n_factors=3, jitters=[20])\n",
    "    X_t.extend(X_row)\n",
    "    labels_t.extend(labels_row)\n",
    "    \n",
    "X_t, labels_t = np.array(X_t) / 2.0, np.array(labels_t) / 10000.0\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(torch.from_numpy(X_t).float())\n",
    "predictions = preds.detach().numpy().squeeze()\n",
    "weights = labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(weights * 1e4, predictions * 1e4)\n",
    "plt.plot([0, 1e4], [0, 1e4], color='red')\n",
    "plt.xlim([0, 1e4])\n",
    "plt.ylim([0, 1e4])\n",
    "plt.xlabel('Ground truth weight (grams)')\n",
    "plt.ylabel('Prediction (grams)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs((predictions - weights)/weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions.mean() - weights.mean())/weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/results/model_lateral_only/results_557ec1732d8bc8bc66951d2ea4e69b935d69b111_model_lateral_only_research-exp-id-01-vikingfjord-20190628-20190630.h5'\n",
    "tdf = pd.read_hdf(f, 'table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_o = []\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "for idx, row in tdf.iterrows():\n",
    "    X_row, _ = process_row(row, n_factors=1, jitters=[0], oos=True)\n",
    "    X_o.extend(X_row)\n",
    "    \n",
    "X_o = np.array(X_o) / 2.0\n",
    "        \n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(torch.from_numpy(X_o).float())\n",
    "predictions = preds.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(predictions * 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_o[:, 0, 0], X_o[:, 0, 2], color='r', alpha=0.5)\n",
    "plt.scatter(X_t[:, 0, 0], X_t[:, 0, 2], color='b', alpha=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['depth'] = tdf.world_keypoints.apply(lambda x: x['EYE'][1] if x else None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:, 0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:, 0, 2], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_o[:, 0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(torch.from_numpy(X_o).float())\n",
    "predictions = preds.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(predictions * 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(tdf[tdf.depth < 0.8].est_weight * 1e4)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(tdf.depth, 1e4 * tdf.est_weight)\n",
    "plt.ylim([-10000, 20000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(tdf.depth, tdf.estimated_biomass_g)\n",
    "plt.ylim([-10000, 20000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
