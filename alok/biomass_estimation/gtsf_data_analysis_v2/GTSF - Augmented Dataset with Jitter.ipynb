{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n",
    "def coord2biomass_blender(world_keypoints, blender):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    # mapping helps for consistency with the kp order\n",
    "    reverse_mapping = blender[\"reverse_mapping\"]\n",
    "    distances = np.array(blender[\"distances\"])\n",
    "    volumes = blender[\"volume\"]\n",
    "    regression_coeff = blender[\"coeff\"]\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # the reverse mapping insure that we listing the kp\n",
    "    # in the same order\n",
    "    measurements = []\n",
    "    number_of_parts = len(world_keypoints)\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    measurements = np.array(measurements)\n",
    "\n",
    "    # absolute diff\n",
    "    diff = np.nanmean(np.abs(distances - measurements), axis=1)\n",
    "    closest = np.argmin(diff)\n",
    "    prediction = volumes[closest]\n",
    "\n",
    "    # here is some machine learning\n",
    "    prediction = prediction*regression_coeff[0] + regression_coeff[1]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('./model.pkl', 'rb'))\n",
    "blender = json.load(open('./volumes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "sfps_all = session.query(StereoFramePair).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for idx, row in enumerate(sfps_all):\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    \n",
    "    # skip bad cases\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        print('Skipping fish ID {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        print('Skipping non-samlon fish: {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # get pairwise distances and biomass predictions\n",
    "    world_keypoint_coordinates = json.loads(row.world_keypoint_coordinates)\n",
    "    predicted_weight_linear = coord2biomass_linear(world_keypoint_coordinates, model)\n",
    "    predicted_weight_blender = coord2biomass_blender(world_keypoint_coordinates, blender)\n",
    "    predicted_length = euclidean_distance(world_keypoint_coordinates['UPPER_LIP'], world_keypoint_coordinates['TAIL_NOTCH'])\n",
    "    \n",
    "    df_row = {}\n",
    "    body_parts = sorted(list(world_keypoint_coordinates.keys()))\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            bp1, bp2 = body_parts[i], body_parts[j]\n",
    "            df_row['{}<->{}'.format(body_parts[i], body_parts[j])] = \\\n",
    "                euclidean_distance(world_keypoint_coordinates[bp1], world_keypoint_coordinates[bp2])\n",
    "            \n",
    "    left_image_keypoint_coordinates = json.loads(row.left_image_keypoint_coordinates)\n",
    "    right_image_keypoint_coordinates = json.loads(row.right_image_keypoint_coordinates)\n",
    "    if row.annotations_project_name == 'Automated keypoints detection':\n",
    "        left_image_keypoint_coordinates = {bp: (x[1], x[0]) for bp, x in left_image_keypoint_coordinates.items()}\n",
    "        right_image_keypoint_coordinates = {bp: (x[1], x[0]) for bp, x in right_image_keypoint_coordinates.items()}\n",
    "    \n",
    "    keypoints_valid = True\n",
    "    \n",
    "    left_keypoint_y_coords = {bp: left_image_keypoint_coordinates[bp][1] for bp in left_image_keypoint_coordinates}\n",
    "    right_keypoint_y_coords = {bp: right_image_keypoint_coordinates[bp][1] for bp in right_image_keypoint_coordinates}\n",
    "    max_y_coordinate_deviation = \\\n",
    "        max([abs(left_keypoint_y_coords[bp] - right_keypoint_y_coords[bp]) for bp in body_parts])\n",
    "            \n",
    "    # append to dataset\n",
    "    df_row.update({\n",
    "        'gtsf_fish_identifier': row.gtsf_fish_identifier,\n",
    "        'weight': ground_truth_metadata['data']['weight'],\n",
    "        'length': ground_truth_metadata['data']['length'],\n",
    "        'width': ground_truth_metadata['data']['width'],\n",
    "        'breadth': ground_truth_metadata['data']['breath'],\n",
    "        'world_keypoint_coordinates': world_keypoint_coordinates,\n",
    "        'left_image_keypoint_coordinates': json.loads(row.left_image_keypoint_coordinates),\n",
    "        'right_image_keypoint_coordinates': json.loads(row.right_image_keypoint_coordinates),\n",
    "        'kfactor': 1e5 * ground_truth_metadata['data']['weight'] / ground_truth_metadata['data']['length']**3,\n",
    "        'date': row.date,\n",
    "        'left_image_s3_key': row.left_image_s3_key,\n",
    "        'right_image_s3_key': row.right_image_s3_key,\n",
    "        'image_s3_bucket': row.image_s3_bucket,\n",
    "        'predicted_weight_linear': predicted_weight_linear,\n",
    "        'predicted_weight_blender': predicted_weight_blender,\n",
    "        'predicted_length': predicted_length,\n",
    "        'error_pct_linear': (predicted_weight_linear - ground_truth_metadata['data']['weight']) / ground_truth_metadata['data']['weight'],\n",
    "        'error_pct_blender': (predicted_weight_blender - ground_truth_metadata['data']['weight']) / ground_truth_metadata['data']['weight'],\n",
    "        'project_name': row.annotations_project_name\n",
    "    })\n",
    "    \n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "df_cache = df.copy()\n",
    "df = df.dropna()\n",
    "df = df[df.project_name != 'Automated keypoints detection']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Construct new df with jitters introduced </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_data_collection = session.query(GtsfDataCollection).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_data_collection = session.query(GtsfDataCollection).first()\n",
    "calibration = session.query(Calibration) \\\n",
    "             .filter(Calibration.enclosure_id == gtsf_data_collection.enclosure_id) \\\n",
    "             .order_by(Calibration.utc_timestamp.desc()) \\\n",
    "             .first()\n",
    "\n",
    "enclosure = session.query(Enclosure).get(calibration.enclosure_id)\n",
    "\n",
    "\n",
    "focal_length = float(calibration.predicted_focal_length_mm) / (1e3)\n",
    "baseline = float(calibration.predicted_baseline_mm) / (1e3)\n",
    "pixel_size_m = float(enclosure.pixel_width_um) / (1e6)\n",
    "focal_length_pixel = focal_length / pixel_size_m\n",
    "image_sensor_width = float(enclosure.sensor_width_mm) / (1e3)\n",
    "image_sensor_height = float(enclosure.sensor_height_mm) / (1e3)\n",
    "pixel_count_width = enclosure.image_num_pixels_width\n",
    "pixel_count_height = enclosure.image_num_pixels_height\n",
    "\n",
    "parameters = {\n",
    "    'baseline': baseline,\n",
    "    'focalLengthPixel': focal_length_pixel,\n",
    "    'imageSensorWidth': image_sensor_width,\n",
    "    'imageSensorHeight': image_sensor_height,\n",
    "    'pixelCountWidth': pixel_count_width,\n",
    "    'pixelCountHeight': pixel_count_height,\n",
    "    'pixelSize': pixel_size_m\n",
    "}\n",
    "\n",
    "parameters['focalLength'] = parameters['focalLengthPixel'] * parameters['pixelSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame()\n",
    "jitter_values_x = [10, 50]\n",
    "trials = 10\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    left_keypoints_original = row.left_image_keypoint_coordinates\n",
    "    right_keypoints_original = row.right_image_keypoint_coordinates\n",
    "    \n",
    "    # introduce small jitter\n",
    "    body_parts = sorted(left_keypoints_original.keys())\n",
    "    for jitter_value_x in jitter_values_x:\n",
    "        for t in range(trials):\n",
    "            jitter_value_y = 0\n",
    "            left_keypoints = {bp: [left_keypoints_original[bp][0] + random.gauss(0, jitter_value_x),\n",
    "                                   left_keypoints_original[bp][1] + random.gauss(0, jitter_value_y)] for bp in body_parts}\n",
    "            right_keypoints = {bp: [right_keypoints_original[bp][0] + random.gauss(0, jitter_value_x),\n",
    "                                    right_keypoints_original[bp][1] + random.gauss(0, jitter_value_y)] for bp in body_parts}\n",
    "\n",
    "            world_keypoints = {}\n",
    "            \n",
    "            for bp in body_parts:\n",
    "                lkp, rkp = left_keypoints[bp], right_keypoints[bp]\n",
    "                d = abs(lkp[0] - rkp[0])\n",
    "\n",
    "                # compute world key point\n",
    "                depth = depth_from_disp(d, parameters)\n",
    "                wkp = convert_to_world_point(lkp[0], lkp[1], depth, parameters)\n",
    "\n",
    "                world_keypoints[bp] = wkp\n",
    "\n",
    "            predicted_weight_linear = coord2biomass_linear(world_keypoints, model)\n",
    "            predicted_weight_blender = coord2biomass_blender(world_keypoints, blender)\n",
    "\n",
    "            df_row = {\n",
    "                'gtsf_fish_identifier': row.gtsf_fish_identifier,\n",
    "                'predicted_weight_linear': predicted_weight_linear,\n",
    "                'predicted_weight_blender': predicted_weight_blender,\n",
    "                'weight': row.weight,\n",
    "                'trial': t,\n",
    "                'jitter_value_x': jitter_value_x\n",
    "            }\n",
    "            \n",
    "            analysis_df = analysis_df.append(df_row, ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "axes[0].scatter(df.weight, df.predicted_weight_linear)\n",
    "axes[0].plot([0, 5000], [0, 5000])\n",
    "axes[0].set_title('Jitter value = 0')\n",
    "mask = analysis_df.jitter_value_x == 10\n",
    "axes[1].scatter(analysis_df[mask].weight, analysis_df[mask].predicted_weight_linear)\n",
    "axes[1].plot([0, 5000], [0, 5000])\n",
    "axes[1].set_title('Jitter value = 10 pixels')\n",
    "mask = analysis_df.jitter_value_x == 50\n",
    "axes[2].scatter(analysis_df[mask].weight, analysis_df[mask].predicted_weight_linear)\n",
    "axes[2].plot([0, 5000], [0, 5000])\n",
    "axes[2].set_title('Jitter value = 50 pixels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((analysis_df[mask].predicted_weight_linear.mean()) - (analysis_df[mask].weight.mean())) / (analysis_df[mask].weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis_df.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
