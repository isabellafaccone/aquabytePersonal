{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF : Ground Truth Weight Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.data_access_utils import DataAccessUtils\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "data_access_utils = DataAccessUtils()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train linear model with PCA + interaction features </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "gtsf_data_collections_all = session.query(GtsfDataCollection).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in enumerate(gtsf_data_collections_all):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    # get fish_id and ground truth metadata\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "\n",
    "    weight = None\n",
    "    if 'data' in ground_truth_metadata.keys():\n",
    "        if 'weight' in ground_truth_metadata['data'].keys():\n",
    "            weight = ground_truth_metadata['data']['weight']\n",
    "    df_row = {}\n",
    "    df_row['gtsf_fish_identifier'] = row.gtsf_fish_identifier\n",
    "    df_row['date'] = row.date\n",
    "    df_row['weight'] = weight\n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply filters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache = df.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(df.weight, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.weight > 5000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "\n",
    "    interaction_values_quadratic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values_quadratic.append(dist1 * dist2)\n",
    "\n",
    "    interaction_values_cubic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            for k in range(j, len(pairwise_distances)):\n",
    "                dist1 = pairwise_distances[i]\n",
    "                dist2 = pairwise_distances[j]\n",
    "                dist3 = pairwise_distances[k]\n",
    "                interaction_values_cubic.append(dist1 * dist2 * dist3)\n",
    "\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values_quadratic + interaction_values_cubic)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(left_keypoints, right_keypoints, world_keypoints, baseline_biomass_model):\n",
    "    filter_out, reason = False, None\n",
    "    \n",
    "    # apply y-coordinate deviation filter\n",
    "    body_parts = sorted(list(left_keypoints.keys()))\n",
    "    max_y_coordinate_deviation = max([abs(left_keypoints[bp][1] - right_keypoints[bp][1]) for bp in body_parts])\n",
    "#     print(max_y_coordinate_deviation, max_x_coordinate_deviation)\n",
    "    if max_y_coordinate_deviation == 2297:\n",
    "        print(left_keypoints, right_keypoints)\n",
    "    if (max_y_coordinate_deviation > 25):\n",
    "        filter_out = True\n",
    "        reason = 'Y-coordinate deviation too high'\n",
    "        \n",
    "    # apply world y-coordinate deviation filter\n",
    "    norm_wkps = normalize_world_keypoints(world_keypoints)\n",
    "    y_world_coordinates = [norm_wkps[bp][1] for bp in body_parts]\n",
    "    max_y_world_coordinate_deviation = max(y_world_coordinates) - min(y_world_coordinates)\n",
    "    if max_y_world_coordinate_deviation > 0.25:\n",
    "        filter_out = True\n",
    "        reason = 'World y-coordinate deviation too high'\n",
    "        \n",
    "    # apply baseline biomass model\n",
    "    baseline_weight_prediction = coord2biomass_linear(world_keypoints, baseline_biomass_model)\n",
    "    if (baseline_weight_prediction < 0) or (baseline_weight_prediction > 15000):\n",
    "        filter_out = True\n",
    "        reason = 'Baseline prediction way too off'\n",
    "        \n",
    "    \n",
    "    return max_y_coordinate_deviation, filter_out, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_biomass_model = pickle.load(open('/root/data/alok/biomass_estimation/models/model_v2.pkl', 'rb'))\n",
    "df['filter_out'] = False\n",
    "df['reason'] = None\n",
    "for idx, row in df.iterrows():\n",
    "    max_y_coordinate_deviation, filter_out, reason = \\\n",
    "        apply_filters(row.left_keypoints, row.right_keypoints, row.world_keypoints, baseline_biomass_model)\n",
    "    if filter_out:\n",
    "        df.at[idx, 'max_y_coordinate_deviation'] = max_y_coordinate_deviation\n",
    "        df.at[idx, 'filter_out'] = True\n",
    "        df.at[idx, 'reason'] = reason\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualize Individual Cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.gtsf_fish_identifier != '190607010041_bolaks-mjanes'].sort_values('max_y_coordinate_deviation', ascending=False)[['gtsf_fish_identifier', 'max_y_coordinate_deviation']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(image, gamma=2.2):\n",
    "    image = image / 255.0\n",
    "    Y = 0.2126*image[:, :, 0]**gamma + 0.7152*image[:, :, 1]**gamma + 0.0722*image[:, :, 2]**gamma\n",
    "    L = 116 * Y**(1.0/3) - 16\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints, \n",
    "                               bp_1='UPPER_LIP', bp_2='TAIL_NOTCH', vertical_search_size=3, window_size=100,\n",
    "                               min_breadth=0.04, max_breadth=0.2):\n",
    "    \n",
    "    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#         left_extrap_kp = (0.5 * left_keypoints[bp_1] + 0.5 * left_keypoints[bp_2]).astype('int64')\n",
    "#         right_extrap_kp = (0.5 * right_keypoints[bp_1] + 0.5 * right_keypoints[bp_2]).astype('int64')\n",
    "        left_extrap_kp = (i * left_keypoints[bp_1] + (1-i) * left_keypoints[bp_2]).astype('int64')\n",
    "        right_extrap_kp = (i * right_keypoints[bp_1] + (1-i) * right_keypoints[bp_2]).astype('int64')\n",
    "        bp_1_depth = world_keypoints[bp_1][1]\n",
    "        bp_2_depth = world_keypoints[bp_2][1]\n",
    "\n",
    "        # need to determine lower and upper bounds here in a data driven fashion from GTSF data\n",
    "        # hardcoded values used here\n",
    "        extrap_kp_max_depth = (bp_1_depth + bp_2_depth) / 2.0 - min_breadth / 2.0\n",
    "        extrap_kp_min_depth = (bp_1_depth + bp_2_depth) / 2.0 - max_breadth / 2.0\n",
    "\n",
    "        # Compute the feature descriptor for the extrapolated keypoint in the left image\n",
    "        extrap_kp_min_disp = disp_from_depth(extrap_kp_max_depth)\n",
    "        extrap_kp_max_disp = disp_from_depth(extrap_kp_min_depth)\n",
    "\n",
    "        left_image_grayscale = convert_to_grayscale(left_image)\n",
    "        right_image_grayscale = convert_to_grayscale(right_image)\n",
    "\n",
    "        left_box = left_image_grayscale[left_extrap_kp[1]-window_size//2:left_extrap_kp[1]+window_size//2, \n",
    "                                        left_extrap_kp[0]-window_size//2:left_extrap_kp[0]+window_size//2]\n",
    "\n",
    "        min_sad = np.inf\n",
    "        i_match, j_match = None, None\n",
    "        for i in range(left_extrap_kp[1]-vertical_search_size//2, left_extrap_kp[1]+vertical_search_size//2):\n",
    "            for j in range(left_extrap_kp[0]-int(extrap_kp_max_disp), left_extrap_kp[0]-int(extrap_kp_min_disp)):\n",
    "#             for j in range(right_extrap_kp[0]-100, right_extrap_kp[0]+100):\n",
    "                right_box = right_image_grayscale[i-window_size//2:i+window_size//2, j-window_size//2:j+window_size//2]\n",
    "                sad = np.abs(left_box - right_box).sum()\n",
    "                if sad < min_sad:\n",
    "                    i_match, j_match = i, j\n",
    "                    min_sad = sad\n",
    "\n",
    "        left_keypoints['BODY_{}'.format(i)] = left_extrap_kp\n",
    "        right_keypoints['BODY_{}'.format(i)] = np.array([j_match, i_match])\n",
    "    return left_keypoints, right_keypoints\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints, \n",
    "                               bp='EYE', horizontal_search_space = 20, vertical_search_size=20, window_size=100,\n",
    "                               min_breadth=0.04, max_breadth=0.2):\n",
    "    left_kp = left_keypoints[bp]\n",
    "    right_kp = right_keypoints[bp]\n",
    "\n",
    "    left_image_grayscale = convert_to_grayscale(left_image)\n",
    "    right_image_grayscale = convert_to_grayscale(right_image)\n",
    "    \n",
    "    left_box = left_image_grayscale[left_kp[1]-window_size//2:left_kp[1]+window_size//2, \n",
    "                                    left_kp[0]-window_size//2:left_kp[0]+window_size//2]\n",
    "    \n",
    "    min_sad = np.inf\n",
    "    i_match, j_match = None, None\n",
    "    for i in range(right_kp[1]-vertical_search_size//2, right_kp[1]+vertical_search_size//2):\n",
    "        for j in range(right_kp[0]-horizontal_search_space, right_kp[0]+horizontal_search_space):\n",
    "            right_box = right_image_grayscale[i-window_size//2:i+window_size//2, j-window_size//2:j+window_size//2]\n",
    "            sad = np.abs(left_box - right_box).sum()\n",
    "            if sad < min_sad:\n",
    "                i_match, j_match = i, j\n",
    "                min_sad = sad\n",
    "    \n",
    "    left_keypoints['BODY'] = left_kp\n",
    "    right_keypoints['BODY'] = np.array([j_match, i_match])\n",
    "    return left_keypoints, right_keypoints\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lateral_keypoint(gtsf_fish_identifier):\n",
    "    sfp = session.query(StereoFramePair).filter(StereoFramePair.gtsf_fish_identifier == gtsf_fish_identifier).all()[0]\n",
    "    \n",
    "    # download left and right images\n",
    "    left_image_s3_key, right_image_s3_key, s3_bucket = sfp.left_image_s3_key, sfp.right_image_s3_key, sfp.image_s3_bucket\n",
    "    \n",
    "    left_image_f = data_access_utils.download_from_s3(s3_bucket, left_image_s3_key)\n",
    "    right_image_f = data_access_utils.download_from_s3(s3_bucket, right_image_s3_key)\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "\n",
    "    left_keypoints = json.loads(sfp.left_image_keypoint_coordinates)\n",
    "    right_keypoints = json.loads(sfp.right_image_keypoint_coordinates)\n",
    "    world_keypoints = json.loads(sfp.world_keypoint_coordinates)\n",
    "    \n",
    "    # convert coordinates from lists to numpy arrays\n",
    "    left_keypoints = {k: np.array(v) for k, v in left_keypoints.items()}\n",
    "    right_keypoints = {k: np.array(v) for k, v in right_keypoints.items()}\n",
    "    world_keypoints = {k: np.array(v) for k, v in world_keypoints.items()}\n",
    "    \n",
    "    left_keypoints, right_keypoints = generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    \n",
    "    \n",
    "    for bp, kp in left_keypoints.items():\n",
    "        axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "#         axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        \n",
    "    for bp, kp in right_keypoints.items():\n",
    "        axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "#         axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_lateral_keypoint('190509010029')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gtsf_fish_id(gtsf_fish_identifier, left_keypoints, right_keypoints):\n",
    "\n",
    "    sfp = session.query(StereoFramePair).filter(StereoFramePair.gtsf_fish_identifier == gtsf_fish_identifier).all()[0]\n",
    "    \n",
    "    # download left and right images\n",
    "    left_image_s3_key, right_image_s3_key, s3_bucket = sfp.left_image_s3_key, sfp.right_image_s3_key, sfp.image_s3_bucket\n",
    "    \n",
    "    left_image_f = data_access_utils.download_from_s3(s3_bucket, left_image_s3_key)\n",
    "    right_image_f = data_access_utils.download_from_s3(s3_bucket, right_image_s3_key)\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "\n",
    "    left_keypoints = json.loads(sfp.left_image_keypoint_coordinates)\n",
    "    right_keypoints = json.loads(sfp.right_image_keypoint_coordinates)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    \n",
    "    \n",
    "    for bp, kp in left_keypoints.items():\n",
    "        print(bp, kp)\n",
    "        axes[0].scatter([kp[0]], [kp[1]], color='red', s=5)\n",
    "        axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        \n",
    "    for bp, kp in right_keypoints.items():\n",
    "        print(bp, kp)\n",
    "        axes[1].scatter([kp[0]], [kp[1]], color='red', s=5)\n",
    "        axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gtsf_fish_id('190509010025')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
