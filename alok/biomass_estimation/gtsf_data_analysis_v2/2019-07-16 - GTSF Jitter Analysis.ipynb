{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase I: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import cholesky\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.data_access_utils import DataAccessUtils\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access_utils = DataAccessUtils('/root/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load GTSF dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/root/data/df_cache.h5')\n",
    "df = df[df.project_name != 'Automated keypoints detection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.project_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define biomass prediction method </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "\n",
    "    interaction_values_quadratic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values_quadratic.append(dist1 * dist2)\n",
    "\n",
    "    interaction_values_cubic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            for k in range(j, len(pairwise_distances)):\n",
    "                dist1 = pairwise_distances[i]\n",
    "                dist2 = pairwise_distances[j]\n",
    "                dist3 = pairwise_distances[k]\n",
    "                interaction_values_cubic.append(dist1 * dist2 * dist3)\n",
    "\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values_quadratic + interaction_values_cubic)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define Stereo Parameters by Project Type </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-axiom parameters\n",
    "\n",
    "parameters_by_project_type = {}\n",
    "gtsf_data_collection = session.query(GtsfDataCollection).first()\n",
    "calibration = session.query(Calibration) \\\n",
    "             .filter(Calibration.enclosure_id == gtsf_data_collection.enclosure_id) \\\n",
    "             .order_by(Calibration.utc_timestamp.desc()) \\\n",
    "             .first()\n",
    "\n",
    "enclosure = session.query(Enclosure).get(calibration.enclosure_id)\n",
    "\n",
    "\n",
    "focal_length = float(calibration.predicted_focal_length_mm) / (1e3)\n",
    "baseline = float(calibration.predicted_baseline_mm) / (1e3)\n",
    "pixel_size_m = float(enclosure.pixel_width_um) / (1e6)\n",
    "focal_length_pixel = focal_length / pixel_size_m\n",
    "image_sensor_width = float(enclosure.sensor_width_mm) / (1e3)\n",
    "image_sensor_height = float(enclosure.sensor_height_mm) / (1e3)\n",
    "pixel_count_width = enclosure.image_num_pixels_width\n",
    "pixel_count_height = enclosure.image_num_pixels_height\n",
    "\n",
    "parameters_pre_axiom = {\n",
    "    'baseline': baseline,\n",
    "    'focalLengthPixel': focal_length_pixel,\n",
    "    'imageSensorWidth': image_sensor_width,\n",
    "    'imageSensorHeight': image_sensor_height,\n",
    "    'pixelCountWidth': pixel_count_width,\n",
    "    'pixelCountHeight': pixel_count_height,\n",
    "    'pixelSize': pixel_size_m\n",
    "}\n",
    "\n",
    "parameters_pre_axiom['focalLength'] = parameters_pre_axiom['focalLengthPixel'] * parameters_pre_axiom['pixelSize']\n",
    "parameters_by_project_type['pre-axiom'] = parameters_pre_axiom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-axiom parameters\n",
    "\n",
    "stereo_params = json.load(open('/root/data/s3/aquabyte-stereo-parameters/L40020185_R40020187/latest_L40020185_R40020187_stereo-parameters.json'))\n",
    "focal_length_pixel = stereo_params['CameraParameters1']['FocalLength'][0]\n",
    "baseline = abs(stereo_params['TranslationOfCamera2'][0] / 1e3) # convert millimeters to meters and use absolute value\n",
    "pixel_size_m = 3.45 * 1e-6\n",
    "focal_length = focal_length_pixel * pixel_size_m\n",
    "image_sensor_width = 0.01412\n",
    "image_sensor_height = 0.01035\n",
    "pixel_count_width = 4096\n",
    "pixel_count_height = 3000\n",
    "\n",
    "parameters_post_axiom = {\n",
    "    'baseline': baseline,\n",
    "    'focalLengthPixel': focal_length_pixel,\n",
    "    'focalLength': focal_length,\n",
    "    'imageSensorWidth': image_sensor_width,\n",
    "    'imageSensorHeight': image_sensor_height,\n",
    "    'pixelCountWidth': pixel_count_width,\n",
    "    'pixelCountHeight': pixel_count_height,\n",
    "    'pixelSize': pixel_size_m\n",
    "}\n",
    "\n",
    "parameters_post_axiom['focalLength'] = parameters_post_axiom['focalLengthPixel'] * parameters_post_axiom['pixelSize']\n",
    "parameters_by_project_type['post-axiom'] = parameters_post_axiom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Construct analysis dataframe </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize analysis df\n",
    "analysis_df = pd.DataFrame()\n",
    "\n",
    "# define baseline biomass model and body_parts\n",
    "model = pickle.load(open('/root/data/alok/biomass_estimation/models/20190716_model_4_eig.pkl', 'rb'))\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "])\n",
    "\n",
    "\n",
    "# define jitter values & number of trials per jitter, and initialize random seed\n",
    "jitter_values_x = [0, 2, 10, 20, 30, 40, 50]\n",
    "np.random.seed(0)\n",
    "trials = 10\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "corr_mat= np.array([[1.0, 0.8],\n",
    "                    [0.8, 1.0]])\n",
    "\n",
    "# Compute the (upper) Cholesky decomposition matrix\n",
    "upper_chol = cholesky(corr_mat)\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    \n",
    "    \n",
    "    left_keypoints_original = row.left_keypoints\n",
    "    right_keypoints_original = row.right_keypoints\n",
    "    project_name = row.project_name\n",
    "    if 'GTSF Phase I Keypoint Annotations' in project_name:\n",
    "        project_type = 'pre-axiom'\n",
    "    elif project_name == 'Underwater Live GTSF - Axiom Calibration Full':\n",
    "        project_type = 'post-axiom'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # introduce small jitter\n",
    "    for jitter_value_x in jitter_values_x:\n",
    "        T = 1 if jitter_value_x == 0 else trials\n",
    "        for t in range(T):\n",
    "            \n",
    "            # Generate 3 series of normally distributed (Gaussian) numbers\n",
    "            rnd = np.random.normal(0.0, jitter_value_x, size=2)\n",
    "            jitters = rnd @ upper_chol\n",
    "            \n",
    "            left_keypoints = {bp: [left_keypoints_original[bp][0] + jitters[0],\n",
    "                                   left_keypoints_original[bp][1]] for bp in body_parts}\n",
    "            right_keypoints = {bp: [right_keypoints_original[bp][0] + jitters[1],\n",
    "                                    right_keypoints_original[bp][1]] for bp in body_parts}\n",
    "\n",
    "            world_keypoints = {}\n",
    "            \n",
    "            for bp in body_parts:\n",
    "                lkp, rkp = left_keypoints[bp], right_keypoints[bp]\n",
    "                d = abs(lkp[0] - rkp[0])\n",
    "\n",
    "                # compute world key point\n",
    "                depth = depth_from_disp(d, parameters_by_project_type[project_type])\n",
    "                wkp = convert_to_world_point(lkp[0], lkp[1], depth, parameters_by_project_type[project_type])\n",
    "                \n",
    "                world_keypoints[bp] = wkp\n",
    "\n",
    "            predicted_weight_linear = coord2biomass_linear(world_keypoints, model)\n",
    "\n",
    "            df_row = {\n",
    "                'gtsf_fish_identifier': row.gtsf_fish_identifier,\n",
    "                'predicted_weight_linear': predicted_weight_linear,\n",
    "                'weight': row.weight,\n",
    "                'trial': t,\n",
    "                'jitter_value_x': jitter_value_x,\n",
    "                'stereo_frame_pair_id': row.stereo_frame_pair_id\n",
    "            }\n",
    "            \n",
    "            analysis_df = analysis_df.append(df_row, ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot precictions vs. ground truth for different jitter values </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = (analysis_df.stereo_frame_pair_id != 2674) & (analysis_df.stereo_frame_pair_id != 1972)\n",
    "for jitter_value_x in jitter_values_x:\n",
    "    mask = (analysis_df.jitter_value_x == jitter_value_x) & m\n",
    "    average_prediction_error = (analysis_df[mask].predicted_weight_linear.mean() - analysis_df[mask].weight.mean())/analysis_df[mask].weight.mean()\n",
    "    print('Average prediction error: {}'.format(average_prediction_error))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(analysis_df[mask].weight, analysis_df[mask].predicted_weight_linear)\n",
    "    plt.title('Mean Jitter along x-axis = {} pixels'.format(jitter_value_x))\n",
    "    plt.xlabel('Predicted weight (grams)')\n",
    "    plt.ylabel('Ground truth weight (grams)')\n",
    "    plt.plot([0, 10000], [0, 10000], color='red')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Per-fish accuracy & precision metrics </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "for jitter_value_x in jitter_values_x:\n",
    "    for stereo_frame_pair_id in sorted(analysis_df.stereo_frame_pair_id.unique()):\n",
    "        if (stereo_frame_pair_id == 2674) or (stereo_frame_pair_id == 1972):\n",
    "            continue\n",
    "        mask = (analysis_df.jitter_value_x == jitter_value_x) & (analysis_df.stereo_frame_pair_id == stereo_frame_pair_id)\n",
    "        error_mean = (analysis_df[mask].predicted_weight_linear.mean() - analysis_df[mask].weight.mean())\n",
    "        error_std = analysis_df[mask].predicted_weight_linear.std()\n",
    "        error_mean_pct = error_mean / analysis_df[mask].weight.mean()\n",
    "        error_std_pct = error_std / analysis_df[mask].weight.mean()\n",
    "        row = {}\n",
    "        row['jitter_value_x'] = jitter_value_x\n",
    "        row['stereo_frame_pair_id'] = stereo_frame_pair_id\n",
    "        row['error_mean'] = error_mean\n",
    "        row['error_std'] = error_std\n",
    "        row['error_mean_pct'] = error_mean_pct\n",
    "        row['error_std_pct'] = error_std_pct\n",
    "        results_df = results_df.append(row, ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jitter_value_x in jitter_values_x:\n",
    "    jitter_mask = results_df.jitter_value_x == jitter_value_x\n",
    "    avg_weight = analysis_df.weight.mean()\n",
    "    avg_error_mean = results_df[jitter_mask].error_mean.mean()\n",
    "    avg_error_std = results_df[jitter_mask].error_std.mean()\n",
    "    avg_error_mean_pct = avg_error_mean / avg_weight\n",
    "    avg_error_std_pct = avg_error_std / avg_weight\n",
    "    print('Jitter value: {}'.format(jitter_value_x))\n",
    "    print('=' * 50)\n",
    "    print('Average prediction error (grams): {}'.format(avg_error_mean))\n",
    "    print('Average prediction spread (grams): {}'.format(avg_error_std))\n",
    "    print('Average prediction error (percentage): {}'.format(avg_error_mean_pct))\n",
    "    print('Average prediction spread (percentage): {}'.format(avg_error_std_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
