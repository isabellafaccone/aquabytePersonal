{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "from aquabyte.data_access_utils import DataAccessUtils\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "# s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "#                          aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "#                          region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n",
    "\n",
    "# data_access_utils = DataAccessUtils()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n",
    "def coord2biomass_blender(world_keypoints, blender):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    # mapping helps for consistency with the kp order\n",
    "    reverse_mapping = blender[\"reverse_mapping\"]\n",
    "    distances = np.array(blender[\"distances\"])\n",
    "    volumes = blender[\"volume\"]\n",
    "    regression_coeff = blender[\"coeff\"]\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # the reverse mapping insure that we listing the kp\n",
    "    # in the same order\n",
    "    measurements = []\n",
    "    number_of_parts = len(world_keypoints)\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    measurements = np.array(measurements)\n",
    "\n",
    "    # absolute diff\n",
    "    diff = np.nanmean(np.abs(distances - measurements), axis=1)\n",
    "    closest = np.argmin(diff)\n",
    "    prediction = volumes[closest]\n",
    "\n",
    "    # here is some machine learning\n",
    "    prediction = prediction*regression_coeff[0] + regression_coeff[1]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('./model.pkl', 'rb'))\n",
    "blender = json.load(open('./volumes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "sfps_all = session.query(StereoFramePair).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for idx, row in enumerate(sfps_all):\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    \n",
    "    # skip bad cases\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        print('Skipping fish ID {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        print('Skipping non-samlon fish: {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # get pairwise distances and biomass predictions\n",
    "    world_keypoint_coordinates = json.loads(row.world_keypoint_coordinates)\n",
    "    predicted_weight_linear = coord2biomass_linear(world_keypoint_coordinates, model)\n",
    "    predicted_weight_blender = coord2biomass_blender(world_keypoint_coordinates, blender)\n",
    "    predicted_length = euclidean_distance(world_keypoint_coordinates['UPPER_LIP'], world_keypoint_coordinates['TAIL_NOTCH'])\n",
    "    \n",
    "    df_row = {}\n",
    "    body_parts = sorted(list(world_keypoint_coordinates.keys()))\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            bp1, bp2 = body_parts[i], body_parts[j]\n",
    "            df_row['{}<->{}'.format(body_parts[i], body_parts[j])] = \\\n",
    "                euclidean_distance(world_keypoint_coordinates[bp1], world_keypoint_coordinates[bp2])\n",
    "            \n",
    "    left_image_keypoint_coordinates = json.loads(row.left_image_keypoint_coordinates)\n",
    "    right_image_keypoint_coordinates = json.loads(row.right_image_keypoint_coordinates)\n",
    "    if row.annotations_project_name == 'Automated keypoints detection':\n",
    "        left_image_keypoint_coordinates = {bp: (x[1], x[0]) for bp, x in left_image_keypoint_coordinates.items()}\n",
    "        right_image_keypoint_coordinates = {bp: (x[1], x[0]) for bp, x in right_image_keypoint_coordinates.items()}\n",
    "    \n",
    "    keypoints_valid = True\n",
    "    threshold = 10\n",
    "    for bp in body_parts:\n",
    "        if abs(left_image_keypoint_coordinates[bp][1] - right_image_keypoint_coordinates[bp][1]) > threshold:\n",
    "            keypoints_valid = False\n",
    "        \n",
    "    if not keypoints_valid:\n",
    "        continue\n",
    "            \n",
    "    # append to dataset\n",
    "    df_row.update({\n",
    "        'weight': ground_truth_metadata['data']['weight'],\n",
    "        'length': ground_truth_metadata['data']['length'],\n",
    "        'width': ground_truth_metadata['data']['width'],\n",
    "        'breadth': ground_truth_metadata['data']['breath'],\n",
    "        'world_keypoint_coordinates': world_keypoint_coordinates,\n",
    "        'left_image_keypoint_coordinates': json.loads(row.left_image_keypoint_coordinates),\n",
    "        'right_image_keypoint_coordinates': json.loads(row.right_image_keypoint_coordinates),\n",
    "        'kfactor': 1e5 * ground_truth_metadata['data']['weight'] / ground_truth_metadata['data']['length']**3,\n",
    "        'date': row.date,\n",
    "        'left_image_s3_key': row.left_image_s3_key,\n",
    "        'right_image_s3_key': row.right_image_s3_key,\n",
    "        'image_s3_bucket': row.image_s3_bucket,\n",
    "        'predicted_weight_linear': predicted_weight_linear,\n",
    "        'predicted_weight_blender': predicted_weight_blender,\n",
    "        'predicted_length': predicted_length,\n",
    "        'error_pct_linear': (predicted_weight_linear - ground_truth_metadata['data']['weight']) / ground_truth_metadata['data']['weight'],\n",
    "        'error_pct_blender': (predicted_weight_blender - ground_truth_metadata['data']['weight']) / ground_truth_metadata['data']['weight'],\n",
    "        'project_name': row.annotations_project_name,\n",
    "        'gtsf_fish_identifier': row.gtsf_fish_identifier\n",
    "    })\n",
    "    \n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "df_cache = df.copy()\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute bioamss estimates for all GTSF data with production model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "features = []\n",
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        feature = '{}<->{}'.format(body_parts[i], body_parts[j])\n",
    "        features.append(feature)\n",
    "\n",
    "mask = (df[features] > 0.95).any(axis=1)\n",
    "\n",
    "plt.scatter(df.loc[mask, 'weight'], df.loc[mask, 'predicted_weight_linear'], color='r')\n",
    "plt.scatter(df.loc[~mask, 'weight'], df.loc[~mask, 'predicted_weight_linear'], color='b')\n",
    "# plt.scatter(df.loc[mask2, 'weight'], df.loc[mask2, 'predicted_weight_linear'], color='r')\n",
    "\n",
    "# plt.scatter(df.loc[~date_mask, 'weight'], df.loc[~date_mask, 'predicted_weight_linear'], color='r')\n",
    "plt.xlabel('Ground Truth Weight')\n",
    "plt.ylabel('Prediction')\n",
    "plt.xlim([0, 8000])\n",
    "plt.ylim([0, 8000])\n",
    "# plt.axis('scaled')\n",
    "plt.plot(range(5000), range(5000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "for path in np.random.choice(glob.glob('/root/data/gtsf_phase_I/2019-05-10/*/rectified/*'), 20):\n",
    "    plt.imshow(cv2.imread(path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Error Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_key, bucket):\n",
    "    truncated_key = image_key.replace('phase_I/small-pen-test-site/1/', '')\n",
    "    f = '/root/data/gtsf_phase_I/{}'.format(truncated_key)\n",
    "    if not os.path.exists(f):\n",
    "        print('here')\n",
    "        s3_client.download_file(bucket, image_key, f)\n",
    "    \n",
    "    \n",
    "    return plt.imread(f)\n",
    "\n",
    "def plot_coordinates(image_key, bucket, side, keypoints, ax):\n",
    "    im = plt.imread(data_access_utils.download_from_s3(bucket, image_key))\n",
    "#     im = load_image(image_key, bucket)\n",
    "    \n",
    "#     plt.figure(figsize=(25, 10))\n",
    "#     im = plt.imread(image_f)\n",
    "    \n",
    "\n",
    "    padding=100\n",
    "    x_values = [coord[1] for body_part, coord in keypoints.items()]\n",
    "    y_values = [coord[0] for body_part, coord in keypoints.items()]\n",
    "    x_min, x_max, y_min, y_max = min(x_values)-padding, max(x_values)+padding, min(y_values)-padding, max(y_values)+padding\n",
    "    \n",
    "    for body_part, coordinates in keypoints.items():\n",
    "        x, y = coordinates[1], coordinates[0]\n",
    "#         x, y = coordinates[0], coordinates[1]\n",
    "        ax.scatter([x-x_min], [y-y_min], c='red')\n",
    "        ax.annotate(body_part, (x-x_min, y-y_min), color='red', )\n",
    "        \n",
    "    \n",
    "    ax.imshow(im[y_min:y_max, x_min:x_max])    \n",
    "    \n",
    "def plot_gtsf(analysis_df, idx):\n",
    "    \n",
    "    data_point = analysis_df[analysis_df.index == idx].iloc[0]\n",
    "    image_s3_bucket = data_point['image_s3_bucket']\n",
    "    left_image_s3_key = data_point['left_image_s3_key']\n",
    "    left_keypoints = data_point['left_image_keypoint_coordinates']\n",
    "    \n",
    "    right_image_s3_key = data_point['right_image_s3_key']\n",
    "    right_keypoints = data_point['right_image_keypoint_coordinates']\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2, figsize=(20, 10))\n",
    "    plot_coordinates(left_image_s3_key, image_s3_bucket, 'left', left_keypoints, axarr[0])\n",
    "    plot_coordinates(right_image_s3_key, image_s3_bucket, 'right', right_keypoints, axarr[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abs_error_pct_linear'] = df.error_pct_linear.abs()\n",
    "# df.sort_values('abs_error_pct_linear', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.project_name == 'Automated keypoints detection'\n",
    "for idx, row in df[mask].sort_values('abs_error_pct_linear', ascending=False).iterrows():\n",
    "    print(row.gtsf_fish_identifier)\n",
    "    plot_gtsf(df, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.sort_values('error_pct', ascending=False).iterrows():\n",
    "    plot_gtsf(analysis_df, idx)\n",
    "    print('Error percentage: {}'.format(row.error_pct))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
