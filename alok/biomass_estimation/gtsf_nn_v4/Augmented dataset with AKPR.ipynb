{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score, generate_akpd_scores\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    print('hello')\n",
    "    \n",
    "    counts, edges = np.histogram(df.weight, bins=np.arange(0, 10000, 1000))\n",
    "    trial_values = (5.0 / (counts / np.max(counts))).astype(int)\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_depth = augmentation_config['min_depth']\n",
    "    max_depth = augmentation_config['max_depth']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        camera_metadata = row.camera_metadata\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        \n",
    "        weight = row.weight\n",
    "        trials = trial_values[min(int(weight / 1000), len(trial_values) - 1)]\n",
    "        for _ in range(trials):\n",
    "            \n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "            wkps = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "            original_depth = np.median(wkps[:, 1])\n",
    "            \n",
    "            depth = np.random.uniform(min_depth, max_depth)\n",
    "            scaling_factor = float(original_depth) / depth\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            \n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)\n",
    "\n",
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation_config = dict(\n",
    "#     trials=10,\n",
    "#     max_jitter_std=10,\n",
    "#     min_depth=0.5,\n",
    "#     max_depth=2.5\n",
    "# )\n",
    "\n",
    "# augmented_df = augment(df, augmentation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.weight_estimation.keypoint_utils.body_parts import core_body_parts\n",
    "from research.utils.image_utils import Picture\n",
    "from scipy.spatial import Delaunay\n",
    "from itertools import compress\n",
    "\n",
    "def in_hull(p, hull):\n",
    "    hull = Delaunay(hull)\n",
    "    return hull.find_simplex(p) >= 0\n",
    "\n",
    "\n",
    "def apply_convex_hull_filter(kp, des, canonical_kps, bbox):\n",
    "    X_canon_kps = np.array(list(canonical_kps.values()))\n",
    "    X_kp = np.array([x.pt for x in kp]).reshape(-1, 2) + np.array([bbox['x_min'], bbox['y_min']])\n",
    "    is_valid = in_hull(X_kp, X_canon_kps)\n",
    "    kp = list(compress(kp, is_valid))\n",
    "    des = des[is_valid]\n",
    "    return kp, des\n",
    "\n",
    "\n",
    "def get_homography_and_matches(sift, left_patch, right_patch,\n",
    "                               left_kps, right_kps,\n",
    "                               left_bbox, right_bbox,\n",
    "                               good_perc=0.7, min_match_count=3):\n",
    "\n",
    "    kp1, des1 = sift.detectAndCompute(left_patch, None)\n",
    "    kp2, des2 = sift.detectAndCompute(right_patch, None)\n",
    "    try:\n",
    "        if not (des1.any() and des2.any()):\n",
    "            return None, kp1, kp2, None, [0]\n",
    "    except AttributeError:\n",
    "        print(\"None type for detectAndComputer descriptor\")\n",
    "        return None, kp1, kp2, None, [0]\n",
    "    # apply convex hull filter\n",
    "    kp1, des1 = apply_convex_hull_filter(kp1, des1, left_kps, left_bbox)\n",
    "    kp2, des2 = apply_convex_hull_filter(kp2, des2, right_kps, right_bbox)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    H, matches_mask = np.eye(3), []\n",
    "    good = []\n",
    "\n",
    "    # check that matches list contains actual pairs\n",
    "    if len(matches) > 0:\n",
    "        if len(matches[0]) != 2:\n",
    "            print('Aborting: matches list does not contain pairs')\n",
    "            return H, kp1, kp2, good, matches_mask\n",
    "\n",
    "    for m, n in matches:\n",
    "        if m.distance < good_perc * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) >= min_match_count:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "    return H, kp1, kp2, good, matches_mask\n",
    "\n",
    "\n",
    "def generate_sift_adjustment(bp, left_crop_metadata, left_fish_picture, left_kps, right_crop_metadata,\n",
    "                             right_fish_picture, right_kps, sift):\n",
    "    left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "    left_crop, left_bbox = left_fish_picture.generate_crop_given_center(left_kp[0], left_kp[1], 600, 200)\n",
    "    right_crop, right_bbox = right_fish_picture.generate_crop_given_center(right_kp[0], right_kp[1], 600, 200)\n",
    "\n",
    "    H, _, _, _, matches_mask = get_homography_and_matches(sift, left_crop, right_crop,\n",
    "                                                          left_kps, right_kps,\n",
    "                                                          left_bbox, right_bbox)\n",
    "    num_matches = sum(matches_mask)\n",
    "    if H is not None:\n",
    "        local_left_kp = [left_kp[0] - left_bbox['x_min'], left_kp[1] - left_bbox['y_min']]\n",
    "        local_right_kp = cv2.perspectiveTransform(\n",
    "            np.array([local_left_kp[0], local_left_kp[1]]).reshape(-1, 1, 2).astype(float), H).squeeze()\n",
    "        right_kp = [local_right_kp[0] + right_bbox['x_min'], local_right_kp[1] + right_bbox['y_min']]\n",
    "    left_item = {\n",
    "        'keypointType': bp,\n",
    "        'xCrop': left_kp[0],\n",
    "        'yCrop': left_kp[1],\n",
    "        'xFrame': left_crop_metadata['x_coord'] + left_kp[0],\n",
    "        'yFrame': left_crop_metadata['y_coord'] + left_kp[1]\n",
    "    }\n",
    "    right_item = {\n",
    "        'keypointType': bp,\n",
    "        'xCrop': right_kp[0],\n",
    "        'yCrop': right_kp[1],\n",
    "        'xFrame': right_crop_metadata['x_coord'] + right_kp[0],\n",
    "        'yFrame': right_crop_metadata['y_coord'] + right_kp[1]\n",
    "    }\n",
    "    return left_item, right_item, num_matches\n",
    "\n",
    "\n",
    "def generate_refined_keypoints(ann, left_crop_url, right_crop_url):\n",
    "\n",
    "    left_kps = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_kps = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "\n",
    "    left_crop_metadata = {\n",
    "        'x_coord': ann['leftCrop'][0]['xFrame'] - ann['leftCrop'][0]['xCrop'],\n",
    "        'y_coord': ann['leftCrop'][0]['yFrame'] - ann['leftCrop'][0]['yCrop']\n",
    "    }\n",
    "    right_crop_metadata = {\n",
    "        'x_coord': ann['rightCrop'][0]['xFrame'] - ann['rightCrop'][0]['xCrop'],\n",
    "        'y_coord': ann['rightCrop'][0]['yFrame'] - ann['rightCrop'][0]['yCrop']\n",
    "    }\n",
    "\n",
    "    left_fish_picture = Picture(image_url=left_crop_url)\n",
    "    right_fish_picture = Picture(image_url=right_crop_url)\n",
    "    left_fish_picture.enhance(in_place=True)\n",
    "    right_fish_picture.enhance(in_place=True)\n",
    "    sift = cv2.KAZE_create()\n",
    "    left_items, right_items = [], []\n",
    "    for bp in core_body_parts:\n",
    "        left_item, right_item, num_matches = generate_sift_adjustment(bp, left_crop_metadata, left_fish_picture,\n",
    "                                                                      left_kps, right_crop_metadata,\n",
    "                                                                      right_fish_picture, right_kps, sift)\n",
    "        left_items.append(left_item)\n",
    "        right_items.append(right_item)\n",
    "    modified_ann = {\n",
    "        'leftCrop': left_items,\n",
    "        'rightCrop': right_items\n",
    "    }\n",
    "    return modified_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_keypoints = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    count = count + 1\n",
    "    \n",
    "    if count > 10:\n",
    "        modified_keypoints.append(None)\n",
    "    if count % 1000 == 0:\n",
    "        print(count, len(df))\n",
    "\n",
    "    ann, cm = (row.keypoints), (row.camera_metadata)\n",
    "    left_crop_url, right_crop_url = row.left_image_url, row.right_image_url\n",
    "    \n",
    "    modified_ann = generate_refined_keypoints(ann, left_crop_url, right_crop_url)\n",
    "    modified_keypoints.append(modified_ann)\n",
    "    \n",
    "df['modified_keypoints'] = modified_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_keypoints[30722]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified = []\n",
    "\n",
    "for i in range(len(modified_keypoints)):\n",
    "    if i >= 10 and i % 2 == 0:\n",
    "        pass\n",
    "    else:\n",
    "        new_modified.append(modified_keypoints[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['modified_keypoints'] = new_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'/root/data/alok/biomass_estimation/playground/gtsf_akpr.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/root/data/alok/biomass_estimation/playground/gtsf_akpr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "s3 = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_keypoints = []\n",
    "camera_metadatas = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    modified_keypoints.append(eval(row['modified_keypoints']))\n",
    "    camera_metadatas.append(eval(row['camera_metadata']))\n",
    "    \n",
    "df['keypoints'] = modified_keypoints\n",
    "df['camera_metadata'] = camera_metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scores = generate_akpd_scores(df, akpd_scorer_f)\n",
    "\n",
    "df['modified_akpd_scores'] = akpd_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['modified_akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df['akpd_score'], df['modified_akpd_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'/root/data/alok/biomass_estimation/playground/gtsf_akpr2.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
