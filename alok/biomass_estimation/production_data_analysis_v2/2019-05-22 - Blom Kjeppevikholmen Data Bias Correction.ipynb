{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker, relationship, join\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import Table, Column, Integer, ForeignKey\n",
    "from sqlalchemy.orm import relationship\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "from aquabyte.data_access_utils import DataAccessUtils\n",
    "\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Establish connection to database and perform query for base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "data_access_utils = DataAccessUtils()\n",
    "\n",
    "# prod SQL credentaials\n",
    "sql_credentials = json.load(open(os.environ[\"PROD_SQL_CREDENTIALS\"]))\n",
    "\n",
    "sql_query = '''\n",
    "select * from keypoint_annotations\n",
    "where captured_at >= '2019-05-15'\n",
    "and site_id = 23\n",
    "and pen_id = 4;\n",
    "'''\n",
    "\n",
    "original_df = data_access_utils.extract_from_database(sql_query)\n",
    "# original_df = original_df.loc[:, ~original_df.columns.duplicated()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iterate over query results and generate 3D coordinates + biomass estimates for each stereo fish detection </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def coord2biomass_blender(world_keypoints, blender):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    reverse_mapping = blender[\"reverse_mapping\"]\n",
    "    distances = np.array(blender[\"distances\"])\n",
    "    volumes = blender[\"volume\"]\n",
    "    regression_coeff = blender[\"coeff\"]\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # the reverse mapping insure that we listing the kp\n",
    "    # in the same order\n",
    "    measurements = []\n",
    "    number_of_parts = len(world_keypoints)\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    measurements = np.array(measurements)\n",
    "\n",
    "    # absolute diff\n",
    "    diff = np.nanmean(np.abs(distances - measurements), axis=1)\n",
    "    closest = np.argmin(diff)\n",
    "    prediction = volumes[closest]\n",
    "\n",
    "    # here is some machine learning\n",
    "    prediction = prediction*regression_coeff[0] + regression_coeff[1]\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def normalize_world_keypoints(world_keypoint_coordinates):\n",
    "    body_parts = sorted(world_keypoint_coordinates.keys())\n",
    "    wkps = {bp: np.array(world_keypoint_coordinates[bp]) for bp in body_parts}\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['DORSAL_FIN'] - np.array([norm_wkps_intermediate['DORSAL_FIN'][0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model parameters for Blender and linear models\n",
    "model = pickle.load(open('/root/data/alok/biomass_estimation/models/model.pkl', 'rb'))\n",
    "blender = json.load(open('/root/data/alok/biomass_estimation/models/volumes.json'))\n",
    "\n",
    "\n",
    "# establish new columns\n",
    "mask = (original_df.is_skipped == False) & (~original_df.keypoints.isnull())\n",
    "for col in ['left_keypoints', 'right_keypoints', 'world_keypoint_coordinates']:\n",
    "    original_df[col] = np.nan\n",
    "    original_df[col] = original_df[col].astype(object)\n",
    "for col in ['predicted_biomass_linear', 'predicted_biomass_blender', \n",
    "            'max_y_coordinate_deviation', 'max_y_world_coordinate_deviation']:\n",
    "    original_df[col] = np.nan\n",
    "\n",
    "\n",
    "# modify the dataframe row-by-row\n",
    "for idx, row in original_df[mask].iterrows():\n",
    "    keypoints = row.keypoints\n",
    "    left_keypoints = keypoints['leftCrop']\n",
    "    right_keypoints = keypoints['rightCrop']\n",
    "            \n",
    "    # compute world coordinates\n",
    "    camera_metadata = row.camera_metadata\n",
    "    camera_metadata['pixelCountHeight'] = 3000\n",
    "    camera_metadata['pixelCountWidth'] = 4096\n",
    "    world_keypoint_coordinates = pixel2world(left_keypoints, right_keypoints, camera_metadata)\n",
    "    \n",
    "    # update dataframe with world keypoint coordinates\n",
    "    original_df.at[idx, 'left_keypoints'] = left_keypoints\n",
    "    original_df.at[idx, 'right_keypoints'] = right_keypoints\n",
    "    original_df.at[idx, 'world_keypoint_coordinates'] = world_keypoint_coordinates\n",
    "    \n",
    "    body_parts = sorted(list(world_keypoint_coordinates.keys()))\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            bp1, bp2 = body_parts[i], body_parts[j]\n",
    "            col = '{}<->{}'.format(body_parts[i], body_parts[j])\n",
    "            if not col in original_df.columns:\n",
    "                original_df[col] = np.nan\n",
    "            original_df.at[idx, col] = \\\n",
    "                euclidean_distance(world_keypoint_coordinates[bp1], world_keypoint_coordinates[bp2])\n",
    "            \n",
    "    for i in range(len(body_parts)):\n",
    "        bp = body_parts[i]\n",
    "        col = '{}_Y'.format(bp)\n",
    "        if not col in original_df.columns:\n",
    "            original_df[col] = np.nan\n",
    "        original_df.at[idx, col] = world_keypoint_coordinates[bp][1]\n",
    "    \n",
    "    # update dataframe with biomass predictions from both models\n",
    "    predicted_biomass_linear = coord2biomass_linear(world_keypoint_coordinates, model)\n",
    "    predicted_biomass_blender = coord2biomass_blender(world_keypoint_coordinates, blender)\n",
    "    original_df.at[idx, 'predicted_biomass_linear'] = predicted_biomass_linear\n",
    "    original_df.at[idx, 'predicted_biomass_blender'] = predicted_biomass_blender\n",
    "    \n",
    "    # update dataframe with keypoint deviation\n",
    "    threshold = 10\n",
    "    left_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in left_keypoints}\n",
    "    right_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in right_keypoints}\n",
    "    max_y_coordinate_deviation = \\\n",
    "        max([abs(left_keypoint_y_coords[bp] - right_keypoint_y_coords[bp]) for bp in body_parts])\n",
    "    \n",
    "    original_df.at[idx, 'max_y_coordinate_deviation'] = max_y_coordinate_deviation\n",
    "    \n",
    "    # add 3D range for world coordinate y-values\n",
    "    \n",
    "    \n",
    "    norm_wkps = normalize_world_keypoints(world_keypoint_coordinates)\n",
    "    norm_wkp_y_values = [norm_wkps[bp][1] for bp in norm_wkps.keys()]\n",
    "    max_y_world_coordinate_deviation = max(norm_wkp_y_values) - min(norm_wkp_y_values)\n",
    "    original_df.at[idx, 'max_y_world_coordinate_deviation'] = max_y_world_coordinate_deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply filters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids = json.load(open('/root/data/alok/biomass_estimation/invalid_fish_detection_ids.json'))\n",
    "df = original_df.copy(deep=True)\n",
    "\n",
    "# define filters\n",
    "valid_linear_prediction_mask = ~df.predicted_biomass_linear.isnull()\n",
    "rectification_valid_mask = (~df.fish_detection_id.isin(invalid_fish_detection_ids))\n",
    "keypoints_valid_mask = (df.max_y_coordinate_deviation < 15)\n",
    "qa_mask = df.is_qa == True\n",
    "\n",
    "orient_mask_1 = (df['DORSAL_FIN_Y'] - df['TAIL_NOTCH_Y'])  * (df['UPPER_LIP_Y'] - df['TAIL_NOTCH_Y']) > 0 \n",
    "orient_mask_2 = (df['UPPER_LIP_Y'] - df['ADIPOSE_FIN_Y'])  * (df['UPPER_LIP_Y'] - df['TAIL_NOTCH_Y']) > 0\n",
    "inlier_mask = (df.predicted_biomass_linear > np.percentile(original_df.predicted_biomass_linear.dropna(), 1.0)) & \\\n",
    "              (df.predicted_biomass_linear < np.percentile(original_df.predicted_biomass_linear.dropna(), 99.0))\n",
    "\n",
    "mask_valid = valid_linear_prediction_mask & rectification_valid_mask & keypoints_valid_mask & qa_mask\n",
    "\n",
    "mask = mask_valid & orient_mask_1 & orient_mask_2 & inlier_mask\n",
    "\n",
    "df = df[mask].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "\n",
    "print(sum(mask_valid), len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_count(working_distance, fish_size, fps=1.0, orientation_angle=0.0):\n",
    "    orientation_angle_rad = np.pi * orientation_angle / 180.0\n",
    "    working_distance = 1.0\n",
    "    field_size = camera_metadata['imageSensorWidth'] * working_distance / camera_metadata['focalLength']\n",
    "    overlapping_field_size = (field_size - camera_metadata['baseline'])\n",
    "    fish_size = fish_size * np.cos(orientation_angle_rad)\n",
    "    fish_speed = fish_size * np.cos(orientation_angle_rad) # meters per second\n",
    "    dist_between_frames = fish_speed / fps\n",
    "    expected_count = (overlapping_field_size - fish_size) / dist_between_frames\n",
    "    return max(expected_count, 0.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "lengths = []\n",
    "thetas = []\n",
    "curvature_thetas = []\n",
    "inverse_expected_counts = []\n",
    "for wkp in df['world_keypoint_coordinates'].tolist():\n",
    "    y_values = [x[1] for x in wkp.values()]\n",
    "    mean_depth = np.array(y_values).mean()\n",
    "    depths.append(mean_depth)\n",
    "    length = euclidean_distance(wkp['UPPER_LIP'], wkp['TAIL_NOTCH'])\n",
    "    lengths.append(length)\n",
    "    direction_vector = np.array(wkp['UPPER_LIP']) - np.array(wkp['TAIL_NOTCH'])\n",
    "    direction_vector[0] = 0\n",
    "    base_vector = np.array([0, 0, 1]) if wkp['UPPER_LIP'][2] > wkp['TAIL_NOTCH'][2] else np.array([0, 0, -1])\n",
    "    theta = (180 / np.pi) * np.arccos(np.dot(direction_vector, base_vector) / np.linalg.norm(direction_vector))\n",
    "    thetas.append(theta)\n",
    "    \n",
    "    # calculate curvature\n",
    "    wkp = {bp: [wkp[bp][2], wkp[bp][1], wkp[bp][0]] for bp in body_parts}\n",
    "    fv1 = np.array(wkp['UPPER_LIP']) - np.array(wkp['DORSAL_FIN'])\n",
    "    fv2 = np.array(wkp['UPPER_LIP']) - np.array(wkp['PELVIC_FIN'])\n",
    "    n1 = np.cross(fv1, fv2)\n",
    "    \n",
    "    bv1 = np.array(wkp['PELVIC_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    bv2 = np.array(wkp['DORSAL_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    n2 = np.cross(bv1, bv2)\n",
    "    curvature_theta = (180 / np.pi) * np.arccos(np.dot(n1, n2) / (np.linalg.norm(n1) * np.linalg.norm(n2)))\n",
    "    curvature_thetas.append(curvature_theta)\n",
    "    \n",
    "    inverse_expected_count = 1.0 / expected_count(mean_depth, length, orientation_angle=theta)\n",
    "    inverse_expected_counts.append(inverse_expected_count)\n",
    "    \n",
    "df['length'] = lengths\n",
    "df['depth'] = depths\n",
    "df['inverse_expected_counts'] = inverse_expected_counts\n",
    "df['theta'] = thetas\n",
    "df['curvature'] = curvature_thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = np.arange(0.8, 2.0, 0.1)\n",
    "x_ticks = []\n",
    "conditional_lengths = []\n",
    "conditional_weights = []\n",
    "modified_conditional_weights = []\n",
    "for i in range(len(depth_values) - 1):\n",
    "    depth_range_mask = (df.depth > depth_values[i]) & (df.depth < depth_values[i+1])\n",
    "    conditional_length = df[depth_range_mask].length.mean()\n",
    "    conditional_lengths.append(conditional_length)\n",
    "    modified_conditional_weight = (df[depth_range_mask].predicted_biomass_linear * df[depth_range_mask].inverse_expected_counts).sum()/df[depth_range_mask].inverse_expected_counts.sum()\n",
    "    conditional_weight = df[depth_range_mask].predicted_biomass_linear.mean()\n",
    "    modified_conditional_weights.append(modified_conditional_weight)\n",
    "    conditional_weights.append(conditional_weight)\n",
    "    x_tick = (depth_values[i] + depth_values[i+1]) / 2.0\n",
    "    x_ticks.append(x_tick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x_ticks, conditional_lengths, width=0.08)\n",
    "plt.xlabel('Fish distance from camera (m)')\n",
    "plt.ylabel('Average fish length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x_ticks, conditional_weights, width=0.08)\n",
    "plt.xlabel('Fish distance from camera (m)')\n",
    "plt.ylabel('Average fish weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x_ticks, conditional_weights, width=0.08, alpha=0.5)\n",
    "plt.bar(x_ticks, modified_conditional_weights, width=0.08, alpha=0.5)\n",
    "plt.xlabel('Fish distance from camera (m)')\n",
    "plt.ylabel('Average fish weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(df.depth, df.predicted_biomass_linear)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(thetas, bins=100)\n",
    "plt.xlabel('Fish orientation relative to lateral (degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(depths, bins=100)\n",
    "plt.xlabel('Fish depth (meters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Curvature </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(curvature_thetas, bins=100)\n",
    "plt.xlabel('Fish curvature angle (degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.max_y_world_coordinate_deviation < 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine the worst cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.is_skipped == True) & (df.is_partial==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coordinates(image_url, side, keypoints):\n",
    "    image_f = './image.jpg'\n",
    "#     bucket = 'aquabyte-frames-resized-inbound'\n",
    "    bucket = 'aquabyte-crops'\n",
    "    key = image_url[image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(bucket, key, image_f)\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    im = plt.imread(image_f)\n",
    "    \n",
    "#     for keypoint in keypoints:\n",
    "#         keypoint_type = keypoint['keypointType']\n",
    "#         x, y = keypoint['xCrop'], keypoint['yCrop']\n",
    "#         plt.scatter([x], [y])\n",
    "#         plt.annotate(keypoint_type, (x, y), color='red')\n",
    "        \n",
    "    plt.imshow(im)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_annotation_id = 96008\n",
    "keypoint_annotation_mask = (df.id == keypoint_annotation_id)\n",
    "left_image_url = df[keypoint_annotation_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[keypoint_annotation_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[keypoint_annotation_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[keypoint_annotation_mask].right_keypoints.iloc[0]\n",
    "\n",
    "world_keypoint_coordinates = df[keypoint_annotation_mask].world_keypoint_coordinates.iloc[0]\n",
    "im_left = plot_coordinates(left_image_url, 'left', left_keypoints)\n",
    "im_right = plot_coordinates(right_image_url, 'right', right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_annotation_id = 128200\n",
    "keypoint_annotation_mask = (df.id == keypoint_annotation_id)\n",
    "left_image_url = df[keypoint_annotation_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[keypoint_annotation_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[keypoint_annotation_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[keypoint_annotation_mask].right_keypoints.iloc[0]\n",
    "\n",
    "world_keypoint_coordinates = df[keypoint_annotation_mask].world_keypoint_coordinates.iloc[0]\n",
    "im_left = plot_coordinates(left_image_url, 'left', left_keypoints)\n",
    "im_right = plot_coordinates(right_image_url, 'right', right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_wkps = normalize_world_keypoints(world_keypoint_coordinates)\n",
    "[(bp, norm_wkps[bp][1]) for bp in norm_wkps.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = 'TAIL_NOTCH'\n",
    "disp = [i['xFrame'] for i in left_keypoints if i['keypointType'] == bp][0] - [i['xFrame'] for i in right_keypoints if i['keypointType'] == bp][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(camera_metadata['focalLengthPixel'] * camera_metadata['baseline'])/disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imleft_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = cv2.imread('./image.jpg')\n",
    "lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "lab_planes = cv2.split(lab)\n",
    "gridsize = 16\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
    "lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "lab = cv2.merge(lab_planes)\n",
    "bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params_file):\n",
    "    params = json.load(open(params_file))\n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    imageSize = (4096, 3000)\n",
    "    \n",
    "    # perform rectification\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T, None, None, None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "    \n",
    "    return left_maps, right_maps\n",
    "\n",
    "def get_remap(crop, side, crop_metadata, stereo_params_f):\n",
    "    left_maps, right_maps = load_params(stereo_params_f)\n",
    "    maps = left_maps if side == 'left' else right_maps\n",
    "    new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "    lower_left = (crop_metadata['y_coord'] + crop_metadata['height'], crop_metadata['x_coord'])\n",
    "    upper_right = (crop_metadata['y_coord'], crop_metadata['x_coord'] + crop_metadata['width'])\n",
    "    new_image[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :] = np.array(crop)\n",
    "    remap = cv2.remap(new_image, maps[0], maps[1], cv2.INTER_LANCZOS4)\n",
    "    nonzero_indices = np.where(remap > 0)\n",
    "    y_min, y_max = nonzero_indices[0].min(), nonzero_indices[0].max() \n",
    "    x_min, x_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    lower_left = (y_max, x_min)\n",
    "    upper_right = (y_min, x_max)\n",
    "    rectified_crop = remap[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :].copy()\n",
    "    print(crop_metadata)\n",
    "    rectified_crop_metadata = crop_metadata.copy()\n",
    "    rectified_crop_metadata['x_coord'] = x_min\n",
    "    rectified_crop_metadata['y_coord'] = y_min\n",
    "    rectified_crop_metadata['width'] = x_max - x_min\n",
    "    rectified_crop_metadata['height'] = y_max - y_min\n",
    "    \n",
    "    return remap, rectified_crop_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_params_f = './2019-04-26_blom_kjeppevikholmen_pen_1.json'\n",
    "left_crop_metadata = df[fish_id_mask].left_crop_metadata.iloc[0]\n",
    "left_new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "left_remap, rectified_left_crop_metadata = get_remap(im_left, 'left', left_crop_metadata, stereo_params_f)\n",
    "\n",
    "right_crop_metadata = df[fish_id_mask].right_crop_metadata.iloc[0]\n",
    "right_new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "right_remap, rectified_right_crop_metadata = get_remap(im_right, 'right', right_crop_metadata, stereo_params_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(kp['keypointType'], kp['yCrop'] + rectified_left_crop_metadata['y_coord']) for kp in left_keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(kp['keypointType'], kp['yCrop'] + rectified_right_crop_metadata['y_coord']) for kp in right_keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(right_remap).save('./right_remap.jpg')\n",
    "Image.fromarray(left_remap).save('./left_remap.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2019-05-02']['predicted_biomass_blender'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.site_id == 23) & (df.pen_id == 4) & (df.index >= '2019-04-27')\n",
    "df[mask].predicted_biomass_blender.resample('D', how=lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[mask].predicted_biomass_blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.predicted_biomass_linear > 500) & (df.predicted_biomass_linear < 6000)\n",
    "plt.scatter(df.ix[mask, 'predicted_biomass_blender'], df.ix[mask, 'predicted_biomass_linear'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine rectification issue </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_bucket = 'aquabyte-crops'\n",
    "left_image_rectified_f = './left_image_rectified.jpg'\n",
    "right_image_rectified_f = './right_image_rectified.jpg'\n",
    "\n",
    "invalid_fish_detection_ids, invalid_urls = [], []\n",
    "i = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    if i < 36132:\n",
    "        continue\n",
    "    left_image_url = row.left_image_url\n",
    "    right_image_url = row.right_image_url\n",
    "    left_rectified_key = left_image_url[left_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, left_rectified_key, left_image_rectified_f)\n",
    "    right_rectified_key = right_image_url[right_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, right_rectified_key, right_image_rectified_f)\n",
    "    \n",
    "    # this is dumb, can probably do this in memory\n",
    "    left_rectified_image = cv2.imread(left_image_rectified_f)\n",
    "    right_rectified_image = cv2.imread(right_image_rectified_f)\n",
    "    \n",
    "    left_crop_metadata = json.loads(row.left_crop_metadata)\n",
    "    right_crop_metadata = json.loads(row.right_crop_metadata)\n",
    "    left_crop_width = left_crop_metadata['width']\n",
    "    left_crop_height = left_crop_metadata['height']\n",
    "    right_crop_width = right_crop_metadata['width']\n",
    "    right_crop_height = right_crop_metadata['height']\n",
    "    \n",
    "    invalid = False\n",
    "    if left_rectified_image.shape[0] == left_crop_height and left_rectified_image.shape[1] == left_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(left_image_url)\n",
    "        print('left image not rectified for id {}!'.format(row.id))\n",
    "    if right_rectified_image.shape[0] == right_crop_height and right_rectified_image.shape[1] == right_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(right_image_url)\n",
    "        print('right image not rectified for id {}!'.format(row.id))\n",
    "    \n",
    "    if invalid:\n",
    "        invalid_fish_detection_ids.append(int(row.id))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(invalid_ids, open('./invalid_ids', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_urls + invalid_urls_old, open('./invalid_urls.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_urls_old = json.load(open('./invalid_urls.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids_old = json.load(open('./invalid_fish_detection_ids.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_fish_detection_ids + invalid_fish_detection_ids_old, open('./invalid_fish_detection_ids.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prod data backfill </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "select \n",
    "\n",
    "k.id, k.fish_detection_id, k.annotated_by_email, k.is_qa, \n",
    "k.is_skipped, k.is_blurry, k.is_dark, k.is_occluded,\n",
    "k.is_bad_orientation, k.is_partial, k.direction, k.keypoints, \n",
    "k.work_duration_left_ms, k.work_duration_right_ms, f.created_at, \n",
    "f.updated_at, f.captured_at, f.site_id, f.pen_id, f.left_image_url, f.right_image_url, \n",
    "f.left_crop_metadata, f.right_crop_metadata, f.camera_metadata\n",
    "\n",
    "from keypoint_annotations k\n",
    "left join fish_detections f\n",
    "on k.fish_detection_id = f.id\n",
    "'''\n",
    "\n",
    "df = extract_from_database(sql_query, sql_credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row.keypoints:\n",
    "        keypoints = row.keypoints\n",
    "        left_crop_metadata = row.left_crop_metadata\n",
    "        right_crop_metadata = row.right_crop_metadata\n",
    "        \n",
    "        for \n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
