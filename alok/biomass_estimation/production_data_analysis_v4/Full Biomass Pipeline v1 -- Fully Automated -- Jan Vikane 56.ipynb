{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AKPD(object):\n",
    "\n",
    "    def __init__(self, aws_credentials):\n",
    "        self.client = boto3.client(\n",
    "            \"sagemaker-runtime\", \n",
    "            region_name=\"eu-west-1\", \n",
    "            aws_access_key_id=aws_credentials['aws_access_key_id'], \n",
    "            aws_secret_access_key=aws_credentials['aws_secret_access_key']\n",
    "        \n",
    "        )\n",
    "\n",
    "    def predict_keypoints(self, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, camera_metadata):\n",
    "        body = [{\n",
    "            'leftCropUrl': left_crop_url,\n",
    "            'rightCropUrl': right_crop_url,\n",
    "            'leftCropMetadata': left_crop_metadata,\n",
    "            'rightCropMetadata': right_crop_metadata,\n",
    "            'cameraMetadata': camera_metadata,\n",
    "            'id': 1\n",
    "        }]\n",
    "\n",
    "        body_str = json.dumps(body).replace(\"'\", '\"')\n",
    "\n",
    "        resp = self.client.invoke_endpoint(EndpointName='auto-keypoints', ContentType='application/json', Body=body_str)\n",
    "        akpd_keypoints_str = resp['Body'].read()\n",
    "        akpd_keypoints = json.loads(akpd_keypoints_str.decode(\"utf-8\"))\n",
    "        return akpd_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM\n",
    "    prod.crop_annotation cas\n",
    "    INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "    WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "    AND cas.annotation is not null\n",
    "    AND cas.pen_id=60\n",
    "    AND cas.group_id='60'\n",
    "    AND cas.captured_at between '2020-01-02' and '2020-01-30';\n",
    "\"\"\"\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "aws_credentials = json.load(open(os.environ['AWS_CREDENTIALS']))\n",
    "akpd = AKPD(aws_credentials)\n",
    "\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "# initialize data transforms so that we can run inference with biomass neural network\n",
    "normalize_centered_2D_transform_biomass = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "\n",
    "# load neural network weights\n",
    "biomass_network = torch.load('/root/data/alok/biomass_estimation/results/neural_network/2019-11-08T00:13:09/nn_epoch_798.pb')\n",
    "akpd_scorer_network = load_model('/root/data/alok/biomass_estimation/playground/akpd_scorer_model_TF.h5') # make this better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Function to generate weight prediction and confidence score </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_score(row_id, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, akpd_keypoints, cm):\n",
    "    \n",
    "    # run AKPD scoring network\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "\n",
    "    # run biomass estimation\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform_biomass.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    akpd_weight_prediction = biomass_network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    \n",
    "    \n",
    "    return akpd_score, akpd_weight_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score_dict = {}\n",
    "\n",
    "args = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "    left_crop_metadata, right_crop_metadata = row.left_crop_metadata, row.right_crop_metadata,\n",
    "    cm = row.camera_metadata\n",
    "    akpd_keypoints = row.annotation\n",
    "    row_id = idx\n",
    "    akpd_score, akpd_weight_prediction = generate_weight_score(row_id, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, akpd_keypoints, cm)\n",
    "    weight_score_dict[row_id] = {\n",
    "        'akpd_score': akpd_score,\n",
    "        'akpd_weight_prediction': akpd_weight_prediction,\n",
    "    }\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['akpd_weight'], df['akpd_score'] = np.nan, np.nan\n",
    "for idx, row in df.iterrows():\n",
    "    if idx in weight_score_dict.keys():\n",
    "        df.at[idx, 'akpd_weight'] = weight_score_dict[idx]['akpd_weight_prediction']\n",
    "        df.at[idx, 'akpd_score'] = weight_score_dict[idx]['akpd_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 9, 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(30, 60))\n",
    "dates = sorted(list(set(df.index.date.astype(str).tolist())))\n",
    "for idx, date in enumerate(dates[:100]):\n",
    "    row, col = idx // cols, idx % cols\n",
    "    axes[row, col].plot(df[date].groupby('hour')['akpd_weight'].agg(lambda x: x.shape[0]), color='blue')\n",
    "    axes[row, col].plot(df[date].groupby('hour')['akpd_weight'].agg(lambda x: x.mean()), color='red')\n",
    "    axes[row, col].set_title(date)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df[df.akpd_score > 0.9].akpd_weight.resample('D').agg(lambda x: x.mean()))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    plt.hist(df[date][df[date].akpd_score > 0.9].akpd_weight)\n",
    "    plt.title(date)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-01-12'\n",
    "hours = list(range(8, 15))\n",
    "for hour in hours:\n",
    "    plt.hist(df[date][(df[date].akpd_score > 0.9) & (df[date].hour == hour)].akpd_weight)\n",
    "    plt.title(df[date][(df[date].akpd_score > 0.9) & (df[date].hour == hour)].akpd_weight.mean())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "for idx, row in df[(df.akpd_score > 0.9)].iterrows():\n",
    "    weight = row.akpd_weight\n",
    "    date = str(idx.date())\n",
    "    print(date)\n",
    "    elements.append({\n",
    "        'date': date,\n",
    "        'estimatedWeightG': weight\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "'''\n",
    "    This function generates biomass estimate for date given historical record of weight estiamtes\n",
    "    Assumptions: \n",
    "        - records_json is JSON blob in string form as specified in instructions\n",
    "        - date is of the format 'YYYY-mm-dd'\n",
    "        - records_json contains up to 30 days of data. The latest date in records_json equals 'date' input\n",
    "'''\n",
    "\n",
    "def generate_biomass_estimate(date, records_json, min_lookback=3, max_lookback=5, min_sample_size=3000, bucket_size=0.1):\n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    # get list of weights by date\n",
    "    weights_by_date = defaultdict(list)\n",
    "    for r in records:       \n",
    "        weights_by_date[r['date']].append(r['estimatedWeightG'])\n",
    "\n",
    "    # calculate daily growth rate (if more than one day is provided)\n",
    "    historical_dates = sorted(list(weights_by_date.keys()))\n",
    "    if len(historical_dates) > 1:\n",
    "        avg_weights = []\n",
    "        for h_date in historical_dates:\n",
    "            avg_weight = np.mean(weights_by_date[h_date])\n",
    "            avg_weights.append(avg_weight)\n",
    "\n",
    "        x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                     dt.datetime.strptime(dates[0], '%Y-%m-%d')).days for date in historical_dates]\n",
    "        X = np.array(x_values).reshape(-1, 1)\n",
    "        y = np.log(np.array(avg_weights))\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        growth_rate = reg.coef_[0]\n",
    "    else:\n",
    "        growth_rate = 0.0\n",
    "    \n",
    "    # calculate moving average weight\n",
    "    date_idx = historical_dates.index(date)\n",
    "    lookback, sample_size, adj_weights = 0, 0, []\n",
    "    while True:\n",
    "        lookback += 1\n",
    "        sample_size += len(weights_by_date[historical_dates[date_idx-lookback+1]])\n",
    "        adj_weights_for_date = list(np.exp(growth_rate*(lookback-1))*np.array(weights_by_date[historical_dates[date_idx-lookback+1]]))\n",
    "        adj_weights.extend(adj_weights_for_date)\n",
    "        if date_idx-lookback+1 == 0:\n",
    "            break\n",
    "        if ((lookback >= min_lookback) & (sample_size >= min_sample_size)) | (lookback >= max_lookback):\n",
    "            break\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    ma_weight = adj_weights.sum() / sample_size\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': ma_weight,\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': lookback,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set([e['date'] for e in elements])))\n",
    "weights = []\n",
    "for date in dates:\n",
    "    weights.append(generate_biomass_estimate(date, json.dumps(elements), min_sample_size=3000)['weightMovingAvg'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "tdf = df[(df.akpd_score > 0.9)].akpd_weight.resample('D', how=lambda x: x.mean())\n",
    "rdf = df[(df.akpd_score > 0.9)].akpd_weight.resample('D', how=lambda x: len(x))\n",
    "ax[0].plot(dates, tdf.dropna().values, label='daily averages', color='blue')\n",
    "ax[0].plot(dates, weights, label='growth-rate-adjusted moving average', color='red')\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Vikane Pen ID 60 Biomass Progression')\n",
    "ax[1].plot(rdf.index, rdf.values)\n",
    "ax[1].grid()\n",
    "ax[1].set_title('Vikane Pen ID 60 Sample Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "mask = df.score > 0.8\n",
    "plt.hist(df[mask].weight, bins=10)\n",
    "plt.xlabel('Predicted weight (grams)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Predicted weight distribution for IMR - 11/09-11/13')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, side='both', overlay_keypoints=True, show_labels=False):\n",
    "    assert side == 'left' or side == 'right' or side == 'both', \\\n",
    "        'Invalid side value: {}'.format(side)\n",
    "\n",
    "    if side == 'left' or side == 'right':\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        image_f = left_image_f if side == 'left' else right_image_f\n",
    "        keypoints = left_keypoints if side == 'left' else right_keypoints\n",
    "        image = plt.imread(image_f)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in keypoints.items():\n",
    "#                 ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                ax.scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    ax.annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "        left_image = plt.imread(left_image_f)\n",
    "        right_image = plt.imread(right_image_f)\n",
    "        axes[0].imshow(left_image)\n",
    "        axes[1].imshow(right_image)\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in left_keypoints.items():\n",
    "#                 axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                axes[0].scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "            for bp, kp in right_keypoints.items():\n",
    "#                 axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                axes[1].scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "row = df[mask].sort_values('weight', ascending=False).iloc[idx]\n",
    "\n",
    "left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(left_crop_url)\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(right_crop_url)\n",
    "left_crop_metadata, right_crop_metadata = row.left_crop_metadata, row.right_crop_metadata,\n",
    "cm = row.camera_metadata\n",
    "row_id = idx\n",
    "\n",
    "# run AKPD\n",
    "akpd_keypoints = akpd.predict_keypoints(left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata)\n",
    "left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in akpd_keypoints[0]['leftCrop']}\n",
    "right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in akpd_keypoints[0]['rightCrop']}\n",
    "\n",
    "\n",
    "# run AKPD scoring network\n",
    "input_sample = {\n",
    "    'keypoints': akpd_keypoints[0],\n",
    "    'cm': cm,\n",
    "    'stereo_pair_id': row_id,\n",
    "    'single_point_inference': True\n",
    "}\n",
    "nomralized_centered_2D_kps = \\\n",
    "    normalize_centered_2D_transform_akpd.__call__(input_sample)\n",
    "\n",
    "akpd_normalized_kps = akpd_normalization_transform.__call__(nomralized_centered_2D_kps)\n",
    "tensorized_kps = to_tensor_transform.__call__(akpd_normalized_kps)\n",
    "score = akpd_scorer_network(tensorized_kps['kp_input']).item()\n",
    "display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, show_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_crop_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3250 = 2070 * (1+r)**60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((3250/2070)**(1/63.0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2070*(1.00718)**(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
