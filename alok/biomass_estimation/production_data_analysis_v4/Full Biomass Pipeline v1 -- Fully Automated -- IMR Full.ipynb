{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AKPD(object):\n",
    "\n",
    "    def __init__(self, aws_credentials):\n",
    "        self.client = boto3.client(\n",
    "            \"sagemaker-runtime\", \n",
    "            region_name=\"eu-west-1\", \n",
    "            aws_access_key_id=aws_credentials['aws_access_key_id'], \n",
    "            aws_secret_access_key=aws_credentials['aws_secret_access_key']\n",
    "        \n",
    "        )\n",
    "\n",
    "    def predict_keypoints(self, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, camera_metadata):\n",
    "        body = [{\n",
    "            'leftCropUrl': left_crop_url,\n",
    "            'rightCropUrl': right_crop_url,\n",
    "            'leftCropMetadata': left_crop_metadata,\n",
    "            'rightCropMetadata': right_crop_metadata,\n",
    "            'cameraMetadata': camera_metadata,\n",
    "            'id': 1\n",
    "        }]\n",
    "\n",
    "        body_str = json.dumps(body).replace(\"'\", '\"')\n",
    "\n",
    "        resp = self.client.invoke_endpoint(EndpointName='auto-keypoints', ContentType='application/json', Body=body_str)\n",
    "        akpd_keypoints_str = resp['Body'].read()\n",
    "        akpd_keypoints = json.loads(akpd_keypoints_str.decode(\"utf-8\"))\n",
    "        return akpd_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/data_dump_1.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-06-from-2019-10-25-to-2019-11-01.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-07-from-2019-11-01-to-2019-11-08.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-08-from-2019-11-08-to-2019-11-15.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-09-from-2019-11-15-to-2019-11-22.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-10-from-2019-11-22-to-2019-11-29.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-11-from-2019-11-29-to-2019-12-06.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-12-from-2019-12-06-to-2019-12-13.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-13-from-2019-12-13-to-2019-12-20.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/pen=61/biomass.csv-61-14-from-2019-12-20-to-2019-12-27.csv')\n",
    "])    \n",
    "\n",
    "aws_credentials = json.load(open(os.environ['AWS_CREDENTIALS']))\n",
    "akpd = AKPD(aws_credentials)\n",
    "\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "# initialize data transforms so that we can run inference with biomass neural network\n",
    "normalize_centered_2D_transform_biomass = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "\n",
    "# load neural network weights\n",
    "biomass_network = torch.load('/root/data/alok/biomass_estimation/results/neural_network/2019-11-08T00:13:09/nn_epoch_798.pb')\n",
    "akpd_scorer_network = load_model('/root/data/alok/biomass_estimation/playground/akpd_scorer_model_TF.h5') # make this better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Function to generate weight prediction and confidence score </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_score(row_id, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, akpd_keypoints, cm):\n",
    "    \n",
    "    # run AKPD scoring network\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "\n",
    "    # run biomass estimation\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform_biomass.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    akpd_weight_prediction = biomass_network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    \n",
    "    \n",
    "    return akpd_score, akpd_weight_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mask = df.akpd_score > 0.9\n",
    "hour_mask = (df.hour >= 7) & (df.hour < 16)\n",
    "plt.figure(figsize=(20, 10))\n",
    "tdf1 = df[score_mask].weight.resample('D', how=lambda x: x.mean())\n",
    "tdf2 = df[score_mask & hour_mask].weight.resample('D', how=lambda x: x.mean())\n",
    "plt.plot(tdf1.index, tdf1.values)\n",
    "plt.plot(tdf2.index, tdf2.values)\n",
    "# plt.plot(tdf.ewm(2).mean())\n",
    "# plt.plot(tdf.index, weights)\n",
    "plt.grid()\n",
    "plt.title('Biomass progression over time (pen_id = 61)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Avg. Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perform Smart Averaging </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute daily growth rate via fitting an exponential curve,\n",
    "# weighting each day by its sample size\n",
    "def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "    x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    y = np.log(tdf.values)\n",
    "    reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "    growth_rate = reg.coef_[0]\n",
    "    trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "    return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# of today's distribution against distribution in the remainder of the window\n",
    "def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "    mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "    distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "    return distribution_confidence\n",
    "\n",
    "\n",
    "# NOTE: we need to think more carefully about this to understand how distribution \n",
    "# confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "    return 5000\n",
    "    \n",
    "# Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# or until the minimum sample size is achieved\n",
    "def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = sorted(list(tdf.index.date.astype(str)))\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# generate date range given current date and window size. If future data\n",
    "# is available relative to current date, windows where the current date\n",
    "# is centered are preferred\n",
    "def compute_date_range(historical_dates, date, window_size_d):\n",
    "    FMT = '%Y-%m-%d'\n",
    "    max_num_days = 0\n",
    "    start_date, end_date = None, None\n",
    "    for i in range(window_size_d // 2 + 1):\n",
    "        lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "                            dt.timedelta(days=i)).strftime(FMT)\n",
    "        upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "        num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "                    (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "        if num_days >= max_num_days:\n",
    "            start_date, end_date = lower_bound_date, upper_bound_date\n",
    "            max_num_days = num_days\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    dts, vals = [], []\n",
    "    for iter_date in records:\n",
    "        for val in records[iter_date]:\n",
    "            dts.append(iter_date)\n",
    "            vals.append(val)\n",
    "\n",
    "    df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    # get raw statistics\n",
    "    raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_weight_g'] = df.weight\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(df.index.date.astype(str)))):\n",
    "    records[date].extend(df[date].estimated_weight_g.dropna().values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize=(10, 30))\n",
    "x_values = df.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3500 / (np.pi * (6**2) * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100000 / (np.pi * (25**2) * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "'''\n",
    "    This function generates biomass estimate for date given historical record of weight estiamtes\n",
    "    Assumptions: \n",
    "        - records_json is JSON blob in string form as specified in instructions\n",
    "        - date is of the format 'YYYY-mm-dd'\n",
    "        - records_json contains up to 30 days of data. The latest date in records_json equals 'date' input\n",
    "'''\n",
    "\n",
    "def generate_biomass_estimate(date, records_json, min_lookback=3, max_lookback=5, min_sample_size=3000, bucket_size=0.1):\n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    # get list of weights by date\n",
    "    weights_by_date = defaultdict(list)\n",
    "    for r in records:       \n",
    "        weights_by_date[r['date']].append(r['estimatedWeightG'])\n",
    "\n",
    "    # calculate daily growth rate (if more than one day is provided)\n",
    "    historical_dates = sorted(list(weights_by_date.keys()))\n",
    "    if len(historical_dates) > 1:\n",
    "        avg_weights = []\n",
    "        for h_date in historical_dates:\n",
    "            avg_weight = np.mean(weights_by_date[h_date])\n",
    "            avg_weights.append(avg_weight)\n",
    "\n",
    "        x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                     dt.datetime.strptime(dates[0], '%Y-%m-%d')).days for date in historical_dates]\n",
    "        X = np.array(x_values).reshape(-1, 1)\n",
    "        y = np.log(np.array(avg_weights))\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        growth_rate = reg.coef_[0]\n",
    "    else:\n",
    "        growth_rate = 0.0\n",
    "    \n",
    "    # calculate moving average weight\n",
    "    date_idx = historical_dates.index(date)\n",
    "    lookback, sample_size, adj_weights = 0, 0, []\n",
    "    while True:\n",
    "        lookback += 1\n",
    "        sample_size += len(weights_by_date[historical_dates[date_idx-lookback+1]])\n",
    "        adj_weights_for_date = list(np.exp(growth_rate*(lookback-1))*np.array(weights_by_date[historical_dates[date_idx-lookback+1]]))\n",
    "        adj_weights.extend(adj_weights_for_date)\n",
    "        if date_idx-lookback+1 == 0:\n",
    "            break\n",
    "        if ((lookback >= min_lookback) & (sample_size >= min_sample_size)) | (lookback >= max_lookback):\n",
    "            break\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    ma_weight = adj_weights.sum() / sample_size\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': ma_weight,\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': lookback,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "for idx, row in df[(df.akpd_score > 0.9)].iterrows():\n",
    "    weight = row.weight\n",
    "    date = str(idx.date())\n",
    "    print(date)\n",
    "    elements.append({\n",
    "        'date': date,\n",
    "        'estimatedWeightG': weight\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str).tolist())))\n",
    "weights = []\n",
    "for date in dates:\n",
    "    weight = generate_biomass_estimate(date, json.dumps(elements))\n",
    "    print(date, weight['weightMovingAvg'])\n",
    "    weights.append(weight['weightMovingAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.akpd_score > 0.9].weight.resample('D').agg(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2019-12-01'][df['2019-12-01'].akpd_score > 0.9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2019-12-01'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(tdf[tdf.index != '2019-12-12'].index, weights)\n",
    "myFmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "fig.autofmt_xdate()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.groupby('hour')['weight'].agg(lambda x: x.mean()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 20, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(30, 60))\n",
    "for idx, date in enumerate(dates[:100]):\n",
    "    row, col = idx // cols, idx % cols\n",
    "    axes[row, col].plot(df[date][df[date].akpd_score > 0.9].groupby('hour')['weight'].agg(lambda x: x.mean()))\n",
    "    axes[row, col].set_title(date)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 20, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(30, 60))\n",
    "for idx, date in enumerate(dates[:100]):\n",
    "    row, col = idx // cols, idx % cols\n",
    "    axes[row, col].plot(df[date].groupby('hour')['weight'].agg(lambda x: x.shape[0]))\n",
    "    axes[row, col].set_title(date)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-11-10'\n",
    "hours = list(range(8, 15))\n",
    "for hour in hours:\n",
    "    plt.hist(df[date][(df[date].akpd_score > 0.9) & (df[date].hour == hour)].weight)\n",
    "    plt.title(df[date][(df[date].akpd_score > 0.9) & (df[date].hour == hour)].weight.mean())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.akpd_score > 0.95\n",
    "plt.figure(figsize=(20, 10))\n",
    "tdf = df[mask].weight.resample('D', how=lambda x: x.shape[0] / 1e5)\n",
    "plt.plot(tdf.index, tdf.values)\n",
    "# plt.plot(tdf.ewm(2).mean())\n",
    "# plt.plot(tdf.index, weights)\n",
    "plt.grid()\n",
    "plt.title('Biomass progression over time (pen_id = 61)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Avg. Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df.weight.resample('D', how=lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-11-26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weight.resample('D', how=lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "'''\n",
    "    This function generates biomass estimate for date given historical record of weight estiamtes\n",
    "    Assumptions: \n",
    "        - records_json is JSON blob in string form as specified in instructions\n",
    "        - date is of the format 'YYYY-mm-dd'\n",
    "        - records_json contains up to 30 days of data. The latest date in records_json equals 'date' input\n",
    "'''\n",
    "\n",
    "def generate_biomass_estimate(date, df, min_lookback=3, max_lookback=5, min_sample_size=3000, bucket_size=0.1):\n",
    "\n",
    "    # calculate daily growth rate (if more than one day is provided)\n",
    "    \n",
    "    tdf = df.weight.resample('D', how=lambda x: x.mean())\n",
    "    sample_tdf = df.weight.resample('D', how=lambda x: x.shape[0])\n",
    "    start_date = dt.datetime.strftime(dt.datetime.strptime(date, '%Y-%m-%d') - dt.timedelta(days=15), '%Y-%m-%d')\n",
    "    historical_dates = tdf.index.date.astype(str).tolist()\n",
    "    if start_date not in historical_dates:\n",
    "        start_date = historical_dates[0]\n",
    "    h_dates = historical_dates[historical_dates.index(start_date):historical_dates.index(date)+1]\n",
    "    if len(historical_dates) > 1:\n",
    "        x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                     dt.datetime.strptime(start_date, '%Y-%m-%d')).days for date in h_dates]\n",
    "        X = np.array(x_values).reshape(-1, 1)\n",
    "        y = np.log(tdf[start_date:date].values)\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        growth_rate = reg.coef_[0]\n",
    "    else:\n",
    "        growth_rate = 0.0\n",
    "    \n",
    "    # calculate moving average weight\n",
    "    date_idx = h_dates.index(date)\n",
    "    lookback, sample_size, adj_weights = 0, 0, []\n",
    "    while True:\n",
    "        lookback += 1\n",
    "        sample_size += sample_tdf[h_dates[date_idx-lookback+1]]\n",
    "        adj_weights_for_date = list(np.exp(growth_rate*(lookback-1))*np.array(df[h_dates[date_idx-lookback+1]].weight))\n",
    "        adj_weights.extend(adj_weights_for_date)\n",
    "        if date_idx-lookback+1 == 0:\n",
    "            break\n",
    "        if ((lookback >= min_lookback) & (sample_size >= min_sample_size)) | (lookback >= max_lookback):\n",
    "            break\n",
    "    \n",
    "    ma_weight = np.nanmean(adj_weights)\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': ma_weight,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': lookback,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_biomass_estimate('2019-12-05', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = tdf.index.date.astype(str).tolist()\n",
    "weights = []\n",
    "for date in dates:\n",
    "    try:\n",
    "        weight = generate_biomass_estimate(date, df, min_sample_size=3000)['weightMovingAvg']\n",
    "        weights.append(weight)\n",
    "    except:\n",
    "        weights.append(None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "tdf = df[(df.akpd_score > 0.9)].akpd_weight.resample('D', how=lambda x: x.mean())\n",
    "rdf = df[(df.akpd_score > 0.9)].akpd_weight.resample('D', how=lambda x: len(x))\n",
    "ax[0].plot(dates, tdf.dropna().values, label='daily averages', color='blue')\n",
    "ax[0].plot(dates, weights, label='growth-rate-adjusted moving average', color='red')\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Vikane Pen ID 60 Biomass Progression')\n",
    "ax[1].plot(rdf.index, rdf.values)\n",
    "ax[1].grid()\n",
    "ax[1].set_title('Vikane Pen ID 60 Sample Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "mask = df.score > 0.8\n",
    "plt.hist(df[mask].weight, bins=10)\n",
    "plt.xlabel('Predicted weight (grams)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Predicted weight distribution for IMR - 11/09-11/13')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, side='both', overlay_keypoints=True, show_labels=False):\n",
    "    assert side == 'left' or side == 'right' or side == 'both', \\\n",
    "        'Invalid side value: {}'.format(side)\n",
    "\n",
    "    if side == 'left' or side == 'right':\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        image_f = left_image_f if side == 'left' else right_image_f\n",
    "        keypoints = left_keypoints if side == 'left' else right_keypoints\n",
    "        image = plt.imread(image_f)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in keypoints.items():\n",
    "#                 ax.scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                ax.scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    ax.annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "        left_image = plt.imread(left_image_f)\n",
    "        right_image = plt.imread(right_image_f)\n",
    "        axes[0].imshow(left_image)\n",
    "        axes[1].imshow(right_image)\n",
    "        if overlay_keypoints:\n",
    "            for bp, kp in left_keypoints.items():\n",
    "#                 axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                axes[0].scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "            for bp, kp in right_keypoints.items():\n",
    "#                 axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "                axes[1].scatter([kp[0]], [kp[1]], color='red', s=200, alpha=0.3)\n",
    "                if show_labels:\n",
    "                    axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "row = df[mask].sort_values('weight', ascending=False).iloc[idx]\n",
    "\n",
    "left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "left_image_f, _, _ = s3_access_utils.download_from_url(left_crop_url)\n",
    "right_image_f, _, _ = s3_access_utils.download_from_url(right_crop_url)\n",
    "left_crop_metadata, right_crop_metadata = row.left_crop_metadata, row.right_crop_metadata,\n",
    "cm = row.camera_metadata\n",
    "row_id = idx\n",
    "\n",
    "# run AKPD\n",
    "akpd_keypoints = akpd.predict_keypoints(left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata)\n",
    "left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in akpd_keypoints[0]['leftCrop']}\n",
    "right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in akpd_keypoints[0]['rightCrop']}\n",
    "\n",
    "\n",
    "# run AKPD scoring network\n",
    "input_sample = {\n",
    "    'keypoints': akpd_keypoints[0],\n",
    "    'cm': cm,\n",
    "    'stereo_pair_id': row_id,\n",
    "    'single_point_inference': True\n",
    "}\n",
    "nomralized_centered_2D_kps = \\\n",
    "    normalize_centered_2D_transform_akpd.__call__(input_sample)\n",
    "\n",
    "akpd_normalized_kps = akpd_normalization_transform.__call__(nomralized_centered_2D_kps)\n",
    "tensorized_kps = to_tensor_transform.__call__(akpd_normalized_kps)\n",
    "score = akpd_scorer_network(tensorized_kps['kp_input']).item()\n",
    "display_crops(left_image_f, right_image_f, left_keypoints, right_keypoints, show_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_crop_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3250 = 2070 * (1+r)**60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((3250/2070)**(1/63.0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2070*(1.00718)**(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_weights = []\n",
    "for i in range(50):\n",
    "    for count in range(100):\n",
    "        ground_truth_weights.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.arange(1000, 200000, 1000)\n",
    "err_pcts = []\n",
    "for sample_size in sample_sizes:\n",
    "    samples = []\n",
    "    for s in range(sample_size):\n",
    "        k = random.sample(ground_truth_weights, 1)[0]\n",
    "        samples.append(k)\n",
    "    err_pct = (np.mean(samples) - 4.5) / 4.5\n",
    "    err_pcts.append(err_pct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (np.array(err_pcts) * 4.5 + 4.5 - 24.5) / 24.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sample_sizes, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mask = df.akpd_score > 0.9\n",
    "date_mask = (df.captured_at > '2019-11-01') & (df.captured_at < '2019-11-08')\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "tdf = df[date_mask & score_mask].weight.resample('D', how=lambda x: x.mean())\n",
    "rdf = df[date_mask & score_mask].weight.resample('D', how=lambda x: x.shape[0])\n",
    "axes[0].plot(tdf.index, tdf.values)\n",
    "axes[0].grid()\n",
    "axes[0].set_title('Biomass progression over time (pen_id = 61)')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Avg. Weight')\n",
    "axes[1].plot(rdf.index, rdf.values)\n",
    "axes[1].grid()\n",
    "axes[1].set_title('Daily Sample Size over time (pen_id = 61)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Num. Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = '2019-11-02', '2019-11-07'\n",
    "fmt = '%Y-%m-%d'\n",
    "days = (dt.datetime.strptime(end_date, fmt) - dt.datetime.strptime(start_date, fmt)).days\n",
    "growth_rate = ((tdf[end_date] - tdf[start_date]) / tdf[start_date]) / days\n",
    "harvest_date = '2019-11-16'\n",
    "days_to_harvest = (dt.datetime.strptime(harvest_date, fmt) - dt.datetime.strptime(end_date, fmt)).days\n",
    "projected_weight = tdf[end_date] * (1 + growth_rate)**days_to_harvest\n",
    "gt_weight = 3370\n",
    "err_pct = (projected_weight - gt_weight) / gt_weight\n",
    "print('Error percentage: {}'.format(err_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mask = df.akpd_score > 0.9\n",
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "N = 4\n",
    "for idx in range(len(dates)-N-1):\n",
    "    start_date, end_date, date = dates[idx], dates[idx+N], dates[idx+N+1]\n",
    "    print(start_date, end_date, date)\n",
    "    x = np.percentile(df[score_mask][start_date:end_date].weight, list(range(100)))\n",
    "    y = np.percentile(df[score_mask][date].weight, list(range(100)))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].scatter(x[1:99], y[1:99])\n",
    "    axes[0].set_title(date)\n",
    "    axes[0].grid()\n",
    "    \n",
    "    lower_bound = int(df[start_date:date].weight.min() * 0.8)\n",
    "    upper_bound = int(df[start_date:date].weight.max() * 1.2)\n",
    "    \n",
    "    axes[1].hist(df[score_mask][start_date:end_date].weight, bins=list(np.arange(lower_bound, upper_bound, 300)), color='blue', alpha=0.5, density=True)\n",
    "    axes[1].hist(df[score_mask][date].weight, bins=list(np.arange(lower_bound, upper_bound, 300)), color='red', alpha=0.5, density=True)\n",
    "    axes[1].set_title(date)\n",
    "    axes[1].grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
