{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from PIL import Image\n",
    "\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute daily growth rate via fitting an exponential curve,\n",
    "# weighting each day by its sample size\n",
    "def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "    x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    y = np.log(tdf.values)\n",
    "    reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "    growth_rate = reg.coef_[0]\n",
    "    trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "    return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# of today's distribution against distribution in the remainder of the window\n",
    "def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "    mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "    distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "    return distribution_confidence\n",
    "\n",
    "\n",
    "# NOTE: we need to think more carefully about this to understand how distribution \n",
    "# confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "    return 5000\n",
    "    \n",
    "# Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# or until the minimum sample size is achieved\n",
    "def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = sorted(list(tdf.index.date.astype(str)))\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# generate date range given current date and window size. If future data\n",
    "# is available relative to current date, windows where the current date\n",
    "# is centered are preferred\n",
    "def compute_date_range(historical_dates, date, window_size_d):\n",
    "    FMT = '%Y-%m-%d'\n",
    "    max_num_days = 0\n",
    "    start_date, end_date = None, None\n",
    "    for i in range(window_size_d // 2 + 1):\n",
    "        lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "                            dt.timedelta(days=i)).strftime(FMT)\n",
    "        upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "        num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "                    (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "        if num_days >= max_num_days:\n",
    "            start_date, end_date = lower_bound_date, upper_bound_date\n",
    "            max_num_days = num_days\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    dts, vals = [], []\n",
    "    for iter_date in records:\n",
    "        for val in records[iter_date]:\n",
    "            dts.append(iter_date)\n",
    "            vals.append(val)\n",
    "\n",
    "    df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    # get raw statistics\n",
    "    raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframe\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "pen_id, group_id = 88, '88'\n",
    "query = \"\"\"\n",
    "    SELECT * FROM\n",
    "    prod.biomass_computations bc\n",
    "    WHERE bc.pen_id={0}\n",
    "    AND bc.group_id='{1}'\n",
    "    AND bc.akpd_score > 0.9;\n",
    "\"\"\".format(pen_id, group_id)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=88) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at between '2020-02-10' and '2020-02-29'\n",
    "      AND prod.biomass_computations.akpd_score > 0.9) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at between '2020-02-10' and '2020-02-29'\n",
    "    AND x.pen_id = 88\n",
    "    AND x.group_id = '88';\n",
    "\"\"\"\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "df = df.sort_values('captured_at')\n",
    "df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann_c = row.annotation\n",
    "    ann_dict_left_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['leftCrop']}\n",
    "    ann_dict_right_kps_c = {item['keypointType']: [item['xFrame'], item['yFrame']] for item in ann_c['rightCrop']}\n",
    "    these_diffs = []\n",
    "    for bp in BODY_PARTS:\n",
    "        diff = ann_dict_left_kps_c[bp][1] - ann_dict_right_kps_c[bp][1]\n",
    "        these_diffs.append(diff)\n",
    "    diffs.append(np.mean(these_diffs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_coordinate_diff'] = diffs\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "axes[0].plot(df.y_coordinate_diff.resample('D', how=lambda x: x.mean()))\n",
    "axes[0].axhline(0, color='red', linestyle='--')\n",
    "axes[1].plot(df.y_coordinate_diff.resample('D', how=lambda x: x.shape[0]))\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[0].grid()\n",
    "axes[1].grid()\n",
    "axes[0].set_title('Daily avg y-coordinate deviation')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Y coordinate deviation (pixels)')\n",
    "axes[1].set_title('Daily Sample Size')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Sample Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Y-Coordinate Deviation Diagnosis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params):\n",
    "    print(\"Loading params...\")\n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    imageSize = (4096, 3000)\n",
    "\n",
    "    # perform rectification\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T, None, None, None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "\n",
    "    print(\"Params loaded.\")\n",
    "    return left_maps, right_maps\n",
    "\n",
    "def rectify_crop(crop, maps, crop_metadata):\n",
    "    print(\"Rectifying...\")\n",
    "    new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "    lower_left = (crop_metadata['y_coord'] + crop_metadata['height'], crop_metadata['x_coord'])\n",
    "    upper_right = (crop_metadata['y_coord'], crop_metadata['x_coord'] + crop_metadata['width'])\n",
    "    new_image[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :] = np.array(crop)\n",
    "    remap = cv2.remap(new_image, maps[0], maps[1], cv2.INTER_LANCZOS4)\n",
    "    nonzero_indices = np.where(remap > 0)\n",
    "    y_min, y_max = nonzero_indices[0].min(), nonzero_indices[0].max()\n",
    "    x_min, x_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    lower_left = (y_max, x_min)\n",
    "    upper_right = (y_min, x_max)\n",
    "    rectified_crop = remap[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :].copy()\n",
    "\n",
    "    # construct rectified crop metadata\n",
    "    rectified_crop_metadata = crop_metadata.copy()\n",
    "    rectified_crop_metadata['x_coord'] = int(x_min)\n",
    "    rectified_crop_metadata['y_coord'] = int(y_min)\n",
    "    rectified_crop_metadata['width'] = int(x_max - x_min)\n",
    "    rectified_crop_metadata['height'] = int(y_max - y_min)\n",
    "\n",
    "    print(\"Rectification done\")\n",
    "    return rectified_crop, rectified_crop_metadata, remap\n",
    "\n",
    "\n",
    "def create_crop_metadata(raw_crop_f):\n",
    "    coords = [int(x) for x in os.path.basename(raw_crop_f).replace('.jpg', '').split('_')[-4:]]\n",
    "    \n",
    "    crop_metadata = {}\n",
    "    crop_metadata['x_coord'] = coords[0]\n",
    "    crop_metadata['y_coord'] = coords[1]\n",
    "    crop_metadata['width'] = coords[2] - coords[0]\n",
    "    crop_metadata['height'] = coords[3] - coords[1]\n",
    "    print(coords[0])\n",
    "    \n",
    "    return crop_metadata\n",
    "\n",
    "def display_crops(left_image, right_image, ann, overlay_keypoints=True, show_labels=False):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    \n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "    if overlay_keypoints:\n",
    "        for bp, kp in left_keypoints.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in right_keypoints.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load crops and metadata\n",
    "i = 0\n",
    "\n",
    "# load maps\n",
    "matlab_stereo_parameters_url = df.camera_metadata.iloc[i]['stereoParametersUrl']\n",
    "matlab_stereo_params_f, _, _ = s3_access_utils.download_from_url(matlab_stereo_parameters_url)\n",
    "matlab_stereo_params = json.load(open(matlab_stereo_params_f))\n",
    "left_maps, right_maps = load_params(matlab_stereo_params)\n",
    "\n",
    "left_crop_url = df.left_crop_url.iloc[i]\n",
    "right_crop_url = df.right_crop_url.iloc[i]\n",
    "ann = df.annotation.iloc[i]\n",
    "\n",
    "_, bucket, left_raw_crop_key = s3_access_utils.download_from_url(left_crop_url)\n",
    "left_raw_crop_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', left_raw_crop_key)\n",
    "left_raw_crop = Image.open(left_raw_crop_f)\n",
    "\n",
    "_, bucket, right_raw_crop_key = s3_access_utils.download_from_url(right_crop_url)\n",
    "right_raw_crop_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', right_raw_crop_key)\n",
    "right_raw_crop = Image.open(right_raw_crop_f)\n",
    "\n",
    "crops_json_key_base = os.path.dirname(left_raw_crop_key)\n",
    "crops_json_key = os.path.join(crops_json_key_base, 'crops.json')\n",
    "crops_json_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', crops_json_key)\n",
    "\n",
    "\n",
    "left_crop_metadata = create_crop_metadata(left_raw_crop_f)\n",
    "right_crop_metadata = create_crop_metadata(right_raw_crop_f)\n",
    "\n",
    "rectified_left_crop, rectified_left_crop_metadata, padded_left_image = rectify_crop(left_raw_crop, left_maps, left_crop_metadata)\n",
    "rectified_right_crop, rectified_right_crop_metadata, padded_right_image = rectify_crop(right_raw_crop, right_maps, right_crop_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_left_crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_left_crop_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_right_crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_right_crop_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left_crop_metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.right_crop_metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_crops(rectified_left_crop, rectified_right_crop, ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(padded_left_image).save('/root/data/alok/biomass_estimation/playground/padded_left_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(padded_right_image).save('/root/data/alok/biomass_estimation/playground/padded_right_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame_resize_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', \n",
    "                                                           os.path.join(os.path.dirname(left_raw_crop_key), 'left_frame.resize_512_512.jpg'))\n",
    "right_frame_resize_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', \n",
    "                                                           os.path.join(os.path.dirname(right_raw_crop_key), 'right_frame.resize_512_512.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame_resize = Image.open(left_frame_resize_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_frame_resize = Image.open(right_frame_resize_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame_resize_unresize = left_frame_resize.resize((4096, 3000))\n",
    "right_frame_resize_unresize = right_frame_resize.resize((4096, 3000))\n",
    "left_frame_resize_unresize_remap = cv2.remap(np.array(left_frame_resize_unresize), left_maps[0], left_maps[1], cv2.INTER_LANCZOS4)\n",
    "right_frame_resize_unresize_remap = cv2.remap(np.array(right_frame_resize_unresize), right_maps[0], right_maps[1], cv2.INTER_LANCZOS4)\n",
    "Image.fromarray(left_frame_resize_unresize_remap).save('/root/data/alok/biomass_estimation/playground/left_frame_resize_unresize_remap.png')\n",
    "Image.fromarray(right_frame_resize_unresize_remap).save('/root/data/alok/biomass_estimation/playground/right_frame_resize_unresize_remap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
