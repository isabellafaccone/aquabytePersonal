{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.weight_estimation.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from research.weight_estimation.weight_estimator import WeightEstimator\n",
    "from research.weight_estimation.visualize import Visualizer\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance\n",
    "from research.weight_estimation.keypoint_utils.keypoint_transformations import get_keypoint_arr, get_raw_3D_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "csv_url = 'https://aquabyte-calibrations.s3-eu-west-1.amazonaws.com/biomass_experiments/bolaks.pen88.matlab.02042020.cal.output.csv'\n",
    "csv_f, bucket, key = s3_access_utils.download_from_url(csv_url)\n",
    "df = pd.read_csv(csv_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotation'] = df.annotation.apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "df['camera_metadata'] = df.camera_metadata.apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "df['left_crop_metadata'] = df.left_crop_metadata.apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "df['right_crop_metadata'] = df.right_crop_metadata.apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/bolaks_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.s3_client.upload_file('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', \n",
    "                            'aquabyte-images-adhoc', 'alok/bolaks_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/data/alok/biomass_estimation/playground/bolaks_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.annotation.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('/root/data/alok/biomass_estimation/playground/bolaks_data.h5', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils.download_from_url('https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/FishID_annotation_data/Precision_annotation/bolaks_results.zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/root/data/s3/aquabyte-images-adhoc/FishID_annotation_data/Precision_annotation/bolaks_results'\n",
    "files = os.listdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(guest_list, filename):\n",
    "    \"\"\"Write the list to csv file.\"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for entries in guest_list:\n",
    "            outfile.write(entries)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file(sorted(files), '/root/data/alok/playground/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate weight estimator class\n",
    "\n",
    "model_f = '/root/data/alok/biomass_estimation/playground/nn_epoch_253.pb'\n",
    "weight_estimator = WeightEstimator(model_f)\n",
    "\n",
    "# generate sample predictions\n",
    "weights = []\n",
    "for idx, row in df.iterrows():\n",
    "    keypoints, camera_metadata = json.loads(row.annotation), json.loads(row.camera_metadata)\n",
    "    weight_prediction = weight_estimator.predict(keypoints, camera_metadata)\n",
    "    weights.append(weight_prediction)\n",
    "    if len(weights) % 1000 == 0:\n",
    "        print(len(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data transforms so that we can run inference with neural network\n",
    "normalize_centered_2D_transform = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "# Get neural network weights from sample training\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2019-11-08T00-13-09/nn_epoch_798.pb'\n",
    "model_f, _, _ = s3_access_utils.download_from_url(model_url)\n",
    "network = torch.load(model_f)\n",
    "\n",
    "weight_predictions = []\n",
    "for idx, row in df.iterrows():\n",
    "    input_sample = {\n",
    "        'keypoints': json.loads(row.annotation),\n",
    "        'cm': json.loads(row.camera_metadata),\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    weight_prediction = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    weight_predictions.append(weight_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_weight'] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(df.new_weight.values - df.estimated_weight_g.values, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    return pixel2world(json.loads(row.annotation)['leftCrop'], \n",
    "                       json.loads(row.annotation)['rightCrop'], \n",
    "                       json.loads(row.camera_metadata))\n",
    "\n",
    "def compute_yaw_angle(wkp):\n",
    "    v = wkp['UPPER_LIP'] - wkp['TAIL_NOTCH']\n",
    "    yaw_angle = np.arctan(v[1] / v[0]) * 180.0 / np.pi\n",
    "    return yaw_angle\n",
    "\n",
    "df['world_keypoints'] = df.apply(lambda x: get_world_keypoints(x), axis=1)\n",
    "df['median_depth'] = df.world_keypoints.apply(lambda x: np.median([wkp[1] for wkp in x.values()]))\n",
    "df['yaw_angle'] = df.world_keypoints.apply(lambda x: compute_yaw_angle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['difference'] = df.new_weight - df.estimated_weight_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image, right_image, ann, overlay_keypoints=True, show_labels=False):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    \n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "    if overlay_keypoints:\n",
    "        for bp, kp in left_keypoints.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in right_keypoints.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 14\n",
    "mask = (df.median_depth > 1.8) & (df.median_depth < 1.9)\n",
    "tdf = df[mask].copy(deep=True)\n",
    "left_crop_url = tdf.left_crop_url.iloc[idx]\n",
    "right_crop_url = tdf.right_crop_url.iloc[idx]\n",
    "print(tdf.difference.iloc[idx])\n",
    "left_crop_f, _, left_crop_key = s3_access_utils.download_from_url(left_crop_url)\n",
    "right_crop_f, _, right_crop_key = s3_access_utils.download_from_url(right_crop_url)\n",
    "left_crop = Image.open(left_crop_f)\n",
    "right_crop = Image.open(right_crop_f)\n",
    "\n",
    "left_raw_image_key = os.path.join(os.path.dirname(left_crop_key), 'left_frame.resize_512_512.jpg')\n",
    "left_raw_image_key = left_raw_image_key.replace('20200318.231820.Js_S', 'production')\n",
    "right_raw_image_key = os.path.join(os.path.dirname(right_crop_key), 'right_frame.resize_512_512.jpg')\n",
    "right_raw_image_key = right_raw_image_key.replace('20200318.231820.Js_S', 'production')\n",
    "left_raw_image_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', left_raw_image_key)\n",
    "right_raw_image_f = s3_access_utils.download_from_s3('aquabyte-frames-resized-inbound', right_raw_image_key)\n",
    "\n",
    "left_image = Image.open(left_crop_f)\n",
    "right_image = Image.open(right_crop_f)\n",
    "ann = json.loads(tdf.annotation.iloc[idx])\n",
    "cm = json.loads(tdf.camera_metadata.iloc[idx])\n",
    "display_crops(left_crop, right_crop, ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(left_raw_image_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(right_raw_image_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_keypoint_arr(ann, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = np.arange(0.5, 2.0, 0.1)\n",
    "for idx in range(len(depth_values) - 1):\n",
    "    low, high = depth_values[idx], depth_values[idx + 1]\n",
    "    mean_difference = df[(df.median_depth > low) & (df.median_depth < high)].difference.median()\n",
    "    print('Mean difference at depth range {}-{}: {}'.format(round(low, 1), round(high, 1), \n",
    "                                                            round(mean_difference, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = np.arange(0.5, 2.0, 0.1)\n",
    "for idx in range(len(depth_values) - 1):\n",
    "    low, high = depth_values[idx], depth_values[idx + 1]\n",
    "    std_difference = df[(df.median_depth > low) & (df.median_depth < high)].difference.std()\n",
    "    print('Std difference at depth range {}-{}: {}'.format(round(low, 1), round(high, 1), \n",
    "                                                            round(std_difference, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaw_values = np.arange(0, 60, 10)\n",
    "low_depth = 1.6\n",
    "for idx in range(len(yaw_values) - 1):\n",
    "    low, high = yaw_values[idx], yaw_values[idx + 1]\n",
    "    mean_difference = df[(df.median_depth > low_depth) & (df.median_depth < low_depth + 0.1) & \\\n",
    "                         (df.yaw_angle.abs() > low) & (df.yaw_angle.abs() < high)].difference.mean()\n",
    "    print('Mean difference at yaw range {}-{}: {}'.format(round(low, 1), round(high, 1), \n",
    "                                                            round(mean_difference, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.world_keypoints.apply(lambda x: x['TAIL_NOTCH'][0] < x['UPPER_LIP'][0])].difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.world_keypoints.apply(lambda x: x['TAIL_NOTCH'][0] > x['UPPER_LIP'][0])].difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df[mask_x & mask_y].head(1).iterrows():\n",
    "    input_sample = {\n",
    "        'keypoints': row.annotation,\n",
    "        'cm': row.camera_metadata,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform.__call__(input_sample)\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    \n",
    "    keypoint_arr = get_keypoint_arr(row.annotation, row.camera_metadata, recover_original_depth=True)\n",
    "    \n",
    "wkps = tensorized_kps['kp_input'].numpy()[0]\n",
    "wkp_1 = (0.5 * 0.1) / wkps[:, 2]\n",
    "x = wkps[:, 0] * wkp_1 / 0.5\n",
    "y = wkps[:, 1] * wkp_1 / 0.5\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(x, y, color='blue')\n",
    "\n",
    "x, y = keypoint_arr[:, 0], keypoint_arr[:, 2]\n",
    "plt.scatter(x, y, color='red')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(keypoint_arr[1], keypoint_arr[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(wkp['ANAL_FIN'], wkp['PELVIC_FIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_medoid(wkp):\n",
    "    return np.median(np.array([np.array(x) for x in wkp.values()]), axis=0)\n",
    "\n",
    "df['medoid'] = df.world_keypoints.apply(lambda x: compute_medoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_x = df.medoid.apply(lambda x: x[0]).abs() < 0.02\n",
    "mask_y = df.medoid.apply(lambda x: x[2]).abs() < 0.02\n",
    "df[mask_x & mask_y].difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask_x & mask_y].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask_x & mask_y].new_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.yaw_angle.abs() > 25\n",
    "df[mask].difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.gtsf_data.body_parts import BodyParts\n",
    "\n",
    "BODY_PARTS = BodyParts().get_core_body_parts()\n",
    "\n",
    "def generate_rotation_matrix(n, theta):\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0] ** 2 * (1 - np.cos(theta)),\n",
    "        n[0] * n[1] * (1 - np.cos(theta)) - n[2] * np.sin(theta),\n",
    "        n[0] * n[2] * (1 - np.cos(theta)) + n[1] * np.sin(theta)\n",
    "    ], [\n",
    "        n[1] * n[0] * (1 - np.cos(theta)) + n[2] * np.sin(theta),\n",
    "        np.cos(theta) + n[1] ** 2 * (1 - np.cos(theta)),\n",
    "        n[1] * n[2] * (1 - np.cos(theta)) - n[0] * np.sin(theta),\n",
    "    ], [\n",
    "        n[2] * n[0] * (1 - np.cos(theta)) - n[1] * np.sin(theta),\n",
    "        n[2] * n[1] * (1 - np.cos(theta)) + n[0] * np.sin(theta),\n",
    "        np.cos(theta) + n[2] ** 2 * (1 - np.cos(theta))\n",
    "    ]])\n",
    "\n",
    "    return R\n",
    "\n",
    "def normalize_3D_coordinates(wkps):\n",
    "\n",
    "    v = np.median(wkps[:8], axis=0)\n",
    "    v /= np.linalg.norm(v)\n",
    "    y = np.array([0, 1, 0])\n",
    "    n = np.cross(y, v)\n",
    "    n /= np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(y, v))\n",
    "    R = generate_rotation_matrix(n, theta)\n",
    "    wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    # rotate about y-axis so that fish is straight\n",
    "    upper_lip_idx = BODY_PARTS.index('UPPER_LIP')\n",
    "    tail_notch_idx = BODY_PARTS.index('TAIL_NOTCH')\n",
    "    v = wkps[upper_lip_idx] - wkps[tail_notch_idx]\n",
    "\n",
    "    n = np.array([0, 1, 0])\n",
    "    theta = np.arctan(v[2] / v[0])\n",
    "    R = generate_rotation_matrix(n, theta)\n",
    "    wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    # perform reflecton if necessary\n",
    "    tail_notch_idx = BODY_PARTS.index('TAIL_NOTCH')\n",
    "    if wkps[upper_lip_idx][0] < wkps[tail_notch_idx][0]:\n",
    "        R = np.array([\n",
    "            [-1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    return wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "raw_wkps = get_raw_3D_coordinates(df.annotation.iloc[idx], df.camera_metadata.iloc[idx])\n",
    "norm_wkps, norm_wkps_2 = normalize_3D_coordinates(raw_wkps)\n",
    "plt.scatter(norm_wkps[:, 0], norm_wkps[:, 2], color='blue')\n",
    "plt.scatter(norm_wkps_2[:, 0], norm_wkps_2[:, 2], color='red')\n",
    "plt.xlim(-0.4, 0.4)\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist(np.dot(raw_wkps, np.array([[1, 0, 0,], [0, 0, 0], [0, 0, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist(np.dot(norm_wkps, np.array([[1, 0, 0,], [0, 0, 0], [0, 0, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist(np.dot(norm_wkps_2, np.array([[1, 0, 0,], [0, 0, 0], [0, 0, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
