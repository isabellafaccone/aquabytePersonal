{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "from multiprocessing import Pool, Manager\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.akpd import AKPD\n",
    "from aquabyte.template_matching import find_matches_and_homography\n",
    "from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute daily growth rate via fitting an exponential curve,\n",
    "# weighting each day by its sample size\n",
    "def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "    x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    y = np.log(tdf.values)\n",
    "    reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "    growth_rate = reg.coef_[0]\n",
    "    trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "    return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# of today's distribution against distribution in the remainder of the window\n",
    "def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "    mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "    distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "    return distribution_confidence\n",
    "\n",
    "\n",
    "# NOTE: we need to think more carefully about this to understand how distribution \n",
    "# confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "    return 5000\n",
    "    \n",
    "# Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# or until the minimum sample size is achieved\n",
    "def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = sorted(list(tdf.index.date.astype(str)))\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# generate date range given current date and window size. If future data\n",
    "# is available relative to current date, windows where the current date\n",
    "# is centered are preferred\n",
    "def compute_date_range(historical_dates, date, window_size_d):\n",
    "    FMT = '%Y-%m-%d'\n",
    "    max_num_days = 0\n",
    "    start_date, end_date = None, None\n",
    "    for i in range(window_size_d // 2 + 1):\n",
    "        lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "                            dt.timedelta(days=i)).strftime(FMT)\n",
    "        upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "        num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "                    (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "        if num_days >= max_num_days:\n",
    "            start_date, end_date = lower_bound_date, upper_bound_date\n",
    "            max_num_days = num_days\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    dts, vals = [], []\n",
    "    for iter_date in records:\n",
    "        for val in records[iter_date]:\n",
    "            dts.append(iter_date)\n",
    "            vals.append(val)\n",
    "\n",
    "    df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    # get raw statistics\n",
    "    raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # test function with gap in data\n",
    "    records = {\n",
    "        '2020-01-01': [1000, 2000, 3000],\n",
    "        '2020-01-07': [5000, 4000, 3000]\n",
    "    }\n",
    "\n",
    "    records_json = json.dumps(records)\n",
    "    date = '2020-01-07'\n",
    "    \n",
    "    smart_average = compute_metrics(date, records_json)\n",
    "    smart_average = json.loads(smart_average)\n",
    "    np.testing.assert_almost_equal(smart_average['weightMovingAvg'], 4000, 6)\n",
    "    np.testing.assert_almost_equal(smart_average['dailyGrowthRate'], 0.11552, 4)\n",
    "    \n",
    "    # test function with no historical data in 7 day window\n",
    "    records = {\n",
    "        '2020-01-01': [1000, 2000, 3000],\n",
    "        '2020-01-08': [5000, 4000, 3000]\n",
    "    }\n",
    "\n",
    "    records_json = json.dumps(records)\n",
    "    date = '2020-01-08'\n",
    "    \n",
    "    \n",
    "    smart_average = compute_metrics(date, records_json)\n",
    "    smart_average = json.loads(smart_average)\n",
    "    np.testing.assert_almost_equal(smart_average['weightMovingAvg'], 4000, 6)\n",
    "    np.testing.assert_almost_equal(smart_average['dailyGrowthRate'], 0.0, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/biomass.csv-72-00-from-2020-01-12-to-2020-01-22.csv'),\n",
    "    pd.read_csv('/root/data/alok/biomass_estimation/playground/biomass.csv-72-01-from-2020-01-22-to-2020-02-01.csv')\n",
    "])\n",
    "\n",
    "df = df.sort_values('captured_at')\n",
    "df['estimated_weight_g'] = df.weight\n",
    "df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "\n",
    "# get daily averages and sample sizes\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "records = defaultdict(list)\n",
    "for date in sorted(list(set(df.index.date.astype(str)))):\n",
    "    records[date].extend(df[date].weight.values.tolist())\n",
    "\n",
    "records_json = json.dumps(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    this_records_json = json.dumps({k: v for k, v in json.loads(records_json).items() if k <= date})\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, this_records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(8, 20))\n",
    "x_values = df.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights)\n",
    "axes[0].plot(x_values, smart_averages)\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes)\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(smart_averages[-1] - 4900) / 4900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(15, 15))\n",
    "tdf = df[df.akpd_score > 0.9].weight.resample('D').agg(lambda x: x.mean())\n",
    "rdf = df[df.akpd_score > 0.9].weight.resample('D').agg(lambda x: x.shape[0])\n",
    "axes[0].plot(tdf.index, tdf.values)\n",
    "axes[0].grid()\n",
    "axes[1].plot(rdf.index, rdf.values)\n",
    "axes[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
