{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Biomass - Erko Rotoy Single Case Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import skimage.io\n",
    "from tqdm import tqdm\n",
    "\n",
    "LIB_DIRECTORY = '/root/alok/repos/cv_research/lib/'\n",
    "sys.path.insert(0, os.path.join(LIB_DIRECTORY, 'maskrcnn'))\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "import mrcnn.model as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "from mrcnn.model import log\n",
    "import mcoco.coco as coco\n",
    "# import mextra.utils as extra_utils\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "LIB_DIRECTORY = '/root/alok/repos/cv_research/lib/'\n",
    "sys.path.insert(0, LIB_DIRECTORY)\n",
    "\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "BASE_DIR = '/root/data/models/erko/mask_rcnn_instance_segmentation'\n",
    "DATA_DIR = '/root/data/erko/'\n",
    "WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"logs\", \"body_part_segmentation_20181031_21H02\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '/root/alok/data/images/annotation_file_test_set.json'\n",
    "coco_output = json.load(open(annotation_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Set up data and helper functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get stereo frame pairs where for each one, we have the left image ID and right image ID\n",
    "'''\n",
    "coco = COCO(annotation_file)\n",
    "image_ids = coco.getImgIds()\n",
    "images = coco.loadImgs(image_ids)\n",
    "stereo_frame_pairs = {}\n",
    "for image in images:\n",
    "    local_path = image['local_path']\n",
    "    sfp_id = int(local_path.split('/')[-3])\n",
    "    side = 'left' if 'left' in local_path else 'right'\n",
    "    if sfp_id not in list(stereo_frame_pairs.keys()):\n",
    "        stereo_frame_pairs[sfp_id] = {}\n",
    "    stereo_frame_pairs[sfp_id][side] = image['id']\n",
    "    \n",
    "stereo_frame_pairs = list(stereo_frame_pairs.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define some helper functions (most of this code is adapted from cv_algorithms)\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "FOCAL_LENGTH = 0.0107\n",
    "BASELINE = 0.135\n",
    "PIXEL_SIZE_M = 3.45 * 1e-6\n",
    "FOCAL_LENGTH_PIXEL = FOCAL_LENGTH / PIXEL_SIZE_M\n",
    "IMAGE_SENSOR_WIDTH = 0.01412\n",
    "IMAGE_SENSOR_HEIGHT = 0.01412\n",
    "RANDOM_STATE = 170\n",
    "DENSITY = 1e6  # g per cubic meter\n",
    "COST_THRESHOLD = 100.0  # another magic number\n",
    "\n",
    "CATEGORIES = [\n",
    "    {'id': 1, 'name': 'salmon'},\n",
    "    {'id': 2, 'name': 'Head'},\n",
    "    {'id': 3, 'name': 'Caudal Fin'},\n",
    "    {'id': 4, 'name': 'Dorsal Fin'},\n",
    "    {'id': 5, 'name': 'Adipose Fin'},\n",
    "    {'id': 6, 'name': 'Anal Fin'},\n",
    "    {'id': 7, 'name': 'Pelvic Fin'},\n",
    "    {'id': 8, 'name': 'Pectoral Fin'},\n",
    "    {'id': 9, 'name': 'Eye'}\n",
    "]\n",
    "\n",
    "SALMON_CATEGORY_ID = [cat['id'] for cat in CATEGORIES if cat['name'] == 'salmon'][0]\n",
    "\n",
    "def convert_to_world_point(x, y, d):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    # TODO (@Thomas) this is hard coded and this bad....\n",
    "    image_center_x = 3000 / 2.0  # depth_map.shape[1] / 2.0\n",
    "    image_center_y = 4096 / 2.0  # depth_map.shape[0] / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (IMAGE_SENSOR_WIDTH / 3000)\n",
    "    sensor_z = px_z * (IMAGE_SENSOR_HEIGHT / 4096)\n",
    "\n",
    "    # d = depth_map[y, x]\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / FOCAL_LENGTH\n",
    "    world_z = (world_y * sensor_z) / FOCAL_LENGTH\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "\n",
    "def depth_from_disp(disp):\n",
    "    depth = FOCAL_LENGTH_PIXEL*BASELINE / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def match_body_parts(annotations, categories, whole_fish=\"salmon\"):\n",
    "    \n",
    "    \"\"\" take the predictions on a single frame and associate body parts with whole salmon using a unique identifier\"\"\"\n",
    "    \n",
    "    whole_fish_id = [cat for cat in categories if cat['name'] == whole_fish][0]['id']\n",
    "    fish = []\n",
    "    body_parts = []\n",
    "    fish_counter = 0\n",
    "\n",
    "    # first we separate whole salmon from body parts and give a unique identifier\n",
    "    for annotation in annotations:\n",
    "        if annotation['category_id'] == whole_fish_id:\n",
    "            annotation['fish_id'] = fish_counter\n",
    "            fish.append(annotation)\n",
    "            fish_counter += 1\n",
    "        else:\n",
    "            body_parts.append(annotation)\n",
    "\n",
    "    # second we match body parts with whole salmon\n",
    "    # @TODO (Thomas) this is not great because it does not take edges cases into account.\n",
    "    for part in body_parts:\n",
    "        bbox = part['bbox']\n",
    "        part_centroid = [np.mean([bbox[0], bbox[2]]), np.mean([bbox[1], bbox[3]])]\n",
    "        for f in fish:\n",
    "            bbox = f['bbox']\n",
    "            if bbox[0] < part_centroid[0] < bbox[2] and bbox[1] < part_centroid[1] < bbox[3]:\n",
    "                part['fish_id'] = f['fish_id']\n",
    "                part['fish_ann_id'] = f['id']\n",
    "                break\n",
    "\n",
    "    matched_body_pars = [ann for ann in annotations if 'fish_id' in ann.keys()]\n",
    "    return matched_body_pars\n",
    "\n",
    "\n",
    "def left_right_matching(left_annotations, right_annotations, categories, whole_fish=\"salmon\"):\n",
    "    \n",
    "    \"\"\"match the bboxes. Return a list of matched bboxes\"\"\"\n",
    "    \n",
    "    # get the salmon category id\n",
    "    whole_fish_id = [cat for cat in categories if cat['name'] == whole_fish][0]['id']\n",
    "\n",
    "    # let's use x1, x2 to match bboxes\n",
    "    left_centroids = []\n",
    "    left_ids = []\n",
    "    for ann in left_annotations:\n",
    "        if ann['category_id'] != whole_fish_id:\n",
    "            continue\n",
    "        bbox = ann['bbox']\n",
    "        # centroid = [(bbox[3] - bbox[1])/2.0, (bbox[2] - bbox[0])/2.0]\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        left_ids.append(ann['fish_id'])\n",
    "        left_centroids.append(corner)\n",
    "            \n",
    "\n",
    "    right_centroids = []\n",
    "    right_ids = []\n",
    "    for ann in right_annotations:\n",
    "        if ann['category_id'] != whole_fish_id:\n",
    "            continue\n",
    "        bbox = ann['bbox']\n",
    "        # centroid = [(bbox[3] - bbox[1])/2.0, (bbox[2] - bbox[0])/2.0]\n",
    "        corner = [bbox[2], bbox[0]]\n",
    "        right_ids.append(ann['fish_id'])\n",
    "        right_centroids.append(corner)\n",
    "\n",
    "    # euclidean distance in (x1, x2) space\n",
    "    cost_matrix = euclidean_distances(left_centroids, right_centroids)\n",
    "    \n",
    "    # hungarian algorithm to minimize weights in bipartite graph\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    matched_annotations = []\n",
    "    for (r, c) in zip(row_ind, col_ind):\n",
    "        if cost_matrix[r, c] < COST_THRESHOLD:\n",
    "            left_matched_parts = [l for l in left_annotations if l['fish_id'] == left_ids[r] and l['category_id'] != whole_fish_id]\n",
    "            right_matched_parts = [r for r in right_annotations if r['fish_id'] == right_ids[c] and r['category_id'] != whole_fish_id]\n",
    "            matched_annotations.append([left_matched_parts, right_matched_parts])\n",
    "\n",
    "    return matched_annotations\n",
    "\n",
    "\n",
    "def calculate_pairwise_distances(left_matched_parts, right_matched_parts, categories):\n",
    "    \n",
    "    \"\"\" Create a dataframe of pairwise distances for each fish detection in this image pair \"\"\"\n",
    "    \n",
    "    left_category_ids = [ann['category_id'] for ann in left_matched_parts]\n",
    "    right_category_ids = [ann['category_id'] for ann in right_matched_parts]\n",
    "\n",
    "    # initialize pairwise distances for this fish detection\n",
    "    category_ids = [str(cat['id']) for cat in categories if cat['name'] != 'salmon']\n",
    "    distances = [c + k for (i, c) in enumerate(category_ids) for k in category_ids[i + 1:]]\n",
    "    dataset = {}\n",
    "    for d in distances:\n",
    "        dataset[d] = np.nan\n",
    "\n",
    "    # calculate disparities & world coordinates\n",
    "    world_coordinates = {}\n",
    "    for cat in categories:\n",
    "        # calculate left centroid\n",
    "        left_parts = [part for part in left_matched_parts if part['category_id'] == cat['id']]\n",
    "        right_parts = [part for part in right_matched_parts if part['category_id'] == cat['id']]\n",
    "        if len(left_parts) > 1 or len(right_parts) > 1:\n",
    "            \n",
    "            # more than one body part of this category has been found on this fish -- skip this body part\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        if left_parts and right_parts:\n",
    "            left_part, right_part = left_parts[0], right_parts[0]\n",
    "            \n",
    "            # calculate left centroid\n",
    "            seg = left_part['segmentation'][0]\n",
    "            poly = np.array(seg).reshape((int(len(seg) / 2), 2))\n",
    "            p = [(r[0], r[1]) for r in poly]\n",
    "            left_mask = Image.new('L', (4096, 3000), 0)\n",
    "            ImageDraw.Draw(left_mask).polygon(p, outline=1, fill=1)\n",
    "            left_mask = np.array(left_mask)\n",
    "            x, y = np.nonzero(left_mask)\n",
    "            left_centroid = [np.mean(x), np.mean(y)]\n",
    "\n",
    "            # calculate right centroid\n",
    "            seg = right_part['segmentation'][0]\n",
    "            poly = np.array(seg).reshape((int(len(seg) / 2), 2))\n",
    "            p = [(r[0], r[1]) for r in poly]\n",
    "            right_mask = Image.new('L', (4096, 3000), 0)\n",
    "            ImageDraw.Draw(right_mask).polygon(p, outline=1, fill=1)\n",
    "            right_mask = np.array(right_mask)\n",
    "            x, y = np.nonzero(right_mask)\n",
    "            right_centroid = [np.mean(x), np.mean(y)]\n",
    "\n",
    "            # calculate disparity\n",
    "            disparities = np.abs(left_centroid[1] - right_centroid[1])\n",
    "            depth = depth_from_disp(disparities)\n",
    "            if left_part['fish_ann_id'] == 13999 and left_part['category_id'] == 2:\n",
    "                print('Disparity: {}'.format(disparities))\n",
    "                print('Depth: {}'.format(depth))\n",
    "            world_coordinates[str(cat['id'])] = convert_to_world_point(left_centroid[0], left_centroid[1], depth)\n",
    "\n",
    "    if left_matched_parts[0]['fish_ann_id'] == 13999:\n",
    "        print(world_coordinates['2'], world_coordinates['4'])\n",
    "    \n",
    "    # now calculate the pairwise distances\n",
    "    for pair in dataset.keys():\n",
    "        cat0, cat1 = pair[0], pair[1]\n",
    "        if cat0 in world_coordinates and cat1 in world_coordinates:\n",
    "            dist = np.linalg.norm(world_coordinates[cat0] - world_coordinates[cat1])\n",
    "            dataset[pair] = dist\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Analyze one case (i.e. one stereo frame pair) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_frame_pair_idx = 1100\n",
    "\n",
    "\n",
    "stereo_frame_pair = stereo_frame_pairs[stereo_frame_pair_idx]\n",
    "left_image_id = stereo_frame_pair['left']\n",
    "left_image = coco.loadImgs([left_image_id])\n",
    "left_annotation_ids = coco.getAnnIds(imgIds=[left_image_id])\n",
    "left_annotations = coco.loadAnns(left_annotation_ids)\n",
    "\n",
    "right_image_id = stereo_frame_pair['right']\n",
    "right_image = coco.loadImgs([left_image_id])\n",
    "right_annotation_ids = coco.getAnnIds(imgIds=[right_image_id])\n",
    "right_annotations = coco.loadAnns(right_annotation_ids)\n",
    "categories = CATEGORIES\n",
    "\n",
    "left_annotations = match_body_parts(left_annotations, categories)\n",
    "right_annotations = match_body_parts(right_annotations, categories)\n",
    "if left_annotations and right_annotations:\n",
    "    matched_annotations = left_right_matching(left_annotations, right_annotations, categories)\n",
    "    for pair in matched_annotations:\n",
    "        if pair[0] and pair[1]:\n",
    "            dataset = calculate_pairwise_distances(pair[0], pair[1], categories)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
