{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase I: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.optics import euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "research_rds_access_utils = RDSAccessUtils(research_sql_credentials)\n",
    "sql_engine = research_rds_access_utils.sql_engine\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create training dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "sfps_all = session.query(StereoFramePair).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "session.rollback()\n",
    "for idx, row in enumerate(sfps_all):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    # get fish_id and ground truth metadata\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "    left_keypoints = json.loads(row.left_image_keypoint_coordinates)\n",
    "    right_keypoints = json.loads(row.right_image_keypoint_coordinates)\n",
    "    wkps = json.loads(row.world_keypoint_coordinates)\n",
    "\n",
    "    df_row = {'0': idx}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(wkps[body_parts[i]], wkps[body_parts[j]])\n",
    "            df_row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    weight, length, kfactor = None, None, None\n",
    "    if 'data' in ground_truth_metadata.keys():\n",
    "        keys = ground_truth_metadata['data'].keys()\n",
    "        if 'weight' in keys or 'weightKgs' in keys:\n",
    "            weightKey = 'weight' if 'weight' in keys else 'weightKgs'\n",
    "            lengthKey = 'length' if 'length' in keys else 'lengthMms'\n",
    "            weight = ground_truth_metadata['data'][weightKey]\n",
    "            length = ground_truth_metadata['data'][lengthKey]\n",
    "            kfactor = (weight / length**3) * 1e5\n",
    "    if not weight:\n",
    "        print('No weight recorded for GTSF fish identifier: {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # calculate curvature\n",
    "    wkp = {bp: [wkps[bp][2], wkps[bp][1], wkps[bp][0]] for bp in body_parts}\n",
    "    fv1 = np.array(wkp['UPPER_LIP']) - np.array(wkp['DORSAL_FIN'])\n",
    "    fv2 = np.array(wkp['UPPER_LIP']) - np.array(wkp['PELVIC_FIN'])\n",
    "    n1 = np.cross(fv1, fv2)\n",
    "    \n",
    "    bv1 = np.array(wkp['PELVIC_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    bv2 = np.array(wkp['DORSAL_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    n2 = np.cross(bv1, bv2)\n",
    "    curvature_theta = (180 / np.pi) * np.arccos(np.dot(n1, n2) / (np.linalg.norm(n1) * np.linalg.norm(n2)))\n",
    "    \n",
    "    df_row['weight'] = weight\n",
    "    df_row['length'] = length\n",
    "    df_row['kfactor'] = kfactor\n",
    "    df_row['date'] = row.date\n",
    "    df_row['project_name'] = row.annotations_project_name\n",
    "    df_row['left_keypoints'] = json.loads(row.left_image_keypoint_coordinates)\n",
    "    df_row['right_keypoints'] = json.loads(row.right_image_keypoint_coordinates)\n",
    "    df_row['world_keypoints'] = wkps\n",
    "    df_row['gtsf_fish_identifier'] = row.gtsf_fish_identifier\n",
    "    df_row['epoch'] = row.epoch\n",
    "    df_row['stereo_frame_pair_id'] = row.id\n",
    "    df_row['curvature_theta'] = curvature_theta\n",
    "        \n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Model with Old Calibration </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "])\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        df[interaction_column] = df[col1] * df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            df[interaction_column] = df[col1] * df[col2] * df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mask = generate_train_mask(df, train_frac=0.8)\n",
    "mask = mask & (~df.gtsf_fish_identifier.str.contains('190620')) & (df.weight > 2000)\n",
    "columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "X_train = df.loc[mask, columns].values\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts_subset\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_model, open('/root/data/models/new_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate world keypoints given new calbration </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_world_point(x, y, d, pixel_count_width, \n",
    "                           pixel_count_height, image_sensor_width, \n",
    "                           image_sensor_height, focal_length):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    \n",
    "    image_center_x = pixel_count_width / 2.0  \n",
    "    image_center_y = pixel_count_height / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (image_sensor_width / pixel_count_width)\n",
    "    sensor_z = px_z * (image_sensor_height / pixel_count_height)\n",
    "\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / focal_length\n",
    "    world_z = (world_y * sensor_z) / focal_length\n",
    "    return [world_x, world_y, world_z]\n",
    "\n",
    "\n",
    "\n",
    "def depth_from_disp(disp, focal_length_pixel, baseline):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    depth = focal_length_pixel*baseline / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params_file):\n",
    "    params = json.load(open(params_file))\n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    imageSize = (4096, 3000)\n",
    "    \n",
    "    # perform rectification\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T, None, None, None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "    \n",
    "\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "    \n",
    "    return left_maps, right_maps, cameraMatrix1, distCoeffs1, R1, P1, cameraMatrix2, distCoeffs2, R2, P2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_params_old_file, stereo_params_new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints_new_list = []\n",
    "right_keypoints_new_list = []\n",
    "world_keypoints_new_list = []\n",
    "\n",
    "\n",
    "# get old stereo parameters files\n",
    "\n",
    "stereo_params_old = {\n",
    "    'bucket': 'aquabyte-stereo-parameters',\n",
    "    'key': 'L40013178_R40014310/2019-03-05T00:00:00Z_L40013178_R40014310_stereo-parameters.json'\n",
    "}\n",
    "\n",
    "stereo_params_new = {\n",
    "    'bucket': 'aquabyte-stereo-parameters',\n",
    "    'key': 'L40013178_R40014310/2019-07-16T00:00:00Z_L40013178_R40014310_stereo-parameters.json'\n",
    "}\n",
    "\n",
    "stereo_params_old_file = s3_access_utils.download_from_s3(stereo_params_old['bucket'], stereo_params_old['key'])\n",
    "stereo_params_new_file = s3_access_utils.download_from_s3(stereo_params_new['bucket'], stereo_params_new['key'])\n",
    "\n",
    "# get parameter for old and new stereo parameters\n",
    "left_maps, right_maps, cameraMatrix1, distCoeffs1, R1, P1, cameraMatrix2, distCoeffs2, R2, P2 = load_params(stereo_params_old_file)\n",
    "stereo_params_old.update({\n",
    "    'left_maps': left_maps,\n",
    "    'right_maps': right_maps,\n",
    "    'cameraMatrix1': cameraMatrix1,\n",
    "    'distCoeffs1': distCoeffs1,\n",
    "    'R1': R1,\n",
    "    'P1': P1,\n",
    "    'cameraMatrix2': cameraMatrix2,\n",
    "    'distCoeffs2': distCoeffs2,\n",
    "    'R2': R2,\n",
    "    'P2': P2\n",
    "})\n",
    "\n",
    "\n",
    "left_maps, right_maps, cameraMatrix1, distCoeffs1, R1, P1, cameraMatrix2, distCoeffs2, R2, P2 = load_params(stereo_params_new_file)\n",
    "stereo_params_new.update({\n",
    "    'left_maps': left_maps,\n",
    "    'right_maps': right_maps,\n",
    "    'cameraMatrix1': cameraMatrix1,\n",
    "    'distCoeffs1': distCoeffs1,\n",
    "    'R1': R1,\n",
    "    'P1': P1,\n",
    "    'cameraMatrix2': cameraMatrix2,\n",
    "    'distCoeffs2': distCoeffs2,\n",
    "    'R2': R2,\n",
    "    'P2': P2\n",
    "})\n",
    "\n",
    "stereo_params = json.load(open(stereo_params_new_file))\n",
    "baseline = abs(stereo_params['TranslationOfCamera2'][0] / 1e3) # convert millimeters to meters and use absolute value\n",
    "focal_length_pixel = stereo_params['CameraParameters1']['FocalLength'][0]\n",
    "pixel_size_m = 3.45 * 1e-6\n",
    "focal_length = focal_length_pixel * pixel_size_m\n",
    "image_sensor_width = 0.01412\n",
    "image_sensor_height = 0.01035\n",
    "pixel_count_width = 4096\n",
    "pixel_count_height = 3000\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # get old keypoint coordinates\n",
    "    left_keypoints_old = row.left_keypoints\n",
    "    right_keypoints_old = row.right_keypoints\n",
    "    \n",
    "    # generate new left keypoint coordinates based on new rectification\n",
    "    left_keypoints_new = {}\n",
    "    for body_part, coordinates in left_keypoints_old.items():\n",
    "        j, i = coordinates[0], coordinates[1]\n",
    "        unrectified_coordinates = stereo_params_old['left_maps'][0][i, j]\n",
    "        rerectified_coordinates = cv2.undistortPoints(\n",
    "            np.array([[unrectified_coordinates]]).astype(float), \n",
    "            stereo_params_new['cameraMatrix1'], \n",
    "            stereo_params_new['distCoeffs1'], \n",
    "            R=stereo_params_new['R1'], \n",
    "            P=stereo_params_new['P1']\n",
    "        )\n",
    "        \n",
    "        i_new, j_new = int(round(rerectified_coordinates[0][0][1])), int(round(rerectified_coordinates[0][0][0]))\n",
    "        new_coordinates = [j_new, i_new]\n",
    "        left_keypoints_new[body_part] = new_coordinates\n",
    "        \n",
    "    # generate new right keypoint coordinates based on new rectification\n",
    "    right_keypoints_new = {}\n",
    "    for body_part, coordinates in right_keypoints_old.items():\n",
    "        j, i = coordinates[0], coordinates[1]\n",
    "        unrectified_coordinates = stereo_params_old['right_maps'][0][i, j]\n",
    "        rerectified_coordinates = cv2.undistortPoints(\n",
    "            np.array([[unrectified_coordinates]]).astype(float), \n",
    "            stereo_params_new['cameraMatrix2'], \n",
    "            stereo_params_new['distCoeffs2'], \n",
    "            R=stereo_params_new['R2'], \n",
    "            P=stereo_params_new['P2']\n",
    "        )\n",
    "        \n",
    "        i_new, j_new = int(round(rerectified_coordinates[0][0][1])), int(round(rerectified_coordinates[0][0][0]))\n",
    "        new_coordinates = [j_new, i_new]\n",
    "        right_keypoints_new[body_part] = new_coordinates\n",
    "        \n",
    "    # generate new world keypoints\n",
    "    world_keypoints_new = {}\n",
    "    for body_part in body_parts:\n",
    "        lkp = left_keypoints_new[body_part]\n",
    "        rkp = right_keypoints_new[body_part]\n",
    "\n",
    "        d = abs(lkp[0] - rkp[0])\n",
    "\n",
    "        # compute world key point\n",
    "        depth = depth_from_disp(d, focal_length_pixel, baseline)\n",
    "        wkp = convert_to_world_point(lkp[0], lkp[1], depth, pixel_count_width, \n",
    "                                     pixel_count_height, image_sensor_width, \n",
    "                                     image_sensor_height, focal_length)\n",
    "\n",
    "        world_keypoints_new[body_part] = wkp\n",
    "        \n",
    "    left_keypoints_new_list.append(left_keypoints_new)\n",
    "    right_keypoints_new_list.append(right_keypoints_new)\n",
    "    world_keypoints_new_list.append(world_keypoints_new)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['left_keypoints_new'] = left_keypoints_new_list\n",
    "df['right_keypoints_new'] = right_keypoints_new_list\n",
    "df['world_keypoints_new'] = world_keypoints_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.gtsf_fish_identifier == '190618010020_vikingfjord-sunde'].right_keypoints_new.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"version\": 2, \"leftCrop\": [{\"xCrop\": 119, \"yCrop\": 324, \"xFrame\": 753, \"yFrame\": 1542, \"keypointType\": \"UPPER_LIP\"}, {\"xCrop\": 211, \"yCrop\": 345, \"xFrame\": 845, \"yFrame\": 1563, \"keypointType\": \"EYE\"}, {\"xCrop\": 954, \"yCrop\": 97, \"xFrame\": 1588, \"yFrame\": 1315, \"keypointType\": \"DORSAL_FIN\"}, {\"xCrop\": 1642, \"yCrop\": 174, \"xFrame\": 2276, \"yFrame\": 1392, \"keypointType\": \"ADIPOSE_FIN\"}, {\"xCrop\": 1875, \"yCrop\": 227, \"xFrame\": 2509, \"yFrame\": 1445, \"keypointType\": \"UPPER_PRECAUDAL_PIT\"}, {\"xCrop\": 2036, \"yCrop\": 311, \"xFrame\": 2670, \"yFrame\": 1529, \"keypointType\": \"HYPURAL_PLATE\"}, {\"xCrop\": 2150, \"yCrop\": 314, \"xFrame\": 2784, \"yFrame\": 1532, \"keypointType\": \"TAIL_NOTCH\"}, {\"xCrop\": 1861, \"yCrop\": 387, \"xFrame\": 2495, \"yFrame\": 1605, \"keypointType\": \"LOWER_PRECAUDAL_PIT\"}, {\"xCrop\": 1541, \"yCrop\": 506, \"xFrame\": 2175, \"yFrame\": 1724, \"keypointType\": \"ANAL_FIN\"}, {\"xCrop\": 1126, \"yCrop\": 610, \"xFrame\": 1760, \"yFrame\": 1828, \"keypointType\": \"PELVIC_FIN\"}, {\"xCrop\": 435, \"yCrop\": 524, \"xFrame\": 1069, \"yFrame\": 1742, \"keypointType\": \"PECTORAL_FIN\"}], \"rightCrop\": [{\"xCrop\": 94, \"yCrop\": 322, \"xFrame\": 266, \"yFrame\": 1540, \"keypointType\": \"UPPER_LIP\"}, {\"xCrop\": 170, \"yCrop\": 341, \"xFrame\": 342, \"yFrame\": 1559, \"keypointType\": \"EYE\"}, {\"xCrop\": 910, \"yCrop\": 94, \"xFrame\": 1082, \"yFrame\": 1312, \"keypointType\": \"DORSAL_FIN\"}, {\"xCrop\": 1623, \"yCrop\": 169, \"xFrame\": 1795, \"yFrame\": 1387, \"keypointType\": \"ADIPOSE_FIN\"}, {\"xCrop\": 1859, \"yCrop\": 224, \"xFrame\": 2031, \"yFrame\": 1442, \"keypointType\": \"UPPER_PRECAUDAL_PIT\"}, {\"xCrop\": 2029, \"yCrop\": 303, \"xFrame\": 2201, \"yFrame\": 1521, \"keypointType\": \"HYPURAL_PLATE\"}, {\"xCrop\": 2149, \"yCrop\": 305, \"xFrame\": 2321, \"yFrame\": 1523, \"keypointType\": \"TAIL_NOTCH\"}, {\"xCrop\": 1851, \"yCrop\": 382, \"xFrame\": 2023, \"yFrame\": 1600, \"keypointType\": \"LOWER_PRECAUDAL_PIT\"}, {\"xCrop\": 1526, \"yCrop\": 501, \"xFrame\": 1698, \"yFrame\": 1719, \"keypointType\": \"ANAL_FIN\"}, {\"xCrop\": 1092, \"yCrop\": 606, \"xFrame\": 1264, \"yFrame\": 1824, \"keypointType\": \"PELVIC_FIN\"}, {\"xCrop\": 393, \"yCrop\": 520, \"xFrame\": 565, \"yFrame\": 1738, \"keypointType\": \"PECTORAL_FIN\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    wkps = row.world_keypoints_new\n",
    "    df_row = {'0': idx}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(wkps[body_parts[i]], wkps[body_parts[j]])\n",
    "            df_row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    df_row['weight'] = row.weight\n",
    "    df_row['gtsf_fish_identifier'] = row.gtsf_fish_identifier\n",
    "    new_df = new_df.append(df_row, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Clean the data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.gtsf_fish_identifier != '190620-4e4e0640-d4eb-405d-8fcf-57fda11d7660'].copy(deep=True)\n",
    "new_df = new_df[new_df.gtsf_fish_identifier != '190620-4e4e0640-d4eb-405d-8fcf-57fda11d7660'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train model on new calibration data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "])\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        new_df[interaction_column] = new_df[col1] * new_df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            new_df[interaction_column] = new_df[col1] * new_df[col2] * new_df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mask = generate_train_mask(new_df, train_frac=0.8)\n",
    "mask = mask & (~new_df.gtsf_fish_identifier.str.contains('190620'))\n",
    "columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "X_train = new_df.loc[mask, columns].values\n",
    "y_train = new_df.loc[mask, 'weight'].values\n",
    "X_test = new_df.loc[~mask, columns].values\n",
    "y_test = new_df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(new_df[columns].values)))\n",
    "new_df['prediction'] = y_pred\n",
    "new_df['error'] = new_df.prediction - new_df.weight\n",
    "new_df['error_pct'] = new_df.error / new_df.weight\n",
    "new_df['abs_error_pct'] = new_df.error_pct.abs()\n",
    "\n",
    "new_model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts_subset\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(np.abs((y_pred - new_df.weight.values)/new_df.weight.values), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(100 * np.abs((y_pred - new_df.weight.values)/new_df.weight.values), bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = []\n",
    "for p in percentiles:\n",
    "    v = np.percentile(np.abs((y_pred - new_df.weight.values)/new_df.weight.values), p)\n",
    "    vs.append(v)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(percentiles, vs)\n",
    "plt.grid()\n",
    "plt.ylim([0, 0.4])\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Percent deviation in biomass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator(mask.values, new_df.prediction.values, new_df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.plot_predictions_vs_ground_truth(impose_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Disparity value comparison </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_diffs = []\n",
    "for idx, row in df.iterrows():\n",
    "    left_keypoints_old = row.left_keypoints\n",
    "    right_keypoints_old = row.right_keypoints\n",
    "    left_keypoints_new = row.left_keypoints_new\n",
    "    right_keypoints_new = row.right_keypoints_new\n",
    "    for body_part in body_parts_subset:\n",
    "        disp_old = abs(left_keypoints_old[body_part][0] - right_keypoints_old[body_part][0])\n",
    "        disp_new = abs(left_keypoints_new[body_part][0] - right_keypoints_new[body_part][0])\n",
    "        disp_diffs.append(disp_new - disp_old)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(np.array(disp_diffs), bins=20)\n",
    "plt.title('Distribution of disparity differences between pre- and post-Axiom calibrations')\n",
    "plt.xlabel('Disparity difference between pre- and post-Axiom calibrations')\n",
    "plt.ylabel('Count')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pairwise Distance Comparison </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame()\n",
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        diffs = new_df['{0}-{1}'.format(i, j)] - df['{0}-{1}'.format(i, j)]\n",
    "        pct_diffs = diffs / df['{0}-{1}'.format(i, j)]\n",
    "        \n",
    "        row = {}\n",
    "        row['pairwise_distance'] = '{0}<->{1}'.format(body_parts[i], body_parts[j])\n",
    "        percentiles = list(np.arange(0, 100, 10))\n",
    "        for percentile in percentiles:\n",
    "            row['diff_{}th_percentile'.format(percentile)] = np.percentile(diffs, percentile)\n",
    "            row['pct_diff_{}th_percentile'.format(percentile)] = np.percentile(pct_diffs, percentile)\n",
    "        analysis_df = analysis_df.append(row, ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diffs_all = []\n",
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        diffs = new_df['{0}-{1}'.format(i, j)] - df['{0}-{1}'.format(i, j)]\n",
    "        pct_diffs = diffs / df['{0}-{1}'.format(i, j)]\n",
    "        pct_diffs_all.extend(pct_diffs.tolist())\n",
    "        \n",
    "pct_diffs_all = np.array(pct_diffs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = list(np.arange(0, 100, 1))\n",
    "values = []\n",
    "for percentile in percentiles:\n",
    "    value = np.percentile(pct_diffs_all, percentile)\n",
    "    values.append(value)\n",
    "    \n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(percentiles, values)\n",
    "plt.grid()\n",
    "plt.ylim([0, 0.4])\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Percent deviation in pairwise distance')\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Figure out worst cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = df.copy(deep=True)\n",
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        analysis_df['{0}-{1}_new'.format(i, j)] = new_df['{0}-{1}'.format(i, j)]\n",
    "        analysis_df['{0}-{1}_diff'.format(i, j)] = new_df['{0}-{1}'.format(i, j)] - df['{0}-{1}'.format(i, j)]\n",
    "        analysis_df['{0}-{1}_pct_diff'.format(i, j)] = analysis_df['{0}-{1}_diff'.format(i, j)] / df['{0}-{1}'.format(i, j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        analysis_df[analysis_df['{}-{}_pct_diff'.format(i, j)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
