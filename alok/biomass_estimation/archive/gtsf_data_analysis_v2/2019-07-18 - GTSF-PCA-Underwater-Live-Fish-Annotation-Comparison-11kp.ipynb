{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase I: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.optics import euclidean_distance\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "research_rds_access_utils = RDSAccessUtils(research_sql_credentials)\n",
    "sql_engine = research_rds_access_utils.sql_engine\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create training dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "sfps_all = session.query(StereoFramePair).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "session.rollback()\n",
    "for idx, row in enumerate(sfps_all):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    # get fish_id and ground truth metadata\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "    left_keypoints = json.loads(row.left_image_keypoint_coordinates)\n",
    "    right_keypoints = json.loads(row.right_image_keypoint_coordinates)\n",
    "    wkps = json.loads(row.world_keypoint_coordinates)\n",
    "\n",
    "    df_row = {'0': idx}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(wkps[body_parts[i]], wkps[body_parts[j]])\n",
    "            df_row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    weight, length, kfactor = None, None, None\n",
    "    if 'data' in ground_truth_metadata.keys():\n",
    "        keys = ground_truth_metadata['data'].keys()\n",
    "        if 'weight' in keys or 'weightKgs' in keys:\n",
    "            weightKey = 'weight' if 'weight' in keys else 'weightKgs'\n",
    "            lengthKey = 'length' if 'length' in keys else 'lengthMms'\n",
    "            weight = ground_truth_metadata['data'][weightKey]\n",
    "            length = ground_truth_metadata['data'][lengthKey]\n",
    "            kfactor = (weight / length**3) * 1e5\n",
    "    if not weight:\n",
    "        print('No weight recorded for GTSF fish identifier: {}'.format(row.gtsf_fish_identifier))\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # calculate curvature\n",
    "    wkp = {bp: [wkps[bp][2], wkps[bp][1], wkps[bp][0]] for bp in body_parts}\n",
    "    fv1 = np.array(wkp['UPPER_LIP']) - np.array(wkp['DORSAL_FIN'])\n",
    "    fv2 = np.array(wkp['UPPER_LIP']) - np.array(wkp['PELVIC_FIN'])\n",
    "    n1 = np.cross(fv1, fv2)\n",
    "    \n",
    "    bv1 = np.array(wkp['PELVIC_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    bv2 = np.array(wkp['DORSAL_FIN']) -  np.array(wkp['TAIL_NOTCH'])\n",
    "    n2 = np.cross(bv1, bv2)\n",
    "    curvature_theta = (180 / np.pi) * np.arccos(np.dot(n1, n2) / (np.linalg.norm(n1) * np.linalg.norm(n2)))\n",
    "    \n",
    "    df_row['weight'] = weight\n",
    "    df_row['length'] = length\n",
    "    df_row['kfactor'] = kfactor\n",
    "    df_row['date'] = row.date\n",
    "    df_row['project_name'] = row.annotations_project_name\n",
    "    df_row['left_keypoints'] = json.loads(row.left_image_keypoint_coordinates)\n",
    "    df_row['right_keypoints'] = json.loads(row.right_image_keypoint_coordinates)\n",
    "    df_row['world_keypoints'] = wkps\n",
    "    df_row['gtsf_fish_identifier'] = row.gtsf_fish_identifier\n",
    "    df_row['epoch'] = row.epoch\n",
    "    df_row['stereo_frame_pair_id'] = row.id\n",
    "    df_row['curvature_theta'] = curvature_theta\n",
    "        \n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply filters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache = df.copy()\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache.to_hdf('/root/data/df_cache.h5', 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from cached location to avoid having to regenerate data\n",
    "\n",
    "df = pd.read_hdf('/root/data/df_cache.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cache.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.stereo_frame_pair_id != 6137)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Labelbox + new calibration + 11 keypoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "#     'UPPER_PRECAUDAL_PIT',\n",
    "#     'LOWER_PRECAUDAL_PIT',\n",
    "#     'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        df[interaction_column] = df[col1] * df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            df[interaction_column] = df[col1] * df[col2] * df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mask = generate_train_mask(df, train_frac=0.8)\n",
    "mask = mask & (~df.gtsf_fish_identifier.str.contains('190620'))\n",
    "columns = pairwise_distance_columns + interaction_columns_quadratic #+ interaction_columns_cubic\n",
    "\n",
    "X_train = df.loc[mask, columns].values\n",
    "print(X_train.sum())\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "print(idx)\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "print(reg.score(X_test_transformed, y_test))\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts   \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.plot_predictions_vs_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "body_parts_subset = sorted([\n",
    "    'HYPURAL_PLATE',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "])\n",
    "\n",
    "body_part_indices = [body_parts.index(bp) for bp in body_parts_subset]\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(body_part_indices, 2))]\n",
    "interaction_columns_quadratic = []\n",
    "interaction_columns_cubic = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        df[interaction_column] = df[col1] * df[col2]\n",
    "        interaction_columns_quadratic.append(interaction_column)\n",
    "        \n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        for k in range(j, len(pairwise_distance_columns)):\n",
    "            col1 = pairwise_distance_columns[i]\n",
    "            col2 = pairwise_distance_columns[j]\n",
    "            col3 = pairwise_distance_columns[k]\n",
    "            interaction_column = '{},{},{}'.format(col1, col2, col3)\n",
    "            df[interaction_column] = df[col1] * df[col2] * df[col3]\n",
    "            interaction_columns_cubic.append(interaction_column)\n",
    "            \n",
    "cs2 = pairwise_distance_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mask = generate_train_mask(df, train_frac=0.8)\n",
    "mask = mask & (~df.gtsf_fish_identifier.str.contains('190620'))\n",
    "columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "X_train = df.loc[mask, columns].values\n",
    "print(X_train.sum())\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "print(idx)\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "print(reg.score(X_test_transformed, y_test))\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts   \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg2 = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)\n",
    "amg2.plot_predictions_vs_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amg2.display_train_test_accuracy_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.prediction < 6000) & (df.weight > 8000)].iloc[0].epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from cached location to avoid having to regenerate data\n",
    "\n",
    "df2 = pd.read_hdf('/root/data/df_cache.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('date').agg(len)['gtsf_fish_identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df2.epoch).difference(set(df.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('date').agg(len)['gtsf_fish_identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ensure apples-to-apples comparison </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\n",
    "    'Underwater Live GTSF - Axiom Calibration Full',\n",
    "    'Underwater Live GTSF - Axiom Calibration - Filtered - Team 1',\n",
    "    'Underwater Live GTSF - Axiom Calibration - Filtered - Team 2'\n",
    "]\n",
    "\n",
    "common_epochs = None\n",
    "\n",
    "for project_name in projects:\n",
    "    project_mask = df.project_name == project_name\n",
    "    epochs = set(df[project_mask].epoch.unique())\n",
    "    if not common_epochs:\n",
    "        common_epochs = epochs\n",
    "    else:\n",
    "        common_epochs = common_epochs.intersection(epochs)\n",
    "    \n",
    "common_epochs = sorted(list(common_epochs))\n",
    "tdf = df[df.epoch.isin(common_epochs)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "biomass_error_pcts = []\n",
    "for i in range(N):\n",
    "    print(i)\n",
    "    mask = generate_train_mask(df, train_frac=0.8)\n",
    "    mask = mask & (~df.gtsf_fish_identifier.str.contains('190620'))\n",
    "    columns = pairwise_distance_columns + interaction_columns_quadratic + interaction_columns_cubic\n",
    "\n",
    "    X_train = df.loc[mask, columns].values\n",
    "    y_train = df.loc[mask, 'weight'].values\n",
    "    X_test = df.loc[~mask, columns].values\n",
    "    y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "    pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "    pca.fit(X_train_normalized)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "    idx = np.where(explained_variance_ratio > 0.999999)[0][0]\n",
    "\n",
    "    pca = PCA(n_components=idx+1)\n",
    "    pca.fit(X_train_normalized)\n",
    "    X_train_transformed = pca.transform(X_train_normalized)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "\n",
    "    y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "    df['prediction'] = y_pred\n",
    "    df['error'] = df.prediction - df.weight\n",
    "    df['error_pct'] = df.error / df.weight\n",
    "    df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "    model = {\n",
    "        'mean': scaler.mean_,\n",
    "        'std': scaler.scale_,\n",
    "        'PCA_components': pca.components_,\n",
    "        'reg_coef': reg.coef_,\n",
    "        'reg_intercept': reg.intercept_,\n",
    "        'body_parts': body_parts   \n",
    "    }\n",
    "    \n",
    "    amg = AccuracyMetricsGenerator(mask.values, df.prediction.values, df.weight.values)\n",
    "    accuracy_metrics = amg.generate_train_test_accuracy_metrics()\n",
    "    biomass_error_pct = accuracy_metrics['test']['biomass_error_pct']\n",
    "    biomass_error_pcts.append(biomass_error_pct)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = sorted(list([abs(x) for x in biomass_error_pcts]))\n",
    "p = 1.0 * np.arange(len(data_sorted)) / (len(data_sorted) - 1)\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(p, data_sorted)\n",
    "ax1.set_xlabel('p')\n",
    "ax1.set_ylabel('OOS error percentage')\n",
    "plt.axvline(x=0.95, linestyle='--', color='red', label='p = 0.95')\n",
    "plt.title('CDF of OOS errors (sample size = 250)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Overall prediction distribution comparison between Team 1 and Team 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\n",
    "    'Underwater Live GTSF - Axiom Calibration Full',\n",
    "    'Underwater Live GTSF - Axiom Calibration - Filtered - Team 1',\n",
    "    'Underwater Live GTSF - Axiom Calibration - Filtered - Team 2'\n",
    "]\n",
    "for project_name in projects:\n",
    "    project_mask = tdf.project_name == project_name\n",
    "    average_error = (tdf[project_mask].prediction.mean() - 1500.0) / 1500.0\n",
    "    print('Average prediction error: {}%'.format(round(average_error * 100.0, 2)))\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('Predicted biomass histogram for project: {}'.format(project_name))\n",
    "    plt.hist(tdf[project_mask].prediction, bins=20)\n",
    "    plt.axvline(1500, color='red', label='Ground Truth Weight')\n",
    "    plt.xlabel('Biomass prediction for single live fish (grams)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Per-stereo-frame level prediction comparison between Team 1 and Team 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_analysis_df = pd.DataFrame()\n",
    "for epoch in common_epochs:\n",
    "    epoch_mask = tdf.epoch == epoch\n",
    "    team_1_prediction = tdf[(tdf.project_name == projects[0]) & epoch_mask].prediction.iloc[0]\n",
    "    team_2_prediction = tdf[(tdf.project_name == projects[1]) & epoch_mask].prediction.iloc[0]\n",
    "    weight = tdf[(tdf.project_name == projects[0]) & epoch_mask].weight.iloc[0]\n",
    "    row = {}\n",
    "    row['epoch'] = epoch\n",
    "    row['team_1_prediction'] = team_1_prediction\n",
    "    row['team_2_prediction'] = team_2_prediction\n",
    "    row['weight'] = weight\n",
    "    \n",
    "    fish_analysis_df = fish_analysis_df.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = fish_analysis_df.team_1_prediction - fish_analysis_df.team_2_prediction\n",
    "difference_mean = differences.mean()\n",
    "difference_std = differences.std()\n",
    "print('Mean difference: {} grams'.format(round(differences.mean(), 2)))\n",
    "print('Standard deviation of difference: {} grams'.format(round(differences.std(), 2)))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Distribution of per-fish prediction differences between Team 1 and Team 2')\n",
    "plt.hist(differences, bins=10)\n",
    "plt.xlabel('Prediction difference (grams)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Per-point level comparison between Team 1 and Team 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_analysis_df = pd.DataFrame()\n",
    "for epoch in common_epochs:\n",
    "    \n",
    "    epoch_mask = tdf.epoch == epoch\n",
    "    team_1_left_keypoints = tdf[epoch_mask & (tdf.project_name == projects[0])].left_keypoints.iloc[0]\n",
    "    team_2_left_keypoints = tdf[epoch_mask & (tdf.project_name == projects[1])].left_keypoints.iloc[0]\n",
    "    team_1_right_keypoints = tdf[epoch_mask & (tdf.project_name == projects[0])].right_keypoints.iloc[0]\n",
    "    team_2_right_keypoints = tdf[epoch_mask & (tdf.project_name == projects[1])].right_keypoints.iloc[0]\n",
    "    \n",
    "    for body_part, team_1_left_keypoint in team_1_left_keypoints.items():\n",
    "        team_2_left_keypoint = team_2_left_keypoints[body_part]\n",
    "        x_diff = team_1_left_keypoint[0] - team_2_left_keypoint[0]\n",
    "        y_diff = team_1_left_keypoint[1] - team_2_left_keypoint[1]\n",
    "        row = {}\n",
    "        row['epoch'] = epoch\n",
    "        row['body_part'] = body_part\n",
    "        row['side'] = 'left'\n",
    "        row['x_diff'] = x_diff\n",
    "        row['y_diff'] = y_diff\n",
    "        \n",
    "        keypoint_analysis_df = keypoint_analysis_df.append(row, ignore_index=True)\n",
    "        \n",
    "    for body_part, team_1_right_keypoint in team_1_right_keypoints.items():\n",
    "        team_2_right_keypoint = team_2_right_keypoints[body_part]\n",
    "        x_diff = team_1_right_keypoint[0] - team_2_right_keypoint[0]\n",
    "        y_diff = team_1_right_keypoint[1] - team_2_right_keypoint[1]\n",
    "        row = {}\n",
    "        row['epoch'] = epoch\n",
    "        row['body_part'] = body_part\n",
    "        row['side'] = 'right'\n",
    "        row['x_diff'] = x_diff\n",
    "        row['y_diff'] = y_diff\n",
    "        \n",
    "        keypoint_analysis_df = keypoint_analysis_df.append(row, ignore_index=True)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('Predicted biomass histogram for underwater live fish')\n",
    "plt.hist(df[df.gtsf_fish_identifier == '190620-4e4e0640-d4eb-405d-8fcf-57fda11d7660'].prediction, bins=20)\n",
    "plt.axvline(1500, color='red', label='Ground Truth Weight')\n",
    "plt.xlabel('Biomass prediction for single live fish (grams)')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.gtsf_fish_identifier == '190620-4e4e0640-d4eb-405d-8fcf-57fda11d7660'].prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Investigate Individual Cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access_utils = DataAccessUtils('/root/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_stereo_frame_pair(stereo_frame_pair_id):\n",
    "    sfp = session.query(StereoFramePair).filter(StereoFramePair.id == stereo_frame_pair_id).all()[0]\n",
    "    left_image_s3_key = sfp.left_image_s3_key\n",
    "    right_image_s3_key = sfp.right_image_s3_key\n",
    "    image_s3_bucket = sfp.image_s3_bucket\n",
    "    left_image_keypoint_coordinates = json.loads(sfp.left_image_keypoint_coordinates)\n",
    "    right_image_keypoint_coordinates = json.loads(sfp.right_image_keypoint_coordinates)\n",
    "    \n",
    "    \n",
    "    left_image_f = data_access_utils.download_from_s3(image_s3_bucket, left_image_s3_key)\n",
    "    right_image_f = data_access_utils.download_from_s3(image_s3_bucket, right_image_s3_key)\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(30, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    for bp, coords in left_image_keypoint_coordinates.items():\n",
    "        axes[0].scatter(coords[0], coords[1], s=2, label=bp, color='red')\n",
    "    \n",
    "    axes[1].imshow(right_image)\n",
    "    for bp, coords in right_image_keypoint_coordinates.items():\n",
    "        axes[1].scatter(coords[0], coords[1], s=2, label=bp, color='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "  'gtsf_fish_identifier', \n",
    "  'epoch', \n",
    "  'prediction', \n",
    "  'stereo_frame_pair_id', \n",
    "  'abs_error_pct',\n",
    "  'error_pct',\n",
    "  '6-7',\n",
    "  'curvature_theta'\n",
    "]\n",
    "tdf = df.ix[df.weight == 1500, features].sort_values('error_pct')\n",
    "\n",
    "for idx, row in tdf.iterrows():\n",
    "    stereo_frame_pair_id = row.stereo_frame_pair_id\n",
    "    prediction = row.prediction\n",
    "    error_pct = row.error_pct\n",
    "    print('Prediction: {0}, Error Percentage: {1}'.format(prediction, error_pct))\n",
    "    visualize_stereo_frame_pair(stereo_frame_pair_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_stereo_frame_pair(3805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ix[df.weight == 1500, \n",
    "      [\n",
    "          'gtsf_fish_identifier', \n",
    "          'epoch', \n",
    "          'prediction', \n",
    "          'stereo_frame_pair_id', \n",
    "          'error_pct',\n",
    "          'abs_error_pct', \n",
    "          '6-7',\n",
    "          'curvature_theta'\n",
    "      ]\n",
    "     ].sort_values('error_pct')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp = session.query(StereoFramePair).filter(StereoFramePair.id == 3756).all()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp.left_image_keypoint_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp.right_image_keypoint_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df.ix[df.weight == 1500, \n",
    "      [\n",
    "          'gtsf_fish_identifier', \n",
    "          'epoch', \n",
    "          'prediction', \n",
    "          'stereo_frame_pair_id', \n",
    "          'abs_error_pct', \n",
    "          '6-7',\n",
    "          'curvature_theta'\n",
    "      ]\n",
    "     ].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tdf.prediction.mean() - 1500)/1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
