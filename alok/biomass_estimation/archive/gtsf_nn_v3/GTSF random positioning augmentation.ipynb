{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.weight_estimation.weight_estimator import WeightEstimator\n",
    "from research.gtsf_data.gtsf_dataset import GTSFDataset\n",
    "from research.gtsf_data.body_parts import BodyParts\n",
    "from research.utils.keypoint_transformations import get_keypoint_arr\n",
    "from research.utils.optics import pixel2world\n",
    "from research.weight_estimation.akpd_scorer import generate_confidence_score\n",
    "from scipy.spatial.distance import pdist\n",
    "from research.weight_estimation.data_loader import *\n",
    "from research.weight_estimation.biomass_estimator import *\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "from research.utils.optics import pixel2world\n",
    "from research.weight_estimation.akpd_scorer import generate_confidence_score\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.gtsf_data.body_parts import BodyParts\n",
    "from research.utils.keypoint_transformations import get_keypoint_arr, get_raw_3d_coordinates\n",
    "\n",
    "BODY_PARTS = BodyParts().get_core_body_parts()\n",
    "\n",
    "\n",
    "class GTSFDataset(object):\n",
    "\n",
    "    def __init__(self, start_date, end_date, akpd_scorer_url, species='salmon', add_template_matching_keypoints=False):\n",
    "        self.s3_access_utils = S3AccessUtils('/root/data')\n",
    "        self.df = self.generate_raw_df(start_date, end_date)\n",
    "        self.prepare_df(akpd_scorer_url, species, add_template_matching_keypoints)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_raw_df(start_date, end_date):\n",
    "        rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "        query = \"\"\"\n",
    "            select * from research.fish_metadata a left join keypoint_annotations b\n",
    "            on a.left_url = b.left_image_url \n",
    "            where b.keypoints -> 'leftCrop' is not null\n",
    "            and b.keypoints -> 'rightCrop' is not null\n",
    "            and b.captured_at between '{0}' and '{1}';\n",
    "        \"\"\".format(start_date, end_date)\n",
    "        df = rds_access_utils.extract_from_database(query)\n",
    "        print('Raw dataframe loaded!')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_world_keypoints(row):\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "\n",
    "    def prepare_df(self, akpd_scorer_url, species, add_template_matching_keypoints):\n",
    "        # use QA'ed entries, and only use Cogito entries when QA data is unavailable\n",
    "        self.df = self.df[self.df.data.apply(lambda x: x['species'].lower()) == species].copy(deep=True)\n",
    "        qa_df = self.df[self.df.is_qa == True]\n",
    "        cogito_df = self.df[(self.df.is_qa != True) & ~(self.df.left_image_url.isin(qa_df.left_image_url))]\n",
    "        self.df = pd.concat([qa_df, cogito_df], axis=0)\n",
    "        print('Dataset preparation beginning...')\n",
    "\n",
    "        # add 3D spatial information\n",
    "        self.df['world_keypoints'] = self.df.apply(lambda x: self.get_world_keypoints(x), axis=1)\n",
    "        self.df['median_depth'] = self.df.world_keypoints.apply(lambda x: np.median([wkp[1] for wkp in x.values()]))\n",
    "        print('3D spatial information added!')\n",
    "\n",
    "        # add k-factor\n",
    "        self.df['k_factor'] = 1e5 * self.df.weight / self.df.data.apply(lambda x: x['lengthMms']**3).astype(float)\n",
    "        \n",
    "        # add AKPD scores and convert world keypoints to matrix form\n",
    "        self.add_akpd_scores(akpd_scorer_url)\n",
    "        if add_template_matching_keypoints:\n",
    "            self.add_template_matching_keypoints()\n",
    "        self.convert_wkps_to_matrix_form()\n",
    "    \n",
    "    @staticmethod\n",
    "    def in_hull(p, hull):\n",
    "        hull = Delaunay(hull)\n",
    "        return hull.find_simplex(p)>=0\n",
    "\n",
    "    def add_template_matching_keypoints(self):\n",
    "        print('Adding template matching body keypoints...')\n",
    "\n",
    "        # load data\n",
    "        gen = self.s3_access_utils.get_matching_s3_keys(\n",
    "            'aquabyte-research', \n",
    "            prefix='template-matching/2019-12-05T02:50:57', \n",
    "            suffixes=['.parquet']\n",
    "        )\n",
    "\n",
    "        keys = [key for key in gen]\n",
    "        f = self.s3_access_utils.download_from_s3('aquabyte-research', keys[0])\n",
    "        pdf = pd.read_parquet(f)\n",
    "        pdf['homography'] = pdf.homography_and_matches.apply(lambda x: np.array(x[0].tolist(), dtype=np.float))\n",
    "        pdf['matches'] = pdf.homography_and_matches.apply(lambda x: np.array(x[1].tolist(), dtype=np.int) if len(x) > 1 else None)\n",
    "\n",
    "        # merge with existing dataframe\n",
    "        self.df = pd.merge(self.df, pdf[['left_image_url', 'homography', 'matches']], how='inner', on='left_image_url')\n",
    "\n",
    "        # generate list of modified keypoints\n",
    "        modified_keypoints_list = []\n",
    "        count = 0\n",
    "        for idx, row in self.df.iterrows():\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "            count += 1\n",
    "            X_keypoints = np.array([[item['xFrame'], item['yFrame']] for item in row.keypoints['leftCrop']])\n",
    "            X_body = np.array(row.matches)\n",
    "            is_valid = self.in_hull(X_body[:, :2], X_keypoints)\n",
    "            X_body = X_body[np.where(is_valid)]\n",
    "            \n",
    "            keypoints = deepcopy(row.keypoints)\n",
    "            left_keypoints, right_keypoints = keypoints['leftCrop'], keypoints['rightCrop']\n",
    "            left_item = {'keypointType': 'BODY', 'xFrame': X_body[:, 0], 'yFrame': X_body[:, 1]}\n",
    "            right_item = {'keypointType': 'BODY', 'xFrame': X_body[:, 2], 'yFrame': X_body[:, 3]}\n",
    "            \n",
    "            left_keypoints.append(left_item)\n",
    "            right_keypoints.append(right_item)\n",
    "            modified_keypoints = {'leftCrop': left_keypoints, 'rightCrop': right_keypoints}\n",
    "            modified_keypoints_list.append(modified_keypoints)\n",
    "\n",
    "        # add modified keypoints information to dataframe\n",
    "        self.df['old_keypoints'] = self.df.keypoints\n",
    "        self.df['keypoints'] = modified_keypoints_list\n",
    "        self.df = self.df[self.df.keypoints.apply(lambda x: x['leftCrop'][-1]['xFrame'].shape[0]) > 500]\n",
    "\n",
    "    def add_akpd_scores(self, akpd_scorer_url):\n",
    "        print('Adding AKPD scores...')\n",
    "        # load neural network weights\n",
    "        akpd_scorer_path, _, _ = self.s3_access_utils.download_from_url(akpd_scorer_url)\n",
    "        akpd_scorer_network = load_model(akpd_scorer_path)\n",
    "\n",
    "        akpd_scores = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            input_sample = {\n",
    "                'keypoints': row.keypoints,\n",
    "                'cm': row.camera_metadata,\n",
    "                'stereo_pair_id': row.id,\n",
    "                'single_point_inference': True\n",
    "            }\n",
    "            akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "            akpd_scores.append(akpd_score)\n",
    "        self.df['akpd_score'] = akpd_scores\n",
    "\n",
    "    def convert_wkps_to_matrix_form(self):\n",
    "        print('Converting world keypoints to matrix form...')\n",
    "        raw_keypoint_arr_list, norm_keypoint_arr_list = [], []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            keypoints, cm = row.keypoints, row.camera_metadata\n",
    "            try:\n",
    "                raw_keypoint_arr, norm_keypoint_arr = get_keypoint_arr(keypoints, cm)\n",
    "            except:\n",
    "                print(row)\n",
    "                raw_keypoint_arr, norm_keypoint_arr = None, None\n",
    "                \n",
    "            raw_keypoint_arr_list.append(raw_keypoint_arr)\n",
    "            norm_keypoint_arr_list.append(norm_keypoint_arr)\n",
    "        self.df['raw_keypoint_arr'] = raw_keypoint_arr_list\n",
    "        self.df['norm_keypoint_arr'] = norm_keypoint_arr_list\n",
    "\n",
    "    def get_prepared_dataset(self):\n",
    "        return self.df\n",
    "\n",
    "    @staticmethod\n",
    "    def randomly_rotate_and_translate(wkps, random_x_addition, random_y_addition,\n",
    "                                      random_z_addition, yaw, pitch, roll):\n",
    "\n",
    "        # convert to radians\n",
    "        yaw, pitch, roll = [theta * np.pi / 180.0 for theta in [yaw, pitch, roll]]\n",
    "\n",
    "        # compute rotation matrix\n",
    "        R_yaw = np.array([\n",
    "            [np.cos(yaw), -np.sin(yaw), 0],\n",
    "            [np.sin(yaw), np.cos(yaw), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "        R_pitch = np.array([\n",
    "            [np.cos(pitch), 0, np.sin(pitch)],\n",
    "            [0, 1, 0],\n",
    "            [-np.sin(pitch), 0, np.cos(pitch)]\n",
    "        ])\n",
    "\n",
    "        R_roll = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(roll), -np.sin(roll)],\n",
    "            [0, np.sin(roll), np.cos(roll)]\n",
    "        ])\n",
    "\n",
    "        R = np.dot(R_yaw, (np.dot(R_pitch, R_roll)))\n",
    "\n",
    "        # apply rotation\n",
    "        wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "        # perform translation\n",
    "        wkps[:, 0] += random_x_addition\n",
    "        wkps[:, 1] += random_y_addition\n",
    "        wkps[:, 2] += random_z_addition\n",
    "        return wkps\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_jitter(wkps, cm, jitter):\n",
    "        ann_left, ann_right = [], []\n",
    "        for idx, body_part in enumerate(BODY_PARTS):\n",
    "            # generate left item\n",
    "            x, y, z = wkps[idx]\n",
    "            x_frame_left = x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] / 2\n",
    "            y_frame_left = -z * cm['focalLengthPixel'] / y + cm['pixelCountHeight'] / 2\n",
    "            item_left = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_left + np.random.normal(0, jitter),\n",
    "                'yFrame': y_frame_left\n",
    "            }\n",
    "            ann_left.append(item_left)\n",
    "\n",
    "            # generate right item\n",
    "            disparity = cm['focalLengthPixel'] * cm['baseline'] / y\n",
    "            x_frame_right = x_frame_left - disparity\n",
    "            y_frame_right = y_frame_left\n",
    "            item_right = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_right + np.random.normal(0, jitter),\n",
    "                'yFrame': y_frame_right\n",
    "            }\n",
    "            ann_right.append(item_right)\n",
    "\n",
    "        jittered_kps = {'leftCrop': ann_left, 'rightCrop': ann_right}\n",
    "        jittered_wkps_arr = get_raw_3d_coordinates(jittered_kps, cm)\n",
    "        return jittered_kps, jittered_wkps_arr\n",
    "\n",
    "    def generate_augmented_dataset(self, x_bounds, y_bounds, z_bounds,\n",
    "                                   yaw_bounds, pitch_bounds, roll_bounds,\n",
    "                                   jitter, trials):\n",
    "\n",
    "        augmented_data = defaultdict(list)\n",
    "        for idx, row in self.df.iterrows():\n",
    "            cm = row.camera_metadata\n",
    "            original_wkps = row.raw_keypoint_arr\n",
    "            original_ann = row.keypoints\n",
    "            norm_wkps = row.norm_keypoint_arr\n",
    "            weight = row.weight\n",
    "            for t in range(trials):\n",
    "\n",
    "                # generate random position and orientation\n",
    "                random_x_addition = np.random.uniform(*x_bounds)\n",
    "                random_y_addition = np.random.uniform(*y_bounds)\n",
    "                random_z_addition = np.random.uniform(*z_bounds)\n",
    "                yaw = np.random.uniform(*yaw_bounds)\n",
    "                pitch = np.random.uniform(*pitch_bounds)\n",
    "                roll = np.random.uniform(*roll_bounds)\n",
    "\n",
    "                # perform rotation and translation\n",
    "                wkps = self.randomly_rotate_and_translate(deepcopy(norm_wkps), random_x_addition, random_y_addition,\n",
    "                                                          random_z_addition, yaw, pitch, roll)\n",
    "\n",
    "                # add jitter\n",
    "                ann, wkps = self.apply_jitter(wkps, cm, jitter)\n",
    "\n",
    "                # create output row\n",
    "                output_row = {\n",
    "                    'original_ann': original_ann,\n",
    "                    'original_wkps': original_wkps,\n",
    "                    'norm_wkps': norm_wkps,\n",
    "                    'wkps': wkps,\n",
    "                    'ann': ann,\n",
    "                    'cm': cm,\n",
    "                    'mean_x': random_x_addition,\n",
    "                    'mean_y': random_y_addition,\n",
    "                    'mean_z': random_z_addition,\n",
    "                    'yaw': yaw,\n",
    "                    'pitch': pitch,\n",
    "                    'roll': roll,\n",
    "                    'weight': weight,\n",
    "                    'jitter': jitter,\n",
    "                    'trial': t\n",
    "                }\n",
    "                for k, v in output_row.items():\n",
    "                    augmented_data[k].append(v)\n",
    "\n",
    "        augmented_df = pd.DataFrame(augmented_data)\n",
    "        return augmented_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.optics import pixel2world\n",
    "from research.gtsf_data.body_parts import BodyParts\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "BODY_PARTS = BodyParts().get_core_body_parts()\n",
    "\n",
    "\n",
    "def get_raw_3d_coordinates(keypoints, cm):\n",
    "    wkps = pixel2world([item for item in keypoints['leftCrop'] if item['keypointType'] != 'BODY'],\n",
    "                       [item for item in keypoints['rightCrop'] if item['keypointType'] != 'BODY'],\n",
    "                       cm)\n",
    "    all_wkps = [list(wkps[bp]) for bp in BODY_PARTS]\n",
    "\n",
    "    # compute BODY world keypoint coordinates\n",
    "    if 'BODY' in [item['keypointType'] for item in keypoints['leftCrop']]:\n",
    "        left_item = [item for item in keypoints['leftCrop'] if item['keypointType'] == 'BODY'][0]\n",
    "        right_item = [item for item in keypoints['rightCrop'] if item['keypointType'] == 'BODY'][0]\n",
    "        disps = np.abs(left_item['xFrame'] - right_item['xFrame'])\n",
    "        focal_length_pixel = cm[\"focalLengthPixel\"]\n",
    "        baseline = cm[\"baseline\"]\n",
    "        depths = focal_length_pixel * baseline / np.array(disps)\n",
    "\n",
    "        pixel_count_width = cm[\"pixelCountWidth\"]\n",
    "        pixel_count_height = cm[\"pixelCountHeight\"]\n",
    "        sensor_width = cm[\"imageSensorWidth\"]\n",
    "        sensor_height = cm[\"imageSensorHeight\"]\n",
    "        focal_length = cm[\"focalLength\"]\n",
    "\n",
    "        image_center_x = pixel_count_width / 2.0\n",
    "        image_center_y = pixel_count_height / 2.0\n",
    "        x = left_item['xFrame']\n",
    "        y = left_item['yFrame']\n",
    "        px_x = x - image_center_x\n",
    "        px_z = image_center_y - y\n",
    "\n",
    "        sensor_x = px_x * (sensor_width / pixel_count_width)\n",
    "        sensor_z = px_z * (sensor_height / pixel_count_height)\n",
    "\n",
    "        world_y = depths\n",
    "        world_x = (world_y * sensor_x) / focal_length\n",
    "        world_z = (world_y * sensor_z) / focal_length\n",
    "        wkps['BODY'] = np.column_stack([world_x, world_y, world_z])\n",
    "\n",
    "        body_wkps = random.sample([list(wkp) for wkp in list(wkps['BODY'])], 500)\n",
    "        all_wkps.extend(body_wkps)\n",
    "    return np.array(all_wkps)\n",
    "\n",
    "\n",
    "def normalize_3d_coordinates(wkps):\n",
    "\n",
    "    # translate fish to origin\n",
    "    v = np.mean(wkps[:8], axis=0)\n",
    "    wkps -= v\n",
    "\n",
    "    # perform PCA decomposition and rotate with respect to new axes\n",
    "    _, eigen_vectors = np.linalg.eig(np.dot(wkps.T, wkps))\n",
    "    wkps = np.dot(eigen_vectors.T, wkps.T).T\n",
    "\n",
    "    return wkps\n",
    "\n",
    "\n",
    "def get_keypoint_arr(keypoints, cm):\n",
    "    dorsal_fin_idx, pelvic_fin_idx = BODY_PARTS.index('DORSAL_FIN'), BODY_PARTS.index('PELVIC_FIN')\n",
    "    raw_wkps = get_raw_3d_coordinates(keypoints, cm)\n",
    "    norm_wkps = normalize_3d_coordinates(deepcopy(raw_wkps))\n",
    "    if any([item['keypointType'] == 'BODY' for item in keypoints['leftCrop']]):\n",
    "        body_norm_wkps = norm_wkps[8:, :]\n",
    "        mid_point = 0.5 * (norm_wkps[dorsal_fin_idx] + norm_wkps[pelvic_fin_idx])\n",
    "        idx = np.argmin(np.linalg.norm(body_norm_wkps[:, [0, 2]] - np.array([mid_point[0], mid_point[2]]), axis=1))\n",
    "        body_wkp = body_norm_wkps[idx]\n",
    "        keypoint_arr = np.vstack([norm_wkps[:8, :], body_wkp])\n",
    "        return raw_wkps, keypoint_arr\n",
    "    else:\n",
    "        return raw_wkps, norm_wkps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-02-01', '2019-09-20', akpd_scorer_url)\n",
    "df = gtsf_dataset.get_prepared_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = gtsf_dataset.generate_augmented_dataset((-0.1, 0.1), (0.8, 1.3), (-0.1, 0.1), \n",
    "                                                    (-45, 45), (-45, 45), (-5, 5), 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.optics import pixel2world\n",
    "from research.gtsf_data.body_parts import BodyParts\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "BODY_PARTS = BodyParts().get_core_body_parts()\n",
    "\n",
    "\n",
    "def get_raw_3D_coordinates(keypoints, cm):\n",
    "    wkps = pixel2world([item for item in keypoints['leftCrop'] if item['keypointType'] != 'BODY'],\n",
    "                       [item for item in keypoints['rightCrop'] if item['keypointType'] != 'BODY'],\n",
    "                       cm)\n",
    "    all_wkps = [list(wkps[bp]) for bp in BODY_PARTS]\n",
    "\n",
    "    # compute BODY world keypoint coordinates\n",
    "    if 'BODY' in [item['keypointType'] for item in keypoints['leftCrop']]:\n",
    "        left_item = [item for item in keypoints['leftCrop'] if item['keypointType'] == 'BODY'][0]\n",
    "        right_item = [item for item in keypoints['rightCrop'] if item['keypointType'] == 'BODY'][0]\n",
    "        disps = np.abs(left_item['xFrame'] - right_item['xFrame'])\n",
    "        focal_length_pixel = cm[\"focalLengthPixel\"]\n",
    "        baseline = cm[\"baseline\"]\n",
    "        depths = focal_length_pixel * baseline / np.array(disps)\n",
    "\n",
    "        pixel_count_width = cm[\"pixelCountWidth\"]\n",
    "        pixel_count_height = cm[\"pixelCountHeight\"]\n",
    "        sensor_width = cm[\"imageSensorWidth\"]\n",
    "        sensor_height = cm[\"imageSensorHeight\"]\n",
    "        focal_length = cm[\"focalLength\"]\n",
    "\n",
    "        image_center_x = pixel_count_width / 2.0\n",
    "        image_center_y = pixel_count_height / 2.0\n",
    "        x = left_item['xFrame']\n",
    "        y = left_item['yFrame']\n",
    "        px_x = x - image_center_x\n",
    "        px_z = image_center_y - y\n",
    "\n",
    "        sensor_x = px_x * (sensor_width / pixel_count_width)\n",
    "        sensor_z = px_z * (sensor_height / pixel_count_height)\n",
    "\n",
    "        world_y = depths\n",
    "        world_x = (world_y * sensor_x) / focal_length\n",
    "        world_z = (world_y * sensor_z) / focal_length\n",
    "        wkps['BODY'] = np.column_stack([world_x, world_y, world_z])\n",
    "\n",
    "        body_wkps = random.sample([list(wkp) for wkp in list(wkps['BODY'])], 500)\n",
    "        all_wkps.extend(body_wkps)\n",
    "    return np.array(all_wkps)\n",
    "\n",
    "\n",
    "def generate_rotation_matrix(n, theta):\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0] ** 2 * (1 - np.cos(theta)),\n",
    "        n[0] * n[1] * (1 - np.cos(theta)) - n[2] * np.sin(theta),\n",
    "        n[0] * n[2] * (1 - np.cos(theta)) + n[1] * np.sin(theta)\n",
    "    ], [\n",
    "        n[1] * n[0] * (1 - np.cos(theta)) + n[2] * np.sin(theta),\n",
    "        np.cos(theta) + n[1] ** 2 * (1 - np.cos(theta)),\n",
    "        n[1] * n[2] * (1 - np.cos(theta)) - n[0] * np.sin(theta),\n",
    "    ], [\n",
    "        n[2] * n[0] * (1 - np.cos(theta)) - n[1] * np.sin(theta),\n",
    "        n[2] * n[1] * (1 - np.cos(theta)) + n[0] * np.sin(theta),\n",
    "        np.cos(theta) + n[2] ** 2 * (1 - np.cos(theta))\n",
    "    ]])\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def normalize_3D_coordinates(wkps):\n",
    "\n",
    "    v = np.median(wkps[:8], axis=0)\n",
    "    v /= np.linalg.norm(v)\n",
    "    y = np.array([0, 1, 0])\n",
    "    n = np.cross(y, v)\n",
    "    n /= np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(y, v))\n",
    "    R = generate_rotation_matrix(n, theta)\n",
    "    wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    # rotate about y-axis so that fish is straight\n",
    "    upper_lip_idx = BODY_PARTS.index('UPPER_LIP')\n",
    "    tail_notch_idx = BODY_PARTS.index('TAIL_NOTCH')\n",
    "    v = wkps[upper_lip_idx] - wkps[tail_notch_idx]\n",
    "\n",
    "    n = np.array([0, 1, 0])\n",
    "    theta = np.arctan(v[2] / v[0])\n",
    "    R = generate_rotation_matrix(n, theta)\n",
    "    wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    # perform reflection if necessary\n",
    "    if wkps[upper_lip_idx][0] < wkps[tail_notch_idx][0]:\n",
    "        R = np.array([\n",
    "            [-1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        wkps = np.dot(R, wkps.T).T\n",
    "\n",
    "    return wkps\n",
    "\n",
    "\n",
    "def get_keypoint_arr(keypoints, cm):\n",
    "    dorsal_fin_idx, pelvic_fin_idx = BODY_PARTS.index('DORSAL_FIN'), BODY_PARTS.index('PELVIC_FIN')\n",
    "    wkps = get_raw_3D_coordinates(keypoints, cm)\n",
    "    norm_wkps = normalize_3D_coordinates(wkps)\n",
    "    if any([item['keypointType'] == 'BODY' for item in keypoints['leftCrop']]):\n",
    "        body_norm_wkps = norm_wkps[8:, :]\n",
    "        mid_point = 0.5 * (norm_wkps[dorsal_fin_idx] + norm_wkps[pelvic_fin_idx])\n",
    "        idx = np.argmin(np.linalg.norm(body_norm_wkps[:, [0, 2]] - np.array([mid_point[0], mid_point[2]]), axis=1))\n",
    "        body_wkp = body_norm_wkps[idx]\n",
    "        keypoint_arr = np.vstack([norm_wkps[:8, :], body_wkp])\n",
    "        return keypoint_arr\n",
    "    else:\n",
    "        return norm_wkps\n",
    "\n",
    "\n",
    "import json, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "\n",
    "# network architecture\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator(object):\n",
    "\n",
    "    def __init__(self, model_f):\n",
    "        self.model = Network()\n",
    "        self.model.load_state_dict(torch.load(model_f))\n",
    "\n",
    "    def predict(self, keypoints, camera_metadata):\n",
    "        norm_keypoint_arr = get_keypoint_arr(keypoints, camera_metadata)\n",
    "        keypoint_tensor = torch.from_numpy(np.array([norm_keypoint_arr])).float()\n",
    "        weight_prediction = 1e4 * self.model(keypoint_tensor).item()\n",
    "        return weight_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = '/root/data/alok/biomass_estimation/playground/nn_epoch_253.pb'\n",
    "weight_estimator = WeightEstimator(model_f)\n",
    "\n",
    "original_preds, preds = [], []\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    original_ann, ann, cm = row.original_ann, row.ann, row.cm\n",
    "    original_pred = weight_estimator.predict(original_ann, cm)\n",
    "    pred = weight_estimator.predict(ann, cm)\n",
    "    preds.append(pred)\n",
    "    original_preds.append(original_pred)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['original_pred'] = original_preds\n",
    "augmented_df['pred'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = augmented_df.original_wkps.iloc[0]\n",
    "x[7] - x[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = augmented_df.wkps.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = np.arange(0.7, 1.4, 0.05)\n",
    "errs = []\n",
    "for idx in range(len(y_vals) - 1):\n",
    "    y_mask = (augmented_df.mean_y > y_vals[idx]) & (augmented_df.mean_y < y_vals[idx + 1])\n",
    "    mask = y_mask & (augmented_df.wkps.apply(lambda x: x[7][0] > x[6][0])) & (augmented_df.wkps.apply(lambda x: x[2][2] > x[5][2]))\n",
    "    error_pct = \\\n",
    "        (augmented_df[mask].pred.mean() - augmented_df[mask].weight.mean()) / (augmented_df[mask].weight.mean())\n",
    "    print('Error at depth {}: {}'.format(round(y_vals[idx], 2), round(100*error_pct, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_breakdown(df, vals, field):\n",
    "    for idx in range(len(vals) - 1):\n",
    "        mask = (df[field] > vals[idx]) & (df[field] < vals[idx + 1])\n",
    "        error_pct = (df[mask].pred.mean() - df[mask].weight.mean()) / (df[mask].weight.mean())\n",
    "        print('Error for {} in range {} <-> {}: {}'.format(\n",
    "            field,\n",
    "            round(vals[idx], 2), \n",
    "            round(vals[idx + 1], 2),\n",
    "            round(100*error_pct, 2))\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_error_breakdown(augmented_df, np.arange(-45, 50, 5), 'pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (augmented_df.wkps.apply(lambda x: x[7][0] > x[6][0])) & (augmented_df.wkps.apply(lambda x: x[2][2] > x[5][2]))\n",
    "(augmented_df[mask].pred.mean() - augmented_df[mask].weight.mean()) / (augmented_df[mask].weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(augmented_df.wkps.apply(lambda x: x[:, 1].mean()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_path, _, _ = s3_access_utils.download_from_url(akpd_scorer_url)\n",
    "akpd_scorer_network = load_model(akpd_scorer_path)\n",
    "akpd_scores = []\n",
    "\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    input_sample = {\n",
    "        'keypoints': row.ann,\n",
    "        'cm': row.cm,\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "    akpd_scores.append(akpd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(n, theta):\n",
    "    theta = theta * np.pi / 180.0\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0] ** 2 * (1 - np.cos(theta)),\n",
    "        n[0] * n[1] * (1 - np.cos(theta)) - n[2] * np.sin(theta),\n",
    "        n[0] * n[2] * (1 - np.cos(theta)) + n[1] * np.sin(theta)\n",
    "    ], [\n",
    "        n[1] * n[0] * (1 - np.cos(theta)) + n[2] * np.sin(theta),\n",
    "        np.cos(theta) + n[1] ** 2 * (1 - np.cos(theta)),\n",
    "        n[1] * n[2] * (1 - np.cos(theta)) - n[0] * np.sin(theta),\n",
    "    ], [\n",
    "        n[2] * n[0] * (1 - np.cos(theta)) - n[1] * np.sin(theta),\n",
    "        n[2] * n[1] * (1 - np.cos(theta)) + n[0] * np.sin(theta),\n",
    "        np.cos(theta) + n[2] ** 2 * (1 - np.cos(theta))\n",
    "    ]])\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.akpd_score > 0.9].world_keypoints.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [0, 0, 1]\n",
    "akpd_scores = []\n",
    "ids, preds, preds_original, gts, thetas, depths = [], [], [], [], [], []\n",
    "analysis_df = pd.DataFrame()\n",
    "for idx, row in df[df.akpd_score > 0.9].iterrows():\n",
    "    wkps = row.raw_keypoint_arr\n",
    "    for t in range(3):\n",
    "        new_wkps = normalize_3D_coordinates(wkps)\n",
    "        depth = np.random.uniform(0.5, 2.0)\n",
    "        new_wkps[:, 1] = new_wkps[:, 1] - new_wkps[:, 1].mean() + depth\n",
    "        theta = np.random.uniform(-30, 30)\n",
    "        R = generate_rotation_matrix([0, 0, 1], theta)\n",
    "        new_wkps = np.dot(R, new_wkps.T).T\n",
    "        \n",
    "        ann_left, ann_right = [], []\n",
    "        for idx, body_part in enumerate(BODY_PARTS):\n",
    "            # generate left item\n",
    "            x, y, z = new_wkps[idx]\n",
    "            x_frame_left = x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] / 2\n",
    "            y_frame_left = -z * cm['focalLengthPixel'] / y + cm['pixelCountHeight'] / 2\n",
    "            item_left = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_left + np.random.normal(0, 10),\n",
    "                'yFrame': y_frame_left\n",
    "            }\n",
    "            ann_left.append(item_left)\n",
    "\n",
    "            # generate right item\n",
    "            disparity = cm['focalLengthPixel'] * cm['baseline'] / y\n",
    "            x_frame_right = x_frame_left - disparity\n",
    "            y_frame_right = y_frame_left\n",
    "            item_right = {\n",
    "                'keypointType': body_part,\n",
    "                'xFrame': x_frame_right + np.random.normal(0, 10),\n",
    "                'yFrame': y_frame_right\n",
    "            }\n",
    "            ann_right.append(item_right)\n",
    "\n",
    "        new_kps = {'leftCrop': ann_left, 'rightCrop': ann_right}\n",
    "        input_sample = {\n",
    "            'keypoints': new_kps,\n",
    "            'cm': row.camera_metadata,\n",
    "            'stereo_pair_id': row.id,\n",
    "            'single_point_inference': True\n",
    "        }\n",
    "        akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "        akpd_scores.append(akpd_score)\n",
    "        \n",
    "        nomralized_centered_2D_kps = \\\n",
    "            normalize_centered_2D_transform.__call__(input_sample)\n",
    "\n",
    "        normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "        tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "        pred_original = network(tensorized_kps['kp_input']).item() * 1e4\n",
    "\n",
    "        pred = weight_estimator.predict(new_kps, row.camera_metadata)\n",
    "        preds.append(pred)\n",
    "        preds_original.append(pred_original)\n",
    "        gts.append(row.weight)\n",
    "        ids.append(row.id)\n",
    "        thetas.append(theta)\n",
    "        depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = pd.DataFrame({\n",
    "    'pred': preds,\n",
    "    'weight': gts,\n",
    "    'id': ids,\n",
    "    'theta': thetas,\n",
    "    'depth': depths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf['error'] = (kdf.pred - kdf.weight) / kdf.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keypoints.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_error_breakdown(kdf, np.arange(-30, 35, 5), 'theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_error_breakdown(kdf, np.arange(0.5, 2.1, 0.1), 'depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
