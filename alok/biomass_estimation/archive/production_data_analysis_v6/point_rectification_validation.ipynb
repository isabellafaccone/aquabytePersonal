{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from weight_estimation.dataset import compute_akpd_score\n",
    "from keras.models import load_model\n",
    "\n",
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "rds = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define helper methods </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params):\n",
    "    print(\"Loading params...\")\n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    imageSize = (4096, 3000)\n",
    "\n",
    "    # perform rectification\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T, None, None, None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "\n",
    "    print(\"Params loaded.\")\n",
    "    return left_maps, right_maps\n",
    "\n",
    "IMAGE_WIDTH = 4096\n",
    "IMAGE_HEIGHT = 3000\n",
    "\n",
    "def get_camera_parameters(params: dict) -> Tuple:\n",
    "    \"\"\"Return individual camera parameters from JSON stereo parameters contents.\"\"\"\n",
    "    \n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "    \n",
    "    imageSize = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, \n",
    "                                                               distCoeffs2, imageSize, R, T, None, None, \n",
    "                                                               None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "    \n",
    "    return left_maps, right_maps, cameraMatrix1, distCoeffs1, R1, P1, cameraMatrix2, distCoeffs2, R2, P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load dataset of unrectified annotations and RGB rectified annotations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/playground/2020-03-01_crop_annotation_88_unrectified_akpd.csv'\n",
    "df1 = pd.read_csv(f, header=None)\n",
    "df1.columns = ['captured_at', 'pen_id', 'left_crop_url', 'right_crop_url', 'left_crop_metadata', \n",
    "             'right_crop_metadata', 'camera_metadata', 'annotation_unrectified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.left_crop_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left_crop_url.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "    * \n",
    "from\n",
    "    prod.biomass_computations\n",
    "where\n",
    "    pen_id = 88 and \n",
    "    group_id = '88' and\n",
    "    captured_at between '2020-03-01' and '2020-03-07' and\n",
    "    camera_metadata is not null\n",
    "order by \n",
    "    captured_at;\n",
    "\"\"\"\n",
    "\n",
    "df2 = rds.extract_from_database(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.merge(df1[['left_crop_url', 'annotation_unrectified']], how='inner', on='left_crop_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load weight estimation modules </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains constants representing core & auxiliary fish body parts.\n",
    "\"\"\"\n",
    "\n",
    "UPPER_LIP = 'UPPER_LIP'\n",
    "EYE = 'EYE'\n",
    "PECTORAL_FIN = 'PECTORAL_FIN'\n",
    "DORSAL_FIN = 'DORSAL_FIN'\n",
    "PELVIC_FIN = 'PELVIC_FIN'\n",
    "ADIPOSE_FIN = 'ADIPOSE_FIN'\n",
    "ANAL_FIN = 'ANAL_FIN'\n",
    "TAIL_NOTCH = 'TAIL_NOTCH'\n",
    "UPPER_PRECAUDAL_PIT = 'UPPER_PRECAUDAL_PIT'\n",
    "LOWER_PRECAUDAL_PIT = 'LOWER_PRECAUDAL_PIT'\n",
    "HYPURAL_PLATE = 'HYPURAL_PLATE'\n",
    "\n",
    "core_body_parts = sorted([UPPER_LIP,\n",
    "                          EYE,\n",
    "                          PECTORAL_FIN,\n",
    "                          DORSAL_FIN,\n",
    "                          PELVIC_FIN,\n",
    "                          ADIPOSE_FIN,\n",
    "                          ANAL_FIN,\n",
    "                          TAIL_NOTCH])\n",
    "\n",
    "auxiliary_body_parts = sorted([UPPER_PRECAUDAL_PIT,\n",
    "                               LOWER_PRECAUDAL_PIT,\n",
    "                               HYPURAL_PLATE])\n",
    "\n",
    "all_body_parts = sorted(core_body_parts + auxiliary_body_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = core_body_parts.index(UPPER_LIP)\n",
    "    tail_notch_idx = core_body_parts.index(TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perform point-based rectification on unrectified key-points </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = df.camera_metadata.iloc[0]\n",
    "cm_obj = CameraMetadata(\n",
    "    focal_length=cm['focalLength'],\n",
    "    focal_length_pixel=cm['focalLengthPixel'],\n",
    "    baseline_m=cm['baseline'],\n",
    "    pixel_count_width=cm['pixelCountWidth'],\n",
    "    pixel_count_height=cm['pixelCountHeight'],\n",
    "    image_sensor_width=cm['imageSensorWidth'],\n",
    "    image_sensor_height=cm['imageSensorHeight']\n",
    ")\n",
    "\n",
    "params_f, _, _ = s3.download_from_url(cm['stereoParametersUrl'])\n",
    "params = json.load(open(params_f))\n",
    "left_maps, right_maps, cameraMatrix1, distCoeffs1, R1, P1, cameraMatrix2, distCoeffs2, R2, P2 = get_camera_parameters(params)\n",
    "\n",
    "weight_model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/playground/nn_epoch_798_v2.pb'\n",
    "kf_model_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb'\n",
    "weight_model_f, _, _ = s3.download_from_url(weight_model_url)\n",
    "kf_model_f, _, _ = s3.download_from_url(weight_model_url)\n",
    "\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "\n",
    "\n",
    "anns, weights, lengths, kfs, akpd_scores = [], [], [], [], []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print('Percentage complete: {}%'.format(round(100 * float(count) / df.shape[0], 2)))\n",
    "    count += 1\n",
    "    \n",
    "    ann_u = json.loads(row.annotation_unrectified)\n",
    "    ann = row.annotation\n",
    "    left_crop_url = row.left_crop_url\n",
    "    right_crop_url = row.right_crop_url\n",
    "    \n",
    "    left_bbox = [int(x) for x in os.path.basename(left_crop_url).split('.')[0].split('_')[-4:]]\n",
    "    right_bbox = [int(x) for x in os.path.basename(right_crop_url).split('.')[0].split('_')[-4:]]\n",
    "    \n",
    "    \n",
    "    # un-rectify with matlab params, re-rectify with circular params\n",
    "    ann_u_r = {'leftCrop': [], 'rightCrop': []}\n",
    "    for side in ['leftCrop', 'rightCrop']:\n",
    "        for item in ann_u[side]:\n",
    "            bp = item['keypointType']\n",
    "            \n",
    "            if side == 'leftCrop':\n",
    "                x = item['xCrop'] + left_bbox[0]\n",
    "                y = item['yFrame'] + left_bbox[1]\n",
    "                x_new, y_new = cv2.undistortPoints(np.array([[[x, y]]]).astype(float), \n",
    "                                    cameraMatrix1, distCoeffs1, R=R1, P=P1)[0][0]\n",
    "            elif side == 'rightCrop':\n",
    "                x = item['xCrop'] + right_bbox[0]\n",
    "                y = item['yFrame'] + right_bbox[1]\n",
    "                x_new, y_new = cv2.undistortPoints(np.array([[[x, y]]]).astype(float), \n",
    "                                    cameraMatrix2, distCoeffs2, R=R2, P=P2)[0][0]\n",
    "                \n",
    "            x_new, y_new = int(round(x_new)), int(round(y_new))\n",
    "            ann_u_r[side].append({\n",
    "                'keypointType': bp,\n",
    "                'xFrame': x_new,\n",
    "                'yFrame': y_new,\n",
    "                'xCrop': x_new - (ann[side][0]['xFrame'] - ann[side][0]['xCrop']),\n",
    "                'yCrop': y_new - (ann[side][0]['yFrame'] - ann[side][0]['yCrop'])\n",
    "                \n",
    "            })\n",
    "    anns.append(ann_u_r)\n",
    "    weight, length, kf = weight_estimator.predict(ann_u_r, cm_obj)\n",
    "    akpd_score = compute_akpd_score(akpd_scorer_network, ann_u_r, cm)\n",
    "    weights.append(weight)\n",
    "    lengths.append(length)\n",
    "    kfs.append(kf)\n",
    "    akpd_scores.append(akpd_score)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_ann'] = anns\n",
    "df['new_weight'] = weights\n",
    "df['new_length'] = lengths\n",
    "df['new_kf'] = kfs\n",
    "df['new_akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df[df.akpd_score > 0.9].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0, 11000, 1000)\n",
    "for low, high in zip(cutoffs, cutoffs[1:]):\n",
    "    mask = (df.akpd_score > 0.9) & (df.estimated_weight_g >= low) & (df.estimated_weight_g < high)\n",
    "    mask2 = (df.new_akpd_score > 0.9) & (df.new_weight >= low) & (df.new_weight < high) \n",
    "    pct_1 = df[mask].shape[0] / df[df.akpd_score > 0.9].shape[0]\n",
    "    pct_2 = df[mask2].shape[0] / df[df.new_akpd_score > 0.9].shape[0]\n",
    "    print(low, high, pct_1, pct_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.new_akpd_score > 0.9].new_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.9) & (df.new_akpd_score > 0.9)\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(df[mask].estimated_weight_g, df[mask].new_weight)\n",
    "plt.plot([0, 10000], [0, 10000], color='red')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean((df[mask].estimated_weight_g - df[mask].new_weight)**2))** 0.5 / df[mask].estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "def density_scatter(x, y, bins=20, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:]+y_e[:-1])), data, np.vstack([x, y]).T,\n",
    "                method=\"splinef2d\", bounds_error=False)\n",
    "\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "    ax.set_xlabel('Pipeline 1 (RGB rectification) prediction (g)')\n",
    "    ax.set_ylabel('Pipeline 2 (key-point rectificatio) prediction (g)')\n",
    "    ax.grid()\n",
    "    ax.set_title('Single fish weight estiamte comparison')\n",
    "    norm = Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm), ax=ax)\n",
    "    cbar.ax.set_ylabel('Density')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_scatter(df[mask].estimated_weight_g.values, df[mask].new_weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - df[df.new_akpd_score > 0.9].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image, right_image, ann):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    \n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['leftCrop']}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in ann['rightCrop']}\n",
    "    \n",
    "    for bp, kp in left_keypoints.items():\n",
    "        axes[0].scatter([kp[0]], [kp[1]], color='red', s=1)\n",
    "    for bp, kp in right_keypoints.items():\n",
    "        axes[1].scatter([kp[0]], [kp[1]], color='red', s=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "rectified_left_image_f, _, _ = s3.download_from_url(df.left_crop_url.iloc[idx])\n",
    "rectified_right_image_f, _, _ = s3.download_from_url(df.right_crop_url.iloc[idx])\n",
    "rectified_left_image = cv2.imread(rectified_left_image_f)\n",
    "rectified_right_image = cv2.imread(rectified_right_image_f)\n",
    "ann = df.annotation.iloc[idx]\n",
    "display_crops(rectified_left_image, rectified_right_image, ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "unrectified_left_image_f, _, _ = s3.download_from_url(df.left_crop_url.iloc[idx].replace('aquabyte-crops', 'aquabyte-frames-resized-inbound'))\n",
    "unrectified_right_image_f, _, _ = s3.download_from_url(df.right_crop_url.iloc[idx].replace('aquabyte-crops', 'aquabyte-frames-resized-inbound'))\n",
    "unrectified_left_image = cv2.imread(unrectified_left_image_f)\n",
    "unrectified_right_image = cv2.imread(unrectified_right_image_f)\n",
    "unrectified_ann = json.loads(df.annotation_unrectified.iloc[idx])\n",
    "display_crops(unrectified_left_image, unrectified_right_image, unrectified_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ann = df.new_ann.iloc[idx]\n",
    "display_crops(rectified_left_image, rectified_right_image, new_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ann['leftCrop'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann['leftCrop'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
