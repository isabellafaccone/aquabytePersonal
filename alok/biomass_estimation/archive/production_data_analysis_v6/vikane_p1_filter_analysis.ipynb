{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from typing import List\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.utils.datetime_utils import add_days, get_dates_in_range\n",
    "from population_metrics.population_metrics_base import generate_pm_base, PopulationMetricsBase\n",
    "from population_metrics.confidence_metrics import compute_biomass_kpi, generate_distribution_consistency\n",
    "from population_metrics.raw_metrics import get_raw_sample_size\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "S3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "RDS = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "OUTPUT_DIR = '/root/data/recommendations'\n",
    "UPLOAD_BUCKET = 'aquabyte-images-adhoc'\n",
    "UPLOAD_KEY_BASE = 'alok/filter_recommendations'\n",
    "\n",
    "\n",
    "class NoDataException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "SamplingFilter = namedtuple('SamplingFilter', 'start_hour end_hour kf_cutoff akpd_score_cutoff')\n",
    "\n",
    "\n",
    "def generate_filter_mask(df: pd.DataFrame, sampling_filter: SamplingFilter):\n",
    "    \"\"\"Generates boolean mask on data-frame of raw biomass computations corresponding to sampling filter.\n",
    "    Args:\n",
    "        df: data-frame of raw biomass computations from data warehouse. This contains:\n",
    "            - group_id, site_id, pen_id, left_crop_url, right_crop_url, estimated_weight_g, captured_at, \n",
    "              akpd_score, estimated_length_mm, estimated_k_factor, process_info, annotation, camera_metadata\n",
    "        sampling_filter: SamplingFilter instance representing filter to apply\n",
    "    Returns:\n",
    "        mask: boolean Pandas series representing df subset that falls into sampling filter\n",
    "    \"\"\"\n",
    "\n",
    "    if sampling_filter.start_hour < sampling_filter.end_hour:\n",
    "        hour_mask = (df.hour >= sampling_filter.start_hour) & (df.hour <= sampling_filter.end_hour)\n",
    "    else:\n",
    "        hour_mask = (df.hour >= sampling_filter.start_hour) | (df.hour <= sampling_filter.end_hour)\n",
    "    kf_mask = (df.estimated_k_factor >= sampling_filter.kf_cutoff)\n",
    "    akpd_score_mask = (df.akpd_score >= sampling_filter.akpd_score_cutoff)\n",
    "    mask = hour_mask & kf_mask & akpd_score_mask\n",
    "    return mask\n",
    "\n",
    "\n",
    "def gen_pm_base(df: pd.DataFrame, sampling_filter: SamplingFilter) -> PopulationMetricsBase:\n",
    "    \"\"\"Generates PopulationMetricsBase instance give data-frame of raw biomass computations and sampling filter.\"\"\"\n",
    "    mask = generate_filter_mask(df, sampling_filter)\n",
    "\n",
    "    # get filtered set of biomass computations\n",
    "    biomass_computations = list(zip(df[mask].date.values,\n",
    "                                    df.loc[mask, 'estimated_weight_g'].values,\n",
    "                                    df[mask].estimated_k_factor.values))\n",
    "\n",
    "    if len(biomass_computations) == 0:\n",
    "        return None\n",
    "    pm_base = generate_pm_base(biomass_computations)\n",
    "    return pm_base\n",
    "\n",
    "\n",
    "def generate_metrics_for_pm_base(pm_base: PopulationMetricsBase, dates: List[str]) -> float:\n",
    "    \"\"\"Generates mean biomass KPI given a PopulationMetricsBase instance and dates to consider.\"\"\"\n",
    "\n",
    "    kpis, sample_sizes = [], []\n",
    "    for date in dates:\n",
    "        sample_size = get_raw_sample_size(pm_base, date)\n",
    "        biomass_kpi = compute_biomass_kpi(pm_base, date)\n",
    "        sample_sizes.append(sample_size)\n",
    "        kpis.append(biomass_kpi)\n",
    "\n",
    "    # compute sample-size weighted kpi and final smart average\n",
    "    kpis = np.array([k if k else np.nan for k in kpis])\n",
    "    sample_sizes = np.array([s if s else np.nan for s in sample_sizes])\n",
    "    mean_kpi = np.nansum(kpis * sample_sizes) / np.nansum(sample_sizes)\n",
    "    return mean_kpi\n",
    "\n",
    "\n",
    "def find_optimal_filter(df: pd.DataFrame, sampling_filters: List[SamplingFilter]) -> SamplingFilter:\n",
    "    \"\"\"Finds optimal filter given data-frame of raw biomass computations and different sampling\n",
    "    filters. \"\"\"\n",
    "\n",
    "    analysis_data = defaultdict(list)\n",
    "    for sampling_filter in sampling_filters:\n",
    "        print('Start hour: {}, End hour: {}, KF cutoff: {}'.format(\n",
    "            sampling_filter.start_hour, sampling_filter.end_hour, sampling_filter.kf_cutoff\n",
    "        ))\n",
    "        pm_base = gen_pm_base(df, sampling_filter)\n",
    "\n",
    "        if pm_base:\n",
    "            unique_dates = sorted(df.date.unique().tolist())\n",
    "            dates = get_dates_in_range(unique_dates[0], unique_dates[-1])\n",
    "            mean_kpi = generate_metrics_for_pm_base(pm_base, dates)\n",
    "        else:\n",
    "            mean_kpi = None\n",
    "\n",
    "        # add to data\n",
    "        analysis_data['mean_kpi'].append(mean_kpi)\n",
    "        analysis_data['start_hour'].append(sampling_filter.start_hour)\n",
    "        analysis_data['end_hour'].append(sampling_filter.end_hour)\n",
    "        analysis_data['kf_cutoff'].append(sampling_filter.kf_cutoff)\n",
    "        analysis_data['akpd_score_cutoff'].append(sampling_filter.akpd_score_cutoff)\n",
    "\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    best_sampling_filter_params = analysis_df.sort_values('mean_kpi', ascending=False).iloc[0]\n",
    "\n",
    "    best_sampling_filter = SamplingFilter(\n",
    "        start_hour=float(best_sampling_filter_params.start_hour),\n",
    "        end_hour=float(best_sampling_filter_params.end_hour),\n",
    "        kf_cutoff=float(best_sampling_filter_params.kf_cutoff),\n",
    "        akpd_score_cutoff=float(best_sampling_filter_params.akpd_score_cutoff)\n",
    "    )\n",
    "    return best_sampling_filter\n",
    "\n",
    "\n",
    "def generate_sampling_filters(start_hours: List[int], end_hours: List[int],\n",
    "                              kf_cutoffs: List[float], akpd_score_cutoff: float = 0.99) -> List[SamplingFilter]:\n",
    "    \"\"\"Generates list of SamplingFilter instances given start hour, end hour, and k-factor values to grid over.\"\"\"\n",
    "    sampling_filters = []\n",
    "    for start_hour in start_hours:\n",
    "        for end_hour in end_hours:\n",
    "            for kf_cutoff in kf_cutoffs:\n",
    "                sampling_filters.append(\n",
    "                    SamplingFilter(\n",
    "                        start_hour=start_hour,\n",
    "                        end_hour=end_hour,\n",
    "                        kf_cutoff=kf_cutoff,\n",
    "                        akpd_score_cutoff=akpd_score_cutoff\n",
    "                    )\n",
    "                )\n",
    "    return sampling_filters\n",
    "\n",
    "\n",
    "def perform_coarse_grid_search(df: pd.DataFrame, max_kf: float = 1.5) -> SamplingFilter:\n",
    "    \"\"\"Perform a coarse but broad grid search to determine best sampling filter.\n",
    "    Args:\n",
    "        - df: DataFrame of raw biomass computations from data-warehouse\n",
    "        - max_kf: Maximum k-factor value to go up to during grid search\n",
    "    Returns:\n",
    "        - best_coarse_filter: SamplingFilter instance corresponding to best coarse-search filter\n",
    "    \"\"\"\n",
    "    start_hours = [0]\n",
    "    end_hours = [24]\n",
    "    min_kf_cutoff = .05 * int(df.estimated_k_factor.min() / .05)\n",
    "    kf_cutoffs = list(np.arange(min_kf_cutoff, max_kf, 0.05))\n",
    "    sampling_filters = generate_sampling_filters(start_hours, end_hours, kf_cutoffs)\n",
    "    best_coarse_filter = find_optimal_filter(df, sampling_filters)\n",
    "    return best_coarse_filter\n",
    "\n",
    "\n",
    "def perform_fine_grid_search(df: pd.DataFrame, best_coarse_filter: SamplingFilter) -> SamplingFilter:\n",
    "    \"\"\"Perform a fine, local grid search around provided sampling filter to determine best sampling filter.\n",
    "    Args:\n",
    "        - df: DataFrame of raw biomass computations from data-warehouse\n",
    "        - max_kf: Maximum k-factor value to go up to during grid search\n",
    "    Returns:\n",
    "        - best_fine_filter: SamplingFilter instance corresponding to best find-search filter\n",
    "    \"\"\"\n",
    "    lo_start_hr, hi_start_hr = max(best_coarse_filter.start_hour - 1, 0), min(best_coarse_filter.start_hour + 1, 24)\n",
    "    lo_end_hr, hi_end_hr = max(best_coarse_filter.end_hour - 1, 0), min(best_coarse_filter.end_hour + 1, 24)\n",
    "    lo_kf, hi_kf = best_coarse_filter.kf_cutoff - 0.1, best_coarse_filter.kf_cutoff + 0.1\n",
    "\n",
    "    start_hours = list(np.arange(lo_start_hr, hi_start_hr, 1))\n",
    "    end_hours = list(np.arange(lo_end_hr, hi_end_hr, 1))\n",
    "    kf_cutoffs = list(np.arange(lo_kf, hi_kf, 0.005))\n",
    "\n",
    "    sampling_filters = generate_sampling_filters(start_hours, end_hours, kf_cutoffs)\n",
    "    best_fine_filter = find_optimal_filter(df, sampling_filters)\n",
    "    return best_fine_filter\n",
    "\n",
    "\n",
    "def generate_global_optimum_filter(df: pd.DataFrame):\n",
    "    \"\"\"Determine best global optimal sampling strategy for given pen_id, start_date, and end_date.\"\"\"\n",
    "\n",
    "\n",
    "    print('Performing coarse grid search...')\n",
    "    best_coarse_filter = perform_coarse_grid_search(df)\n",
    "    print(f'Coarse grid search complete with best start hour of {best_coarse_filter.start_hour}, '\n",
    "          f'best end hour of {best_coarse_filter.end_hour}, best kf cutoff of {best_coarse_filter.kf_cutoff}')\n",
    "\n",
    "    print('Perform fine grid search...')\n",
    "    best_fine_filter = perform_fine_grid_search(df, best_coarse_filter)\n",
    "    return best_fine_filter\n",
    "\n",
    "\n",
    "def get_active_pen_ids():\n",
    "    \"\"\"Get all active customer pen IDs.\"\"\"\n",
    "\n",
    "    query = 'SELECT id FROM customer.pens WHERE is_active=TRUE;'\n",
    "    pdf = RDS.extract_from_database(query)\n",
    "    pen_ids = sorted(pdf.id.values.tolist())\n",
    "    return pen_ids\n",
    "\n",
    "\n",
    "def get_historical_date_range(pen_id, curr_date, lookback_days):\n",
    "    \"\"\"Get date range corresponding to past two weeks if pen has data, else raise NoDataException.\"\"\"\n",
    "\n",
    "    # TODO @alok: weigh KPI for each date by sample size\n",
    "\n",
    "    today, two_weeks_ago = curr_date, add_days(curr_date, -lookback_days)\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM\n",
    "            (SELECT CAST(captured_at as DATE) as date, COUNT(estimated_weight_g)\n",
    "            FROM prod.biomass_computations\n",
    "            WHERE pen_id={}\n",
    "            AND akpd_score >= 0.99\n",
    "            GROUP BY date\n",
    "            ORDER BY date DESC) AS COUNT_BY_DATE\n",
    "        WHERE date >= '{}'\n",
    "        AND date <= '{}';\n",
    "    \"\"\".format(pen_id, two_weeks_ago, today)\n",
    "    print(query)\n",
    "    tdf = RDS.extract_from_database(query)\n",
    "    if not tdf.shape[0]:\n",
    "        raise NoDataException('No data present was found in last two weeks for this pen!')\n",
    "    return two_weeks_ago, today\n",
    "\n",
    "\n",
    "def _add_date_hour_columns(df):\n",
    "    \"\"\"Adds date and hour columns to DataFrame of biomass computations\"\"\"\n",
    "    df.index = list(range(df.shape[0]))\n",
    "    df = df.sort_values('captured_at').copy(deep=True)\n",
    "    df.index = pd.to_datetime(df.captured_at)\n",
    "    dates = df.index.date.astype(str)\n",
    "    df['date'] = dates\n",
    "    df['hour'] = df.index.hour\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_biomass_data(pen_id, start_date, end_date, akpd_score_cutoff):\n",
    "    \"\"\"Get raw biomass computations for given pen_id, date range, and AKPD score cutoff.\"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM\n",
    "        prod.biomass_computations bc\n",
    "        WHERE bc.pen_id={}\n",
    "        AND bc.akpd_score >= {}\n",
    "        AND bc.captured_at BETWEEN '{}' and '{}'\n",
    "        AND bc.estimated_weight_g > 0.0\n",
    "    \"\"\".format(pen_id, akpd_score_cutoff, start_date, end_date)\n",
    "\n",
    "    df = RDS.extract_from_database(query)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = _add_date_hour_columns(df)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "pen_ids = [56]\n",
    "\n",
    "recommendations_by_date = {}\n",
    "\n",
    "#     curr_date = dt.datetime.strftime(dt.datetime.utcnow(), '%Y-%m-%d')\n",
    "curr_date = '2020-03-01'\n",
    "while True:\n",
    "    f = os.path.join(OUTPUT_DIR, 'recommendations_{}.json'.format(curr_date))\n",
    "    tomorrow = add_days(dt.datetime.strftime(dt.datetime.utcnow(), '%Y-%m-%d'), 1)\n",
    "    if curr_date < tomorrow:\n",
    "        recommendations = {}\n",
    "        for pen_id in pen_ids:\n",
    "\n",
    "            print('Optimizing filters for Pen ID: {}'.format(pen_id))\n",
    "\n",
    "            # get date range corresponding to last two weeks\n",
    "            try:\n",
    "                start_date, end_date = get_historical_date_range(pen_id, curr_date, 14)\n",
    "            except NoDataException as err:\n",
    "                print(str(err))\n",
    "                continue\n",
    "\n",
    "            # get best overall start hour, end hour, and k-factor cutoff\n",
    "            df = extract_biomass_data(pen_id, start_date, end_date, 0.99)\n",
    "            best_global_filter = generate_global_optimum_filter(df)\n",
    "            recommendations[pen_id] = dict(\n",
    "                best_start_hr=best_global_filter.start_hour,\n",
    "                best_end_hr=best_global_filter.end_hour,\n",
    "                best_kf_cutoff=best_global_filter.kf_cutoff\n",
    "            )\n",
    "\n",
    "            print(f'Best Start Hour: {best_global_filter.start_hour}')\n",
    "            print(f'Best End Hour: {best_global_filter.end_hour}')\n",
    "            print(f'Best KF Cutoff: {best_global_filter.kf_cutoff}')\n",
    "\n",
    "            recommendations_by_date[curr_date] = recommendations\n",
    "#                 json.dump(recommendations, open(f, 'w'))\n",
    "\n",
    "        curr_date = add_days(curr_date, 1)\n",
    "    else:\n",
    "        print('Now sleeping for one hour...')\n",
    "        time.sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from population_metrics.population_metrics_base import generate_pm_base\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.raw_metrics import generate_raw_average_weight, generate_raw_average_kf, get_raw_sample_size\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_standard_deviation, \\\n",
    "     generate_smart_distribution, generate_smart_avg_kf, get_smart_sample_size, \\\n",
    "     get_smart_growth_rate\n",
    "from population_metrics.confidence_metrics import generate_distribution_consistency, generate_trend_stability, \\\n",
    "     compute_biomass_kpi\n",
    "from research.utils.datetime_utils import add_days, get_dates_in_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.datetime_utils import add_days\n",
    "\n",
    "pen_id = 56\n",
    "start_date = '2020-03-01'\n",
    "end_date = '2020-09-25'\n",
    "dates_to_include = get_dates_in_range(add_days(start_date, -7), add_days(end_date, 7))\n",
    "dates_to_compute = get_dates_in_range(start_date, end_date)\n",
    "\n",
    "print('Extracting Data...')\n",
    "df = extract_biomass_data(pen_id, dates_to_include[0], dates_to_include[-1], 0.99)\n",
    "print('Data extracted!')\n",
    "\n",
    "results_by_date_fh = {}\n",
    "\n",
    "for date in dates_to_compute:\n",
    "    print('On date: {}'.format(date))\n",
    "\n",
    "#     sampling_filter_dict = recommendations_by_date[date][pen_id]\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=7,\n",
    "        end_hour=15,\n",
    "        kf_cutoff=0.0,\n",
    "        akpd_score_cutoff=0.99\n",
    "    )\n",
    "    start_date, end_date = add_days(date, -4), add_days(date, 4)\n",
    "    tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    smart_average_weight = generate_smart_avg_weight(pm_base, date)\n",
    "    smart_standard_deviation = generate_smart_standard_deviation(pm_base, date)\n",
    "    if smart_average_weight and smart_standard_deviation:\n",
    "        coefficient_of_variation = smart_standard_deviation / smart_average_weight\n",
    "    else:\n",
    "        coefficient_of_variation = None\n",
    "\n",
    "    raw_data = dict(\n",
    "        avgWeight=generate_raw_average_weight(pm_base, date),\n",
    "        avgKFactor=generate_raw_average_kf(pm_base, date),\n",
    "        numFish=get_raw_sample_size(pm_base, date)\n",
    "    )\n",
    "\n",
    "    smart_data = dict(\n",
    "        avgWeight=smart_average_weight,\n",
    "        avgKFactor=generate_smart_avg_kf(pm_base, date),\n",
    "        numFish=get_smart_sample_size(pm_base, date),\n",
    "        weightDistribution=generate_smart_distribution(pm_base, date),\n",
    "        standardDeviation=smart_standard_deviation,\n",
    "        coefficientOfVariation=coefficient_of_variation,\n",
    "        growthRate=compute_local_growth_rate(pm_base, date),\n",
    "        growthRateForSmartAvg=get_smart_growth_rate(pm_base, date),\n",
    "        distributionConsistency=generate_distribution_consistency(pm_base, date),\n",
    "        trendStability=generate_trend_stability(pm_base, date),\n",
    "        biomassKPI=compute_biomass_kpi(pm_base, date)\n",
    "    )\n",
    "    \n",
    "    results_by_date_fh[date] = {}\n",
    "    results_by_date_fh[date]['raw_data'] = raw_data\n",
    "    results_by_date_fh[date]['smart_data'] = smart_data\n",
    "    print(date, smart_data['avgWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.datetime_utils import add_days\n",
    "\n",
    "pen_id = 56\n",
    "start_date = '2020-03-01'\n",
    "end_date = '2020-09-25'\n",
    "dates_to_include = get_dates_in_range(add_days(start_date, -7), add_days(end_date, 7))\n",
    "dates_to_compute = get_dates_in_range(start_date, end_date)\n",
    "\n",
    "print('Extracting Data...')\n",
    "df = extract_biomass_data(pen_id, dates_to_include[0], dates_to_include[-1], 0.99)\n",
    "print('Data extracted!')\n",
    "\n",
    "results_by_date_u = {}\n",
    "\n",
    "for date in dates_to_compute:\n",
    "    print('On date: {}'.format(date))\n",
    "\n",
    "#     sampling_filter_dict = recommendations_by_date[date][pen_id]\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=0,\n",
    "        end_hour=24,\n",
    "        kf_cutoff=0.0,\n",
    "        akpd_score_cutoff=0.99\n",
    "    )\n",
    "    start_date, end_date = add_days(date, -4), add_days(date, 4)\n",
    "    tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    smart_average_weight = generate_smart_avg_weight(pm_base, date)\n",
    "    smart_standard_deviation = generate_smart_standard_deviation(pm_base, date)\n",
    "    if smart_average_weight and smart_standard_deviation:\n",
    "        coefficient_of_variation = smart_standard_deviation / smart_average_weight\n",
    "    else:\n",
    "        coefficient_of_variation = None\n",
    "\n",
    "    raw_data = dict(\n",
    "        avgWeight=generate_raw_average_weight(pm_base, date),\n",
    "        avgKFactor=generate_raw_average_kf(pm_base, date),\n",
    "        numFish=get_raw_sample_size(pm_base, date)\n",
    "    )\n",
    "\n",
    "    smart_data = dict(\n",
    "        avgWeight=smart_average_weight,\n",
    "        avgKFactor=generate_smart_avg_kf(pm_base, date),\n",
    "        numFish=get_smart_sample_size(pm_base, date),\n",
    "        weightDistribution=generate_smart_distribution(pm_base, date),\n",
    "        standardDeviation=smart_standard_deviation,\n",
    "        coefficientOfVariation=coefficient_of_variation,\n",
    "        growthRate=compute_local_growth_rate(pm_base, date),\n",
    "        growthRateForSmartAvg=get_smart_growth_rate(pm_base, date),\n",
    "        distributionConsistency=generate_distribution_consistency(pm_base, date),\n",
    "        trendStability=generate_trend_stability(pm_base, date),\n",
    "        biomassKPI=compute_biomass_kpi(pm_base, date)\n",
    "    )\n",
    "    \n",
    "    results_by_date_u[date] = {}\n",
    "    results_by_date_u[date]['raw_data'] = raw_data\n",
    "    results_by_date_u[date]['smart_data'] = smart_data\n",
    "    print(date, smart_data['avgWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.datetime_utils import add_days\n",
    "\n",
    "pen_id = 56\n",
    "start_date = '2020-03-01'\n",
    "end_date = '2020-09-25'\n",
    "dates_to_include = get_dates_in_range(add_days(start_date, -7), add_days(end_date, 7))\n",
    "dates_to_compute = get_dates_in_range(start_date, end_date)\n",
    "\n",
    "print('Extracting Data...')\n",
    "df = extract_biomass_data(pen_id, dates_to_include[0], dates_to_include[-1], 0.99)\n",
    "print('Data extracted!')\n",
    "\n",
    "results_by_date_ws = {}\n",
    "\n",
    "for date in dates_to_compute:\n",
    "    print('On date: {}'.format(date))\n",
    "\n",
    "#     sampling_filter_dict = recommendations_by_date[date][pen_id]\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=0,\n",
    "        end_hour=24,\n",
    "        kf_cutoff=1.085,\n",
    "        akpd_score_cutoff=0.99\n",
    "    )\n",
    "    start_date, end_date = add_days(date, -4), add_days(date, 4)\n",
    "    tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    smart_average_weight = generate_smart_avg_weight(pm_base, date)\n",
    "    smart_standard_deviation = generate_smart_standard_deviation(pm_base, date)\n",
    "    if smart_average_weight and smart_standard_deviation:\n",
    "        coefficient_of_variation = smart_standard_deviation / smart_average_weight\n",
    "    else:\n",
    "        coefficient_of_variation = None\n",
    "\n",
    "    raw_data = dict(\n",
    "        avgWeight=generate_raw_average_weight(pm_base, date),\n",
    "        avgKFactor=generate_raw_average_kf(pm_base, date),\n",
    "        numFish=get_raw_sample_size(pm_base, date)\n",
    "    )\n",
    "\n",
    "    smart_data = dict(\n",
    "        avgWeight=smart_average_weight,\n",
    "        avgKFactor=generate_smart_avg_kf(pm_base, date),\n",
    "        numFish=get_smart_sample_size(pm_base, date),\n",
    "        weightDistribution=generate_smart_distribution(pm_base, date),\n",
    "        standardDeviation=smart_standard_deviation,\n",
    "        coefficientOfVariation=coefficient_of_variation,\n",
    "        growthRate=compute_local_growth_rate(pm_base, date),\n",
    "        growthRateForSmartAvg=get_smart_growth_rate(pm_base, date),\n",
    "        distributionConsistency=generate_distribution_consistency(pm_base, date),\n",
    "        trendStability=generate_trend_stability(pm_base, date),\n",
    "        biomassKPI=compute_biomass_kpi(pm_base, date)\n",
    "    )\n",
    "    \n",
    "    results_by_date_ws[date] = {}\n",
    "    results_by_date_ws[date]['raw_data'] = raw_data\n",
    "    results_by_date_ws[date]['smart_data'] = smart_data\n",
    "    print(date, smart_data['avgWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(results_by_date.keys()))\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid()\n",
    "avg_weights = [results_by_date[date]['smart_data']['avgWeight'] for date in sorted(results_by_date.keys())]\n",
    "avg_weights_fh = [results_by_date_fh[date]['smart_data']['avgWeight'] for date in sorted(results_by_date_fh.keys())]\n",
    "avg_weights_u = [results_by_date_u[date]['smart_data']['avgWeight'] for date in sorted(results_by_date_u.keys())]\n",
    "avg_weights_ws = [results_by_date_ws[date]['smart_data']['avgWeight'] for date in sorted(results_by_date_ws.keys())]\n",
    "plt.plot(dates, avg_weights, color='blue')\n",
    "# plt.plot(dates, avg_weights_fh, color='red')\n",
    "# plt.plot(dates, avg_weights_u, color='green')\n",
    "plt.plot(dates, prod_sys_ws, color='purple')\n",
    "plt.plot(dates, avg_weights_ws, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = results_by_date['2020-09-22']['smart_data']['avgWeight']\n",
    "b = results_by_date_fh['2020-09-22']['smart_data']['avgWeight']\n",
    "a, b, (a - b)/ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv('/root/data/alok/biomass_estimation/playground/biomass_prod_system_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sys_ws = []\n",
    "for date in dates:\n",
    "    try:\n",
    "        prod_sys_w = pdf.ix[pdf.Date == date, 'Avg weight'].iloc[0]\n",
    "    except IndexError as e:\n",
    "        prod_sys_w = None\n",
    "    prod_sys_ws.append(prod_sys_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
