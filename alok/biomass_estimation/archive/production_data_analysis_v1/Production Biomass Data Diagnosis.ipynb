{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker, relationship, join\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import Table, Column, Integer, ForeignKey\n",
    "from sqlalchemy.orm import relationship\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Establish connection to database and perform query for base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "# prod SQL credentaials\n",
    "prod_sql_credentials = json.load(open(os.environ[\"PROD_SQL_CREDENTIALS\"]))\n",
    "prod_sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(prod_sql_credentials[\"user\"], prod_sql_credentials[\"password\"],\n",
    "                           prod_sql_credentials[\"host\"], prod_sql_credentials[\"port\"],\n",
    "                           prod_sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=prod_sql_engine)\n",
    "session = Session()\n",
    "Base = automap_base()\n",
    "Base.prepare(prod_sql_engine, reflect=True)\n",
    "KeypointAnnotations = Base.classes.keypoint_annotations\n",
    "FishDetections = Base.classes.fish_detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform join between KeypointAnnotatios and FishDetections\n",
    "\n",
    "records = session.query(\n",
    "    KeypointAnnotations, FishDetections\n",
    ").outerjoin(\n",
    "    FishDetections, KeypointAnnotations.fish_detection_id == FishDetections.id\n",
    ").all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iterate over query results and generate 3D coordinates for each stereo fish detection </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for record in records:\n",
    "    keypoint_annotation = record.keypoint_annotations\n",
    "    fish_detection = record.fish_detections\n",
    "    keypoints = keypoint_annotation.keypoints\n",
    "    \n",
    "    if keypoints:\n",
    "        if keypoints.get('leftCrop') and keypoints.get('rightCrop'):\n",
    "\n",
    "            # record image URLs and annotated keypoint coordinates\n",
    "            left_image_url = fish_detection.left_image_url\n",
    "            right_image_url = fish_detection.right_image_url\n",
    "            left_keypoints = keypoints['leftCrop']\n",
    "            right_keypoints = keypoints['rightCrop']\n",
    "            \n",
    "            # compute world coordinates\n",
    "            camera_metadata = fish_detection.camera_metadata\n",
    "            camera_metadata['pixelCountHeight'] = 3000\n",
    "            camera_metadata['pixelCountWidth'] = 4096\n",
    "            world_keypoint_coordinates = pixel2world(left_keypoints, right_keypoints, camera_metadata)\n",
    "            \n",
    "            row = {\n",
    "                'keypoint_annotation_id': keypoint_annotation.id,\n",
    "                'fish_detection_id': fish_detection.id,\n",
    "                'captured_at': fish_detection.captured_at,\n",
    "                'is_qa': keypoint_annotation.is_qa,\n",
    "                'left_image_url': left_image_url,\n",
    "                'right_image_url': right_image_url,\n",
    "                'left_keypoints': left_keypoints,\n",
    "                'right_keypoints': right_keypoints,\n",
    "                'world_keypoint_coordinates': world_keypoint_coordinates,\n",
    "                'site_id': fish_detection.site_id,\n",
    "                'pen_id': fish_detection.pen_id,\n",
    "                'camera_metadata': camera_metadata,\n",
    "                'left_crop_metadata': fish_detection.left_crop_metadata,\n",
    "                'right_crop_metadata': fish_detection.right_crop_metadata\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)\n",
    "\n",
    "df.index = pd.to_datetime(df.captured_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute biomass estimate using Linear Model + PCA + interaction features for pairwise distances </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    print(pairwise_distances)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def coord2biomass_blender(world_keypoints, blender):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    reverse_mapping = blender[\"reverse_mapping\"]\n",
    "    distances = np.array(blender[\"distances\"])\n",
    "    volumes = blender[\"volume\"]\n",
    "    regression_coeff = blender[\"coeff\"]\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # the reverse mapping insure that we listing the kp\n",
    "    # in the same order\n",
    "    measurements = []\n",
    "    number_of_parts = len(world_keypoints)\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    measurements = np.array(measurements)\n",
    "\n",
    "    # absolute diff\n",
    "    diff = np.nanmean(np.abs(distances - measurements), axis=1)\n",
    "    closest = np.argmin(diff)\n",
    "    prediction = volumes[closest]\n",
    "\n",
    "    # here is some machine learning\n",
    "    prediction = prediction*regression_coeff[0] + regression_coeff[1]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('./model.pkl', 'rb'))\n",
    "blender = json.load(open('./volumes.json'))\n",
    "linear_biomass_values, blender_biomass_values, lengths = [], [], []\n",
    "i = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    \n",
    "    linear_biomass_values.append(coord2biomass_linear(row.world_keypoint_coordinates, model))\n",
    "    blender_biomass_values.append(coord2biomass_blender(row.world_keypoint_coordinates, blender))\n",
    "    lengths.append(euclidean_distance(row.world_keypoint_coordinates['UPPER_LIP'], row.world_keypoint_coordinates['TAIL_NOTCH']))\n",
    "df['predicted_biomass_linear'] = linear_biomass_values\n",
    "df['predicted_biomass_blender'] = blender_biomass_values\n",
    "df['length'] = lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids = json.load(open('./invalid_fish_detection_ids.json'))\n",
    "df['is_valid'] = 1\n",
    "for invalid_fish_detection_id in invalid_fish_detection_ids:\n",
    "    df.loc[df.fish_detection_id == invalid_fish_detection_id, 'is_valid'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.index >= '2019-04-28') & (df.site_id == 23) & (df.pen_id == 4) & (df.is_valid == 1) & (df.is_qa == 1)\n",
    "df[mask].sort_values('predicted_biomass_linear', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coordinates(image_url, side, keypoints):\n",
    "    image_f = './image.jpg'\n",
    "    bucket = 'aquabyte-crops'\n",
    "    key = image_url[image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(bucket, key, image_f)\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    im = plt.imread(image_f)\n",
    "    print(image_url, im.shape)\n",
    "    \n",
    "    for keypoint in keypoints:\n",
    "        keypoint_type = keypoint['keypointType']\n",
    "        x, y = keypoint['xCrop'], keypoint['yCrop']\n",
    "        plt.scatter([x], [y])\n",
    "        plt.annotate(keypoint_type, (x, y), color='red')\n",
    "        \n",
    "    plt.imshow(im)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_detection_id = 239557\n",
    "fish_id_mask = df.fish_detection_id == fish_detection_id\n",
    "left_image_url = df[fish_id_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[fish_id_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[fish_id_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[fish_id_mask].right_keypoints.iloc[0]\n",
    "\n",
    "world_keypoint_coordinates = df[fish_id_mask].world_keypoint_coordinates.iloc[0]\n",
    "print(euclidean_distance(world_keypoint_coordinates['UPPER_LIP'], world_keypoint_coordinates['TAIL_NOTCH']))\n",
    "plot_coordinates(right_image_url, 'right', right_keypoints)\n",
    "plot_coordinates(left_image_url, 'left', left_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2019-05-02']['predicted_biomass_blender'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.site_id == 23) & (df.pen_id == 4) & (df.index >= '2019-04-27')\n",
    "df[mask].predicted_biomass_blender.resample('D', how=lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[mask].predicted_biomass_blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.predicted_biomass_linear > 500) & (df.predicted_biomass_linear < 6000)\n",
    "plt.scatter(df.ix[mask, 'predicted_biomass_blender'], df.ix[mask, 'predicted_biomass_linear'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine rectification issue </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_bucket = 'aquabyte-crops'\n",
    "left_image_rectified_f = './left_image_rectified.jpg'\n",
    "right_image_rectified_f = './right_image_rectified.jpg'\n",
    "\n",
    "invalid_fish_detection_ids, invalid_urls = [], []\n",
    "i = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    if i < 36132:\n",
    "        continue\n",
    "    left_image_url = row.left_image_url\n",
    "    right_image_url = row.right_image_url\n",
    "    left_rectified_key = left_image_url[left_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, left_rectified_key, left_image_rectified_f)\n",
    "    right_rectified_key = right_image_url[right_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, right_rectified_key, right_image_rectified_f)\n",
    "    \n",
    "    # this is dumb, can probably do this in memory\n",
    "    left_rectified_image = cv2.imread(left_image_rectified_f)\n",
    "    right_rectified_image = cv2.imread(right_image_rectified_f)\n",
    "    \n",
    "    left_crop_metadata = json.loads(row.left_crop_metadata)\n",
    "    right_crop_metadata = json.loads(row.right_crop_metadata)\n",
    "    left_crop_width = left_crop_metadata['width']\n",
    "    left_crop_height = left_crop_metadata['height']\n",
    "    right_crop_width = right_crop_metadata['width']\n",
    "    right_crop_height = right_crop_metadata['height']\n",
    "    \n",
    "    invalid = False\n",
    "    if left_rectified_image.shape[0] == left_crop_height and left_rectified_image.shape[1] == left_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(left_image_url)\n",
    "        print('left image not rectified for id {}!'.format(row.id))\n",
    "    if right_rectified_image.shape[0] == right_crop_height and right_rectified_image.shape[1] == right_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(right_image_url)\n",
    "        print('right image not rectified for id {}!'.format(row.id))\n",
    "    \n",
    "    if invalid:\n",
    "        invalid_fish_detection_ids.append(int(row.id))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(invalid_ids, open('./invalid_ids', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_urls + invalid_urls_old, open('./invalid_urls.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_urls_old = json.load(open('./invalid_urls.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids_old = json.load(open('./invalid_fish_detection_ids.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_fish_detection_ids + invalid_fish_detection_ids_old, open('./invalid_fish_detection_ids.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
