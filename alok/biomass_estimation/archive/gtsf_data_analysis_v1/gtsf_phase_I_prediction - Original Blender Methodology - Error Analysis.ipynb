{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.float_format', lambda x: str(x))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/data/alok/blender_data/volumes_all.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.array(data[\"dimensions\"])[:, 1], data[\"volume\"])\n",
    "# plt.ylabel(\"Volume (cm^3)\")\n",
    "# plt.xlabel(\"Length (mm)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"volume\"])\n",
    "plt.title(\"Blender volume histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility functions for world keypoint normalization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    if type(p1) == list:\n",
    "        p1 = np.array(p1)\n",
    "    if type(p2) == list:\n",
    "        p2 = np.array(p2)\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "\n",
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_world_keypoints(wkps):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['DORSAL_FIN'] - np.array([norm_wkps_intermediate['DORSAL_FIN'][0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform reflecton if necessary\n",
    "    if norm_wkps['PECTORAL_FIN'][1] > 0:\n",
    "        norm_wkps = {bp: np.array([\n",
    "            norm_wkps[bp][0],\n",
    "            -norm_wkps[bp][1],\n",
    "            norm_wkps[bp][2]\n",
    "        ]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get normalized world keyponts of all cached Blender models </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "sfps = session.query(StereoFramePair).all()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for idx, sfp in enumerate(sfps):\n",
    "    \n",
    "    if sfp.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    \n",
    "    ground_truth_metadata = json.loads(sfp.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "    wkps = json.loads(sfp.world_keypoint_coordinates)\n",
    "    body_parts = list(wkps.keys())\n",
    "    wkps = {bp: np.array(wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    row = {'0': idx}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(wkps[body_parts[i]], wkps[body_parts[j]])\n",
    "            row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    \n",
    "    weight = ground_truth_metadata['data']['weight']\n",
    "    length = ground_truth_metadata['data']['length']\n",
    "    row['weight'] = weight\n",
    "    row['length'] = length\n",
    "    row['kfactor'] = 1e5 * weight / length**3\n",
    "    df = df.append(row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_train_mask(df, train_frac=0.615)\n",
    "\n",
    "\n",
    "columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(list(range(8)), 2))]\n",
    "X_train = df.loc[mask, columns].values\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame()\n",
    "\n",
    "predictions = []\n",
    "predictions_average = []\n",
    "ground_truth = []\n",
    "\n",
    "for row in sfps:\n",
    "    \n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "    # load keypoints\n",
    "    world_keypoints_from_db = json.loads(row.world_keypoint_coordinates)\n",
    "    world_keypoints = {bp: np.array(world_keypoints_from_db[bp]) for bp in world_keypoints_from_db.keys()}\n",
    "    # calculate distances\n",
    "    measurements = []\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    \n",
    "    # find closest blender volume\n",
    "    # calculate l1 distance\n",
    "    diff = np.nanmean(np.abs(np.array(df)[:, :-1] - measurements), axis=1)\n",
    "    closest = np.argsort(diff)\n",
    "    idx = 10\n",
    "    closest5 = np.array(df)[closest[:idx], -1]\n",
    "    print(\"closest volumes\", closest5)\n",
    "    print(\"standard dev:\", np.std(closest5))\n",
    "    print(\"estimated length\", measurements[13])\n",
    "    closest_length = np.array(list(df[\"2-3\"].iloc()[closest[:idx]]))\n",
    "    kfactor = 10**5*closest5 / closest_length**3\n",
    "    print(\"closest length\", closest_length)\n",
    "    print(\"closest kfactor\", kfactor)\n",
    "    print(\"closest height\", list(df[\"4-6\"].iloc()[closest[:idx]]))\n",
    "    print(\"#\"*50)\n",
    "    pred_volume = np.array(df)[closest[0], -1]\n",
    "    predictions.append(pred_volume)\n",
    "    pred_avg = np.mean(closest5)\n",
    "    predictions_average.append(np.mean(closest5))\n",
    "    \n",
    "    closest_wkps = data['coordinates'][closest[0]]\n",
    "    closest_wkps = {bp: [x / 1e3 for x in closest_wkps[bp]] for bp in closest_wkps.keys()}\n",
    "    \n",
    "    # ground truth\n",
    "    \n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    ground_truth_weight = ground_truth_metadata['data']['weight']\n",
    "    ground_truth_width = ground_truth_metadata['data']['width']\n",
    "    ground_truth_breadth = ground_truth_metadata['data']['breath']\n",
    "    ground_truth_length = ground_truth_metadata['data']['length']\n",
    "    ground_truth_kfactor = 1e5 * (ground_truth_weight / ground_truth_length**3)\n",
    "    \n",
    "    ground_truth.append([ground_truth_weight, ground_truth_kfactor])\n",
    "    \n",
    "    row = {\n",
    "        'gtsf_data_collection_id': row.gtsf_data_collection_id,\n",
    "        'gtsf_fish_identifier': int(row.gtsf_fish_identifier),\n",
    "        'ground_truth_weight': ground_truth_weight,\n",
    "        'ground_truth_length': ground_truth_length,\n",
    "        'ground_truth_width': ground_truth_width,\n",
    "        'ground_truth_breadth': ground_truth_breadth,\n",
    "        'ground_truth_kfactor': ground_truth_kfactor,\n",
    "        'pred_volume': pred_volume,\n",
    "        'pred_avg_volume': pred_avg,\n",
    "        'pred_length': closest_length[0],\n",
    "        'wkps': row.world_keypoint_coordinates,\n",
    "        'closest_wkps': json.dumps(closest_wkps),\n",
    "        'closest_idx': closest[0]\n",
    "    }\n",
    "    \n",
    "    analysis_df = analysis_df.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "predictions_average = np.array(predictions_average)\n",
    "ground_truth = np.array(ground_truth)\n",
    "gt_weight = ground_truth[:, 0]\n",
    "gt_kfactor = ground_truth[:, 1]\n",
    "predictions = predictions[:, np.newaxis]\n",
    "reg = LinearRegression().fit(predictions, gt_weight)\n",
    "print(reg.coef_, reg.intercept_)\n",
    "print(\"R2 : {}\".format(reg.score(predictions, gt_weight)))\n",
    "predictions = np.squeeze(predictions)\n",
    "analysis_df['prediction'] = predictions*reg.coef_ + reg.intercept_\n",
    "analysis_df['error'] = analysis_df.prediction - analysis_df.ground_truth_weight\n",
    "analysis_df['error_pct'] = analysis_df.error / analysis_df.ground_truth_weight\n",
    "analysis_df['abs_error_pct'] = analysis_df.error_pct.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(df[columns].values)\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.grid()\n",
    "plt.xlabel('K Factor')\n",
    "plt.ylabel('Error')\n",
    "plt.scatter(df.loc[~mask, 'weight'], df.loc[~mask, 'error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "plt.scatter(gt_weight, predictions*reg.coef_ + reg.intercept_,  c=gt_kfactor)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Ground truth weight\")\n",
    "plt.ylabel(\"Predicted weight\")\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.sort_values('abs_error_pct', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "# perform N-fold cross validation\n",
    "abs_err_pcts = []\n",
    "for i in range(N):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    mask = generate_train_mask(df, train_frac=0.615)\n",
    "\n",
    "    columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(list(range(8)), 2))]\n",
    "    X_train = df.loc[mask, columns].values\n",
    "    y_train = df.loc[mask, 'weight'].values\n",
    "    X_test = df.loc[~mask, columns].values\n",
    "    y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = reg.predict(df[columns].values)\n",
    "    df['prediction'] = y_pred\n",
    "    df['error'] = df.prediction - df.weight\n",
    "    df['error_pct'] = df.error / df.weight\n",
    "    df['abs_error_pct'] = df.error_pct.abs()\n",
    "    \n",
    "    avg_biomass_err = df.loc[~mask, 'prediction'].mean() - df.loc[~mask, 'weight'].mean()\n",
    "    abs_err_pct = abs(avg_biomass_err) / df.loc[~mask, 'weight'].mean()\n",
    "    abs_err_pcts.append(abs_err_pct)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = sorted(list(abs_err_pcts))\n",
    "p = 1.0 * np.arange(len(data_sorted)) / (len(data_sorted) - 1)\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(p, data_sorted)\n",
    "ax1.set_xlabel('p')\n",
    "ax1.set_ylabel('OOS error percentage')\n",
    "plt.axvline(x=0.95, linestyle='--', color='red', label='p = 0.95')\n",
    "plt.title('CDF of OOS errors (sample size = 250)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(df[~mask].weight, df[~mask].prediction)\n",
    "plt.xlabel('Ground Truth Weight')\n",
    "plt.ylabel('Prediction')\n",
    "plt.plot(range(5000), range(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Conduct Error Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Plot 3D coordinates of keypoints </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def plot_3D_keypoints(norm_wkps, norm_closest_wkps):\n",
    "    body_parts = norm_wkps.keys()\n",
    "    xs = [norm_wkps[bp][0] for bp in body_parts]\n",
    "    ys = [norm_wkps[bp][1] for bp in body_parts]\n",
    "    zs = [norm_wkps[bp][2] for bp in body_parts]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlim3d(0, max(xs))\n",
    "    ax.set_ylim3d(-0.3, 0.3)\n",
    "    ax.set_zlim3d(-0.3, 0.3)\n",
    "\n",
    "    \n",
    "    ax.scatter(xs, ys, zs, color='blue')\n",
    "\n",
    "    for i, txt in enumerate(body_parts):\n",
    "        ax.text(xs[i], ys[i], zs[i], txt, size=5, color='blue')\n",
    "        \n",
    "    xs = [norm_closest_wkps[bp][0] for bp in body_parts]\n",
    "    ys = [norm_closest_wkps[bp][1] for bp in body_parts]\n",
    "    zs = [norm_closest_wkps[bp][2] for bp in body_parts]\n",
    "    ax.scatter(xs, ys, zs, color='red')\n",
    "    \n",
    "    for i, txt in enumerate(body_parts):\n",
    "        ax.text(xs[i], ys[i], zs[i], txt, size=5, color='red')\n",
    "        \n",
    "#     plt.axis('scaled')\n",
    "        \n",
    "    \n",
    "\n",
    "def plot_fish_id(gtsf_fish_identifier):\n",
    "    row = analysis_df[analysis_df.gtsf_fish_identifier == gtsf_fish_identifier].iloc[0]\n",
    "    \n",
    "    # get GTSF normalized world keypoint coordinates\n",
    "    wkps = json.loads(row.wkps)\n",
    "    body_parts = wkps.keys()\n",
    "    wkps = {bp: np.array(wkps[bp]) for bp in body_parts}\n",
    "    norm_wkps = normalize_world_keypoints(wkps)\n",
    "    \n",
    "    # get closest Blender model normalized world keypoint coordinates\n",
    "    closest_wkps = json.loads(row.closest_wkps)\n",
    "    closest_wkps = {bp: np.array(closest_wkps[bp]) for bp in body_parts}\n",
    "    norm_closest_wkps = normalize_world_keypoints(closest_wkps)\n",
    "    \n",
    "    plot_3D_keypoints(norm_wkps, norm_closest_wkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plot_fish_id(190325010034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
