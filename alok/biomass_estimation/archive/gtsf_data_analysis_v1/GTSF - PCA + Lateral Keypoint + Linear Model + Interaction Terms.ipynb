{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from multiprocessing import Pool, Manager\n",
    "import copy\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility functions for world keypoint normalization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    if type(p1) == list:\n",
    "        p1 = np.array(p1)\n",
    "    if type(p2) == list:\n",
    "        p2 = np.array(p2)\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "\n",
    "def normalize_world_keypoints(wkps):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['DORSAL_FIN'] - np.array([norm_wkps_intermediate['DORSAL_FIN'][0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform reflecton if necessary\n",
    "    if norm_wkps['PECTORAL_FIN'][1] > 0:\n",
    "        norm_wkps = {bp: np.array([\n",
    "            norm_wkps[bp][0],\n",
    "            -norm_wkps[bp][1],\n",
    "            norm_wkps[bp][2]\n",
    "        ]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility Method: World Keypoint Calculation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE OPTICAL PROPERTIES\n",
    "\n",
    "# all distance are in meters\n",
    "FOCAL_LENGTH = 0.00843663\n",
    "BASELINE = 0.128096\n",
    "PIXEL_SIZE_M = 3.45 * 1e-6\n",
    "FOCAL_LENGTH_PIXEL = FOCAL_LENGTH / PIXEL_SIZE_M\n",
    "IMAGE_SENSOR_WIDTH = 0.01412\n",
    "IMAGE_SENSOR_HEIGHT = 0.01035\n",
    "PIXEL_COUNT_WIDTH = 4096\n",
    "PIXEL_COUNT_HEIGHT = 3000\n",
    "\n",
    "def convert_to_world_point(x, y, d):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    \n",
    "    image_center_x = PIXEL_COUNT_WIDTH / 2.0  \n",
    "    image_center_y = PIXEL_COUNT_HEIGHT / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (IMAGE_SENSOR_WIDTH / PIXEL_COUNT_WIDTH)\n",
    "    sensor_z = px_z * (IMAGE_SENSOR_HEIGHT / PIXEL_COUNT_HEIGHT)\n",
    "\n",
    "    # d = depth_map[y, x]\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / FOCAL_LENGTH\n",
    "    world_z = (world_y * sensor_z) / FOCAL_LENGTH\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "def depth_from_disp(disp):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    depth = FOCAL_LENGTH_PIXEL*BASELINE / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "def disp_from_depth(depth):\n",
    "    disp = FOCAL_LENGTH_PIXEL * BASELINE / depth\n",
    "    return disp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate accuracy metrics on GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints, \n",
    "                               bp_1='UPPER_LIP', bp_2='TAIL_NOTCH', left_window_size=100, \n",
    "                               min_breadth=0.04, max_breadth=0.2):\n",
    "    left_extrap_kp = (0.5 * left_keypoints[bp_1] + 0.5 * left_keypoints[bp_2]).astype('int64')\n",
    "    bp_1_depth = world_keypoints[bp_1][1]\n",
    "    bp_2_depth = world_keypoints[bp_2][1]\n",
    "\n",
    "    # need to determine lower and upper bounds here in a data driven fashion from GTSF data\n",
    "    # hardcoded values used here\n",
    "    extrap_kp_max_depth = (bp_1_depth + bp_2_depth) / 2.0 - min_breadth / 2.0\n",
    "    extrap_kp_min_depth = (bp_1_depth + bp_2_depth) / 2.0 - max_breadth / 2.0\n",
    "\n",
    "    # Compute the feature descriptor for the extrapolated keypoint in the left image\n",
    "    extrap_kp_min_disp = disp_from_depth(extrap_kp_max_depth)\n",
    "    extrap_kp_max_disp = disp_from_depth(extrap_kp_min_depth)\n",
    "    \n",
    "    left_box = left_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2, \n",
    "                          left_extrap_kp[0]-left_window_size//2:left_extrap_kp[0]+left_window_size//2]\n",
    "    right_box = right_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2,\n",
    "                            left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2:left_extrap_kp[0]-int(extrap_kp_min_disp)+left_window_size//2]\n",
    "\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(left_box,None)\n",
    "    kp2, des2 = orb.detectAndCompute(right_box,None)\n",
    "    \n",
    "    # get top five matches\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1,des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)[:5]\n",
    "    \n",
    "    # get world coordinates of lateral keypoints\n",
    "    lateral_wkps = []\n",
    "    for match in matches[:5]:\n",
    "        \n",
    "        lateral_left_coordinates = np.array(kp1[match.queryIdx].pt).astype(int)\n",
    "        lateral_left_coordinates[0] += left_extrap_kp[0]-left_window_size//2\n",
    "        lateral_left_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        lateral_right_coordinates = np.array(kp2[match.trainIdx].pt).astype(int)\n",
    "        lateral_right_coordinates[0] += left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2\n",
    "        lateral_right_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        disp = abs(lateral_left_coordinates[0] - lateral_right_coordinates[0])\n",
    "        depth = depth_from_disp(disp)\n",
    "        lateral_wkp = convert_to_world_point(lateral_left_coordinates[0], lateral_left_coordinates[1], depth)\n",
    "        lateral_wkps.append(lateral_wkp)\n",
    "        \n",
    "    return np.array(lateral_wkps)\n",
    "\n",
    "\n",
    "def process_stereo_frame_pair(sfp, world_keypoints_dict):\n",
    "    if sfp['gtsf_fish_identifier'] == '190321010002':\n",
    "        return\n",
    "    ground_truth_metadata = json.loads(sfp['ground_truth_metadata'])\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        return\n",
    "        \n",
    "    # download left and right images\n",
    "    random_hash = uuid.uuid4().hex\n",
    "    left_image_f = 'left_image_{}.jpg'.format(random_hash)\n",
    "    right_image_f = 'right_image_{}.jpg'.format(random_hash)\n",
    "\n",
    "    left_image_s3_key, right_image_s3_key, s3_bucket = sfp['left_image_s3_key'], sfp['right_image_s3_key'], sfp['image_s3_bucket']\n",
    "    s3_client.download_file(s3_bucket, left_image_s3_key, left_image_f)\n",
    "    s3_client.download_file(s3_bucket, right_image_s3_key, right_image_f)\n",
    "    \n",
    "    left_image = cv2.imread(left_image_f)\n",
    "    right_image = cv2.imread(right_image_f)\n",
    "    \n",
    "    # get left, right, and world keypoints\n",
    "    left_keypoints = json.loads(sfp['left_image_keypoint_coordinates'])\n",
    "    right_keypoints = json.loads(sfp['right_image_keypoint_coordinates'])\n",
    "    world_keypoints = json.loads(sfp['world_keypoint_coordinates'])\n",
    "    \n",
    "    # convert coordinates from lists to numpy arrays\n",
    "    left_keypoints = {k: np.array(v) for k, v in left_keypoints.items()}\n",
    "    right_keypoints = {k: np.array(v) for k, v in right_keypoints.items()}\n",
    "    world_keypoints = {k: np.array(v) for k, v in world_keypoints.items()}\n",
    "     \n",
    "    lateral_wkps = generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints)\n",
    "    world_keypoints['BODY'] = lateral_wkps\n",
    "    world_keypoints_dict[sfp['id']] = world_keypoints\n",
    "    if len(world_keypoints_dict) % 10 == 0:\n",
    "        print(len(world_keypoints_dict))\n",
    "    \n",
    "    os.remove(left_image_f)\n",
    "    os.remove(right_image_f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "world_keypoints_dict = manager.dict()\n",
    "\n",
    "session.rollback()\n",
    "sfps_all = session.query(StereoFramePair).all()\n",
    "args = []\n",
    "for row in sfps_all:\n",
    "    row_copy = copy.copy(row)\n",
    "    sfp = row_copy.__dict__\n",
    "    del sfp['_sa_instance_state']\n",
    "    args.append((sfp, world_keypoints_dict))\n",
    "\n",
    "pool = Pool(processes=20)\n",
    "pool.starmap(process_stereo_frame_pair, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train linear model with PCA + interaction features </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfps_all = session.query(StereoFramePair).all()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "])\n",
    "\n",
    "session.rollback()\n",
    "for idx, row in enumerate(sfps_all):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        \n",
    "    # get fish_id and ground truth metadata\n",
    "    if row.gtsf_fish_identifier == '190321010002':\n",
    "        continue\n",
    "    gtsf_data_collection_id = row.gtsf_data_collection_id\n",
    "    gtsf_data_collection = session.query(GtsfDataCollection).get(gtsf_data_collection_id)\n",
    "    ground_truth_metadata = json.loads(gtsf_data_collection.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "#     world_keypoints = world_keypoints_dict[row.id]\n",
    "    wkps = json.loads(row.world_keypoint_coordinates)\n",
    "#     body_wkp = world_keypoints['BODY'].mean(axis=0)\n",
    "#     wkps = copy.copy(world_keypoints)\n",
    "#     wkps['BODY'] = body_wkp\n",
    "    df_row = {'0': idx}\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            d = euclidean_distance(wkps[body_parts[i]], wkps[body_parts[j]])\n",
    "            df_row['{0}-{1}'.format(i, j)] = d\n",
    "    \n",
    "    \n",
    "    weight = ground_truth_metadata['data']['weight']\n",
    "    length = ground_truth_metadata['data']['length']\n",
    "    width = ground_truth_metadata['data']['width']\n",
    "    breadth = ground_truth_metadata['data']['breath']\n",
    "    df_row['weight'] = weight\n",
    "    df_row['length'] = length\n",
    "    df_row['width'] = width\n",
    "    df_row['breadth'] = breadth\n",
    "    df_row['kfactor'] = 1e5 * weight / length**3\n",
    "    df_row['date'] = row.date\n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "df_cache = df.copy()\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_mask(df, train_frac, randomize=True):\n",
    "    x = np.zeros((df.shape[0]), dtype=bool)\n",
    "    x[:int(train_frac * df.shape[0])] = True\n",
    "    np.random.shuffle(x)\n",
    "    mask = pd.Series(x)\n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all features\n",
    "\n",
    "\n",
    "pairwise_distance_columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(list(range(len(body_parts))), 2))]\n",
    "interaction_columns = []\n",
    "for i in range(len(pairwise_distance_columns)):\n",
    "    for j in range(i, len(pairwise_distance_columns)):\n",
    "        col1 = pairwise_distance_columns[i]\n",
    "        col2 = pairwise_distance_columns[j]\n",
    "        interaction_column = '{},{}'.format(col1, col2)\n",
    "        df[interaction_column] = df[col1] * df[col2]\n",
    "        interaction_columns.append(interaction_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_train_mask(df, train_frac=0.8)\n",
    "mask = mask & (df.date.astype(str) < '2019-05-02')\n",
    "\n",
    "\n",
    "columns = pairwise_distance_columns + interaction_columns\n",
    "\n",
    "X_train = df.loc[mask, columns].values\n",
    "y_train = df.loc[mask, 'weight'].values\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=min(X_train_normalized.shape[0], X_train_normalized.shape[1]))\n",
    "pca.fit(X_train_normalized)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "idx = np.where(explained_variance_ratio > 0.99999)[0][0]\n",
    "print(idx)\n",
    "\n",
    "pca = PCA(n_components=idx+1)\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_transformed = pca.transform(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_test_transformed = pca.transform(X_test_normalized)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_transformed, y_train)\n",
    "print(reg.score(X_test_transformed, y_test))\n",
    "\n",
    "y_pred = reg.predict(pca.transform(scaler.transform(df[columns].values)))\n",
    "df['prediction'] = y_pred\n",
    "df['error'] = df.prediction - df.weight\n",
    "df['error_pct'] = df.error / df.weight\n",
    "df['abs_error_pct'] = df.error_pct.abs()\n",
    "\n",
    "model = {\n",
    "    'mean': scaler.mean_,\n",
    "    'std': scaler.scale_,\n",
    "    'PCA_components': pca.components_,\n",
    "    'reg_coef': reg.coef_,\n",
    "    'reg_intercept': reg.intercept_,\n",
    "    'body_parts': body_parts   \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_train_mask(df, train_frac=0.8)\n",
    "columns = pairwise_distance_columns + interaction_columns\n",
    "\n",
    "X_test = df.loc[~mask, columns].values\n",
    "y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "X_test_normalized = (X_test - output['mean']) / output['std']\n",
    "X_test_transformed = np.dot(X_test_normalized, output['PCA_components'].T)\n",
    "\n",
    "y_pred = np.dot(X_test_transformed, reg.coef_) + reg.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - output['mean']) / output['std']\n",
    "    X_transformed = np.dot(X_normalized, output['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg.coef_) + reg.intercept_\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = model['mean']\n",
    "std = model['std']\n",
    "PCA_components = model['PCA_components']\n",
    "reg_coef = model['reg_coef']\n",
    "reg_intercept = model['reg_intercept']\n",
    "body_parts = model['body_parts']\n",
    "\n",
    "# calculate pairwise distances for production coord\n",
    "# based on the exact ordering reflected in the body_parts\n",
    "# variable above\n",
    "\n",
    "pairwise_distances = []\n",
    "for i in range(len(body_parts)-1):\n",
    "    for j in range(i+1, len(body_parts)):\n",
    "        dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "        pairwise_distances.append(dist)\n",
    "\n",
    "interaction_values = []\n",
    "for i in range(len(pairwise_distances)):\n",
    "    for j in range(i, len(pairwise_distances)):\n",
    "        dist1 = pairwise_distances[i]\n",
    "        dist2 = pairwise_distances[j]\n",
    "        interaction_values.append(dist1 * dist2)\n",
    "\n",
    "X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "X_normalized = (X - model['mean']) / model['std']\n",
    "X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "prediction = np.dot(X_transformed, reg.coef_) + reg.intercept_\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "date_mask = df.date.astype(str) != '2019-05-02'\n",
    "plt.scatter(df[date_mask]['weight'], df[date_mask]['prediction'])\n",
    "plt.scatter(df[~date_mask]['weight'], df[~date_mask]['prediction'], color='r')\n",
    "plt.xlabel('Ground Truth Weight')\n",
    "plt.ylabel('Prediction')\n",
    "plt.plot(range(5000), range(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.grid()\n",
    "plt.xlabel('K Factor')\n",
    "plt.ylabel('Error')\n",
    "plt.scatter(df['kfactor'], df['error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "# perform N-fold cross validation\n",
    "abs_err_pcts = []\n",
    "error_stds = []\n",
    "for i in range(N):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    mask = generate_train_mask(df, train_frac=0.8)\n",
    "\n",
    "    columns = ['{0}-{1}'.format(x, y) for x, y in list(combinations(list(range(8)), 2))]\n",
    "    X_train = df.loc[mask, columns].values\n",
    "    y_train = df.loc[mask, 'weight'].values\n",
    "    X_test = df.loc[~mask, columns].values\n",
    "    y_test = df.loc[~mask, 'weight'].values\n",
    "\n",
    "    pca = PCA(n_components=min(X_train.shape[0], X_train.shape[1]))\n",
    "    pca.fit(X_train)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "    idx = np.where(explained_variance_ratio > 0.99999)[0][0]\n",
    "\n",
    "    pca = PCA(n_components=idx+1)\n",
    "    pca.fit(X_train)\n",
    "    X_train_modified = pca.transform(X_train)\n",
    "    X_test_modified = pca.transform(X_test)\n",
    "    reg = LinearRegression().fit(X_train_modified, y_train)\n",
    "\n",
    "\n",
    "    y_pred = reg.predict(pca.transform(df[columns].values))\n",
    "    df['prediction'] = y_pred\n",
    "    df['error'] = df.prediction - df.weight\n",
    "    df['error_pct'] = df.error / df.weight\n",
    "    df['abs_error_pct'] = df.error_pct.abs()\n",
    "    error_std = df.loc[~mask, 'error_pct'].std()\n",
    "    error_stds.append(error_std)\n",
    "\n",
    "    \n",
    "    avg_biomass_err = df.loc[~mask, 'prediction'].mean() - df.loc[~mask, 'weight'].mean()\n",
    "    abs_err_pct = abs(avg_biomass_err) / df.loc[~mask, 'weight'].mean()\n",
    "    abs_err_pcts.append(abs_err_pct)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = sorted(list(abs_err_pcts))\n",
    "p = 1.0 * np.arange(len(data_sorted)) / (len(data_sorted) - 1)\n",
    "fig = plt.figure(figsize=(30, 7))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(p, data_sorted)\n",
    "ax1.set_xlabel('p')\n",
    "ax1.set_ylabel('OOS error percentage')\n",
    "plt.axvline(x=0.95, linestyle='--', color='red', label='p = 0.95')\n",
    "plt.title('CDF of OOS errors (sample size = 250)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prod implementation test </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sqlalchemy import create_engine, MetaData, Table, exc\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**0.5\n",
    "\n",
    "\n",
    "def convert_to_world_point(x, y, d, parameters):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    # get relevant parameters\n",
    "    pixel_count_height = parameters[\"pixelCountHeight\"]\n",
    "    pixel_count_width = parameters[\"pixelCountWidth\"]\n",
    "    sensor_width = parameters[\"imageSensorWidth\"]\n",
    "    sensor_height = parameters[\"imageSensorHeight\"]\n",
    "    focal_length = parameters[\"focalLength\"]\n",
    "\n",
    "    image_center_x = pixel_count_height / 2.0\n",
    "    image_center_y = pixel_count_width / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (sensor_height / pixel_count_height)\n",
    "    sensor_z = px_z * (sensor_width / pixel_count_width)\n",
    "\n",
    "    # now move to world coordinates\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / focal_length\n",
    "    world_z = (world_y * sensor_z) / focal_length\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "\n",
    "def depth_from_disp(disp, parameters):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    focal_length_pixel = parameters[\"focalLengthPixel\"]\n",
    "\n",
    "    baseline = parameters[\"baseline\"]\n",
    "    depth = focal_length_pixel * baseline / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "def pixel2world(left_crop, right_crop, parameters):\n",
    "    \"\"\"2D pixel coordinates to 3D world coordinates\"\"\"\n",
    "\n",
    "    # first create a dic with crop keypoints\n",
    "    image_coordinates = {\"leftCrop\": {},\n",
    "                         \"rightCrop\": {}}\n",
    "    for keypoint in left_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        image_coordinates[\"leftCrop\"][name] = [keypoint[\"xFrame\"], keypoint[\"yFrame\"]]\n",
    "    for keypoint in right_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        image_coordinates[\"rightCrop\"][name] = [keypoint[\"xFrame\"], keypoint[\"yFrame\"]]\n",
    "\n",
    "    # then loop through the right crop keypoints and calculate the world coordinates\n",
    "    world_coordinates = {}\n",
    "    for keypoint in left_crop:\n",
    "        name = keypoint[\"keypointType\"]\n",
    "        disparity = image_coordinates[\"leftCrop\"][name][0] - image_coordinates[\"rightCrop\"][name][0]\n",
    "        depth = depth_from_disp(disparity, parameters)\n",
    "        world_point = convert_to_world_point(image_coordinates[\"leftCrop\"][name][1],\n",
    "                                             image_coordinates[\"leftCrop\"][name][0],\n",
    "                                             depth,\n",
    "                                             parameters)\n",
    "        world_coordinates[name] = world_point\n",
    "    return world_coordinates\n",
    "\n",
    "\n",
    "def coord2biomass(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "    print(body_parts)\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "            print(body_parts[i], body_parts[j], dist)\n",
    "    print(pairwise_distances)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondata = json.load(open('sample_message.json'))\n",
    "model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_id = jsondata[\"annotationId\"]\n",
    "parameters = jsondata[\"cameraParameters\"]\n",
    "right_crop = jsondata[\"rightCrop\"]\n",
    "left_crop = jsondata[\"leftCrop\"]\n",
    "site_id = jsondata[\"siteId\"]\n",
    "pen_id = jsondata[\"penId\"]\n",
    "\n",
    "\n",
    "\n",
    "# pixel coordinates to world coordinates\n",
    "coordinates = pixel2world(left_crop, right_crop, parameters)\n",
    "biomass = coord2biomass(coordinates, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(coordinates['TAIL_NOTCH'], coordinates['UPPER_LIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
