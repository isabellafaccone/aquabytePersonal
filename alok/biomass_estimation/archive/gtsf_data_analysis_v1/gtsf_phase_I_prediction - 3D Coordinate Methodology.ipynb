{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/data/alok/blender_data/volumes_all.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(np.array(data[\"dimensions\"])[:, 1], data[\"volume\"])\n",
    "# plt.ylabel(\"Volume (cm^3)\")\n",
    "# plt.xlabel(\"Length (mm)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"volume\"])\n",
    "plt.title(\"Blender volume histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility functions for world keypoint normalization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_world_keypoints(wkps):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['DORSAL_FIN'] - np.array([norm_wkps_intermediate['DORSAL_FIN'][0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get normalized world keyponts of all cached Blender models </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.linalg.norm(p1-p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bp = {\n",
    "    'UPPER_LIP': 1.0,\n",
    "    'PECTORAL_FIN': 1.0,\n",
    "    'TAIL_NOTCH': 1.0,\n",
    "    'DORSAL_FIN': 1.0,\n",
    "    'ANAL_FIN': 1.0,\n",
    "    'ADIPOSE_FIN': 1.0,\n",
    "    'EYE': 1.0,\n",
    "    'PELVIC_FIN': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfps = session.query(StereoFramePair).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_wkps = {bp: 1e-3*np.array(data['coordinates'][0][bp]) for bp in data['mapping'].keys()}\n",
    "\n",
    "norm_canonical_wkps = normalize_world_keypoints(canonical_wkps)\n",
    "canonical_volume = data['volume'][0]\n",
    "\n",
    "analysis_df = pd.DataFrame()\n",
    "predicted_volumes = []\n",
    "gt_biomass = []\n",
    "gt_kfactor = []\n",
    "y_factors = []\n",
    "ys = []\n",
    "for idx, row in enumerate(sfps):\n",
    "    # extract and normalize the predicted 3D keypoints\n",
    "    wkps = json.loads(row.world_keypoint_coordinates)\n",
    "    wkps = {bp: np.array(wkps[bp]) for bp in wkps.keys()}\n",
    "    norm_wkps = normalize_world_keypoints(wkps)\n",
    "    \n",
    "    ground_truth_metadata = json.loads(row.ground_truth_metadata)\n",
    "    species = ground_truth_metadata['data'].get('species')\n",
    "    \n",
    "    if species != 'salmon':\n",
    "        continue\n",
    "    ground_truth_biomass = ground_truth_metadata['data']['weight']\n",
    "    ground_truth_length = ground_truth_metadata['data']['length']\n",
    "    \n",
    "    x_factor = abs(sum([norm_canonical_wkps[bp][0]*norm_wkps[bp][0]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][0]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    y_factor = abs(sum([norm_canonical_wkps[bp][1]*norm_wkps[bp][1]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][1]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    z_factor = abs(sum([norm_canonical_wkps[bp][2]*norm_wkps[bp][2]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][2]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    volume = canonical_volume * x_factor * z_factor * (1 + (y_factor - 0.9) * 0.12)\n",
    "    y = norm_wkps['PECTORAL_FIN'][1]-norm_wkps['UPPER_LIP'][1]\n",
    "    ys.append(y_factor)\n",
    "    if ground_truth_biomass > 1000:\n",
    "        gt_biomass.append(ground_truth_biomass)\n",
    "        predicted_volumes.append(volume)\n",
    "        gt_kfactor.append(1e5 * ground_truth_biomass / ground_truth_length**3)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predicted_volumes)[:, np.newaxis]\n",
    "reg = LinearRegression().fit(predictions, gt_biomass)\n",
    "print(reg.coef_, reg.intercept_)\n",
    "print(\"R2 : {}\".format(reg.score(predictions, gt_biomass)))\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "plt.scatter(gt_biomass, predictions*reg.coef_ + reg.intercept_,  c=gt_kfactor)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Ground truth weight\")\n",
    "plt.ylabel(\"Predicted weight\")\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.array(list(zip(gt_biomass, gt_kfactor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_predictions = predictions*reg.coef_ + reg.intercept_\n",
    "error = fitted_predictions-gt_biomass\n",
    "print(\"Average absolute error: {}\".format(np.nanmean(np.abs(error))))\n",
    "print(\"Average error: {}\".format(np.nanmean(error)))\n",
    "# error5 = predictions_average-ground_truth\n",
    "#print(\"Average absolute error5: {}\".format(np.nanmean(np.abs(error5))))\n",
    "relative_error = ((fitted_predictions-gt_biomass) / gt_biomass)*100\n",
    "print(\"Average relative error: {} %\".format(np.nanmean(relative_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(error)\n",
    "dist_space = np.linspace( min(error), max(error), 100 )\n",
    "plt.hist(error, bins=20, density=True)\n",
    "plt.plot( dist_space, kde(dist_space) )\n",
    "plt.title(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(relative_error)\n",
    "dist_space = np.linspace( min(relative_error), max(relative_error), 100 )\n",
    "plt.hist(relative_error, bins=20, density=True)\n",
    "plt.plot( dist_space, kde(dist_space) )\n",
    "plt.title(\"Relative Error (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(0, 101, 5)\n",
    "percentiles = np.percentile(np.abs(relative_error), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(percentiles, values)\n",
    "plt.yticks(np.arange(0,101,5))\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Absolute relative error (%)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = norm.fit(fitted_predictions)\n",
    "print(\"Mean: {}, Standard deviation: {}\".format(mean, std))\n",
    "plt.hist(fitted_predictions, bins=20, normed=True)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "y = norm.pdf(x, mean, std)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(fitted_predictions, norm(loc=mean, scale=std).cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cross validation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.squeeze(predictions)\n",
    "all_errors = []\n",
    "all_relative_errors = []\n",
    "for i in range(1000):\n",
    "    predictions = predictions[:, np.newaxis]\n",
    "    test_size = i\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictions, gt_biomass, test_size=0.2)\n",
    "    X_test= np.squeeze(X_test)\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    # print(reg.coef_, reg.intercept_)\n",
    "    # print(\"R2 : {}\".format(reg.score(X_train, y_train)))\n",
    "    predictions = np.squeeze(predictions)\n",
    "    \n",
    "    \n",
    "    fitted_X_test = X_test*reg.coef_ + reg.intercept_\n",
    "    error = fitted_X_test-y_test\n",
    "    relative_error = ((fitted_X_test-y_test) / y_test)*100\n",
    "    all_errors.append(np.nanmean(error))\n",
    "    all_relative_errors.append(np.nanmean(relative_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_errors)\n",
    "plt.xlabel(\"Average error distribution\")\n",
    "plt.show()\n",
    "plt.hist(all_relative_errors)\n",
    "plt.xlabel(\"Average relative error distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Extra Metrics </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "errors_means = []\n",
    "kfactors = []\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "for i in range(1000):\n",
    "    predictions = predictions[:, np.newaxis]\n",
    "    test_size = i\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictions, ground_truth, test_size=0.2)\n",
    "    X_test = np.squeeze(X_test)\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train, y_train[:, 0])\n",
    "    predictions = np.squeeze(predictions)\n",
    "    \n",
    "    fitted_X_test = X_test*reg.coef_ + reg.intercept_\n",
    "    error_mean = np.mean(fitted_X_test) - np.mean(y_test[:, 0])\n",
    "    error = fitted_X_test - y_test[:, 0]\n",
    "    errors_means.append(error_mean)\n",
    "    errors.append(error)\n",
    "    kfactors.append(y_test[:, 1])\n",
    "#     relative_error = ((fitted_X_test-y_test) / y_test)*100\n",
    "#     all_errors.append(np.nanmean(error))\n",
    "#     all_relative_errors.append(np.nanmean(relative_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(errors_means)\n",
    "plt.title(\"Error on mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, 1001)\n",
    "abs_error = np.abs(errors[idx])\n",
    "plt.scatter(kfactors[idx], errors[idx])\n",
    "plt.xlabel(\"K factor\")\n",
    "plt.ylabel(\"absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "# isolate 50% of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictions, gt_biomass, test_size=0.5)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.squeeze(X_test)\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "fitted_X_test = X_test*reg.coef_ + reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50, 260, 50):\n",
    "    predictions = predictions[:, np.newaxis]\n",
    "    test_size = i\n",
    "    tmp = []\n",
    "    for j in range(100):\n",
    "        random_idx = np.random.choice(range(len(X_test)), size=i, replace=False)\n",
    "        fitted_X_test_subset = fitted_X_test[random_idx]\n",
    "        y_test_subset = np.array(y_test)[random_idx]\n",
    "\n",
    "        # error = fitted_X_test - y_test[:, 0]\n",
    "        # relative_error = np.abs(((fitted_X_test-y_test_subset) / y_test_subset)*100)\n",
    "        err = (np.mean(fitted_X_test) - np.mean(y_test_subset))*100 / np.mean(y_test_subset)\n",
    "        err = np.abs(err)\n",
    "        tmp.append(err)\n",
    "        # tmp.append(np.mean(relative_error))\n",
    "    errors.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(50, 260, 50):\n",
    "    values = np.arange(0, 101, 5)\n",
    "    percentiles = np.percentile(errors[c], values)\n",
    "    plt.plot(values, percentiles, label=\"Sample size {}\".format(i))\n",
    "    c += 1\n",
    "    \n",
    "plt.xticks(np.arange(0,101,5))\n",
    "plt.yticks(np.arange(0, 9, 1))\n",
    "plt.xlabel(\"Percentage\")\n",
    "# plt.ylabel(\"Mean Absolute relative error (%)\")\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
