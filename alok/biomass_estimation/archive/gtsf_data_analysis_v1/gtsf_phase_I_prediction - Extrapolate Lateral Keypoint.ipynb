{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSF phase: biomass prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are forecasting the weights by finding the closest blender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the volumes created with blender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load blender data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/data/alok/blender_data/volumes_all.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"volume\"])\n",
    "plt.title(\"Blender volume histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get world keypoint coordinates from GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "sql_credentials = json.load(open(os.environ[\"SQL_CREDENTIALS\"]))\n",
    "sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                           sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                           sql_credentials[\"database\"]))\n",
    "\n",
    "Session = sessionmaker(bind=sql_engine)\n",
    "session = Session()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(sql_engine, reflect=True)\n",
    "Enclosure = Base.classes.enclosures\n",
    "Calibration = Base.classes.calibrations\n",
    "GtsfDataCollection = Base.classes.gtsf_data_collections\n",
    "StereoFramePair = Base.classes.stereo_frame_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_data = []\n",
    "body_parts = ['ADIPOSE_FIN', 'ANAL_FIN', 'TAIL_NOTCH', 'PECTORAL_FIN', 'PELVIC_FIN', 'UPPER_LIP', 'EYE', 'DORSAL_FIN']\n",
    "for idx, coord in enumerate(data['coordinates']):\n",
    "    obj = {bp: [1e-2*x for x in coord[bp]] for bp in body_parts}\n",
    "    obj['biomass'] = 1.88*data['volume'][idx]\n",
    "    challenge_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(challenge_data[:10], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility functions for world keypoint normalization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_world_keypoints(wkps):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp].T).T for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "#     k = np.array([norm_wkps_intermediate['DORSAL_FIN'][0], \n",
    "#                   (norm_wkps_intermediate['DORSAL_FIN'][1] + norm_wkps_intermediate['ADIPOSE_FIN'][1])/2.0,\n",
    "#                   norm_wkps_intermediate['DORSAL_FIN'][2]])\n",
    "    \n",
    "    k = norm_wkps_intermediate['ANAL_FIN']\n",
    "    v = k - np.array([k[0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp].T).T for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility Method: World Keypoint Calculation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE OPTICAL PROPERTIES\n",
    "\n",
    "# all distance are in meters\n",
    "FOCAL_LENGTH = 0.00843663\n",
    "BASELINE = 0.128096\n",
    "PIXEL_SIZE_M = 3.45 * 1e-6\n",
    "FOCAL_LENGTH_PIXEL = FOCAL_LENGTH / PIXEL_SIZE_M\n",
    "IMAGE_SENSOR_WIDTH = 0.01412\n",
    "IMAGE_SENSOR_HEIGHT = 0.01035\n",
    "PIXEL_COUNT_WIDTH = 4096\n",
    "PIXEL_COUNT_HEIGHT = 3000\n",
    "\n",
    "def convert_to_world_point(x, y, d):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    \n",
    "    image_center_x = PIXEL_COUNT_WIDTH / 2.0  \n",
    "    image_center_y = PIXEL_COUNT_HEIGHT / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (IMAGE_SENSOR_WIDTH / PIXEL_COUNT_WIDTH)\n",
    "    sensor_z = px_z * (IMAGE_SENSOR_HEIGHT / PIXEL_COUNT_HEIGHT)\n",
    "\n",
    "    # d = depth_map[y, x]\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / FOCAL_LENGTH\n",
    "    world_z = (world_y * sensor_z) / FOCAL_LENGTH\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "def depth_from_disp(disp):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    depth = FOCAL_LENGTH_PIXEL*BASELINE / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "def disp_from_depth(depth):\n",
    "    disp = FOCAL_LENGTH_PIXEL * BASELINE / depth\n",
    "    return disp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load canonical Blender model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraboloid_fn(reg):\n",
    "    c = reg.coef_\n",
    "    i = reg.intercept_\n",
    "    \n",
    "    def paraboloid_fn(x, z):\n",
    "        return c[0]*x**2 + c[1]*z**2 + c[2]*x*z + c[3]*x + c[4]*z + i\n",
    "    \n",
    "    return paraboloid_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_model_json = json.load(open('./single.json'))\n",
    "blender_model = {bp: 1e-2*np.array(blender_model_json['coordinates'][0][bp]) for bp in blender_model_json['coordinates'][0].keys()} \n",
    "norm_canonical_wkps = normalize_world_keypoints(blender_model)\n",
    "\n",
    "# find best fit paraboloid for lateral keypoints\n",
    "lateral_wkps = norm_canonical_wkps['BODY']\n",
    "A = np.empty([lateral_wkps.shape[0], 5])\n",
    "A[:, 0] = lateral_wkps[:, 0]**2\n",
    "A[:, 1] = lateral_wkps[:, 2]**2\n",
    "A[:, 2] = lateral_wkps[:, 0] * lateral_wkps[:, 2]\n",
    "A[:, 3] = lateral_wkps[:, 0]\n",
    "A[:, 4] = lateral_wkps[:, 2]\n",
    "\n",
    "b = lateral_wkps[:, 1]\n",
    "\n",
    "\n",
    "reg = LinearRegression().fit(A, b)\n",
    "paraboloid_fn = generate_paraboloid_fn(reg)\n",
    "\n",
    "canonical_volume = blender_model_json['volume'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate accuracy metrics on GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = [\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lateral_keypoints(left_keypoints, right_keypoints, world_keypoints, \n",
    "                               bp_1='UPPER_LIP', bp_2='TAIL_NOTCH', left_window_size=100, \n",
    "                               min_breadth=0.04, max_breadth=0.2):\n",
    "    left_extrap_kp = (0.5 * left_keypoints[bp_1] + 0.5 * left_keypoints[bp_2]).astype('int64')\n",
    "    bp_1_depth = world_keypoints[bp_1][1]\n",
    "    bp_2_depth = world_keypoints[bp_2][1]\n",
    "\n",
    "    # need to determine lower and upper bounds here in a data driven fashion from GTSF data\n",
    "    # hardcoded values used here\n",
    "    extrap_kp_max_depth = (bp_1_depth + bp_2_depth) / 2.0 - min_breadth / 2.0\n",
    "    extrap_kp_min_depth = (bp_1_depth + bp_2_depth) / 2.0 - max_breadth / 2.0\n",
    "\n",
    "    # Compute the feature descriptor for the extrapolated keypoint in the left image\n",
    "    extrap_kp_min_disp = disp_from_depth(extrap_kp_max_depth)\n",
    "    extrap_kp_max_disp = disp_from_depth(extrap_kp_min_depth)\n",
    "    \n",
    "    left_box = left_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2, \n",
    "                          left_extrap_kp[0]-left_window_size//2:left_extrap_kp[0]+left_window_size//2]\n",
    "    right_box = right_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2,\n",
    "                            left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2:left_extrap_kp[0]-int(extrap_kp_min_disp)+left_window_size//2]\n",
    "\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(left_box,None)\n",
    "    kp2, des2 = orb.detectAndCompute(right_box,None)\n",
    "    \n",
    "    # get top five matches\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1,des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)[:5]\n",
    "    \n",
    "    # get world coordinates of lateral keypoints\n",
    "    lateral_wkps = []\n",
    "    for match in matches[:5]:\n",
    "        \n",
    "        lateral_left_coordinates = np.array(kp1[match.queryIdx].pt).astype(int)\n",
    "        lateral_left_coordinates[0] += left_extrap_kp[0]-left_window_size//2\n",
    "        lateral_left_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        lateral_right_coordinates = np.array(kp2[match.trainIdx].pt).astype(int)\n",
    "        lateral_right_coordinates[0] += left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2\n",
    "        lateral_right_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        disp = abs(lateral_left_coordinates[0] - lateral_right_coordinates[0])\n",
    "        depth = depth_from_disp(disp)\n",
    "        lateral_wkp = convert_to_world_point(lateral_left_coordinates[0], lateral_left_coordinates[1], depth)\n",
    "        lateral_wkps.append(lateral_wkp)\n",
    "        \n",
    "    return np.array(lateral_wkps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfps = session.query(StereoFramePair).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "left_image_f = 'left_image.jpg'\n",
    "right_image_f = 'right_image.jpg'\n",
    "\n",
    "world_keypoints_dict = {}\n",
    "for row in tqdm.tqdm(sfps):\n",
    "        \n",
    "    # download left and right images\n",
    "    left_image_s3_key, right_image_s3_key, s3_bucket = row.left_image_s3_key, row.right_image_s3_key, row.image_s3_bucket\n",
    "    s3_client.download_file(s3_bucket, left_image_s3_key, left_image_f)\n",
    "    s3_client.download_file(s3_bucket, right_image_s3_key, right_image_f)\n",
    "    \n",
    "    left_image = cv2.imread(left_image_f)\n",
    "    right_image = cv2.imread(right_image_f)\n",
    "    \n",
    "    # get left, right, and world keypoints\n",
    "    left_keypoints = json.loads(row.left_image_keypoint_coordinates)\n",
    "    right_keypoints = json.loads(row.right_image_keypoint_coordinates)\n",
    "    world_keypoints = json.loads(row.world_keypoint_coordinates)\n",
    "    \n",
    "    # convert coordinates from lists to numpy arrays\n",
    "    left_keypoints = {k: np.array(v) for k, v in left_keypoints.items()}\n",
    "    right_keypoints = {k: np.array(v) for k, v in right_keypoints.items()}\n",
    "    world_keypoints = {k: np.array(v) for k, v in world_keypoints.items()}\n",
    "     \n",
    "    lateral_wkps = generate_lateral_keypoints(left_keypoints, right_keypoints, world_keypoints)\n",
    "    world_keypoints['BODY'] = lateral_wkps\n",
    "    world_keypoints_dict[row.id] = world_keypoints\n",
    "#     norm_wkps = normalize_world_keypoints(world_keypoints)\n",
    "        \n",
    "#     # Determine how to fit canonical Blender model to this GTSF fish\n",
    "#     x_factor = abs(sum([norm_canonical_wkps[bp][0]*norm_wkps[bp][0] for bp in body_parts]) / \\\n",
    "#                sum([norm_canonical_wkps[bp][0]**2 for bp in body_parts]))\n",
    "    \n",
    "#     z_factor = abs(sum([norm_canonical_wkps[bp][2]*norm_wkps[bp][2] for bp in body_parts]) / \\\n",
    "#                sum([norm_canonical_wkps[bp][2]**2 for bp in body_parts]))\n",
    "    \n",
    "#     y_factor =  sum([coordinate[1]*paraboloid_fn(coordinate[0], coordinate[2]) for coordinate in norm_wkps['BODY']]) / \\\n",
    "#                 sum([paraboloid_fn(coordinate[0], coordinate[2])**2 for coordinate in norm_wkps['BODY']])\n",
    "    \n",
    "#     predicted_volume = canonical_volume*abs(x_factor)*abs(y_factor)*abs(z_factor)\n",
    "#     predicted_volumes.append(predicted_volume)\n",
    "#     gt_biomass.append(ground_truth_metadata['data']['weight'])\n",
    "#     print(ground_truth_metadata['data']['breath'], y_factor)\n",
    "#     breadths.append(ground_truth_metadata['data']['breath'])\n",
    "#     y_factors.append(y_factor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(world_keypoints_dict, open('world_keypoints_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoints_dict = pickle.load(open('world_keypoints_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "for idx, row in enumerate(sfps):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    # get fish_id and ground truth metadata\n",
    "    gtsf_data_collection_id = row.gtsf_data_collection_id\n",
    "    gtsf_data_collection = session.query(GtsfDataCollection).get(gtsf_data_collection_id)\n",
    "    ground_truth_metadata = json.loads(gtsf_data_collection.ground_truth_metadata)\n",
    "    if ground_truth_metadata['data'].get('species') != 'salmon':\n",
    "        continue\n",
    "    \n",
    "    world_keypoints = world_keypoints_dict[row.id]\n",
    "    try:\n",
    "        norm_wkps = normalize_world_keypoints(world_keypoints)\n",
    "\n",
    "        # Determine how to fit canonical Blender model to this GTSF fish\n",
    "        x_factor = abs(sum([norm_canonical_wkps[bp][0]*norm_wkps[bp][0] for bp in body_parts]) / \\\n",
    "                   sum([norm_canonical_wkps[bp][0]**2 for bp in body_parts]))\n",
    "\n",
    "        z_factor = abs(sum([norm_canonical_wkps[bp][2]*norm_wkps[bp][2] for bp in body_parts]) / \\\n",
    "                   sum([norm_canonical_wkps[bp][2]**2 for bp in body_parts]))\n",
    "\n",
    "        y_factor =  sum([coordinate[1]*paraboloid_fn(coordinate[0] / x_factor, coordinate[2] / z_factor) for coordinate in norm_wkps['BODY']]) / \\\n",
    "                    sum([paraboloid_fn(coordinate[0] / x_factor, coordinate[2] / z_factor)**2 for coordinate in norm_wkps['BODY']])\n",
    "\n",
    "\n",
    "        # get deviation\n",
    "        deviation = \\\n",
    "        x_factor * sum([norm_canonical_wkps[bp][0]**2 for bp in body_parts]) - sum([norm_wkps[bp][0]**2 for bp in body_parts]) + \\\n",
    "        z_factor * sum([norm_canonical_wkps[bp][2]**2 for bp in body_parts]) - sum([norm_wkps[bp][2]**2 for bp in body_parts]) + \\\n",
    "        sum([norm_canonical_wkps[bp][1]**2 for bp in body_parts]) - sum([norm_wkps[bp][1]**2 for bp in body_parts])\n",
    "\n",
    "        predicted_volume = canonical_volume*abs(x_factor)*abs(z_factor)\n",
    "\n",
    "        row = {\n",
    "            'deviation': deviation,\n",
    "            'predicted_volume': predicted_volume,\n",
    "            'gt_biomass': ground_truth_metadata['data']['weight'],\n",
    "            'gtsf_data_collection_id': row.gtsf_data_collection_id\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(row, ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['predictions'] = results_df['predicted_volume']*reg.coef_ + reg.intercept_\n",
    "results_df['abs_difference'] = (results_df.predictions - results_df.gt_biomass).abs()\n",
    "results_df['pct_difference'] = (results_df.predictions - results_df.gt_biomass).abs() / results_df.gt_biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('pct_difference', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_volumes = results_df.predicted_volume.values\n",
    "gt_biomass = results_df.gt_biomass.values\n",
    "predictions = np.array(predicted_volumes)[:, np.newaxis]\n",
    "reg = LinearRegression().fit(predictions, gt_biomass)\n",
    "print(reg.coef_, reg.intercept_)\n",
    "print(\"R2 : {}\".format(reg.score(predictions, gt_biomass)))\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "plt.scatter(gt_biomass, predictions*reg.coef_ + reg.intercept_)\n",
    "plt.xlabel(\"Ground truth weight\")\n",
    "plt.ylabel(\"Predicted weight\")\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp = session.query(StereoFramePair).filter(StereoFramePair.gtsf_data_collection_id == 311).all()[0]\n",
    "\n",
    "# download left and right images\n",
    "left_image_f = 'left_image.jpg'\n",
    "right_image_f = 'right_image.jpg'\n",
    "\n",
    "left_image_s3_key, right_image_s3_key, s3_bucket = sfp.left_image_s3_key, sfp.right_image_s3_key, sfp.image_s3_bucket\n",
    "s3_client.download_file(s3_bucket, left_image_s3_key, left_image_f)\n",
    "s3_client.download_file(s3_bucket, right_image_s3_key, right_image_f)\n",
    "\n",
    "left_image = cv2.imread(left_image_f)\n",
    "right_image = cv2.imread(right_image_f)\n",
    "\n",
    "left_keypoints = json.loads(sfp.left_image_keypoint_coordinates)\n",
    "right_keypoints = json.loads(sfp.right_image_keypoint_coordinates)\n",
    "\n",
    "im = Image.fromarray(left_image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "r = 5\n",
    "for bp, kp in left_keypoints.items():\n",
    "    draw.ellipse((kp[0]-r, kp[1]-r, kp[0]+r, kp[1]+r), fill=(255,0,0,255))\n",
    "    draw.text((kp[0], kp[1]), bp)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(right_image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "r = 5\n",
    "for bp, kp in right_keypoints.items():\n",
    "    draw.ellipse((kp[0]-r, kp[1]-r, kp[0]+r, kp[1]+r), fill=(255,0,0,255))\n",
    "    draw.text((kp[0], kp[1]), bp)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([paraboloid_fn(coordinate[0], coordinate[2])**2 for coordinate in norm_wkps['BODY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([coordinate[1]*paraboloid_fn(coordinate[0], coordinate[2]) for coordinate in norm_wkps['BODY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_wkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_data_collection_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get normalized world keyponts of all cached Blender models </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.linalg.norm(p1-p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfps = session.query(StereoFramePair).all()\n",
    "filtered_sfps = []\n",
    "\n",
    "# get vector of ground truth biomass\n",
    "gt_biomass = []\n",
    "gt_length = []\n",
    "gt_width = []\n",
    "gt_breadth = []\n",
    "gt_kfactor = []\n",
    "for row in sfps:\n",
    "    # get ground truth biomass\n",
    "    gtsf_data_collection_id = row.gtsf_data_collection_id\n",
    "    gtsf_data_collection = session.query(GtsfDataCollection).get(gtsf_data_collection_id)\n",
    "    ground_truth_metadata = json.loads(gtsf_data_collection.ground_truth_metadata)\n",
    "    species = ground_truth_metadata['data'].get('species')\n",
    "    if species == 'trout':\n",
    "        continue\n",
    "    ground_truth_biomass = ground_truth_metadata['data']['weight']\n",
    "    ground_truth_length = ground_truth_metadata['data']['length']\n",
    "    ground_truth_width = ground_truth_metadata['data']['width']\n",
    "    ground_truth_breadth = ground_truth_metadata['data']['breath']\n",
    "    \n",
    "    gt_biomass.append(ground_truth_biomass)\n",
    "    gt_length.append(ground_truth_length)\n",
    "    gt_width.append(ground_truth_width)\n",
    "    gt_breadth.append(ground_truth_breadth)\n",
    "    gt_kfactor.append(ground_truth_biomass / ground_truth_length**3)\n",
    "    filtered_sfps.append(row)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_wkps = {bp: 1e-3*np.array(data['coordinates'][0][bp]) for bp in data['mapping'].keys()}\n",
    "\n",
    "norm_canonical_wkps = normalize_world_keypoints(canonical_wkps)\n",
    "canonical_volume = data['volume'][0]\n",
    "\n",
    "analysis_df = pd.DataFrame()\n",
    "predicted_volumes = []\n",
    "y_factors = []\n",
    "ys = []\n",
    "for idx, row in enumerate(filtered_sfps):\n",
    "    # extract and normalize the predicted 3D keypoints\n",
    "    wkps = json.loads(row.world_keypoint_coordinates)\n",
    "    wkps = {bp: np.array(wkps[bp]) for bp in wkps.keys()}\n",
    "    norm_wkps = normalize_world_keypoints(wkps)\n",
    "    \n",
    "    x_factor = abs(sum([norm_canonical_wkps[bp][0]*norm_wkps[bp][0]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][0]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    y_factor = abs(sum([norm_canonical_wkps[bp][1]*norm_wkps[bp][1]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][1]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    z_factor = abs(sum([norm_canonical_wkps[bp][2]*norm_wkps[bp][2]*weight_bp[bp] for bp in data['mapping'].keys()]) / \\\n",
    "               sum([norm_canonical_wkps[bp][2]**2*weight_bp[bp] for bp in data['mapping'].keys()]))\n",
    "    \n",
    "    volume = canonical_volume * x_factor * z_factor * (1 + (y_factor - 1.0) * 0.12)\n",
    "    y = norm_wkps['PECTORAL_FIN'][1]-norm_wkps['UPPER_LIP'][1]\n",
    "    ys.append(y)\n",
    "    predicted_volumes.append(volume)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predicted_volumes)[:, np.newaxis]\n",
    "reg = LinearRegression().fit(predictions, gt_biomass)\n",
    "print(reg.coef_, reg.intercept_)\n",
    "print(\"R2 : {}\".format(reg.score(predictions, gt_biomass)))\n",
    "predictions = np.squeeze(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_biomass), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 5000], [0, 5000], \"--\", c=\"r\", linewidth=2)\n",
    "plt.scatter(gt_biomass, predictions*reg.coef_ + reg.intercept_)\n",
    "plt.xlabel(\"Ground truth weight\")\n",
    "plt.ylabel(\"Predicted weight\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_predictions = predictions*reg.coef_ + reg.intercept_\n",
    "error = fitted_predictions-gt_biomass\n",
    "print(\"Average absolute error: {}\".format(np.nanmean(np.abs(error))))\n",
    "print(\"Average error: {}\".format(np.nanmean(error)))\n",
    "# error5 = predictions_average-ground_truth\n",
    "#print(\"Average absolute error5: {}\".format(np.nanmean(np.abs(error5))))\n",
    "relative_error = ((fitted_predictions-gt_biomass) / gt_biomass)*100\n",
    "print(\"Average relative error: {} %\".format(np.nanmean(relative_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['coordinates'])):\n",
    "    canonical_wkps = {bp: 1e-3*np.array(data['coordinates'][i][bp]) for bp in data['mapping'].keys()}\n",
    "    norm_canonical_wkps = normalize_world_keypoints(canonical_wkps)\n",
    "    print((norm_canonical_wkps['PECTORAL_FIN'][1]-norm_canonical_wkps['UPPER_LIP'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gt_breadth, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
