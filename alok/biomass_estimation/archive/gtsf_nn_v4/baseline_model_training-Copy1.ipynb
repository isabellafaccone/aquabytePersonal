{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load Raw GTSF Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Augment the Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_estimation.utils import get_left_right_keypoint_arrs, get_ann_from_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "\n",
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    print('hello')\n",
    "    \n",
    "    counts, edges = np.histogram(df.weight, bins=np.arange(0, 10000, 1000))\n",
    "    trial_values = (5.0 / (counts / np.max(counts))).astype(int)\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_depth = augmentation_config['min_depth']\n",
    "    max_depth = augmentation_config['max_depth']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        camera_metadata = row.camera_metadata\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        \n",
    "        weight = row.weight\n",
    "        trials = trial_values[min(int(weight / 1000), len(trial_values) - 1)]\n",
    "        for _ in range(trials):\n",
    "            \n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "            wkps = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "            original_depth = np.median(wkps[:, 1])\n",
    "            \n",
    "            depth = np.random.uniform(min_depth, max_depth)\n",
    "            scaling_factor = float(original_depth) / depth\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            \n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_config = dict(\n",
    "    trials=10,\n",
    "    max_jitter_std=10,\n",
    "    min_depth=0.5,\n",
    "    max_depth=2.5\n",
    ")\n",
    "\n",
    "augmented_df = augment(df, augmentation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_csv('/root/data/alok/biomass_estimation/playground/augmented_df_depth_weight_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Note: cell below takes about 1.5 hrs to run. To load cached version, run the cell below this one </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "akpd_scores = []\n",
    "akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print('Percentage complete: {}%'.format(round(100 * count / augmented_df.shape[0], 2)))\n",
    "    count += 1\n",
    "    akpd_score = compute_akpd_score(akpd_scorer_network, row.annotation, row.camera_metadata)\n",
    "    akpd_scores.append(akpd_score)\n",
    "\n",
    "\n",
    "augmented_df['akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> If you ran the cell above, do not run the one here </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.read_csv('/root/data/alok/biomass_estimation/playground/gtsf_augmented_dataset.csv')\n",
    "\n",
    "new_anns, new_cms = [], []\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    cm = row.camera_metadata\n",
    "    new_cm = json.loads(cm.replace(\"'\", '\"'))\n",
    "    new_cms.append(new_cm)\n",
    "    \n",
    "    ann = row.annotation\n",
    "    new_ann = json.loads(ann.replace(\"'\", '\"'))\n",
    "    new_anns.append(new_ann)\n",
    "    \n",
    "augmented_df['annotation'] = new_anns\n",
    "augmented_df['camera_metadata'] = new_cms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, get_ann_from_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "from weight_estimation.dataset import prepare_gtsf_data\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_intermediate(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor and get final hiddel layer weights.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def normalize(anns: List, camera_metadatas: List) -> np.ndarray:\n",
    "    norm_anns = []\n",
    "    for ann, camera_metadata in zip(anns, camera_metadatas):\n",
    "\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        norm_ann = convert_to_nn_input(ann, cm)\n",
    "        norm_anns.append(norm_ann)\n",
    "    return np.array(norm_anns)\n",
    "\n",
    "\n",
    "def get_data_split(X: np.ndarray, y: np.ndarray, fish_ids: np.ndarray, train_pct: float,\n",
    "                   val_pct: float) -> Tuple:\n",
    "    # select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "    test_pct = 1.0 - train_pct - val_pct\n",
    "    unique_fish_ids = np.array(list(set(fish_ids)))\n",
    "    train_cnt, val_cnt, test_cnt = np.random.multinomial(len(unique_fish_ids),\n",
    "                                                         [train_pct, val_pct, test_pct])\n",
    "\n",
    "    assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "    np.random.shuffle(assignments)\n",
    "    train_fish_ids = unique_fish_ids[np.where(assignments == 0)]\n",
    "    val_fish_ids = unique_fish_ids[np.where(assignments == 1)]\n",
    "    test_fish_ids = unique_fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "    train_mask = np.isin(fish_ids, train_fish_ids)\n",
    "    val_mask = np.isin(fish_ids, val_fish_ids)\n",
    "    test_mask = np.isin(fish_ids, test_fish_ids)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, train_config):\n",
    "    inputs = Input(shape=(24,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(1)(x)\n",
    "    model = Model(inputs, pred)\n",
    "\n",
    "    epochs = train_config['epochs']\n",
    "    batch_size = train_config['batch_size']\n",
    "    lr = train_config['learning_rate']\n",
    "    patience = train_config['patience']\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=patience,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_pytorch(model):\n",
    "    pytorch_model = Network()\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    pytorch_model.fc1.weight.data = torch.from_numpy(np.transpose(weights[0]))\n",
    "    pytorch_model.fc1.bias.data = torch.from_numpy(np.transpose(weights[1]))\n",
    "    pytorch_model.fc2.weight.data = torch.from_numpy(np.transpose(weights[2]))\n",
    "    pytorch_model.fc2.bias.data = torch.from_numpy(np.transpose(weights[3]))\n",
    "    pytorch_model.fc3.weight.data = torch.from_numpy(np.transpose(weights[4]))\n",
    "    pytorch_model.fc3.bias.data = torch.from_numpy(np.transpose(weights[5]))\n",
    "    pytorch_model.output.weight.data = torch.from_numpy(np.transpose(weights[6]))\n",
    "    pytorch_model.output.bias.data = torch.from_numpy(np.transpose(weights[7]))\n",
    "    \n",
    "    return pytorch_model\n",
    "\n",
    "\n",
    "def apply_final_layer_ols(pytorch_model):\n",
    "    X_ols = pytorch_model.forward_intermediate(torch.from_numpy(X_train).float()).detach().numpy()\n",
    "    lr = LinearRegression().fit(X_ols, y_train)\n",
    "    pytorch_model.output.weight.data = torch.from_numpy(np.array(lr.coef_).reshape(1, -1))\n",
    "    pytorch_model.output.bias.data = torch.from_numpy(np.array([lr.intercept_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "anns = augmented_df.annotation.values.tolist()\n",
    "cms = augmented_df.camera_metadata.values.tolist()\n",
    "X = normalize(anns, cms)\n",
    "\n",
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "y = 1e-4 * augmented_df.weight.values\n",
    "fish_ids = augmented_df.fish_id.values\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask = get_data_split(X, y, fish_ids,\n",
    "                                                                train_config['train_pct'],\n",
    "                                                                train_config['val_pct'])\n",
    "\n",
    "tf_model = train_model(X_train, y_train, X_val, y_val, train_config)\n",
    "pytorch_model = convert_to_pytorch(tf_model)\n",
    "apply_final_layer_ols(pytorch_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['is_train'] = train_mask.astype(int)\n",
    "augmented_df['is_val'] = val_mask.astype(int)\n",
    "augmented_df['is_test'] = test_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = convert_to_pytorch(tf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Overall accuracy stats </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "print('Train stats')\n",
    "train_errs = (y_pred[train_mask] - y_train) / y_train\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[train_mask]) - np.mean(y_train)) / np.mean(y_train)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(train_errs))))\n",
    "print('='*20)\n",
    "print('Val stats')\n",
    "val_errs = (y_pred[val_mask] - y_val) / y_val\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[val_mask]) - np.mean(y_val)) / np.mean(y_val)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(val_errs))))\n",
    "print('='*20)\n",
    "print('Test stats')\n",
    "test_errs = (y_pred[test_mask] - y_test) / y_test\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[test_mask]) - np.mean(y_test)) / np.mean(y_test)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(test_errs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "print('Train stats')\n",
    "train_errs = (y_pred[train_mask] - y_train) / y_train\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[train_mask]) - np.mean(y_train)) / np.mean(y_train)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(train_errs))))\n",
    "print('='*20)\n",
    "print('Val stats')\n",
    "val_errs = (y_pred[val_mask] - y_val) / y_val\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[val_mask]) - np.mean(y_val)) / np.mean(y_val)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(val_errs))))\n",
    "print('='*20)\n",
    "print('Test stats')\n",
    "test_errs = (y_pred[test_mask] - y_test) / y_test\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[test_mask]) - np.mean(y_test)) / np.mean(y_test)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(test_errs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_bucket_error(X, y):\n",
    "    y_pred = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "    print(np.mean(y_pred - y))\n",
    "\n",
    "    buckets = np.arange(0, 10000, 1000) * 1e-4\n",
    "    bucket_strs = []\n",
    "    mean_errs = []\n",
    "    maes = []\n",
    "    for low, high in zip(buckets, buckets[1:]):\n",
    "        bucket_str = '{}-{}'.format(round(1e4 * low), round(1e4 * high))\n",
    "        mask = (y >= low) & (y < high)\n",
    "        mean_err = np.mean((y_pred[mask] - y[mask]) / y[mask])\n",
    "        mae = np.mean(np.abs((y_pred[mask] - y[mask]) / y[mask]))\n",
    "        mean_errs.append(mean_err)\n",
    "        maes.append(mae)\n",
    "        bucket_strs.append(bucket_str)\n",
    "    \n",
    "    return pd.DataFrame({'bucket': bucket_strs, 'mean_err': mean_errs, 'mae': maes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate errors with respect to depth </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get depth array and add as column to augmented data-frame\n",
    "\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, convert_to_world_point_arr\n",
    "\n",
    "depths = []\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    ann, camera_metadata = row.annotation, row.camera_metadata\n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    X = convert_to_world_point_arr(*get_left_right_keypoint_arrs(ann), cm)\n",
    "    median_depth = np.median(X[:, 1])\n",
    "    depths.append(median_depth)\n",
    "    \n",
    "augmented_df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "augmented_df['y_pred'] = predictions\n",
    "augmented_df['y'] = y\n",
    "\n",
    "depths = np.arange(0.2, 2.7, 0.1)\n",
    "mean_pct_errs = []\n",
    "depth_buckets = []\n",
    "for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "    depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "    depth_buckets.append(depth_bucket)\n",
    "    mask = (augmented_df.depth >= low_depth) & (augmented_df.depth <= high_depth) & train_mask\n",
    "    mean_pct_err = np.mean((augmented_df[mask].y_pred - augmented_df[mask].y) / augmented_df[mask].y)\n",
    "    mean_pct_errs.append(mean_pct_err)\n",
    "    \n",
    "\n",
    "pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "augmented_df['y_pred'] = predictions\n",
    "augmented_df['y'] = y\n",
    "\n",
    "depths = np.arange(0.2, 2.7, 0.1)\n",
    "mean_pct_errs = []\n",
    "depth_buckets = []\n",
    "for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "    depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "    depth_buckets.append(depth_bucket)\n",
    "    mask = (augmented_df.depth >= low_depth) & (augmented_df.depth <= high_depth) & test_mask\n",
    "    mean_pct_err = np.mean((augmented_df[mask].y_pred - augmented_df[mask].y) / augmented_df[mask].y)\n",
    "    mean_pct_errs.append(mean_pct_err)\n",
    "    \n",
    "\n",
    "pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pytorch_model(torch.from_numpy(X).float()).detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pytorch_model(torch.from_numpy(X_test).float()).detach().numpy().squeeze() - y_test) / y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_bucket_error_2(y_pred, y):\n",
    "\n",
    "    buckets = np.arange(0, 10000, 1000) * 1e-4\n",
    "    bucket_strs = []\n",
    "    mean_errs = []\n",
    "    for low, high in zip(buckets, buckets[1:]):\n",
    "        bucket_str = '{}-{}'.format(round(1e4 * low), round(1e4 * high))\n",
    "        mask = (y >= low) & (y < high)\n",
    "        mean_err = np.mean((y_pred[mask] - y[mask]) / y[mask])\n",
    "        mean_errs.append(mean_err)\n",
    "        bucket_strs.append(bucket_str)\n",
    "    \n",
    "    return pd.DataFrame({'bucket': bucket_strs, 'mean_err': mean_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_bucket_error_2(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X)\n",
    "augmented_df['y_pred_2'] = preds\n",
    "augmented_df['y'] = y\n",
    "\n",
    "depths = np.arange(0.2, 2.7, 0.1)\n",
    "mean_pct_errs = []\n",
    "depth_buckets = []\n",
    "for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "    depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "    depth_buckets.append(depth_bucket)\n",
    "    mask = (augmented_df.depth >= low_depth) & (augmented_df.depth <= high_depth) & train_mask\n",
    "    mean_pct_err = np.mean((augmented_df[mask].y_pred_2 - augmented_df[mask].y) / augmented_df[mask].y)\n",
    "    mean_pct_errs.append(mean_pct_err)\n",
    "    \n",
    "\n",
    "pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def density_scatter(x, y, bins=20, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:]+y_e[:-1])), data, np.vstack([x, y]).T,\n",
    "                method=\"splinef2d\", bounds_error=False)\n",
    "\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "    norm = Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm), ax=ax)\n",
    "    cbar.ax.set_ylabel('Density')\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    ax.grid()\n",
    "\n",
    "    return ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
