{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load canonical GTSF dataset (to be versioned) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.read_csv('/root/data/alok/biomass_estimation/playground/augmented_df_depth_weight_balanced.csv')\n",
    "\n",
    "new_anns, new_cms = [], []\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    cm = row.camera_metadata\n",
    "    new_cm = json.loads(cm.replace(\"'\", '\"'))\n",
    "    new_cms.append(new_cm)\n",
    "    \n",
    "    ann = row.annotation\n",
    "    new_ann = json.loads(ann.replace(\"'\", '\"'))\n",
    "    new_anns.append(new_ann)\n",
    "    \n",
    "augmented_df['annotation'] = new_anns\n",
    "augmented_df['camera_metadata'] = new_cms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Alternatively, run the section below to create augmented GTSF dataset on the fly </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load raw GTSF data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)\n",
    "\n",
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Add synthetic data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import pixel2world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.keypoints, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    lengths.append(np.linalg.norm(wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']))\n",
    "    \n",
    "df['length'] = lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.length, df.weight)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "res = lr.fit(np.log(df.length.values).reshape(-1, 1), np.log(df.weight.values))\n",
    "exp = res.coef_[0]\n",
    "print('Expoonent to use on length term: {}'.format(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Simulate large fish and add to GTSF dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_larger_fish(world_keypoints, weight, pct_inflation):\n",
    "    modified_world_keypoints = {}\n",
    "    for body_part in core_body_parts:\n",
    "        kps = world_keypoints[body_part]\n",
    "        modified_kps = (1.0 + pct_inflation) * kps\n",
    "        modified_world_keypoints[body_part] = modified_kps\n",
    "        \n",
    "    modified_weight = (1.0 + pct_inflation)**exp * weight\n",
    "    return modified_world_keypoints, modified_weight\n",
    "    \n",
    "\n",
    "def get_ann_from_world_keypoints(world_keypoints, cm):\n",
    "    ann = {'leftCrop': [], 'rightCrop': []}\n",
    "    for body_part in core_body_parts:\n",
    "        x, y, z = world_keypoints[body_part]\n",
    "        px_x = round(x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] / 2.0)\n",
    "        px_y = round(cm['pixelCountHeight'] / 2.0 - z * cm['focalLengthPixel'] / y)\n",
    "        disparity = round(cm['focalLengthPixel'] * cm['baseline'] / y)\n",
    "        \n",
    "        left_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x,\n",
    "            'yFrame': px_y\n",
    "        }\n",
    "        \n",
    "        right_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x - disparity,\n",
    "            'yFrame': px_y\n",
    "        }\n",
    "        \n",
    "        ann['leftCrop'].append(left_item)\n",
    "        ann['rightCrop'].append(right_item)\n",
    "    return ann\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoints = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.keypoints, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    world_keypoints.append(wkps)\n",
    "    \n",
    "    \n",
    "df['world_keypoints'] = world_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*np.tan((70 * np.pi / 180.0) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_estimation.body_parts import core_body_parts\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "mask = (df.weight >= 7000) & (df.weight <= 9000) \n",
    "max_pct_inflation = 0.15\n",
    "\n",
    "\n",
    "modified_ann_list = []\n",
    "modified_weight_list = []\n",
    "cm_list = []\n",
    "for idx, row in df[mask].iterrows():\n",
    "    pct_inflation = np.random.uniform(0, max_pct_inflation)\n",
    "    world_keypoints = row.world_keypoints\n",
    "    cm = row.camera_metadata\n",
    "    weight = row.weight\n",
    "    modified_world_keypoints, modified_weight = simulate_larger_fish(world_keypoints, weight, pct_inflation)\n",
    "    modified_ann = get_ann_from_world_keypoints(modified_world_keypoints, cm)\n",
    "    \n",
    "    modified_ann_list.append(modified_ann)\n",
    "    modified_weight_list.append(modified_weight)\n",
    "    cm_list.append(cm)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "mask = df.weight < 10000\n",
    "tdf_original = df.loc[mask, ['keypoints', 'fish_id', 'weight', 'k_factor', 'camera_metadata']].copy(deep=True)\n",
    "\n",
    "annotation_list = []\n",
    "fish_id_list = []\n",
    "weight_list = []\n",
    "kf_list = []\n",
    "camera_metadata_list = []\n",
    "for ann, weight, camera_metadata in zip(modified_ann_list, modified_weight_list, cm_list):\n",
    "    annotation_list.append(ann)\n",
    "    fish_id_list.append(uuid.uuid1())\n",
    "    weight_list.append(weight)\n",
    "    kf_list.append(1.0)\n",
    "    camera_metadata_list.append(camera_metadata)\n",
    "    \n",
    "tdf_synthetic = pd.DataFrame({\n",
    "    'keypoints': annotation_list,\n",
    "    'fish_id': fish_id_list,\n",
    "    'weight': weight_list,\n",
    "    'k_factor': kf_list,\n",
    "    'camera_metadata': camera_metadata_list\n",
    "})\n",
    "\n",
    "tdf = pd.concat([tdf_original, tdf_synthetic])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Perform data augmentation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_estimation.utils import convert_to_world_point_arr\n",
    "\n",
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    print('hello')\n",
    "    \n",
    "    counts, edges = np.histogram(df.weight, bins=np.arange(0, 10000, 1000))\n",
    "    trial_values = (5.0 / (counts / np.max(counts))).astype(int)\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_depth = augmentation_config['min_depth']\n",
    "    max_depth = augmentation_config['max_depth']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        camera_metadata = row.camera_metadata\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        \n",
    "        weight = row.weight\n",
    "        trials = trial_values[min(int(weight / 1000), len(trial_values) - 1)]\n",
    "        for _ in range(trials):\n",
    "            \n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "            wkps = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "            original_depth = np.median(wkps[:, 1])\n",
    "            \n",
    "            depth = np.random.uniform(min_depth, max_depth)\n",
    "            scaling_factor = float(original_depth) / depth\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            \n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "augmentation_config = dict(\n",
    "    trials=10,\n",
    "    max_jitter_std=10,\n",
    "    min_depth=0.5,\n",
    "    max_depth=2.5\n",
    ")\n",
    "\n",
    "augmented_df = augment(tdf, augmentation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "akpd_scores = []\n",
    "akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print('Percentage complete: {}%'.format(round(100 * count / augmented_df.shape[0], 2)))\n",
    "    count += 1\n",
    "    akpd_score = compute_akpd_score(akpd_scorer_network, row.annotation, row.camera_metadata)\n",
    "    akpd_scores.append(akpd_score)\n",
    "\n",
    "\n",
    "augmented_df['akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, get_ann_from_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "from weight_estimation.dataset import prepare_gtsf_data\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_intermediate(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor and get final hiddel layer weights.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def normalize(anns: List, camera_metadatas: List) -> np.ndarray:\n",
    "    norm_anns = []\n",
    "    for ann, camera_metadata in zip(anns, camera_metadatas):\n",
    "\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        norm_ann = convert_to_nn_input(ann, cm)\n",
    "        norm_anns.append(norm_ann)\n",
    "    return np.array(norm_anns)\n",
    "\n",
    "\n",
    "def get_data_split(X: np.ndarray, y: np.ndarray, fish_ids: np.ndarray, train_pct: float,\n",
    "                   val_pct: float) -> Tuple:\n",
    "    # select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "    test_pct = 1.0 - train_pct - val_pct\n",
    "    unique_fish_ids = np.array(list(set(fish_ids)))\n",
    "    train_cnt, val_cnt, test_cnt = np.random.multinomial(len(unique_fish_ids),\n",
    "                                                         [train_pct, val_pct, test_pct])\n",
    "\n",
    "    assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "    np.random.shuffle(assignments)\n",
    "    train_fish_ids = unique_fish_ids[np.where(assignments == 0)]\n",
    "    val_fish_ids = unique_fish_ids[np.where(assignments == 1)]\n",
    "    test_fish_ids = unique_fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "    train_mask = np.isin(fish_ids, train_fish_ids)\n",
    "    val_mask = np.isin(fish_ids, val_fish_ids)\n",
    "    test_mask = np.isin(fish_ids, test_fish_ids)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, train_config):\n",
    "    inputs = Input(shape=(24,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(1)(x)\n",
    "    model = Model(inputs, pred)\n",
    "\n",
    "    epochs = train_config['epochs']\n",
    "    batch_size = train_config['batch_size']\n",
    "    lr = train_config['learning_rate']\n",
    "    patience = train_config['patience']\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=patience,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_pytorch(model):\n",
    "    pytorch_model = Network()\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    pytorch_model.fc1.weight.data = torch.from_numpy(np.transpose(weights[0]))\n",
    "    pytorch_model.fc1.bias.data = torch.from_numpy(np.transpose(weights[1]))\n",
    "    pytorch_model.fc2.weight.data = torch.from_numpy(np.transpose(weights[2]))\n",
    "    pytorch_model.fc2.bias.data = torch.from_numpy(np.transpose(weights[3]))\n",
    "    pytorch_model.fc3.weight.data = torch.from_numpy(np.transpose(weights[4]))\n",
    "    pytorch_model.fc3.bias.data = torch.from_numpy(np.transpose(weights[5]))\n",
    "    pytorch_model.output.weight.data = torch.from_numpy(np.transpose(weights[6]))\n",
    "    pytorch_model.output.bias.data = torch.from_numpy(np.transpose(weights[7]))\n",
    "    \n",
    "    return pytorch_model\n",
    "\n",
    "\n",
    "def apply_final_layer_ols(pytorch_model):\n",
    "    X_ols = pytorch_model.forward_intermediate(torch.from_numpy(X_train).float()).detach().numpy()\n",
    "    lr = LinearRegression().fit(X_ols, y_train)\n",
    "    pytorch_model.output.weight.data = torch.from_numpy(np.array(lr.coef_).reshape(1, -1))\n",
    "    pytorch_model.output.bias.data = torch.from_numpy(np.array([lr.intercept_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tensorflow.random.set_seed(0)\n",
    "\n",
    "anns = augmented_df.annotation.values.tolist()\n",
    "cms = augmented_df.camera_metadata.values.tolist()\n",
    "X = normalize(anns, cms)\n",
    "\n",
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "y = 1e-4 * augmented_df.weight.values\n",
    "fish_ids = augmented_df.fish_id.values\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask = get_data_split(X, y, fish_ids,\n",
    "                                                                train_config['train_pct'],\n",
    "                                                                train_config['val_pct'])\n",
    "\n",
    "tf_model = train_model(X_train, y_train, X_val, y_val, train_config)\n",
    "pytorch_model = convert_to_pytorch(tf_model)\n",
    "apply_final_layer_ols(pytorch_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['is_train'] = train_mask.astype(int)\n",
    "augmented_df['is_val'] = val_mask.astype(int)\n",
    "augmented_df['is_test'] = test_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_csv('/root/data/alok/biomass_estimation/playground/2020-01-05_gtsf_augmented_dataset_synthetic_static_max_jitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.s3_client.upload_file(\n",
    "    '/root/data/alok/biomass_estimation/playground/2020-01-05_gtsf_augmented_dataset_synthetic_static_max_jitter.csv',\n",
    "    'aquabyte-images-adhoc',\n",
    "    'alok/gtsf_datasets/2020-01-05_gtsf_augmented_dataset_synthetic_static_max_jitter.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Overall accuracy stats </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "print('Train stats')\n",
    "train_errs = (y_pred[train_mask] - y_train) / y_train\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[train_mask]) - np.mean(y_train)) / np.mean(y_train)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(train_errs))))\n",
    "print('='*20)\n",
    "print('Val stats')\n",
    "val_errs = (y_pred[val_mask] - y_val) / y_val\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[val_mask]) - np.mean(y_val)) / np.mean(y_val)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(val_errs))))\n",
    "print('='*20)\n",
    "print('Test stats')\n",
    "test_errs = (y_pred[test_mask] - y_test) / y_test\n",
    "print('Mean error pct: {}'.format((np.mean(y_pred[test_mask]) - np.mean(y_test)) / np.mean(y_test)))\n",
    "print('Mean absolute error pct: {}'.format(np.mean(np.abs(test_errs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_bucket_error(X, y):\n",
    "    y_pred = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "\n",
    "    buckets = np.arange(0, 13000, 1000) * 1e-4\n",
    "    bucket_strs = []\n",
    "    mean_errs = []\n",
    "    maes = []\n",
    "    for low, high in zip(buckets, buckets[1:]):\n",
    "        bucket_str = '{}-{}'.format(round(1e4 * low), round(1e4 * high))\n",
    "        mask = (y >= low) & (y < high)\n",
    "        mean_err = np.mean((y_pred[mask] - y[mask]) / y[mask])\n",
    "        mae = np.mean(np.abs((y_pred[mask] - y[mask]) / y[mask]))\n",
    "        mean_errs.append(mean_err)\n",
    "        maes.append(mae)\n",
    "        bucket_strs.append(bucket_str)\n",
    "    \n",
    "    return pd.DataFrame({'bucket': bucket_strs, 'mean_err': mean_errs, 'mae': maes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset')\n",
    "print('\\n')\n",
    "print(generate_per_bucket_error(X_train, y_train))\n",
    "print('='*20)\n",
    "print('\\n')\n",
    "print('Testing dataset')\n",
    "print('\\n')\n",
    "print(generate_per_bucket_error(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate errors with respect to depth </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_depth_bucket_error(X, y, d):\n",
    "    predictions = (pytorch_model(torch.from_numpy(X).float())).detach().numpy().squeeze()\n",
    "\n",
    "    depths = np.arange(0.4, 2.6, 0.1)\n",
    "    mean_pct_errs = []\n",
    "    depth_buckets = []\n",
    "    for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "        mask = (d >= low_depth) & (d < high_depth)\n",
    "        depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "        depth_buckets.append(depth_bucket)\n",
    "        mean_pct_err = np.nanmean((predictions[mask] - y[mask]) / y[mask])\n",
    "        mean_pct_errs.append(mean_pct_err)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset')\n",
    "print('\\n')\n",
    "print(generate_per_depth_bucket_error(X_train, y_train, augmented_df[train_mask].depth.values))\n",
    "print('\\n')\n",
    "print('='*20)\n",
    "print('\\n')\n",
    "print('Testing dataset')\n",
    "print('\\n')\n",
    "print(generate_per_depth_bucket_error(X_test, y_test, augmented_df[test_mask].depth.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save model (Can be loaded in backtesting notebook) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/playground/output_model.pb'\n",
    "torch.save(pytorch_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.s3_client.upload_file(f, 'aquabyte-models', 'biomass/playground/2020-01-05_gtsf_augmented_dataset_synthetic_static_max_jitter_standard_training_params.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
