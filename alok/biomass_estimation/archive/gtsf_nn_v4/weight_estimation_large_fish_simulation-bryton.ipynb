{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains constants representing core & auxiliary fish body parts.\n",
    "\"\"\"\n",
    "\n",
    "UPPER_LIP = 'UPPER_LIP'\n",
    "EYE = 'EYE'\n",
    "PECTORAL_FIN = 'PECTORAL_FIN'\n",
    "DORSAL_FIN = 'DORSAL_FIN'\n",
    "PELVIC_FIN = 'PELVIC_FIN'\n",
    "ADIPOSE_FIN = 'ADIPOSE_FIN'\n",
    "ANAL_FIN = 'ANAL_FIN'\n",
    "TAIL_NOTCH = 'TAIL_NOTCH'\n",
    "UPPER_PRECAUDAL_PIT = 'UPPER_PRECAUDAL_PIT'\n",
    "LOWER_PRECAUDAL_PIT = 'LOWER_PRECAUDAL_PIT'\n",
    "HYPURAL_PLATE = 'HYPURAL_PLATE'\n",
    "\n",
    "core_body_parts = sorted([UPPER_LIP,\n",
    "                          EYE,\n",
    "                          PECTORAL_FIN,\n",
    "                          DORSAL_FIN,\n",
    "                          PELVIC_FIN,\n",
    "                          ADIPOSE_FIN,\n",
    "                          ANAL_FIN,\n",
    "                          TAIL_NOTCH])\n",
    "\n",
    "auxiliary_body_parts = sorted([UPPER_PRECAUDAL_PIT,\n",
    "                               LOWER_PRECAUDAL_PIT,\n",
    "                               HYPURAL_PLATE])\n",
    "\n",
    "all_body_parts = sorted(core_body_parts + auxiliary_body_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = core_body_parts.index(UPPER_LIP)\n",
    "    tail_notch_idx = core_body_parts.index(TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "# from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "# from weight_estimation.utils import body_parts, normalize_left_right_keypoint_arrs\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "# from weight_estimation.dataset import prepare_gtsf_data\n",
    "# from weight_estimation.weight_estimator import WeightEstimator\n",
    "# from weight_estimation.utils import CameraMetadata\n",
    "import pandas as pd\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "# from weight_estimation.body_parts import core_body_parts\n",
    "from keras.models import load_model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m', 'pixel_count_width',\n",
    "                             'pixel_count_height', 'image_sensor_width', 'image_sensor_height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/tmp/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate base data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "s3 = S3AccessUtils('/tmp/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "mask = df.k_factor < 3.0\n",
    "df = df[mask].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/alok/repos/research-exploration/bryton/biomass/gtsf_data.csv')\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = eval(row.keypoints), eval(row.camera_metadata)\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    lengths.append(np.linalg.norm(wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']))\n",
    "    \n",
    "df['length'] = lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get length -> weight allometric model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['length'] = df.data.apply(lambda x: x['lengthMms'])\n",
    "plt.scatter(df.length, df.weight)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "res = lr.fit(np.log(df.length.values).reshape(-1, 1), np.log(df.weight.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weight_from_length(length):\n",
    "    weight = np.exp(res.intercept_) * length**(res.coef_[0])\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_weight'] = df.length.apply(lambda x: predict_weight_from_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot relationship between weight and predicted length </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoints = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = eval(row.keypoints), eval(row.camera_metadata)\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    world_keypoints.append(wkps)\n",
    "    \n",
    "    \n",
    "df['world_keypoints'] = world_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_length'] = df.world_keypoints.apply(lambda x: np.linalg.norm(x['UPPER_LIP'] - x['TAIL_NOTCH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.pred_length.values, df.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['weight'], df['pred_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using fish in 3-4 kg, simulate larger fish and assess representativeness </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_larger_fish(world_keypoints, weight, pct_inflation):\n",
    "    modified_world_keypoints = {}\n",
    "    for body_part in core_body_parts:\n",
    "        kps = world_keypoints[body_part]\n",
    "        modified_kps = (1.0 + pct_inflation) * kps\n",
    "        modified_world_keypoints[body_part] = modified_kps\n",
    "        \n",
    "    modified_weight = (1.0 + pct_inflation)**3.08 * weight\n",
    "    return modified_world_keypoints, modified_weight\n",
    "    \n",
    "\n",
    "def get_ann_from_world_keypoints(world_keypoints, cm):\n",
    "    ann = {'leftCrop': [], 'rightCrop': []}\n",
    "    for body_part in core_body_parts:\n",
    "        x, y, z = world_keypoints[body_part]\n",
    "        px_x = round(x * cm['focalLengthPixel'] / y + cm['pixelCountWidth'] / 2.0)\n",
    "        px_y = round(cm['pixelCountHeight'] / 2.0 - z * cm['focalLengthPixel'] / y)\n",
    "        disparity = round(cm['focalLengthPixel'] * cm['baseline'] / y)\n",
    "        \n",
    "        left_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x,\n",
    "            'yFrame': px_y\n",
    "        }\n",
    "        \n",
    "        right_item = {\n",
    "            'keypointType': body_part,\n",
    "            'xFrame': px_x - disparity,\n",
    "            'yFrame': px_y\n",
    "        }\n",
    "        \n",
    "        ann['leftCrop'].append(left_item)\n",
    "        ann['rightCrop'].append(right_item)\n",
    "    return ann\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.weight <= 5000) \n",
    "max_pct_inflation = 0.15\n",
    "\n",
    "\n",
    "modified_ann_list = []\n",
    "modified_weight_list = []\n",
    "cm_list = []\n",
    "for idx, row in df[mask].iterrows():\n",
    "    pct_inflation = np.random.uniform(0, max_pct_inflation)\n",
    "    world_keypoints = row.world_keypoints\n",
    "    cm = eval(row.camera_metadata)\n",
    "    weight = row.weight\n",
    "    modified_world_keypoints, modified_weight = simulate_larger_fish(world_keypoints, weight, pct_inflation)\n",
    "    modified_ann = get_ann_from_world_keypoints(modified_world_keypoints, cm)\n",
    "    \n",
    "    modified_ann_list.append(modified_ann)\n",
    "    modified_weight_list.append(modified_weight)\n",
    "    cm_list.append(cm)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/src/weight_estimation/weight_model.h5'\n",
    "# kf_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/src/weight_estimation/kf_model.h5'\n",
    "weight_model_f = '/root/alok/repos/research-exploration/bryton/biomass/weight_model_synthetic_data.pb'\n",
    "kf_model_f = '/root/alok/repos/research-exploration/bryton/biomass/kf_model.pb'\n",
    "    \n",
    "    \n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "weights, lengths, kfs = [], [], []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weights = []\n",
    "for ann, camera_metadata in zip(modified_ann_list, cm_list):\n",
    "    camera_metadata_obj = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    weight, _, _ = weight_estimator.predict(ann, camera_metadata_obj)\n",
    "    pred_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.array(modified_weight_list), np.array(pred_weights))\n",
    "plt.plot([min(modified_weight_list), max(modified_weight_list)], \n",
    "         [min(modified_weight_list), max(modified_weight_list)], color='red')\n",
    "plt.xlabel('Simulated weight of synthetic fish')\n",
    "plt.ylabel('Production model prediction on synthetic fish')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(pred_weights) - np.mean(modified_weight_list)) / np.mean(modified_weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((np.abs(np.array(pred_weights) - np.array(modified_weight_list))) / np.array(modified_weight_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modify the training dataset with synthetic data and apply augmentation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.weight < 10000\n",
    "tdf_original = df.loc[mask, ['keypoints', 'fish_id', 'weight', 'k_factor', 'camera_metadata']].copy(deep=True)\n",
    "\n",
    "annotation_list = []\n",
    "fish_id_list = []\n",
    "weight_list = []\n",
    "kf_list = []\n",
    "camera_metadata_list = []\n",
    "for ann, weight, camera_metadata in zip(modified_ann_list, modified_weight_list, cm_list):\n",
    "    annotation_list.append(ann)\n",
    "    fish_id_list.append(uuid.uuid1())\n",
    "    weight_list.append(weight)\n",
    "    kf_list.append(1.0)\n",
    "    camera_metadata_list.append(camera_metadata)\n",
    "    \n",
    "tdf_synthetic = pd.DataFrame({\n",
    "    'keypoints': annotation_list,\n",
    "    'fish_id': fish_id_list,\n",
    "    'weight': weight_list,\n",
    "    'k_factor': kf_list,\n",
    "    'camera_metadata': camera_metadata_list\n",
    "})\n",
    "\n",
    "tdf = pd.concat([tdf_original, tdf_synthetic])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.weight, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from research.weight_estimation.akpd_utils.akpd_scorer import generate_confidence_score\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, convert_to_world_point_arr, \\\n",
    "    CameraMetadata\n",
    "\n",
    "\n",
    "# generate raw GTSF dataframe from database\n",
    "def generate_raw_df(start_date, end_date):\n",
    "    rds = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "    query = \"\"\"\n",
    "        select * from research.fish_metadata a left join keypoint_annotations b\n",
    "        on a.left_url = b.left_image_url \n",
    "        where b.keypoints -> 'leftCrop' is not null\n",
    "        and b.keypoints -> 'rightCrop' is not null\n",
    "        and b.captured_at between '{0}' and '{1}';\n",
    "    \"\"\".format(start_date, end_date)\n",
    "    df = rds.extract_from_database(query)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df.data.apply(lambda x: x['species'].lower()) == 'salmon'].copy(deep=True)\n",
    "    qa_df = df[df.is_qa == True]\n",
    "    cogito_df = df[(df.is_qa != True) & ~(df.left_image_url.isin(qa_df.left_image_url))]\n",
    "    df = pd.concat([qa_df, cogito_df], axis=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_akpd_score(akpd_scorer_network, keypoints: Dict, camera_metadata: Dict) -> float:\n",
    "    input_sample = {\n",
    "        'keypoints': keypoints,\n",
    "        'cm': camera_metadata,\n",
    "        'stereo_pair_id': 0,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "    return akpd_score\n",
    "\n",
    "\n",
    "def generate_akpd_scores(df: pd.DataFrame, akpd_scorer_f: str) -> List[float]:\n",
    "    akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "    akpd_scores = []\n",
    "    count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if count % 1000 == 0:\n",
    "            print('Percentage complete: {}%'.format(round(100 * count / df.shape[0], 2)))\n",
    "        count += 1\n",
    "        akpd_score = compute_akpd_score(akpd_scorer_network, row.keypoints, row.camera_metadata)\n",
    "        akpd_scores.append(akpd_score)\n",
    "    return akpd_scores\n",
    "\n",
    "\n",
    "def generate_depths(df: pd.DataFrame):\n",
    "    depths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        annotation = row.keypoints\n",
    "        camera_metadata = row.camera_metadata\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "        X_world = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "        depths.append(np.mean(X_world[:, ]))\n",
    "    return depths\n",
    "\n",
    "\n",
    "def prepare_gtsf_data(start_date: str, end_date: str, akpd_scorer_f: str,\n",
    "                      akpd_score_cutoff: float, depth_cutoff: float) -> pd.DataFrame:\n",
    "    df = generate_raw_df(start_date, end_date)\n",
    "    print('Raw data loaded!')\n",
    "    df = process(df)\n",
    "    print('Data preprocessed!')\n",
    "    df['k_factor'] = 1e5 * df.weight / df.data.apply(lambda x: x['lengthMms']**3).astype(float)\n",
    "    df['akpd_score'] = generate_akpd_scores(df, akpd_scorer_f)\n",
    "    df['depth'] = generate_depths(df)\n",
    "    mask = (df.akpd_score > akpd_score_cutoff) & (df.depth < depth_cutoff)\n",
    "    df = df[mask].copy(deep=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_config = dict(\n",
    "    trials=10,\n",
    "    max_jitter_std=10,\n",
    "    min_scaling_factor=0.3,\n",
    "    max_scaling_factor=2.0\n",
    ")\n",
    "\n",
    "augmented_df = augment(tdf, augmentation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "akpd_scores = []\n",
    "akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    if count % 1000 == 0:\n",
    "        print('Percentage complete: {}%'.format(round(100 * count / augmented_df.shape[0], 2)))\n",
    "    count += 1\n",
    "    akpd_score = compute_akpd_score(akpd_scorer_network, row.annotation, row.camera_metadata)\n",
    "    akpd_scores.append(akpd_score)\n",
    "\n",
    "\n",
    "augmented_df['akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train the synthetic-data based model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, get_ann_from_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "from weight_estimation.dataset import prepare_gtsf_data\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "\n",
    "\n",
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    trials = augmentation_config['trials']\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_scaling_factor = augmentation_config['min_scaling_factor']\n",
    "    max_scaling_factor = augmentation_config['max_scaling_factor']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        for _ in range(trials):\n",
    "            scaling_factor = np.random.uniform(min_scaling_factor, max_scaling_factor)\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "def normalize(anns: List, camera_metadatas: List) -> np.ndarray:\n",
    "    norm_anns = []\n",
    "    for ann, camera_metadata in zip(anns, camera_metadatas):\n",
    "\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        norm_ann = convert_to_nn_input(ann, cm)\n",
    "        norm_anns.append(norm_ann)\n",
    "    return np.array(norm_anns)\n",
    "\n",
    "\n",
    "def get_data_split(X: np.ndarray, y: np.ndarray, fish_ids: np.ndarray, train_pct: float,\n",
    "                   val_pct: float) -> Tuple:\n",
    "    # select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "    test_pct = 1.0 - train_pct - val_pct\n",
    "    unique_fish_ids = np.array(list(set(fish_ids)))\n",
    "    train_cnt, val_cnt, test_cnt = np.random.multinomial(len(unique_fish_ids),\n",
    "                                                         [train_pct, val_pct, test_pct])\n",
    "\n",
    "    assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "    np.random.shuffle(assignments)\n",
    "    train_fish_ids = unique_fish_ids[np.where(assignments == 0)]\n",
    "    val_fish_ids = unique_fish_ids[np.where(assignments == 1)]\n",
    "    test_fish_ids = unique_fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "    train_mask = np.isin(fish_ids, train_fish_ids)\n",
    "    val_mask = np.isin(fish_ids, val_fish_ids)\n",
    "    test_mask = np.isin(fish_ids, test_fish_ids)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, train_config):\n",
    "    inputs = Input(shape=(24,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(1)(x)\n",
    "    model = Model(inputs, pred)\n",
    "\n",
    "    epochs = train_config['epochs']\n",
    "    batch_size = train_config['batch_size']\n",
    "    lr = train_config['learning_rate']\n",
    "    patience = train_config['patience']\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=patience,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def density_scatter(x, y, bins=20, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:]+y_e[:-1])), data, np.vstack([x, y]).T,\n",
    "                method=\"splinef2d\", bounds_error=False)\n",
    "\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "    norm = Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm), ax=ax)\n",
    "    cbar.ax.set_ylabel('Density')\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    ax.grid()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_accuracy_details(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train).squeeze().astype(float)\n",
    "    y_test_pred = model.predict(X_test).squeeze().astype(float)\n",
    "    ax_train = density_scatter(1e4 * y_train, 1e4 * y_train_pred)\n",
    "    ax_test = density_scatter(1e4 * y_test, 1e4 * y_test_pred)\n",
    "    train_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_train_pred - y_train) / y_train)),\n",
    "        'mean_error_pct': 100 * np.mean(y_train_pred - y_train) / np.mean(y_train)\n",
    "    }\n",
    "    test_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_test_pred - y_test) / y_test)),\n",
    "        'mean_error_pct': 100 * np.mean(y_test_pred - y_test) / np.mean(y_test)\n",
    "    }\n",
    "\n",
    "    return ax_train, ax_test, train_stats, test_stats\n",
    "\n",
    "\n",
    "def train(augmented_df, train_config, weight):\n",
    "    print('here')\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    anns = augmented_df.annotation.values.tolist()\n",
    "    cms = augmented_df.camera_metadata.values.tolist()\n",
    "    X = normalize(anns, cms)\n",
    "\n",
    "    if weight:\n",
    "        y = 1e-4 * augmented_df.weight.values\n",
    "    else:\n",
    "        y = (augmented_df.kf.values - 1.2) / 0.3\n",
    "    print(y)\n",
    "    fish_ids = augmented_df.fish_id.values\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(X, y, fish_ids,\n",
    "                                                                    train_config['train_pct'],\n",
    "                                                                    train_config['val_pct'])\n",
    "    model = train_model(X_train, y_train, X_val, y_val, train_config)\n",
    "    ax_train, ax_test, train_stats, test_stats = \\\n",
    "        generate_accuracy_details(model, X_train, y_train, X_test, y_test)\n",
    "    return model, ax_train, ax_test, train_stats, test_stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "key = 'weight'\n",
    "mask = (augmented_df.akpd_score >= 0.9) & (augmented_df.weight > 7500)\n",
    "model, ax_train, ax_test, train_stats, test_stats = train(augmented_df[mask], train_config,\n",
    "                                                          weight=True if key == 'weight' else\n",
    "                                                          False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/tmp/synthetic_data_model_large-bryton.h5'\n",
    "# model.save(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Test out on Leivesthamran pen </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from filter_optimization.filter_optimization_task import NoDataException, SamplingFilter, generate_filter_mask, \\\n",
    "     extract_biomass_data\n",
    "from population_metrics.population_metrics_base import generate_pm_base, PopulationMetricsBase\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.raw_metrics import get_raw_kf_values, generate_raw_average_weight, get_raw_sample_size\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, \\\n",
    "     generate_smart_distribution, generate_smart_avg_kf, get_smart_sample_size, get_smart_growth_rate, \\\n",
    "     generate_smart_standard_deviation\n",
    "from population_metrics.confidence_metrics import generate_trend_stability, generate_distribution_consistency, \\\n",
    "     compute_biomass_kpi, get_raw_and_historical_weights\n",
    "from research.utils.datetime_utils import get_dates_in_range\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, start_date, end_date = 204, '2020-11-25', '2020-12-04'\n",
    "sampling_filter = SamplingFilter(start_hour=7, end_hour=15, kf_cutoff=0.0, akpd_score_cutoff=0.95)\n",
    "rdf = extract_biomass_data(pen_id, start_date, end_date, sampling_filter.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.s3_client.upload_file('/tmp/large_weight_pytorch_model.pb', 'aquabyte-models', 'biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/playground/nn_epoch_798_v2.pb')\n",
    "kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "weights, lengths, kfs = [], [], []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in rdf.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print('Percentage completion: {}%'.format(round(100 * count / rdf.shape[0], 2)))\n",
    "        print(count)\n",
    "    count += 1\n",
    "    annotation = row.annotation\n",
    "    if not annotation:\n",
    "        weights.append(None)\n",
    "        continue\n",
    "    camera_metadata = row.camera_metadata\n",
    "    if not camera_metadata:\n",
    "        camera_metadata = rdf.camera_metadata.iloc[0]\n",
    "\n",
    "    camera_metadata_obj = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "\n",
    "    weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "    weights.append(weight)\n",
    "    lengths.append(length)\n",
    "    kfs.append(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf['estimated_weight_g'] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(rdf, sampling_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Large fish synthetic model Result </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Original Production Model Result </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-12-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(rdf.new_weight.values, bins=20, alpha=0.5, weights=np.ones(rdf.shape[0]) / rdf.shape[0], label='synthetic model weights')\n",
    "plt.hist(rdf.estimated_weight_g.values, bins=20, weights=np.ones(rdf.shape[0]) / rdf.shape[0], alpha=0.5, label='original model weights')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.estimated_weight_g.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.new_weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(rdf.new_weight.values, bins=20, alpha=0.5, weights=np.ones(rdf.shape[0]) / rdf.shape[0], label='synthetic model weights')\n",
    "plt.hist(rdf.estimated_weight_g.values, bins=20, weights=np.ones(rdf.shape[0]) / rdf.shape[0], alpha=0.5, label='original model weights')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n",
    "\n",
    "def gen_pm_base(df: pd.DataFrame, sampling_filter: SamplingFilter) -> PopulationMetricsBase:\n",
    "    \"\"\"\n",
    "    Returns PopulationMetricsBase instance given input biomass computations\n",
    "    data-frame (see README for more details) and SamplingFilter instance.\n",
    "    \"\"\"\n",
    "\n",
    "    mask = generate_filter_mask(df, sampling_filter)\n",
    "\n",
    "    # get filtered set of biomass computations\n",
    "    biomass_computations = list(zip(df[mask].date.values,\n",
    "                                    df.loc[mask, 'estimated_weight_g'].values,\n",
    "                                    df[mask].estimated_k_factor.values))\n",
    "\n",
    "    # generate population metrics estimator\n",
    "    if not biomass_computations:\n",
    "        raise NoDataException('No data found for given filter!')\n",
    "    return generate_pm_base(biomass_computations)\n",
    "\n",
    "\n",
    "def generate_ts_data(df: pd.DataFrame, sampling_filter: SamplingFilter) -> defaultdict:\n",
    "    \"\"\"\n",
    "    Given input data-frame of biomass computations and SamplingFilter instance,\n",
    "    generates time-series data for different raw metrics, smart metrics, growth rate metrics,\n",
    "    and confidence metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    pm_base = gen_pm_base(df, sampling_filter)\n",
    "    start_date, end_date = pm_base.unique_dates[0], pm_base.unique_dates[-1]\n",
    "    dates = get_dates_in_range(start_date, end_date)\n",
    "    ts_data = defaultdict(list)\n",
    "    ts_data['date'].extend(dates)\n",
    "    for date in dates:\n",
    "\n",
    "        # raw metrics\n",
    "        raw_average_weight = generate_raw_average_weight(pm_base, date)\n",
    "        raw_sample_size = get_raw_sample_size(pm_base, date)\n",
    "\n",
    "        # growth rate metrics\n",
    "        growth_rate = compute_local_growth_rate(pm_base, date)\n",
    "\n",
    "        # confidence metrics\n",
    "        distribution_consistency = generate_distribution_consistency(pm_base, date)\n",
    "        kpi = compute_biomass_kpi(pm_base, date)\n",
    "\n",
    "        # smart metrics\n",
    "        smart_average_weight = generate_smart_avg_weight(pm_base, date)\n",
    "        smart_average_kf = generate_smart_avg_kf(pm_base, date)\n",
    "        smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "        smart_growth_rate = get_smart_growth_rate(pm_base, date)\n",
    "\n",
    "        ts_data['raw_average_weight'].append(raw_average_weight)\n",
    "        ts_data['raw_sample_size'].append(raw_sample_size)\n",
    "        ts_data['growth_rate'].append(growth_rate)\n",
    "        ts_data['distribution_consistency'].append(distribution_consistency)\n",
    "        ts_data['kpi'].append(kpi)\n",
    "        ts_data['smart_average_weight'].append(smart_average_weight)\n",
    "        ts_data['smart_average_kf'].append(smart_average_kf)\n",
    "        ts_data['smart_sample_size'].append(smart_sample_size)\n",
    "        ts_data['smart_growth_rate'].append(smart_growth_rate)\n",
    "\n",
    "    return ts_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(rdf, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-11-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-11-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_smart_avg_weight(pm_base, '2020-11-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_distribution(weights, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        bucket_count = weights[(weights >= low) & (weights < high)].shape[0]\n",
    "        dist[bucket] = bucket_count\n",
    "        count += bucket_count\n",
    "    \n",
    "    dist = {k: round(100 * v / count, 1) for k, v in dist.items()}\n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_kf_breakdown(weights, kfs, bucket_cutoffs):\n",
    "    dist = {}\n",
    "    count = 0\n",
    "    for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "        bucket = f'{1e-3 * low}-{1e-3 * high}'\n",
    "        mean_kf = kfs[(weights >= low) & (weights < high)].mean()\n",
    "        dist[bucket] = round(mean_kf, 2)\n",
    "    \n",
    "    return dist\n",
    "        \n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "    \n",
    "\n",
    "def generate_info(pm_base, date, loss_factor, adjustment_pct):\n",
    "    weights, kfs = generate_smart_individual_values(pm_base, date, 3, True, True, 0.9)\n",
    "    vals = (1.0 + 0.01 * adjustment_pct) * weights * (1.0 - loss_factor)\n",
    "    smart_avg = np.mean(vals)\n",
    "    smart_kf = np.mean(kfs)\n",
    "    smart_sample_size = get_smart_sample_size(pm_base, date)\n",
    "    smart_std = np.std(vals)\n",
    "    cov = smart_std / smart_avg\n",
    "    weight_dist = get_distribution(vals, np.arange(0, 15000, 1000))\n",
    "    kf_breakdown = get_kf_breakdown(vals, kfs, np.arange(0, 15000, 1000))\n",
    "    \n",
    "    print('Loss Factor: {}%'.format(round(100 * loss_factor)))\n",
    "    print('-----------')\n",
    "    print('Smart Avg Weight: {}g'.format(round(smart_avg)))\n",
    "    print('Smart K Factor: {}'.format(round(smart_kf, 2)))\n",
    "    print('Smart Sample Size: {}'.format(smart_sample_size))\n",
    "    print('Smart Standard Deviation: {}g'.format(round(smart_std)))\n",
    "    print('Coefficient of Variation: {}%'.format(round(100 * cov, 1)))\n",
    "    print('Weight Distribution:')\n",
    "    print(json.dumps(weight_dist, indent=4))\n",
    "    print('KF Breakdown:')\n",
    "    print(json.dumps(kf_breakdown, indent=4))\n",
    "    \n",
    "    return {\n",
    "        'loss_factor': round(100 * loss_factor),\n",
    "        'smart_average_weight': round(smart_avg),\n",
    "        'smart_k_factor': round(smart_kf, 2),\n",
    "        'smart_sample_size': smart_sample_size,\n",
    "        'smart_standard_deviation': smart_std,\n",
    "        'coefficient_of_variation': round(100 * cov, 1),\n",
    "        'weight_distribution': weight_dist,\n",
    "        'kf_breakdown': kf_breakdown\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-12-03'\n",
    "adjustment_pct = -1.5\n",
    "\n",
    "output = []\n",
    "for loss_factor in [0] + list(np.arange(0.13, 0.19, 0.01)):\n",
    "    \n",
    "    output.append(generate_info(pm_base, date, loss_factor, adjustment_pct))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output, indent=4).replace('NaN', 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate weight trend line over larger period </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, start_date, end_date = 153, '2020-08-01', '2020-11-22'\n",
    "sampling_filter = SamplingFilter(start_hour=7, end_hour=15, kf_cutoff=0.0, akpd_score_cutoff=0.95)\n",
    "rdf = extract_biomass_data(pen_id, start_date, end_date, sampling_filter.akpd_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_base = gen_pm_base(rdf, sampling_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(rdf.date.unique())\n",
    "original_weights = []\n",
    "for date in dates:\n",
    "    weight = generate_smart_avg_weight(pm_base, date)\n",
    "    original_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(rdf.date.unique())\n",
    "new_weights = []\n",
    "for date in dates:\n",
    "    weight = generate_smart_avg_weight(pm_base, date)\n",
    "    new_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(dates, original_weights, label='Original Weight')\n",
    "ax.plot(dates, new_weights, label='Corrected Weight')\n",
    "ax.set_ylim([2000, 7000])\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Weight (g)')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matplotlib\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'original_weight': original_weights,\n",
    "    'new_weight': new_weights\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.to_csv('/Users/aloksaxena/Desktop/original_new_weights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = generate_smart_individual_values(pm_base, '2020-05-12', 3, True, True, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(x, bins=100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = generate_smart_individual_values(pm_base, '2020-05-12', 3, True, True, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(x, color='blue', alpha=0.5, weights=np.ones(len(x)) / len(x), bins=20, label='prediction')\n",
    "plt.hist(gt_df.weight * 1000 * 1.17, color='red', alpha=0.5, weights=np.ones(gt_df.shape[0]) / gt_df.shape[0], bins=20, label='ground truth')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv('/Users/aloksaxena/Desktop/eide_langoy_singleweights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Convert to PyTorch </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_estimation.weight_estimator import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.fc1.weight.data = torch.from_numpy(np.transpose(weights[0]))\n",
    "pytorch_model.fc1.bias.data = torch.from_numpy(np.transpose(weights[1]))\n",
    "pytorch_model.fc2.weight.data = torch.from_numpy(np.transpose(weights[2]))\n",
    "pytorch_model.fc2.bias.data = torch.from_numpy(np.transpose(weights[3]))\n",
    "pytorch_model.fc3.weight.data = torch.from_numpy(np.transpose(weights[4]))\n",
    "pytorch_model.fc3.bias.data = torch.from_numpy(np.transpose(weights[5]))\n",
    "pytorch_model.output.weight.data = torch.from_numpy(np.transpose(weights[6]))\n",
    "pytorch_model.output.bias.data = torch.from_numpy(np.transpose(weights[7]))\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pytorch_model.state_dict(), '/tmp/large_weight_pytorch_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluate model on standard GTSF dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_model_f = f\n",
    "# kf_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/src/weight_estimation/kf_model.h5'\n",
    "\n",
    "weight_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/tests/artifacts/weight_model.pb'\n",
    "kf_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/tests/artifacts/kf_model.pb'\n",
    "    \n",
    "    \n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "weights, lengths, kfs = [], [], []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print('Percentage completion: {}%'.format(round(100 * count / df.shape[0], 2)))\n",
    "        print(count)\n",
    "    count += 1\n",
    "    annotation = row.keypoints\n",
    "    camera_metadata = row.camera_metadata\n",
    "\n",
    "    camera_metadata_obj = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "\n",
    "    weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "    weights.append(weight)\n",
    "    lengths.append(length)\n",
    "    kfs.append(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['estimated_weight_g'] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_cutoffs = np.arange(0, 13000, 1000)\n",
    "buckets, biases, maes = [], [], []\n",
    "for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "    bucket = '{}-{}'.format(low, high)\n",
    "    mask = (df.weight > low) & (df.weight <= high) & (df.akpd_score > 0.9)\n",
    "    bias = (df[mask].estimated_weight_g.mean() - df[mask].weight.mean()) / (df[mask].weight.mean())\n",
    "    mae = np.mean(np.abs((df[mask].estimated_weight_g.values - df[mask].weight.values) / (df[mask].weight.values)))\n",
    "    \n",
    "    buckets.append(bucket)\n",
    "    biases.append(bias)\n",
    "    maes.append(mae)\n",
    "    print(bucket, bias, mae)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = pd.DataFrame({\n",
    "    'bucket': buckets,\n",
    "    'bias': biases,\n",
    "    'mae': maes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(kdf.bucket, kdf.bias * 100)\n",
    "plt.xlabel('Weight Bucket (g)')\n",
    "plt.ylabel('Bias (%)')\n",
    "plt.title('Single Fish Accuracy vs. Weight Bucket')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_cutoffs = np.arange(0, 10000, 1000)\n",
    "buckets, biases, maes = [], [], []\n",
    "for low, high in zip(bucket_cutoffs, bucket_cutoffs[1:]):\n",
    "    bucket = '{}-{}'.format(low, high)\n",
    "    mask = (df.weight > low) & (df.weight <= high) & (df.akpd_score > 0.9)\n",
    "    bias = (df[mask].estimated_weight_g_2.mean() - df[mask].weight.mean()) / (df[mask].weight.mean())\n",
    "    mae = np.mean(np.abs((df[mask].estimated_weight_g_2.values - df[mask].weight.values) / (df[mask].weight.values)))\n",
    "    \n",
    "    buckets.append(bucket)\n",
    "    biases.append(bias)\n",
    "    maes.append(mae)\n",
    "    print(bucket, bias, mae)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'bucket': buckets,\n",
    "    'bias': biases,\n",
    "    'mae': maes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train the model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from weight_estimation import body_parts\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length_pixel', 'baseline_m', 'pixel_count_width',\n",
    "                             'pixel_count_height', 'image_sensor_width', 'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in body_parts.core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def get_ann_from_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Dict:\n",
    "    \"\"\"Constructs annotation from left and right key-point arrays (i.e. inverse of\n",
    "    get_left_right_keypoint_arrs method.\"\"\"\n",
    "\n",
    "    ann = {'leftCrop': [], 'rightCrop': []}\n",
    "    for idx in range(X_left.shape[0]):\n",
    "        x_left, y_left = tuple(X_left[idx, :])\n",
    "        x_right, y_right = tuple(X_right[idx, :])\n",
    "        body_part = body_parts.core_body_parts[idx]\n",
    "        left_item = dict(keypointType=body_part, xFrame=x_left, yFrame=y_left)\n",
    "        right_item = dict(keypointType=body_part, xFrame=x_right, yFrame=y_right)\n",
    "        ann['leftCrop'].append(left_item)\n",
    "        ann['rightCrop'].append(right_item)\n",
    "\n",
    "    return ann\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = body_parts.core_body_parts.index(body_parts.UPPER_LIP)\n",
    "    tail_notch_idx = body_parts.core_body_parts.index(body_parts.TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "    x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> np.ndarray:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world).reshape(1, -1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, CameraMetadata\n",
    "from weight_estimation.dataset import prepare_gtsf_data\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> np.ndarray:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world).reshape(1, -1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    trials = augmentation_config['trials']\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_scaling_factor = augmentation_config['min_scaling_factor']\n",
    "    max_scaling_factor = augmentation_config['max_scaling_factor']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        for _ in range(trials):\n",
    "            scaling_factor = np.random.uniform(min_scaling_factor, max_scaling_factor)\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "def normalize(anns: List, camera_metadatas: List) -> np.ndarray:\n",
    "    norm_anns = []\n",
    "    for ann, camera_metadata in zip(anns, camera_metadatas):\n",
    "\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        norm_ann = convert_to_nn_input(ann, cm)\n",
    "        norm_anns.append(norm_ann)\n",
    "    return np.array(norm_anns)\n",
    "\n",
    "\n",
    "def get_data_split(X: np.ndarray, y: np.ndarray, fish_ids: np.ndarray, train_pct: float,\n",
    "                   val_pct: float) -> Tuple:\n",
    "    # select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "    test_pct = 1.0 - train_pct - val_pct\n",
    "    unique_fish_ids = np.array(list(set(fish_ids)))\n",
    "    train_cnt, val_cnt, test_cnt = np.random.multinomial(len(unique_fish_ids),\n",
    "                                                         [train_pct, val_pct, test_pct])\n",
    "\n",
    "    assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "    np.random.shuffle(assignments)\n",
    "    train_fish_ids = unique_fish_ids[np.where(assignments == 0)]\n",
    "    val_fish_ids = unique_fish_ids[np.where(assignments == 1)]\n",
    "    test_fish_ids = unique_fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "    train_mask = np.isin(fish_ids, train_fish_ids)\n",
    "    val_mask = np.isin(fish_ids, val_fish_ids)\n",
    "    test_mask = np.isin(fish_ids, test_fish_ids)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, train_config):\n",
    "    inputs = Input(shape=(24,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(1)(x)\n",
    "    model = Model(inputs, pred)\n",
    "\n",
    "    epochs = train_config['epochs']\n",
    "    batch_size = train_config['batch_size']\n",
    "    lr = train_config['learning_rate']\n",
    "    patience = train_config['patience']\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=patience,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def density_scatter(x, y, bins=20, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:]+y_e[:-1])), data, np.vstack([x, y]).T,\n",
    "                method=\"splinef2d\", bounds_error=False)\n",
    "\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "    norm = Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm), ax=ax)\n",
    "    cbar.ax.set_ylabel('Density')\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    ax.grid()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_accuracy_details(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train).squeeze().astype(float)\n",
    "    y_test_pred = model.predict(X_test).squeeze().astype(float)\n",
    "    ax_train = density_scatter(1e4 * y_train, 1e4 * y_train_pred)\n",
    "    ax_test = density_scatter(1e4 * y_test, 1e4 * y_test_pred)\n",
    "    train_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_train_pred - y_train) / y_train)),\n",
    "        'mean_error_pct': 100 * np.mean(y_train_pred - y_train) / np.mean(y_train)\n",
    "    }\n",
    "    test_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_test_pred - y_test) / y_test)),\n",
    "        'mean_error_pct': 100 * np.mean(y_test_pred - y_test) / np.mean(y_test)\n",
    "    }\n",
    "\n",
    "    return ax_train, ax_test, train_stats, test_stats\n",
    "\n",
    "\n",
    "def train(augmented_df, train_config, weight):\n",
    "    print('here')\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    anns = augmented_df.annotation.values.tolist()\n",
    "    cms = augmented_df.camera_metadata.values.tolist()\n",
    "    X = normalize(anns, cms)\n",
    "\n",
    "    if weight:\n",
    "        y = 1e-4 * augmented_df.weight.values\n",
    "    else:\n",
    "        y = (augmented_df.kf.values - 1.2) / 0.3\n",
    "    fish_ids = augmented_df.fish_id.values\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(X, y, fish_ids,\n",
    "                                                                    train_config['train_pct'],\n",
    "                                                                    train_config['val_pct'])\n",
    "    model = train_model(X_train, y_train, X_val, y_val, train_config)\n",
    "    ax_train, ax_test, train_stats, test_stats = \\\n",
    "        generate_accuracy_details(model, X_train, y_train, X_test, y_test)\n",
    "    return model, ax_train, ax_test, train_stats, test_stats\n",
    "\n",
    "def get_ann_from_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Dict:\n",
    "    \"\"\"Constructs annotation from left and right key-point arrays (i.e. inverse of\n",
    "    get_left_right_keypoint_arrs method.\"\"\"\n",
    "\n",
    "    ann = {'leftCrop': [], 'rightCrop': []}\n",
    "    for idx in range(X_left.shape[0]):\n",
    "        x_left, y_left = tuple(X_left[idx, :])\n",
    "        x_right, y_right = tuple(X_right[idx, :])\n",
    "        body_part = body_parts.core_body_parts[idx]\n",
    "        left_item = dict(keypointType=body_part, xFrame=x_left, yFrame=y_left)\n",
    "        right_item = dict(keypointType=body_part, xFrame=x_right, yFrame=y_right)\n",
    "        ann['leftCrop'].append(left_item)\n",
    "        ann['rightCrop'].append(right_item)\n",
    "\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/aloksaxena/Documents/general_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(augmented_df.weight)\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Experiment #1: Filter on AKPD > 0.9; Incorporate all data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "key = 'weight'\n",
    "mask = (augmented_df.akpd_score >= 0.9) & (augmented_df.weight < 5000)\n",
    "model, ax_train, ax_test, train_stats, test_stats = train(augmented_df[mask], train_config,\n",
    "                                                          weight=True if key == 'weight' else\n",
    "                                                          False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = augmented_df.akpd_score > 0.9\n",
    "tdf = augmented_df[mask].copy(deep=True)\n",
    "\n",
    "anns = tdf.annotation.values.tolist()\n",
    "cms = tdf.camera_metadata.values.tolist()\n",
    "X = normalize(anns, cms)\n",
    "pred = 1e4 * model(X).numpy().squeeze()\n",
    "tdf['pred1'] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = (tdf.weight >= 0)\n",
    "error_pcts = ((tdf[mask2].pred1 - tdf[mask2].weight) / tdf[mask2].weight).values\n",
    "mean_abs_pct_err = np.mean(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = (tdf.weight > 1500)# & (tdf.weight < 3000)\n",
    "error_pcts = ((tdf[mask2].pred1 - tdf[mask2].weight) / tdf[mask2].weight).values\n",
    "mean_abs_pct_err = np.median(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(np.arange(0, 10000, 1000))\n",
    "for lo, hi in zip(weights, weights[1:]):\n",
    "    mask2 = (tdf.weight > lo) & (tdf.weight <= hi)\n",
    "    error_pcts = ((tdf[mask2].pred1 - tdf[mask2].weight) / tdf[mask2].weight).values\n",
    "    mean_abs_pct_err = np.median(np.abs(error_pcts))\n",
    "    mean_pct_err = np.mean(error_pcts)\n",
    "#     print('Weight bucket: {}-{}'.format(lo, hi))\n",
    "#     print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "#     print('Mean percentage error: {}'.format(mean_pct_err)) \n",
    "    weight_bucket = '{}-{}'.format(lo, hi)\n",
    "    print(weight_bucket, mean_pct_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from prod.biomass_computations\n",
    "where akpd_score > 0.9\n",
    "and pen_id = 193\n",
    "and captured_at > '2020-10-01'\n",
    "limit 10000;\n",
    "\"\"\"\n",
    "\n",
    "kdf = rds.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_model_f = '/Users/aloksaxena/Documents/model.h5'\n",
    "kf_model_f = '/Users/aloksaxena/Documents/repos/production_algo/weight_estimation/src/weight_estimation/kf_model.h5'\n",
    "    \n",
    "    \n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "weights, lengths, kfs = [], [], []\n",
    "count = 0\n",
    "\n",
    "for idx, row in kdf.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print('Percentage completion: {}%'.format(round(100 * count / df.shape[0], 2)))\n",
    "        print(count)\n",
    "    count += 1\n",
    "    annotation = row.annotation\n",
    "    camera_metadata = row.camera_metadata\n",
    "\n",
    "    camera_metadata_obj = CameraMetadata(\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "\n",
    "    if row.estimated_weight_g < 1000:\n",
    "        weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "        weights.append(weight)\n",
    "        lengths.append(length)\n",
    "        kfs.append(kf)\n",
    "    else:\n",
    "        weights.append(row.estimated_weight_g)\n",
    "        lengths.append(row.estimated_length_mm)\n",
    "        kfs.append(row.estimated_k_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(weights, bins=50)\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(kdf.estimated_weight_g, bins=50)\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.estimated_weight_g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(error_pcts, bins=100)\n",
    "plt.xlim([-0.2, 0.2])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = augmented_df.weight >= 1500\n",
    "error_pcts = ((augmented_df[mask].pred - augmented_df[mask].weight) / augmented_df[mask].weight).values\n",
    "mean_abs_pct_err = np.mean(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = augmented_df.weight < 1500\n",
    "error_pcts = ((augmented_df[mask].pred2 - augmented_df[mask].weight) / augmented_df[mask].weight).values\n",
    "mean_abs_pct_err = np.mean(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = augmented_df.weight >= 1500\n",
    "error_pcts = ((augmented_df[mask].pred2 - augmented_df[mask].weight) / augmented_df[mask].weight).values\n",
    "mean_abs_pct_err = np.mean(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "key = 'weight'\n",
    "model_small, ax_train, ax_test, train_stats, test_stats = train(augmented_df[augmented_df.weight < 1500], train_config,\n",
    "                                                          weight=True if key == 'weight' else\n",
    "                                                          False)\n",
    "# model.save(f'{key}_model.h5')\n",
    "# ax_train.figure.savefig(f'{key}_train_plot.png')\n",
    "# ax_test.figure.savefig(f'{key}_test_plot.png')\n",
    "# json.dump(train_stats, open(f'{key}_train_stats.json', 'w'))\n",
    "# json.dump(test_stats, open(f'{key}_test_stats.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = 1e4 * model_small(X).numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['pred3'] = pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = augmented_df.weight <= 1500\n",
    "error_pcts = ((augmented_df[mask].pred3 - augmented_df[mask].weight) / augmented_df[mask].weight).values\n",
    "mean_abs_pct_err = np.mean(np.abs(error_pcts))\n",
    "mean_pct_err = np.mean(error_pcts)\n",
    "print('Mean absolute percentage error: {}'.format(mean_abs_pct_err))\n",
    "print('Mean percentage error: {}'.format(mean_pct_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
