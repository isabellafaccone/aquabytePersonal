{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker, relationship, join\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import Table, Column, Integer, ForeignKey\n",
    "from sqlalchemy.orm import relationship\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "from aquabyte.data_access_utils import DataAccessUtils\n",
    "\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Establish connection to database and perform query for base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "data_access_utils = DataAccessUtils()\n",
    "\n",
    "# prod SQL credentaials\n",
    "sql_credentials = json.load(open(os.environ[\"PROD_SQL_CREDENTIALS\"]))\n",
    "\n",
    "sql_query = '''\n",
    "select * from keypoint_annotations\n",
    "where pen_id = 7;\n",
    "'''\n",
    "\n",
    "original_df = data_access_utils.extract_from_database(sql_query)\n",
    "# original_df = original_df.loc[:, ~original_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iterate over query results and generate 3D coordinates + biomass estimates for each stereo fish detection </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values_quadratic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values_quadratic.append(dist1 * dist2)\n",
    "            \n",
    "    interaction_values_cubic = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            for k in range(j, len(pairwise_distances)):\n",
    "                dist1 = pairwise_distances[i]\n",
    "                dist2 = pairwise_distances[j]\n",
    "                dist3 = pairwise_distances[k]\n",
    "                interaction_values_cubic.append(dist1 * dist2 * dist3)\n",
    "            \n",
    "    \n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values_quadratic + interaction_values_cubic)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def normalize_world_keypoints(world_keypoint_coordinates):\n",
    "    body_parts = sorted(world_keypoint_coordinates.keys())\n",
    "    wkps = {bp: np.array(world_keypoint_coordinates[bp]) for bp in body_parts}\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    translated_wkps = {bp: wkps[bp] - wkps['TAIL_NOTCH'] for bp in body_parts}\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps['UPPER_LIP']\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    # perform second rotation\n",
    "    u_base = np.array([0, 0, 1])\n",
    "    v = norm_wkps_intermediate['DORSAL_FIN'] - np.array([norm_wkps_intermediate['DORSAL_FIN'][0], 0, 0])\n",
    "    R = generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps = {bp: np.dot(R, norm_wkps_intermediate[bp]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model parameters for Blender and linear models\n",
    "model = pickle.load(open('/root/data/alok/biomass_estimation/models/model_v2.pkl', 'rb'))\n",
    "# blender = json.load(open('/root/data/alok/biomass_estimation/models/volumes.json'))\n",
    "\n",
    "qa_mask = original_df.annotated_by_email.str.contains('aquabyte')\n",
    "# establish new columns\n",
    "mask = (~original_df.keypoints.isnull())\n",
    "for col in ['left_keypoints', 'right_keypoints', 'world_keypoint_coordinates', 'camera_metadata']:\n",
    "    original_df[col] = np.nan\n",
    "    original_df[col] = original_df[col].astype(object)\n",
    "for col in ['predicted_biomass_linear', 'predicted_biomass_blender', \n",
    "            'max_y_coordinate_deviation', 'max_y_world_coordinate_deviation']:\n",
    "    original_df[col] = np.nan\n",
    "    \n",
    "\n",
    "# modify the dataframe row-by-row\n",
    "for idx, row in original_df[mask].iterrows():\n",
    "    keypoints = row.keypoints\n",
    "    left_image_url = row.left_image_url\n",
    "    try:\n",
    "        keypoints = original_df[(~qa_mask) & (original_df.left_image_url == left_image_url)].iloc[0].keypoints\n",
    "    except:\n",
    "        continue\n",
    "    original_df.at[idx, 'keypoints'] = keypoints\n",
    "\n",
    "    try:\n",
    "        left_keypoints = keypoints['leftCrop']\n",
    "        right_keypoints = keypoints['rightCrop']\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "    # compute world coordinates\n",
    "    camera_metadata = row.camera_metadata\n",
    "    camera_metadata['pixelCountHeight'] = 3000\n",
    "    camera_metadata['pixelCountWidth'] = 4096\n",
    "    world_keypoint_coordinates = pixel2world(left_keypoints, right_keypoints, camera_metadata)\n",
    "    original_df.at[idx, 'camera_metadata'] = camera_metadata\n",
    "    \n",
    "    # update dataframe with world keypoint coordinates\n",
    "    original_df.at[idx, 'left_keypoints'] = left_keypoints\n",
    "    original_df.at[idx, 'right_keypoints'] = right_keypoints\n",
    "    original_df.at[idx, 'world_keypoint_coordinates'] = world_keypoint_coordinates\n",
    "    \n",
    "    body_parts = sorted(list(world_keypoint_coordinates.keys()))\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            bp1, bp2 = body_parts[i], body_parts[j]\n",
    "            col = '{}<->{}'.format(body_parts[i], body_parts[j])\n",
    "            if not col in original_df.columns:\n",
    "                original_df[col] = np.nan\n",
    "            original_df.at[idx, col] = \\\n",
    "                euclidean_distance(world_keypoint_coordinates[bp1], world_keypoint_coordinates[bp2])\n",
    "    \n",
    "    # update dataframe with biomass predictions from both models\n",
    "    predicted_biomass_linear = coord2biomass_linear(world_keypoint_coordinates, model)\n",
    "    original_df.at[idx, 'predicted_biomass_linear'] = predicted_biomass_linear\n",
    "    \n",
    "    # update dataframe with keypoint deviation\n",
    "    threshold = 10\n",
    "    left_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in left_keypoints}\n",
    "    right_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in right_keypoints}\n",
    "    max_y_coordinate_deviation = \\\n",
    "        max([abs(left_keypoint_y_coords[bp] - right_keypoint_y_coords[bp]) for bp in body_parts])\n",
    "    \n",
    "    original_df.at[idx, 'max_y_coordinate_deviation'] = max_y_coordinate_deviation\n",
    "    \n",
    "    # add 3D range for world coordinate y-values\n",
    "    \n",
    "    \n",
    "    norm_wkps = normalize_world_keypoints(world_keypoint_coordinates)\n",
    "    norm_wkp_y_values = [norm_wkps[bp][1] for bp in norm_wkps.keys()]\n",
    "    max_y_world_coordinate_deviation = max(norm_wkp_y_values) - min(norm_wkp_y_values)\n",
    "    original_df.at[idx, 'max_y_world_coordinate_deviation'] = max_y_world_coordinate_deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply filters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids = json.load(open('/root/data/alok/biomass_estimation/invalid_fish_detection_ids.json'))\n",
    "df = original_df.copy(deep=True)\n",
    "\n",
    "# define filters\n",
    "valid_linear_prediction_mask = ~df.predicted_biomass_linear.isnull()\n",
    "rectification_valid_mask = (~df.fish_detection_id.isin(invalid_fish_detection_ids))\n",
    "keypoints_valid_mask = (df.max_y_coordinate_deviation < 15)\n",
    "qa_mask = df.is_qa == True\n",
    "\n",
    "inlier_mask = (df.predicted_biomass_linear > np.percentile(original_df.predicted_biomass_linear.dropna(), 1.0)) & \\\n",
    "              (df.predicted_biomass_linear < np.percentile(original_df.predicted_biomass_linear.dropna(), 99.0))\n",
    "\n",
    "mask_valid = valid_linear_prediction_mask & rectification_valid_mask & keypoints_valid_mask & qa_mask\n",
    "\n",
    "mask = mask_valid & inlier_mask\n",
    "\n",
    "df = df[mask].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Display left and right crops with annotations overlayed </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coordinates(image_url, side, keypoints):\n",
    "    bucket = 'aquabyte-crops'\n",
    "    key = image_url[image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    image_f = data_access_utils.download_from_s3(bucket, key)\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    im = plt.imread(image_f)\n",
    "    \n",
    "    for keypoint in keypoints:\n",
    "        keypoint_type = keypoint['keypointType']\n",
    "        x, y = keypoint['xCrop'], keypoint['yCrop']\n",
    "        plt.scatter([x], [y])\n",
    "        plt.annotate(keypoint_type, (x, y), color='red')\n",
    "        \n",
    "    plt.imshow(im)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_annotation_id = 173849\n",
    "keypoint_annotation_mask = (df.id == keypoint_annotation_id)\n",
    "left_image_url = df[keypoint_annotation_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[keypoint_annotation_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[keypoint_annotation_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[keypoint_annotation_mask].right_keypoints.iloc[0]\n",
    "\n",
    "world_keypoint_coordinates = df[keypoint_annotation_mask].world_keypoint_coordinates.iloc[0]\n",
    "im_left = plot_coordinates(left_image_url, 'left', left_keypoints)\n",
    "im_right = plot_coordinates(right_image_url, 'right', right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate lateral keypoint </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lateral_keypoints(left_image, right_image, left_keypoints, right_keypoints, world_keypoints, \n",
    "                               bp_1='UPPER_LIP', bp_2='TAIL_NOTCH', left_window_size=100, \n",
    "                               min_breadth=0.04, max_breadth=0.2):\n",
    "    left_extrap_kp = (0.5 * left_keypoints[bp_1] + 0.5 * left_keypoints[bp_2]).astype('int64')\n",
    "    bp_1_depth = world_keypoints[bp_1][1]\n",
    "    bp_2_depth = world_keypoints[bp_2][1]\n",
    "\n",
    "    # need to determine lower and upper bounds here in a data driven fashion from GTSF data\n",
    "    # hardcoded values used here\n",
    "    extrap_kp_max_depth = (bp_1_depth + bp_2_depth) / 2.0 - min_breadth / 2.0\n",
    "    extrap_kp_min_depth = (bp_1_depth + bp_2_depth) / 2.0 - max_breadth / 2.0\n",
    "\n",
    "    # Compute the feature descriptor for the extrapolated keypoint in the left image\n",
    "    extrap_kp_min_disp = disp_from_depth(extrap_kp_max_depth)\n",
    "    extrap_kp_max_disp = disp_from_depth(extrap_kp_min_depth)\n",
    "    \n",
    "    left_box = left_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2, \n",
    "                          left_extrap_kp[0]-left_window_size//2:left_extrap_kp[0]+left_window_size//2]\n",
    "    right_box = right_image[left_extrap_kp[1]-left_window_size//2:left_extrap_kp[1]+left_window_size//2,\n",
    "                            left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2:left_extrap_kp[0]-int(extrap_kp_min_disp)+left_window_size//2]\n",
    "\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(left_box,None)\n",
    "    kp2, des2 = orb.detectAndCompute(right_box,None)\n",
    "    \n",
    "    # get top five matches\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1,des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)[:5]\n",
    "    \n",
    "    # get world coordinates of lateral keypoints\n",
    "    lateral_wkps = []\n",
    "    for match in matches[:5]:\n",
    "        \n",
    "        lateral_left_coordinates = np.array(kp1[match.queryIdx].pt).astype(int)\n",
    "        lateral_left_coordinates[0] += left_extrap_kp[0]-left_window_size//2\n",
    "        lateral_left_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        lateral_right_coordinates = np.array(kp2[match.trainIdx].pt).astype(int)\n",
    "        lateral_right_coordinates[0] += left_extrap_kp[0]-int(extrap_kp_max_disp)-left_window_size//2\n",
    "        lateral_right_coordinates[1] += left_extrap_kp[1]-left_window_size//2\n",
    "        \n",
    "        disp = abs(lateral_left_coordinates[0] - lateral_right_coordinates[0])\n",
    "        depth = depth_from_disp(disp)\n",
    "        lateral_wkp = convert_to_world_point(lateral_left_coordinates[0], lateral_left_coordinates[1], depth)\n",
    "        lateral_wkps.append(lateral_wkp)\n",
    "        \n",
    "    return np.array(lateral_wkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'aquabyte-crops'\n",
    "\n",
    "left_image_url = df[keypoint_annotation_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[keypoint_annotation_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[keypoint_annotation_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[keypoint_annotation_mask].right_keypoints.iloc[0]\n",
    "world_keypoints = df[keypoint_annotation_mask].world_keypoint_coordinates.iloc[0]\n",
    "\n",
    "left_key = left_image_url[left_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "left_image_f = data_access_utils.download_from_s3(bucket, left_key)\n",
    "left_image = plt.imread(left_image_f)\n",
    "\n",
    "right_key = right_image_url[right_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "right_image_f = data_access_utils.download_from_s3(bucket, right_key)\n",
    "right_image = plt.imread(right_image_f)\n",
    "\n",
    "left_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints}\n",
    "right_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lateral_keypoints(left_image, right_image, left_kps, right_kps, world_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
