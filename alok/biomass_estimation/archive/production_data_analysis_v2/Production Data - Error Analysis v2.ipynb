{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import boto3\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, and_, func\n",
    "from sqlalchemy.orm import sessionmaker, relationship, join\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import Table, Column, Integer, ForeignKey\n",
    "from sqlalchemy.orm import relationship\n",
    "from aquabyte.optics import convert_to_world_point, depth_from_disp, pixel2world, euclidean_distance\n",
    "from aquabyte.database_access_utils import extract_from_database\n",
    "\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Establish connection to database and perform query for base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "aws_credentials = json.load(open(os.environ[\"AWS_CREDENTIALS\"]))\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")\n",
    "\n",
    "\n",
    "# prod SQL credentaials\n",
    "sql_credentials = json.load(open(os.environ[\"PROD_SQL_CREDENTIALS\"]))\n",
    "\n",
    "sql_query = '''\n",
    "select * from keypoint_annotations\n",
    "where captured_at >= '2019-05-15'\n",
    "and site_id = 23\n",
    "and pen_id = 4;\n",
    "'''\n",
    "\n",
    "original_df = extract_from_database(sql_query, sql_credentials)\n",
    "original_df = original_df.loc[:, ~original_df.columns.duplicated()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iterate over query results and generate 3D coordinates + biomass estimates for each stereo fish detection </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2biomass_linear(world_keypoints, model):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    mean = model['mean']\n",
    "    std= model['std']\n",
    "    PCA_components = model['PCA_components']\n",
    "    reg_coef = model['reg_coef']\n",
    "    reg_intercept = model['reg_intercept']\n",
    "    body_parts = model['body_parts']\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # based on the exact ordering reflected in the body_parts\n",
    "    # variable above\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            dist = euclidean_distance(world_keypoints[body_parts[i]], world_keypoints[body_parts[j]])\n",
    "            pairwise_distances.append(dist)\n",
    "    \n",
    "    interaction_values = []\n",
    "    for i in range(len(pairwise_distances)):\n",
    "        for j in range(i, len(pairwise_distances)):\n",
    "            dist1 = pairwise_distances[i]\n",
    "            dist2 = pairwise_distances[j]\n",
    "            interaction_values.append(dist1 * dist2)\n",
    "\n",
    "    X = np.array(pairwise_distances + interaction_values)\n",
    "\n",
    "    X_normalized = (X - model['mean']) / model['std']\n",
    "    X_transformed = np.dot(X_normalized, model['PCA_components'].T)\n",
    "    prediction = np.dot(X_transformed, reg_coef) + reg_intercept\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def coord2biomass_blender(world_keypoints, blender):\n",
    "    \"\"\"from coordinates to biomass\"\"\"\n",
    "\n",
    "    reverse_mapping = blender[\"reverse_mapping\"]\n",
    "    distances = np.array(blender[\"distances\"])\n",
    "    volumes = blender[\"volume\"]\n",
    "    regression_coeff = blender[\"coeff\"]\n",
    "\n",
    "    # calculate pairwise distances for production coord\n",
    "    # the reverse mapping insure that we listing the kp\n",
    "    # in the same order\n",
    "    measurements = []\n",
    "    number_of_parts = len(world_keypoints)\n",
    "    for k in range(number_of_parts):\n",
    "        v = world_keypoints[reverse_mapping[str(k)]]\n",
    "        for k0 in range(k+1, number_of_parts):\n",
    "            v0 = world_keypoints[reverse_mapping[str(k0)]]\n",
    "            dist = euclidean_distance(v, v0)*1000 # mm to m\n",
    "            measurements.append(dist)\n",
    "    measurements = np.array(measurements)\n",
    "\n",
    "    # absolute diff\n",
    "    diff = np.nanmean(np.abs(distances - measurements), axis=1)\n",
    "    closest = np.argmin(diff)\n",
    "    prediction = volumes[closest]\n",
    "\n",
    "    # here is some machine learning\n",
    "    prediction = prediction*regression_coeff[0] + regression_coeff[1]\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model parameters for Blender and linear models\n",
    "model = pickle.load(open('/root/data/alok/biomass_estimation/models/model.pkl', 'rb'))\n",
    "blender = json.load(open('/root/data/alok/biomass_estimation/models/volumes.json'))\n",
    "\n",
    "\n",
    "# establish new columns\n",
    "mask = (original_df.is_skipped == False) & (~original_df.keypoints.isnull())\n",
    "for col in ['left_keypoints', 'right_keypoints', 'world_keypoint_coordinates']:\n",
    "    original_df[col] = np.nan\n",
    "    original_df[col] = original_df[col].astype(object)\n",
    "for col in ['predicted_biomass_linear', 'predicted_biomass_blender', 'max_y_coordinate_deviation']:\n",
    "    original_df[col] = np.nan\n",
    "\n",
    "\n",
    "# modify the dataframe row-by-row\n",
    "for idx, row in original_df[mask].iterrows():\n",
    "    keypoints = row.keypoints\n",
    "    left_keypoints = keypoints['leftCrop']\n",
    "    right_keypoints = keypoints['rightCrop']\n",
    "            \n",
    "    # compute world coordinates\n",
    "    camera_metadata = row.camera_metadata\n",
    "    camera_metadata['pixelCountHeight'] = 3000\n",
    "    camera_metadata['pixelCountWidth'] = 4096\n",
    "    world_keypoint_coordinates = pixel2world(left_keypoints, right_keypoints, camera_metadata)\n",
    "    \n",
    "    # update dataframe with world keypoint coordinates\n",
    "    original_df.at[idx, 'left_keypoints'] = left_keypoints\n",
    "    original_df.at[idx, 'right_keypoints'] = right_keypoints\n",
    "    original_df.at[idx, 'world_keypoint_coordinates'] = world_keypoint_coordinates\n",
    "    \n",
    "    body_parts = sorted(list(world_keypoint_coordinates.keys()))\n",
    "    for i in range(len(body_parts)-1):\n",
    "        for j in range(i+1, len(body_parts)):\n",
    "            bp1, bp2 = body_parts[i], body_parts[j]\n",
    "            col = '{}<->{}'.format(body_parts[i], body_parts[j])\n",
    "            if not col in original_df.columns:\n",
    "                original_df[col] = np.nan\n",
    "            original_df.at[idx, col] = \\\n",
    "                euclidean_distance(world_keypoint_coordinates[bp1], world_keypoint_coordinates[bp2])\n",
    "    \n",
    "    # update dataframe with biomass predictions from both models\n",
    "    predicted_biomass_linear = coord2biomass_linear(world_keypoint_coordinates, model)\n",
    "    predicted_biomass_blender = coord2biomass_blender(world_keypoint_coordinates, blender)\n",
    "    original_df.at[idx, 'predicted_biomass_linear'] = predicted_biomass_linear\n",
    "    original_df.at[idx, 'predicted_biomass_blender'] = predicted_biomass_blender\n",
    "    \n",
    "    # update dataframe with keypoint deviation\n",
    "    threshold = 10\n",
    "    left_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in left_keypoints}\n",
    "    right_keypoint_y_coords = {bp['keypointType']: bp['yFrame'] for bp in right_keypoints}\n",
    "    max_y_coordinate_deviation = \\\n",
    "        max([abs(left_keypoint_y_coords[bp] - right_keypoint_y_coords[bp]) for bp in body_parts])\n",
    "    \n",
    "    original_df.at[idx, 'max_y_coordinate_deviation'] = max_y_coordinate_deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apply filters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids = json.load(open('/root/data/alok/biomass_estimation/invalid_fish_detection_ids.json'))\n",
    "df = original_df.copy(deep=True)\n",
    "\n",
    "# define filters\n",
    "valid_linear_prediction_mask = ~df.predicted_biomass_linear.isnull()\n",
    "rectification_valid_mask = (~df.fish_detection_id.isin(invalid_fish_detection_ids))\n",
    "keypoints_valid_mask = (df.max_y_coordinate_deviation < 15)\n",
    "qa_mask = df.is_qa = True\n",
    "mask = valid_linear_prediction_mask & rectification_valid_mask & keypoints_valid_mask & qa_mask\n",
    "\n",
    "df = df[mask].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pairwise Distance Distributions </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot the distributions of the pairwise distance features. This will allow us to visibly spot and potentially diagnose bad biomass estimates, and give more insights as to why the linear PCA model may be failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = sorted([item['keypointType'] for item in df.left_keypoints.iloc[0]])\n",
    "N = len(body_parts)\n",
    "fig, axes = plt.subplots(N, N, figsize=(30, 30))\n",
    "i = 1\n",
    "for i, bp1 in enumerate(body_parts):\n",
    "    for j, bp2 in enumerate(body_parts):\n",
    "        if bp1 >= bp2:\n",
    "            continue\n",
    "        axes[i, j].scatter(df['{}<->{}'.format(bp1, bp2)], df['TAIL_NOTCH<->UPPER_LIP'])\n",
    "        axes[i, j].grid()\n",
    "        axes[i, j].set_title('{}<->{}'.format(bp1, bp2))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.max_y_coordinate_deviation, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Compute biomass estimate using Linear Model + PCA + interaction features for pairwise distances </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('predicted_biomass_linear', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.predicted_biomass_linear, bins=100)\n",
    "plt.xlabel('Predicted biomass')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine the worst cases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coordinates(image_url, side, keypoints):\n",
    "    image_f = './image.jpg'\n",
    "#     bucket = 'aquabyte-frames-resized-inbound'\n",
    "    bucket = 'aquabyte-crops'\n",
    "    key = image_url[image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(bucket, key, image_f)\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    im = plt.imread(image_f)\n",
    "    \n",
    "    for keypoint in keypoints:\n",
    "        keypoint_type = keypoint['keypointType']\n",
    "        x, y = keypoint['xCrop'], keypoint['yCrop']\n",
    "        plt.scatter([x], [y])\n",
    "        plt.annotate(keypoint_type, (x, y), color='red')\n",
    "        \n",
    "    plt.imshow(im)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_annotation_id = 97971\n",
    "keypoint_annotation_mask = (df.id == keypoint_annotation_id)\n",
    "left_image_url = df[keypoint_annotation_mask].left_image_url.iloc[0]\n",
    "left_keypoints = df[keypoint_annotation_mask].left_keypoints.iloc[0]\n",
    "right_image_url = df[keypoint_annotation_mask].right_image_url.iloc[0]\n",
    "right_keypoints = df[keypoint_annotation_mask].right_keypoints.iloc[0]\n",
    "\n",
    "world_keypoint_coordinates = df[keypoint_annotation_mask].world_keypoint_coordinates.iloc[0]\n",
    "im_left = plot_coordinates(left_image_url, 'left', left_keypoints)\n",
    "im_right = plot_coordinates(right_image_url, 'right', right_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoint_y_coords = sorted([(item['keypointType'], item['yFrame']) for item in left_keypoints])\n",
    "right_keypoint_y_coords = sorted([(item['keypointType'], item['yFrame']) for item in right_keypoints])\n",
    "left_keypoint_x_coords = sorted([(item['keypointType'], item['xFrame']) for item in left_keypoints])\n",
    "right_keypoint_x_coords = sorted([(item['keypointType'], item['xFrame']) for item in right_keypoints])\n",
    "left_keypoint_y_coords, right_keypoint_y_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_keypoint_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params_file):\n",
    "    params = json.load(open(params_file))\n",
    "    cameraMatrix1 = np.array(params['CameraParameters1']['IntrinsicMatrix']).transpose()\n",
    "    cameraMatrix2 = np.array(params['CameraParameters2']['IntrinsicMatrix']).transpose()\n",
    "\n",
    "    distCoeffs1 = params['CameraParameters1']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters1']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters1']['RadialDistortion'][2]]\n",
    "    distCoeffs1 = np.array(distCoeffs1)\n",
    "\n",
    "    distCoeffs2 = params['CameraParameters2']['RadialDistortion'][0:2] + \\\n",
    "                   params['CameraParameters2']['TangentialDistortion'] + \\\n",
    "                   [params['CameraParameters2']['RadialDistortion'][2]]\n",
    "    distCoeffs2 = np.array(distCoeffs2)\n",
    "\n",
    "    R = np.array(params['RotationOfCamera2']).transpose()\n",
    "    T = np.array(params['TranslationOfCamera2']).transpose()\n",
    "\n",
    "    imageSize = (4096, 3000)\n",
    "    \n",
    "    # perform rectification\n",
    "    (R1, R2, P1, P2, Q, leftROI, rightROI) = cv2.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T, None, None, None, None, None, cv2.CALIB_ZERO_DISPARITY, 0)\n",
    "\n",
    "    left_maps = cv2.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, R1, P1, imageSize, cv2.CV_16SC2)\n",
    "    right_maps = cv2.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, R2, P2, imageSize, cv2.CV_16SC2)\n",
    "    \n",
    "    return left_maps, right_maps\n",
    "\n",
    "def get_remap(crop, side, crop_metadata, stereo_params_f):\n",
    "    left_maps, right_maps = load_params(stereo_params_f)\n",
    "    maps = left_maps if side == 'left' else right_maps\n",
    "    new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "    lower_left = (crop_metadata['y_coord'] + crop_metadata['height'], crop_metadata['x_coord'])\n",
    "    upper_right = (crop_metadata['y_coord'], crop_metadata['x_coord'] + crop_metadata['width'])\n",
    "    new_image[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :] = np.array(crop)\n",
    "    remap = cv2.remap(new_image, maps[0], maps[1], cv2.INTER_LANCZOS4)\n",
    "    nonzero_indices = np.where(remap > 0)\n",
    "    y_min, y_max = nonzero_indices[0].min(), nonzero_indices[0].max() \n",
    "    x_min, x_max = nonzero_indices[1].min(), nonzero_indices[1].max()\n",
    "    lower_left = (y_max, x_min)\n",
    "    upper_right = (y_min, x_max)\n",
    "    rectified_crop = remap[upper_right[0]:lower_left[0], lower_left[1]:upper_right[1], :].copy()\n",
    "    print(crop_metadata)\n",
    "    rectified_crop_metadata = crop_metadata.copy()\n",
    "    rectified_crop_metadata['x_coord'] = x_min\n",
    "    rectified_crop_metadata['y_coord'] = y_min\n",
    "    rectified_crop_metadata['width'] = x_max - x_min\n",
    "    rectified_crop_metadata['height'] = y_max - y_min\n",
    "    \n",
    "    return remap, rectified_crop_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_params_f = './2019-04-26_blom_kjeppevikholmen_pen_1.json'\n",
    "left_crop_metadata = df[fish_id_mask].left_crop_metadata.iloc[0]\n",
    "left_new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "left_remap, rectified_left_crop_metadata = get_remap(im_left, 'left', left_crop_metadata, stereo_params_f)\n",
    "\n",
    "right_crop_metadata = df[fish_id_mask].right_crop_metadata.iloc[0]\n",
    "right_new_image = np.zeros([3000, 4096, 3]).astype('uint8')\n",
    "right_remap, rectified_right_crop_metadata = get_remap(im_right, 'right', right_crop_metadata, stereo_params_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(kp['keypointType'], kp['yCrop'] + rectified_left_crop_metadata['y_coord']) for kp in left_keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(kp['keypointType'], kp['yCrop'] + rectified_right_crop_metadata['y_coord']) for kp in right_keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(right_remap).save('./right_remap.jpg')\n",
    "Image.fromarray(left_remap).save('./left_remap.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2019-05-02']['predicted_biomass_blender'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.site_id == 23) & (df.pen_id == 4) & (df.index >= '2019-04-27')\n",
    "df[mask].predicted_biomass_blender.resample('D', how=lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[mask].predicted_biomass_blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.predicted_biomass_linear > 500) & (df.predicted_biomass_linear < 6000)\n",
    "plt.scatter(df.ix[mask, 'predicted_biomass_blender'], df.ix[mask, 'predicted_biomass_linear'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Examine rectification issue </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_bucket = 'aquabyte-crops'\n",
    "left_image_rectified_f = './left_image_rectified.jpg'\n",
    "right_image_rectified_f = './right_image_rectified.jpg'\n",
    "\n",
    "invalid_fish_detection_ids, invalid_urls = [], []\n",
    "i = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    if i < 36132:\n",
    "        continue\n",
    "    left_image_url = row.left_image_url\n",
    "    right_image_url = row.right_image_url\n",
    "    left_rectified_key = left_image_url[left_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, left_rectified_key, left_image_rectified_f)\n",
    "    right_rectified_key = right_image_url[right_image_url.index('aquabyte-crops') + len('aquabyte-crops') + 1:]\n",
    "    s3_client.download_file(rectified_bucket, right_rectified_key, right_image_rectified_f)\n",
    "    \n",
    "    # this is dumb, can probably do this in memory\n",
    "    left_rectified_image = cv2.imread(left_image_rectified_f)\n",
    "    right_rectified_image = cv2.imread(right_image_rectified_f)\n",
    "    \n",
    "    left_crop_metadata = json.loads(row.left_crop_metadata)\n",
    "    right_crop_metadata = json.loads(row.right_crop_metadata)\n",
    "    left_crop_width = left_crop_metadata['width']\n",
    "    left_crop_height = left_crop_metadata['height']\n",
    "    right_crop_width = right_crop_metadata['width']\n",
    "    right_crop_height = right_crop_metadata['height']\n",
    "    \n",
    "    invalid = False\n",
    "    if left_rectified_image.shape[0] == left_crop_height and left_rectified_image.shape[1] == left_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(left_image_url)\n",
    "        print('left image not rectified for id {}!'.format(row.id))\n",
    "    if right_rectified_image.shape[0] == right_crop_height and right_rectified_image.shape[1] == right_crop_width:\n",
    "        invalid = True\n",
    "        invalid_urls.append(right_image_url)\n",
    "        print('right image not rectified for id {}!'.format(row.id))\n",
    "    \n",
    "    if invalid:\n",
    "        invalid_fish_detection_ids.append(int(row.id))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(invalid_ids, open('./invalid_ids', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_urls + invalid_urls_old, open('./invalid_urls.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_urls_old = json.load(open('./invalid_urls.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_fish_detection_ids_old = json.load(open('./invalid_fish_detection_ids.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(invalid_fish_detection_ids + invalid_fish_detection_ids_old, open('./invalid_fish_detection_ids.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prod data backfill </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "select \n",
    "\n",
    "k.id, k.fish_detection_id, k.annotated_by_email, k.is_qa, \n",
    "k.is_skipped, k.is_blurry, k.is_dark, k.is_occluded,\n",
    "k.is_bad_orientation, k.is_partial, k.direction, k.keypoints, \n",
    "k.work_duration_left_ms, k.work_duration_right_ms, f.created_at, \n",
    "f.updated_at, f.captured_at, f.site_id, f.pen_id, f.left_image_url, f.right_image_url, \n",
    "f.left_crop_metadata, f.right_crop_metadata, f.camera_metadata\n",
    "\n",
    "from keypoint_annotations k\n",
    "left join fish_detections f\n",
    "on k.fish_detection_id = f.id\n",
    "'''\n",
    "\n",
    "df = extract_from_database(sql_query, sql_credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row.keypoints:\n",
    "        keypoints = row.keypoints\n",
    "        left_crop_metadata = row.left_crop_metadata\n",
    "        right_crop_metadata = row.right_crop_metadata\n",
    "        \n",
    "        for \n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
