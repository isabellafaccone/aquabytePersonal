{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from aquabyte.visualize import Visualizer, _normalize_world_keypoints\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import copy\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from research.fish_metadata a left join keypoint_annotations b\n",
    "    on a.left_url = b.left_image_url \n",
    "    where b.keypoints is not null\n",
    "    and (b.is_qa = false or b.captured_at > '2019-09-19');\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_keypoint_annotation_ids = [\n",
    "    606484, \n",
    "    635806, \n",
    "    637801, \n",
    "    508773, \n",
    "    640493, \n",
    "    639409, \n",
    "    648536, \n",
    "    507003,\n",
    "    706002,\n",
    "    507000,\n",
    "    709298,\n",
    "    714073,\n",
    "    719239\n",
    "]\n",
    "\n",
    "df = df[~df.id.isin(blacklisted_keypoint_annotation_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Apend world keypoints to this data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_rotation_matrix(u_base, v):\n",
    "    u = v / np.linalg.norm(v)\n",
    "    n = np.cross(u_base, u)\n",
    "    n = n / np.linalg.norm(n)\n",
    "    theta = -np.arccos(np.dot(u, u_base))\n",
    "\n",
    "    R = np.array([[\n",
    "        np.cos(theta) + n[0]**2*(1-np.cos(theta)), \n",
    "        n[0]*n[1]*(1-np.cos(theta)) - n[2]*np.sin(theta),\n",
    "        n[0]*n[2]*(1-np.cos(theta)) + n[1]*np.sin(theta)\n",
    "    ], [\n",
    "        n[1]*n[0]*(1-np.cos(theta)) + n[2]*np.sin(theta),\n",
    "        np.cos(theta) + n[1]**2*(1-np.cos(theta)),\n",
    "        n[1]*n[2]*(1-np.cos(theta)) - n[0]*np.sin(theta),\n",
    "    ], [\n",
    "        n[2]*n[0]*(1-np.cos(theta)) - n[1]*np.sin(theta),\n",
    "        n[2]*n[1]*(1-np.cos(theta)) + n[0]*np.sin(theta),\n",
    "        np.cos(theta) + n[2]**2*(1-np.cos(theta))\n",
    "    ]])\n",
    "    \n",
    "    return R\n",
    "\n",
    "def _normalize_world_keypoints(wkps, rotate=True):\n",
    "    body_parts = wkps.keys()\n",
    "    \n",
    "    # translate keypoints such that tail notch is at origin\n",
    "    if wkps['UPPER_LIP'][0] > wkps['HYPURAL_PLATE'][0]:\n",
    "        front_bp, back_bp = 'UPPER_LIP', 'HYPURAL_PLATE'\n",
    "    else:\n",
    "        front_bp, back_bp = 'HYPURAL_PLATE', 'UPPER_LIP'\n",
    "        \n",
    "    translated_wkps = {bp: wkps[bp] - wkps[back_bp] for bp in body_parts}\n",
    "\n",
    "    if not rotate:\n",
    "        return translated_wkps\n",
    "    \n",
    "    # perform first rotation\n",
    "    u_base=np.array([1, 0, 0])\n",
    "    v = translated_wkps[front_bp]\n",
    "    R = _generate_rotation_matrix(u_base, v)\n",
    "    norm_wkps_intermediate = {bp: np.dot(R, translated_wkps[bp]) for bp in body_parts}\n",
    "    \n",
    "    return norm_wkps_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_fish_identifiers = list(df.fish_id.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "date_mask = (df.captured_at < '2019-09-10')\n",
    "train_mask = date_mask & df.fish_id.isin(fish_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_behaved(wkps, cutoff_depth=10.0):\n",
    "    if any([abs(wkp[1]) > cutoff_depth for wkp in wkps.values()]):\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def flip_center_kps(left_kps, right_kps):\n",
    "    \n",
    "    x_min_l = min([kp[0] for kp in left_kps.values()])\n",
    "    x_max_l = max([kp[0] for kp in left_kps.values()])\n",
    "    x_mid_l = np.mean([x_min_l, x_max_l])\n",
    "    \n",
    "    y_min_l = min([kp[1] for kp in left_kps.values()])\n",
    "    y_max_l = max([kp[1] for kp in left_kps.values()])\n",
    "    y_mid_l = np.mean([y_min_l, y_max_l])\n",
    "    \n",
    "    x_min_r = min([kp[0] for kp in right_kps.values()])\n",
    "    x_max_r = max([kp[0] for kp in right_kps.values()])\n",
    "    x_mid_r = np.mean([x_min_r, x_max_r])\n",
    "    \n",
    "    y_min_r = min([kp[1] for kp in right_kps.values()])\n",
    "    y_max_r = max([kp[1] for kp in right_kps.values()])\n",
    "    y_mid_r = np.mean([y_min_r, y_max_r])\n",
    "        \n",
    "    fc_left_kps, fc_right_kps = {}, {}\n",
    "    flip_factor = 1 if left_kps['UPPER_LIP'][0] > left_kps['TAIL_NOTCH'][0] else -1\n",
    "    for bp in BODY_PARTS:\n",
    "        left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "        if flip_factor > 0:\n",
    "            fc_left_kp = np.array([left_kp[0] - x_mid_l, left_kp[1] - y_mid_l])\n",
    "            fc_right_kp = np.array([right_kp[0] - x_mid_l, right_kp[1] - y_mid_l])\n",
    "        else:\n",
    "            fc_right_kp = np.array([x_mid_r - left_kp[0], left_kp[1] - y_mid_r])\n",
    "            fc_left_kp = np.array([x_mid_r - right_kp[0], right_kp[1] - y_mid_r])\n",
    "        fc_left_kps[bp] = fc_left_kp\n",
    "        fc_right_kps[bp] = fc_right_kp\n",
    "        \n",
    "    return fc_left_kps, fc_right_kps\n",
    "\n",
    "\n",
    "def _rotate_cc(p, theta):\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "    \n",
    "    rotated_kp = np.dot(R, p)\n",
    "    return rotated_kp\n",
    "\n",
    "\n",
    "def rotate_kps(left_kps, right_kps):\n",
    "    upper_lip_x, upper_lip_y = left_kps['UPPER_LIP']\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    r_left_kps, r_right_kps = {}, {}\n",
    "    for bp in BODY_PARTS:\n",
    "        rotated_kp = _rotate_cc(left_kps[bp], -theta)\n",
    "        r_left_kps[bp] = rotated_kp\n",
    "        disp = abs(left_kps[bp][0] - right_kps[bp][0])\n",
    "        r_right_kps[bp] = np.array([rotated_kp[0] - disp, rotated_kp[1]])\n",
    "        \n",
    "    return r_left_kps, r_right_kps\n",
    "\n",
    "\n",
    "def translate_kps(left_kps, right_kps, factor):\n",
    "    t_left_kps, t_right_kps = {}, {}\n",
    "    for bp in BODY_PARTS:\n",
    "        left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "        t_left_kps[bp] = factor * np.array(left_kps[bp])\n",
    "        t_right_kps[bp] = factor * np.array(right_kps[bp])\n",
    "    \n",
    "    return t_left_kps, t_right_kps\n",
    "\n",
    "\n",
    "def jitter_kps(left_kps, right_kps, jitter):\n",
    "    j_left_kps, j_right_kps = {}, {}\n",
    "    for bp in BODY_PARTS:\n",
    "        j_left_kps[bp] = np.array([left_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                   left_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "        j_right_kps[bp] = np.array([right_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                    right_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "    \n",
    "    return j_left_kps, j_right_kps\n",
    "\n",
    "\n",
    "\n",
    "def modify_kps(left_kps, right_kps, factor, jitter, cm):\n",
    "    fc_left_kps, fc_right_kps = flip_center_kps(left_kps, right_kps)\n",
    "    r_left_kps, r_right_kps = rotate_kps(fc_left_kps, fc_right_kps)\n",
    "    t_left_kps, t_right_kps = translate_kps(r_left_kps, r_right_kps, factor)\n",
    "    j_left_kps, j_right_kps  = jitter_kps(t_left_kps, t_right_kps, jitter)\n",
    "    j_left_kps_list, j_right_kps_list = [], []\n",
    "    for bp in BODY_PARTS:\n",
    "        l_item = {\n",
    "            'keypointType': bp,\n",
    "            'xFrame': j_left_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "            'yFrame': j_left_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "        }\n",
    "        \n",
    "        r_item = {\n",
    "            'keypointType': bp,\n",
    "            'xFrame': j_right_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "            'yFrame': j_right_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "        }\n",
    "        \n",
    "        j_left_kps_list.append(l_item)\n",
    "        j_right_kps_list.append(r_item)\n",
    "        \n",
    "    modified_kps = {\n",
    "        'leftCrop': j_left_kps_list,\n",
    "        'rightCrop': j_right_kps_list\n",
    "    }\n",
    "    \n",
    "    return modified_kps\n",
    "\n",
    "\n",
    "def process_row(row, n_factors=1, jitters=[0], low=0.3, high=2, oos=False, network=None):\n",
    "    X_row, labels_row, est_weights = [], [], []\n",
    "    keypoints = row.keypoints\n",
    "    left_keypoints_list = keypoints.get('leftCrop')\n",
    "    right_keypoints_list = keypoints.get('rightCrop')\n",
    "    cm = row.camera_metadata\n",
    "    \n",
    "    if left_keypoints_list and right_keypoints_list:\n",
    "        wkps = pixel2world(left_keypoints_list, right_keypoints_list, cm)\n",
    "        left_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints_list}\n",
    "        right_kps = {item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints_list}\n",
    "        if well_behaved(wkps):\n",
    "            for n in range(n_factors):\n",
    "                factor = 1.0 if n_factors == 1 else np.random.uniform(low=low, high=high)\n",
    "                for jitter in jitters:\n",
    "                    trials = 3 if jitter > 0 else 1\n",
    "                    for t in range(trials):\n",
    "                        modified_kps = modify_kps(left_kps, right_kps, factor, jitter, cm)\n",
    "                        modified_wkps = pixel2world(modified_kps['leftCrop'], modified_kps['rightCrop'], cm)\n",
    "                        data_point = []\n",
    "                        for bp in BODY_PARTS:\n",
    "                            wkp = modified_wkps[bp]\n",
    "                            data_point.append([wkp[0] / wkp[1], wkp[2] / wkp[1], 0.1 / wkp[1]])\n",
    "                        X_row.append(data_point)\n",
    "                        if not oos:\n",
    "                            labels_row.append(row.weight)\n",
    "                        if network:\n",
    "                            u = torch.from_numpy(np.array(data_point) / 2.0).float()\n",
    "                            est_weights.append(network(u.view(1, *u.shape)).item())\n",
    "\n",
    "    return X_row, labels_row, est_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in tdf[tdf.index == tdf.index[0]].iterrows():\n",
    "    X_row, labels_row, est_weights = process_row(row, n_factors=1, jitters=[0], network=network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in tdf[tdf.index == tdf.index[68]].iterrows():\n",
    "    X_row, labels_row, est_weights = process_row(row, n_factors=1, jitters=[0], network=network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.scatter(np.array(X_row[0])[:, 0], np.array(X_row[0])[:, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.scatter(np.array(X[74620])[:, 0], np.array(X[74620])[:, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[74620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.scatter(np.array(X_row[0])[:, 0], np.array(X_row[0])[:, 2])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X, labels = [], []\n",
    "\n",
    "row_count = 0\n",
    "for idx, row in df[train_mask].iterrows():\n",
    "    \n",
    "    X_row, labels_row, _ = process_row(row, n_factors=5, jitters=[0, 5, 10])\n",
    "    X.extend(X_row)\n",
    "    labels.extend(labels_row)\n",
    "    \n",
    "    if row_count % 1000 == 0:\n",
    "        print('Percentage complete: {}'.format(row_count / df[train_mask].shape[0]))\n",
    "    row_count += 1\n",
    "    \n",
    "X, labels = np.array(X) / 2.0, np.array(labels) / 10000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    \"\"\"Keypoints dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, labels, transform=None):\n",
    "        self.X = X\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(np.array([y])).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = KeypointsDataset(X, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=True, num_workers=1)\n",
    "\n",
    "val_dataset = KeypointsDataset(X_t, labels_t)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=25, shuffl=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "epochs = 2000\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = data_batch\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(running_loss / i)\n",
    "    else:\n",
    "        # print validation loss\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data_batch in enumerate(val_dataloader):\n",
    "                X_batch, y_batch = data_batch\n",
    "                y_pred = network(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss_for_epoch = val_loss / len(val_dataloader)\n",
    "    loss_for_epoch = running_loss / len(dataloader)\n",
    "    \n",
    "    print('-'*20)\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Train Loss: {}'.format(loss_for_epoch))\n",
    "    print('Validation Loss: {}'.format(val_loss_for_epoch))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     preds = network(torch.from_numpy(X_t).float())\n",
    "#     predictions = preds.detach().numpy().squeeze()\n",
    "#     weights = labels_t\n",
    "#     accuracy_1 = np.mean(abs((predictions - weights) / weights))\n",
    "#     accuracy_2 = (np.mean(predictions) - np.mean(weights)) / np.mean(weights)\n",
    "    \n",
    "#     print('Loss for epoch {}: {}'.format(epoch, loss_for_epoch))\n",
    "#     print('Validation accuracy (per-fish): {}'.format(accuracy_1))\n",
    "#     print('Validation accuracy (per-fish): {}'.format(accuracy_2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction: {}'.format(predictions[idx] * 1e4))\n",
    "print('Ground Truth: {}'.format(labels_t[idx] * 1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.scatter(X[idx][:, 0], X[idx][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.from_numpy(X[idx]).float()\n",
    "pred = network(u.view(1, *u.shape)).item() * 1e4\n",
    "print('Prediction: {}'.format(pred))\n",
    "print('Ground Truth: {}'.format(labels[idx] * 1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.where(0.05 / X[:, :, 2].mean(axis=1) > 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(0.05 / X_t[:, :, 2].mean(axis=1) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1833]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpids, X_t, labels_t, est_weights = [], [], [], []\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "# for idx, row in df[~train_mask].iterrows():\n",
    "for idx, row in df[~train_mask].iterrows():\n",
    "#     X_row, labels_row, est_weight = process_row(row, n_factors=1, jitters=[0, 5, 10], network=network)\n",
    "    X_row, labels_row, est_weight = process_row(row, n_factors=1, jitters=[0])\n",
    "    X_t.extend(X_row)\n",
    "    labels_t.extend(labels_row)\n",
    "    kpids.append(row.id)\n",
    "    if len(est_weight) > 0:\n",
    "        est_weights.extend(est_weight)\n",
    "    else:\n",
    "        est_weights.append(None)\n",
    "    \n",
    "    \n",
    "X_t, labels_t = np.array(X_t) / 2.0, np.array(labels_t) / 10000.0\n",
    "# tdf = df[~train_mask].copy(deep=True)\n",
    "# tdf['prediction'] = est_weights\n",
    "# tdf['prediction'] = tdf['prediction'] * 1e4\n",
    "# tdf['depth'] = tdf.world_keypoints.apply(lambda x: np.mean([wkp[1] for wkp in x.values()]) if x else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['error'] = tdf.prediction - tdf.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "amg = AccuracyMetricsGenerator()\n",
    "test_mask = ~train_mask\n",
    "amg.set_data(train_mask, tdf.prediction.values, tdf.weight.values, test_mask=test_mask)\n",
    "amg.plot_predictions_vs_ground_truth(impose_bounds=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(torch.from_numpy(X).float())\n",
    "predictions = preds.detach().numpy().squeeze()\n",
    "weights = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_t = network(torch.from_numpy(X_t).float())\n",
    "predictions_t = preds_t.detach().numpy().squeeze()\n",
    "weights_t = labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(weights * 1e4, predictions * 1e4, color='blue')\n",
    "plt.scatter(weights_t * 1e4, predictions_t * 1e4, color='red')\n",
    "plt.plot([0, 1e4], [0, 1e4])\n",
    "plt.xlabel('Ground Truth Weight (grams)')\n",
    "plt.ylabel('Predicted Weight (grams)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(np.abs((predictions - weights) / weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sc = ax.scatter(tdf.weight, tdf.prediction, c=tdf.depth)\n",
    "plt.colorbar(sc)\n",
    "plt.plot([0, 1e4], [0, 1e4], color='red')\n",
    "plt.xlim([0, 1e4])\n",
    "plt.ylim([0, 1e4])\n",
    "plt.title('Prediction vs. Ground Truth: fish far away from camera, Neural Network')\n",
    "plt.xlabel('Ground truth weight (grams)')\n",
    "plt.ylabel('Prediction (grams)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/data/alok/imr_austevoll_yolo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['captured_at_local'] = pd.to_datetime(df.captured_at).dt.tz_convert('Europe/Oslo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/imr_austevoll_yolo_localized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [round(x, 1) for x in list(np.arange(0.6, 1.2, 0.1))]\n",
    "error_pcts = []\n",
    "for idx in range(len(depths)-1):\n",
    "    mask = (tdf.depth > depths[idx]) & (tdf.depth < depths[idx+1])\n",
    "    mean_prediction = tdf[mask].prediction.mean()\n",
    "    mean_ground_truth = tdf[mask].weight.mean()\n",
    "    error_pct = (mean_prediction - mean_ground_truth) / mean_ground_truth\n",
    "    error_pcts.append(error_pct)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ind = list(range(len(depths[:-1])))\n",
    "plt.bar(ind, np.array(error_pcts) * 1e2)\n",
    "plt.xticks(ind, depths[:-1])\n",
    "plt.xlabel('Mean Distance (meters)')\n",
    "plt.ylabel('Mean error (%)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs((tdf.prediction - tdf.weight)/tdf.weight).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tdf.prediction.mean() - tdf.weight.mean()) / tdf.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.error < -500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X.reshape(X.shape[0], -1)[0], X_t.reshape(X_t.shape[0], -1)[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X.reshape(X.shape[0], -1), X.reshape(X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = []\n",
    "for i in range(X.shape[0]):\n",
    "    p = np.corrcoef(X.reshape(X.shape[0], -1)[i], X_t.reshape(X_t.shape[0], -1)[68])[0, 1]\n",
    "    ps.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(ps) > 0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(abs((predictions - weights)/weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions.mean() - weights.mean())/weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/results/model_lateral_only/results_557ec1732d8bc8bc66951d2ea4e69b935d69b111_model_lateral_only_research-exp-id-03-vikingfjord-20190709-20190710.h5'\n",
    "tdf = pd.read_hdf(f, 'table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_o, est_weights = [], []\n",
    "body_parts = sorted([\n",
    "    'TAIL_NOTCH',\n",
    "    'ADIPOSE_FIN',\n",
    "    'ANAL_FIN',\n",
    "    'PECTORAL_FIN',\n",
    "    'PELVIC_FIN',\n",
    "    'DORSAL_FIN',\n",
    "    'UPPER_LIP',\n",
    "    'EYE',\n",
    "    'UPPER_PRECAUDAL_PIT', \n",
    "    'LOWER_PRECAUDAL_PIT',\n",
    "    'HYPURAL_PLATE'\n",
    "])\n",
    "\n",
    "for idx, row in tdf.iterrows():\n",
    "    X_row, _, est_weight = process_row(row, n_factors=1, jitters=[0], oos=True, network=network)\n",
    "    X_o.extend(X_row)\n",
    "    if len(est_weight) > 0:\n",
    "        est_weights.extend(est_weight)\n",
    "    else:\n",
    "        est_weights.append(None)\n",
    "    \n",
    "X_o = np.array(X_o) / 2.0\n",
    "        \n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['est_weight'] = est_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.est_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(torch.from_numpy(X_o).float())\n",
    "predictions = preds.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(predictions * 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predictions * 1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "credentials = json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS']))\n",
    "rds_access_utils = RDSAccessUtils(credentials)\n",
    "v = Visualizer(s3_access_utils, rds_access_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpids.index(726962)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "v.load_data(726962)\n",
    "v.display_crops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "credentials = json.load(open(os.environ['PROD_SQL_CREDENTIALS']))\n",
    "rds_access_utils = RDSAccessUtils(credentials)\n",
    "v = Visualizer(s3_access_utils, rds_access_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "v.load_data(725633)\n",
    "v.display_crops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "v.display_3d_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fov_cutoffs(fov, cm):\n",
    "    fov = fov * np.pi / 180.0\n",
    "    field_size_px = 2*cm['focalLengthPixel'] * np.tan(fov / 2.0)\n",
    "    min_cutoff = (cm['pixelCountWidth'] - field_size_px) / 2.0\n",
    "    max_cutoff = (cm['pixelCountWidth'] + field_size_px) / 2.0\n",
    "    return min_cutoff, max_cutoff\n",
    "\n",
    "def is_preserved(keypoints, min_cutoff, max_cutoff):\n",
    "    min_x_left = min([item['xFrame'] for item in keypoints['leftCrop']])\n",
    "    max_x_left = max([item['xFrame'] for item in keypoints['leftCrop']])\n",
    "    min_x_right = min([item['xFrame'] for item in keypoints['rightCrop']])\n",
    "    max_x_right = max([item['xFrame'] for item in keypoints['rightCrop']])\n",
    "    \n",
    "    if (min_x_left < min_cutoff) or (min_x_right < min_cutoff) or (max_x_left > max_cutoff) or (max_x_right > max_cutoff):\n",
    "        return False\n",
    "    return True\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs = list(np.arange(10, 55, 1))\n",
    "for fov in fovs:\n",
    "    min_cutoff, max_cutoff = get_fov_cutoffs(fov, tdf.camera_metadata.iloc[0])\n",
    "    is_preserved_list = []\n",
    "    for idx, row in tdf.iterrows():\n",
    "        keypoints = row.keypoints\n",
    "        if 'leftCrop' in keypoints and 'rightCrop' in keypoints:\n",
    "            is_preserved_list.append(is_preserved(keypoints, min_cutoff, max_cutoff))\n",
    "        else:\n",
    "            is_preserved_list.append(False)\n",
    "\n",
    "    tdf['is_preserved_{}'.format(fov)] = is_preserved_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weight_means, sample_sizes = [], []\n",
    "for fov in fovs:\n",
    "    mask = tdf['is_preserved_{}'.format(fov)] == True\n",
    "#     pred_weight_means.append(tdf[mask].est_weight.mean() * 1e4)\n",
    "    pred_weight_means.append(tdf[mask].estimated_biomass_g.median())\n",
    "    sample_sizes.append(tdf[mask].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Waiting pen ID #1 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, pred_weight_means, s=80)\n",
    "plt.xlabel('Field of View (degrees)')\n",
    "plt.ylabel('Estimated biomass (g)')\n",
    "plt.title('Empirlcal Optical Samling Bias')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, sample_sizes)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Waiting pen ID #2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, pred_weight_means, s=80)\n",
    "plt.xlabel('Field of View (degrees)')\n",
    "plt.ylabel('Estimated biomass (g)')\n",
    "plt.title('Empirlcal Optical Samling Bias')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, sample_sizes)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Waiting Pen ID #3 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, pred_weight_means, s=80)\n",
    "plt.xlabel('Field of View (degrees)')\n",
    "plt.ylabel('Estimated biomass (g)')\n",
    "plt.title('Empirlcal Optical Samling Bias')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, sample_sizes)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, pred_weight_means, s=80)\n",
    "plt.xlabel('Field of View (degrees)')\n",
    "plt.ylabel('Estimated biomass (g)')\n",
    "plt.title('Empirlcal Optical Samling Bias')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(fovs, sample_sizes)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.depth > 1.75].est_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.is_preserved_29 == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
