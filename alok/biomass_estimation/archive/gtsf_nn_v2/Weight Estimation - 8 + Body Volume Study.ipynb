{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.visualize import Visualizer, _normalize_world_keypoints\n",
    "from aquabyte.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "import random\n",
    "import torch\n",
    "from aquabyte.data_loader import KeypointsDataset, NormalizeCentered2D, ToTensor, BODY_PARTS\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import copy, deepcopy\n",
    "import pyarrow.parquet as pq\n",
    "from scipy.spatial import Delaunay\n",
    "from pyobb.obb import OBB\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_RESEARCH_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from research.fish_metadata a left join keypoint_annotations b\n",
    "    on a.left_url = b.left_image_url \n",
    "    where b.keypoints -> 'leftCrop' is not null\n",
    "    and b.keypoints -> 'rightCrop' is not null\n",
    "    and b.is_qa = false\n",
    "    and b.captured_at < '2019-09-20';\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_keypoint_annotation_ids = [\n",
    "    606484, \n",
    "    635806, \n",
    "    637801, \n",
    "    508773, \n",
    "    640493, \n",
    "    639409, \n",
    "    648536, \n",
    "    507003,\n",
    "    706002,\n",
    "    507000,\n",
    "    709298,\n",
    "    714073,\n",
    "    719239\n",
    "]\n",
    "\n",
    "df = df[~df.id.isin(blacklisted_keypoint_annotation_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Append World Keypoints to this Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def is_well_behaved(wkps, cutoff_depth=10.0):\n",
    "    if any([abs(wkp[1]) > cutoff_depth for wkp in wkps.values()]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")\n",
    "\n",
    "is_well_behaved_mask = df.world_keypoints.apply(lambda x: is_well_behaved(x))\n",
    "df = df[is_well_behaved_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add template matchingr results to this base dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "\n",
    "gen = s3_access_utils.get_matching_s3_keys('aquabyte-research', prefix='template-matching/2019-12-05T02:50:57', suffixes=['.parquet'])\n",
    "keys = []\n",
    "for key in gen:\n",
    "    keys.append(key)\n",
    "\n",
    "f = s3_access_utils.download_from_s3('aquabyte-research', keys[0])\n",
    "pdf = pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['homography'] = pdf.homography_and_matches.apply(lambda x: np.array(x[0].tolist(), dtype=np.float))\n",
    "pdf['matches'] = pdf.homography_and_matches.apply(lambda x: np.array(x[1].tolist(), dtype=np.int) if len(x) > 1 else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, pdf[['left_image_url', 'homography', 'matches']], how='inner', on='left_image_url')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> OBB Class </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray, array, asarray, dot, cross, cov, array, finfo, min as npmin, max as npmax\n",
    "from numpy.linalg import eigh, norm\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# adapted from: http://jamesgregson.blogspot.com/2011/03/latex-test.html\n",
    "########################################################################################################################\n",
    "class OBB:\n",
    "    def __init__(self):\n",
    "        self.rotation = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def transform(self, point):\n",
    "        return dot(array(point), self.rotation)\n",
    "\n",
    "    @property\n",
    "    def centroid(self):\n",
    "        return self.transform((self.min + self.max) / 2.0)\n",
    "\n",
    "    @property\n",
    "    def extents(self):\n",
    "        return abs(self.transform((self.max - self.min) / 2.0))\n",
    "\n",
    "    @property\n",
    "    def points(self):\n",
    "        return [\n",
    "            # upper cap: ccw order in a right-hand system\n",
    "            # rightmost, topmost, farthest\n",
    "            self.transform((self.max[0], self.max[1], self.min[2])),\n",
    "            # leftmost, topmost, farthest\n",
    "            self.transform((self.min[0], self.max[1], self.min[2])),\n",
    "            # leftmost, topmost, closest\n",
    "            self.transform((self.min[0], self.max[1], self.max[2])),\n",
    "            # rightmost, topmost, closest\n",
    "            self.transform(self.max),\n",
    "            # lower cap: cw order in a right-hand system\n",
    "            # leftmost, bottommost, farthest\n",
    "            self.transform(self.min),\n",
    "            # rightmost, bottommost, farthest\n",
    "            self.transform((self.max[0], self.min[1], self.min[2])),\n",
    "            # rightmost, bottommost, closest\n",
    "            self.transform((self.max[0], self.min[1], self.max[2])),\n",
    "            # leftmost, bottommost, closest\n",
    "            self.transform((self.min[0], self.min[1], self.max[2])),\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def build_from_covariance_matrix(cls, covariance_matrix, points):\n",
    "        if not isinstance(points, ndarray):\n",
    "            points = array(points, dtype=float)\n",
    "        assert points.shape[1] == 3\n",
    "\n",
    "        obb = OBB()\n",
    "\n",
    "        _, eigen_vectors = eigh(covariance_matrix)\n",
    "\n",
    "        def try_to_normalize(v):\n",
    "            n = norm(v)\n",
    "            if n < finfo(float).resolution:\n",
    "                raise ZeroDivisionError\n",
    "            return v / n\n",
    "\n",
    "        r = try_to_normalize(eigen_vectors[:, 0])\n",
    "        u = try_to_normalize(eigen_vectors[:, 1])\n",
    "        f = try_to_normalize(eigen_vectors[:, 2])\n",
    "\n",
    "        obb.rotation = array((r, u, f)).T\n",
    "\n",
    "        # apply the rotation to all the position vectors of the array\n",
    "        # TODO : this operation could be vectorized with tensordot\n",
    "        p_primes = asarray([obb.rotation.dot(p) for p in points])\n",
    "        obb.min = npmin(p_primes, axis=0)\n",
    "        obb.max = npmax(p_primes, axis=0)\n",
    "\n",
    "        return obb, eigen_vectors\n",
    "\n",
    "    @classmethod\n",
    "    def build_from_points(cls, points):\n",
    "        if not isinstance(points, ndarray):\n",
    "            points = array(points, dtype=float)\n",
    "        assert points.shape[1] == 3, 'points have to have 3-elements'\n",
    "        # no need to store the covariance matrix\n",
    "        return OBB.build_from_covariance_matrix(cov(points, y=None, rowvar=0, bias=1), points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add Convex Hull Filtration and Volume Computation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "idx = 30\n",
    "row = df[~(df.left_image_url.str.contains('aquabyte-crops'))].iloc[idx]\n",
    "X_body = np.array(row.matches)\n",
    "X_keypoints = np.array([[item['xFrame'], item['yFrame']] for item in row.keypoints['rightCrop']])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X_body[:, 2], X_body[:, 3], color='blue')\n",
    "plt.scatter(X_keypoints[:, 0], X_keypoints[:, 1], color='red')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_hull(p, hull):\n",
    "    hull = Delaunay(hull)\n",
    "    return hull.find_simplex(p)>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[3]\n",
    "X_keypoints = np.array([[item['xFrame'], item['yFrame']] for item in row.keypoints['leftCrop']])\n",
    "X_body = np.array(row.matches)\n",
    "is_valid = in_hull(X_body[:, :2], X_keypoints)\n",
    "X_body = X_body[np.where(is_valid)]\n",
    "\n",
    "# generate 3D point cloud\n",
    "cm = row.camera_metadata\n",
    "all_wkps = []\n",
    "for i in range(X_body.shape[0]):\n",
    "    d = depth_from_disp(abs(X_body[i, 2] - X_body[i, 0]), cm)\n",
    "    wkp = convert_to_world_point(X_body[i, 0], X_body[i, 1], d, cm)\n",
    "    all_wkps.append(list(wkp))\n",
    "    \n",
    "additional_wkps = pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], cm)\n",
    "all_wkps.extend([list(additional_wkps[bp]) for bp in additional_wkps.keys()])\n",
    "\n",
    "obb, eigen_vectors = OBB.build_from_points(all_wkps)\n",
    "obb_points = np.array(obb.points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# get x, y, and z lists\n",
    "x_values = list(obb_points[:,0])\n",
    "y_values = list(obb_points[:,1])\n",
    "z_values = list(obb_points[:,2])\n",
    "\n",
    "x_values.extend(list(np.array(all_wkps)[:,0]))\n",
    "y_values.extend(list(np.array(all_wkps)[:,1]))\n",
    "z_values.extend(list(np.array(all_wkps)[:,2]))\n",
    "\n",
    "x_values = np.array(x_values)\n",
    "y_values = np.array(y_values)\n",
    "z_values = np.array(z_values)\n",
    "\n",
    "\n",
    "ax.scatter(x_values, y_values, z_values)\n",
    "for point_pair in [(0, 1), (1, 2), (2, 3), (3, 0), \\\n",
    "                   (4, 5), (5, 6), (6, 7), (7, 4), \\\n",
    "                   (0, 5), (1, 4), (2, 7), (3, 6)]:\n",
    "    i, j = point_pair\n",
    "    edge_x_values = [obb_points[i][0], obb_points[j][0]]\n",
    "    edge_y_values = [obb_points[i][1], obb_points[j][1]]\n",
    "    edge_z_values = [obb_points[i][2], obb_points[j][2]]\n",
    "    ax.plot(edge_x_values, edge_y_values, edge_z_values, color='red')\n",
    "    \n",
    "# Create cubic bounding box to simulate equal aspect ratio\n",
    "max_range = np.array([x_values.max()-x_values.min(), y_values.max()-y_values.min(), z_values.max()-z_values.min()]).max()\n",
    "Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x_values.max()+x_values.min())\n",
    "Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(y_values.max()+y_values.min())\n",
    "Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(z_values.max()+z_values.min())\n",
    "# Comment or uncomment following both lines to test the fake bounding box:\n",
    "for xb, yb, zb in zip(Xb, Yb, Zb):\n",
    "    ax.plot([xb], [yb], [zb], 'w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Neural Network </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_world_point(x, y, d, parameters):\n",
    "    \"\"\" from pixel coordinates to world coordinates \"\"\"\n",
    "    # get relevant parameters\n",
    "    pixel_count_width = parameters[\"pixelCountWidth\"]\n",
    "    pixel_count_height = parameters[\"pixelCountHeight\"]\n",
    "    sensor_width = parameters[\"imageSensorWidth\"]\n",
    "    sensor_height = parameters[\"imageSensorHeight\"]\n",
    "    focal_length = parameters[\"focalLength\"]\n",
    "\n",
    "    image_center_x = pixel_count_width / 2.0\n",
    "    image_center_y = pixel_count_height / 2.0\n",
    "    px_x = x - image_center_x\n",
    "    px_z = image_center_y - y\n",
    "\n",
    "    sensor_x = px_x * (sensor_width / pixel_count_width)\n",
    "    sensor_z = px_z * (sensor_height / pixel_count_height)\n",
    "\n",
    "    # now move to world coordinates\n",
    "    world_y = d\n",
    "    world_x = (world_y * sensor_x) / focal_length\n",
    "    world_z = (world_y * sensor_z) / focal_length\n",
    "    return np.array([world_x, world_y, world_z])\n",
    "\n",
    "\n",
    "def depth_from_disp(disp, parameters):\n",
    "    \"\"\" calculate the depth of the point based on the disparity value \"\"\"\n",
    "    focal_length_pixel = parameters[\"focalLengthPixel\"]\n",
    "\n",
    "    baseline = parameters[\"baseline\"]\n",
    "    depth = focal_length_pixel * baseline / np.array(disp)\n",
    "    return depth\n",
    "\n",
    "\n",
    "class NormalizeCentered2D(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the 2D left and right keypoints such that:\n",
    "        (1) The center of the left image 2D keypoints is located at the center of the left image\n",
    "            (i.e. 2D translation)\n",
    "        (2) The left image keypoints are possibly flipped such that the upper-lip x-coordinate \n",
    "            is greater than the tail-notch coordinate. This is done to reduce the total number of \n",
    "            spatial orientations the network must learn from -> reduces the training size\n",
    "        (3) The left image keypoints are then rotated such that upper-lip is located on the x-axis.\n",
    "            As in (2), this is done to reduce the total number of spatial orientations the network \n",
    "            must learn from -> reduces the training size\n",
    "        (4) Rescale all left image keypoints by some random number between 'lo' and 'hi' args\n",
    "        (5) Apply Gaussian random noise \"jitter\" to each keypoint to mimic annotation error\n",
    "        (5) For all transformations above, the right image keypoint coordinates are accordingly\n",
    "            transformed such that the original disparity values are preserved for all keypoints\n",
    "            (or adjusted during rescaling event)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def flip_center_kps(self, left_kps, right_kps):\n",
    "\n",
    "        x_min_l = min([left_kps[bp][0] for bp in BODY_PARTS])\n",
    "        x_max_l = max([left_kps[bp][0] for bp in BODY_PARTS])\n",
    "        x_mid_l = np.mean([x_min_l, x_max_l])\n",
    "\n",
    "        y_min_l = min([left_kps[bp][1] for bp in BODY_PARTS])\n",
    "        y_max_l = max([left_kps[bp][1] for bp in BODY_PARTS])\n",
    "        y_mid_l = np.mean([y_min_l, y_max_l])\n",
    "\n",
    "        x_min_r = min([right_kps[bp][0] for bp in BODY_PARTS])\n",
    "        x_max_r = max([right_kps[bp][0] for bp in BODY_PARTS])\n",
    "        x_mid_r = np.mean([x_min_r, x_max_r])\n",
    "\n",
    "        y_min_r = min([right_kps[bp][1] for bp in BODY_PARTS])\n",
    "        y_max_r = max([right_kps[bp][1] for bp in BODY_PARTS])\n",
    "        y_mid_r = np.mean([y_min_r, y_max_r])\n",
    "\n",
    "        fc_left_kps, fc_right_kps = {}, {}\n",
    "        flip_factor = 1 if left_kps['UPPER_LIP'][0] > left_kps['TAIL_NOTCH'][0] else -1\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            if flip_factor > 0:\n",
    "                fc_left_kp = np.array([left_kp[0] - x_mid_l, left_kp[1] - y_mid_l])\n",
    "                fc_right_kp = np.array([right_kp[0] - x_mid_l, right_kp[1] - y_mid_l])\n",
    "            else:\n",
    "                fc_right_kp = np.array([x_mid_r - left_kp[0], left_kp[1] - y_mid_r])\n",
    "                fc_left_kp = np.array([x_mid_r - right_kp[0], right_kp[1] - y_mid_r])\n",
    "            fc_left_kps[bp] = fc_left_kp\n",
    "            fc_right_kps[bp] = fc_right_kp\n",
    "\n",
    "        if 'BODY' in left_kps.keys():\n",
    "            left_body_kps, right_body_kps = np.array(left_kps['BODY']), np.array(right_kps['BODY'])\n",
    "            if flip_factor > 0:\n",
    "                fc_left_body_kps = left_body_kps - np.array([x_mid_l, y_mid_l])\n",
    "                fc_right_body_kps = right_body_kps - np.array([x_mid_l, y_mid_l])\n",
    "            else:\n",
    "                fc_left_body_kps = np.dot(left_body_kps - np.array([x_mid_r, y_mid_r]), np.array([[-1, 0], [0, 1]]))\n",
    "                fc_right_body_kps = np.dot(right_body_kps - np.array([x_mid_r, y_mid_r]), np.array([[-1, 0], [0, 1]]))\n",
    "            fc_left_kps['BODY'] = fc_left_body_kps\n",
    "            fc_right_kps['BODY'] = fc_right_body_kps\n",
    "        \n",
    "        return fc_left_kps, fc_right_kps\n",
    "\n",
    "\n",
    "    def _rotate_cc(self, p, theta):\n",
    "        R = np.array([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "\n",
    "        rotated_kp = np.dot(R, p)\n",
    "        return rotated_kp\n",
    "\n",
    "\n",
    "    def rotate_kps(self, left_kps, right_kps):\n",
    "        upper_lip_x, upper_lip_y = left_kps['UPPER_LIP']\n",
    "        theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "        r_left_kps, r_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            rotated_kp = self._rotate_cc(left_kps[bp], -theta)\n",
    "            r_left_kps[bp] = rotated_kp\n",
    "            disp = abs(left_kps[bp][0] - right_kps[bp][0])\n",
    "            r_right_kps[bp] = np.array([rotated_kp[0] - disp, rotated_kp[1]])\n",
    "            \n",
    "        if 'BODY' in left_kps.keys():\n",
    "            left_body_kps, right_body_kps = np.array(left_kps['BODY']), np.array(right_kps['BODY'])\n",
    "            r_left_body_kps = self._rotate_cc(left_body_kps.T, -theta).T\n",
    "            disp = np.abs(left_body_kps[:, 0] - right_body_kps[:, 0])\n",
    "            r_right_body_kps = np.column_stack([r_left_body_kps[:, 0] - disp, r_left_body_kps[:, 1]])\n",
    "            r_left_kps['BODY'] = r_left_body_kps\n",
    "            r_right_kps['BODY'] = r_right_body_kps\n",
    "\n",
    "        return r_left_kps, r_right_kps\n",
    "\n",
    "\n",
    "    def scale_kps(self, left_kps, right_kps, factor):\n",
    "        s_left_kps, s_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            left_kp, right_kp = left_kps[bp], right_kps[bp]\n",
    "            s_left_kps[bp] = factor * np.array(left_kps[bp])\n",
    "            s_right_kps[bp] = factor * np.array(right_kps[bp])\n",
    "        \n",
    "        if 'BODY' in left_kps.keys():\n",
    "            left_body_kps, right_body_kps = np.array(left_kps['BODY']), np.array(right_kps['BODY'])\n",
    "            s_left_body_kps = factor * left_body_kps\n",
    "            s_right_body_kps = factor * right_body_kps\n",
    "            s_left_kps['BODY'] = s_left_body_kps\n",
    "            s_right_kps['BODY'] = s_right_body_kps\n",
    "            \n",
    "        return s_left_kps, s_right_kps\n",
    "\n",
    "\n",
    "    def jitter_kps(self, left_kps, right_kps, jitter):\n",
    "        j_left_kps, j_right_kps = {}, {}\n",
    "        for bp in BODY_PARTS:\n",
    "            j_left_kps[bp] = np.array([left_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                       left_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "            j_right_kps[bp] = np.array([right_kps[bp][0] + np.random.normal(0, jitter), \n",
    "                                        right_kps[bp][1] + np.random.normal(0, jitter)])\n",
    "            \n",
    "        if 'BODY' in left_kps.keys():\n",
    "            j_left_kps['BODY'] = left_kps['BODY']\n",
    "            j_right_kps['BODY'] = right_kps['BODY']\n",
    "\n",
    "        return j_left_kps, j_right_kps\n",
    "\n",
    "\n",
    "\n",
    "    def modify_kps(self, left_kps, right_kps, factor, jitter, cm, rotate=True, center=False):\n",
    "        fc_left_kps, fc_right_kps = self.flip_center_kps(left_kps, right_kps)\n",
    "        if rotate:\n",
    "            r_left_kps, r_right_kps = self.rotate_kps(fc_left_kps, fc_right_kps)\n",
    "            s_left_kps, s_right_kps = self.scale_kps(r_left_kps, r_right_kps, factor)\n",
    "        else:\n",
    "            s_left_kps, s_right_kps = self.scale_kps(fc_left_kps, fc_right_kps, factor)\n",
    "        j_left_kps, j_right_kps  = self.jitter_kps(s_left_kps, s_right_kps, jitter)\n",
    "        j_left_kps_list, j_right_kps_list = [], []\n",
    "        if not center:\n",
    "            for bp in BODY_PARTS:\n",
    "                l_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_left_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_left_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "\n",
    "                r_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_right_kps[bp][0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_right_kps[bp][1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "\n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "                \n",
    "            if 'BODY' in left_kps.keys():\n",
    "                l_item = {\n",
    "                    'keypointType': 'BODY',\n",
    "                    'xFrame': j_left_kps['BODY'][:, 0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_left_kps['BODY'][:, 1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "                \n",
    "                r_item = {\n",
    "                    'keypointType': 'BODY',\n",
    "                    'xFrame': j_right_kps['BODY'][:, 0] + cm['pixelCountWidth'] / 2.0,\n",
    "                    'yFrame': j_right_kps['BODY'][:, 1] + cm['pixelCountHeight'] / 2.0\n",
    "                }\n",
    "                \n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "            \n",
    "        else:\n",
    "            for bp in BODY_PARTS:\n",
    "                l_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_left_kps[bp][0],\n",
    "                    'yFrame': j_left_kps[bp][1]\n",
    "                }\n",
    "\n",
    "                r_item = {\n",
    "                    'keypointType': bp,\n",
    "                    'xFrame': j_right_kps[bp][0],\n",
    "                    'yFrame': j_right_kps[bp][1]\n",
    "                }\n",
    "\n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "                \n",
    "            if 'BODY' in left_kps.keys():\n",
    "                l_item = {\n",
    "                    'keypointType': 'BODY',\n",
    "                    'xFrame': j_left_kps['BODY'][:, 0],\n",
    "                    'yFrame': j_left_kps['BODY'][:, 1]\n",
    "                }\n",
    "                \n",
    "                r_item = {\n",
    "                    'keypointType': 'BODY',\n",
    "                    'xFrame': j_right_kps['BODY'][:, 0],\n",
    "                    'yFrame': j_right_kps['BODY'][:, 1]\n",
    "                }\n",
    "                \n",
    "                j_left_kps_list.append(l_item)\n",
    "                j_right_kps_list.append(r_item)\n",
    "\n",
    "\n",
    "        modified_kps = {\n",
    "            'leftCrop': j_left_kps_list,\n",
    "            'rightCrop': j_right_kps_list\n",
    "        }\n",
    "\n",
    "        return modified_kps\n",
    "    \n",
    "    def __init__(self, lo=None, hi=None, jitter=0.0, rotate=True, center=False):\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "        self.jitter = jitter\n",
    "        self.rotate = rotate\n",
    "        self.center = center\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        keypoints, cm, stereo_pair_id, label = \\\n",
    "            sample['keypoints'], sample['cm'], sample.get('stereo_pair_id'), sample.get('label')\n",
    "        left_keypoints_list = keypoints['leftCrop']\n",
    "        right_keypoints_list = keypoints['rightCrop']\n",
    "        left_kps = {item['keypointType']: np.column_stack([item['xFrame'], item['yFrame']]) \\\n",
    "                    for item in left_keypoints_list if item['keypointType'] == 'BODY'}\n",
    "        right_kps = {item['keypointType']: np.column_stack([item['xFrame'], item['yFrame']]) \\\n",
    "                    for item in right_keypoints_list if item['keypointType'] == 'BODY'}\n",
    "        left_kps.update({item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in left_keypoints_list if item['keypointType'] != 'BODY'})\n",
    "        right_kps.update({item['keypointType']: np.array([item['xFrame'], item['yFrame']]) for item in right_keypoints_list if item['keypointType'] != 'BODY'})\n",
    "        factor = 1.0 \n",
    "        if self.lo and self.hi:\n",
    "            factor = np.random.uniform(low=self.lo, high=self.hi)\n",
    "\n",
    "        jitter = np.random.uniform(high=self.jitter)\n",
    "\n",
    "        modified_kps = self.modify_kps(left_kps, right_kps, factor, jitter, cm, \n",
    "            rotate=self.rotate, center=self.center)\n",
    "\n",
    "        kp_input = {}\n",
    "        for idx, _ in enumerate(modified_kps['leftCrop']):\n",
    "            left_item, right_item = modified_kps['leftCrop'][idx], modified_kps['rightCrop'][idx]\n",
    "            bp = left_item['keypointType']\n",
    "            kp_input[bp] = [left_item['xFrame'], left_item['yFrame'], right_item['xFrame'], right_item['yFrame']]\n",
    "\n",
    "\n",
    "        transformed_sample = {\n",
    "            'kp_input': kp_input,\n",
    "            'modified_kps': modified_kps,\n",
    "            'label': label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'cm': cm,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "\n",
    "        return transformed_sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldKeypointTransform(object):\n",
    "    \"\"\"\n",
    "        Transforms into world keypoints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        modified_kps, label, stereo_pair_id, cm = \\\n",
    "            sample['modified_kps'], sample['label'], sample['stereo_pair_id'], sample['cm']\n",
    "        \n",
    "        modified_wkps = pixel2world([item for item in modified_kps['leftCrop'] if item['keypointType'] != 'BODY'], \n",
    "                                    [item for item in modified_kps['rightCrop'] if item['keypointType'] != 'BODY'],\n",
    "                                    cm)\n",
    "        \n",
    "        # compute BODY world keypoint coordinates\n",
    "        if 'BODY' in modified_wkps.keys():\n",
    "            disps = np.abs(modified_kps['leftCrop']['BODY'][:, 0] - modified_kps['rightCrop']['BODY'][:, 0])\n",
    "            focal_length_pixel = cm[\"focalLengthPixel\"]\n",
    "            baseline = cm[\"baseline\"]\n",
    "            depths = focal_length_pixel * baseline / np.array(disps)\n",
    "\n",
    "            pixel_count_width = parameters[\"pixelCountWidth\"]\n",
    "            pixel_count_height = parameters[\"pixelCountHeight\"]\n",
    "            sensor_width = parameters[\"imageSensorWidth\"]\n",
    "            sensor_height = parameters[\"imageSensorHeight\"]\n",
    "            focal_length = parameters[\"focalLength\"]\n",
    "\n",
    "            image_center_x = pixel_count_width / 2.0\n",
    "            image_center_y = pixel_count_height / 2.0\n",
    "            x = modified_kps['leftCrop']['BODY'][:, 0]\n",
    "            y = modified_kps['leftCrop']['BODY'][:, 1]\n",
    "            px_x = x - image_center_x\n",
    "            px_z = image_center_y - y\n",
    "\n",
    "            sensor_x = px_x * (sensor_width / pixel_count_width)\n",
    "            sensor_z = px_z * (sensor_height / pixel_count_height)\n",
    "\n",
    "            world_y = depths\n",
    "            world_x = (world_y * sensor_x) / focal_length\n",
    "            world_z = (world_y * sensor_z) / focal_length\n",
    "            modified_wkps['BODY'] = np.column_stack([world_x, world_y, world_z])\n",
    "        \n",
    "        \n",
    "        transformed_sample = {\n",
    "            'modified_wkps': modified_wkps,\n",
    "            'label': label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample\n",
    "    \n",
    "class PrismTransform(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        modified_wkps, label, stereo_pair_id = \\\n",
    "            sample['modified_wkps'], sample['label'], sample['stereo_pair_id']\n",
    "        \n",
    "        all_wkps = [list(modified_wkps[bp]) for bp in BODY_PARTS]\n",
    "        if 'BODY' in modified_wkps.keys():\n",
    "            all_wkps.extend([list(wkp) for wkp in list(modified_wkps['BODY'])])\n",
    "        obb, eigen_vectors = OBB.build_from_points(all_wkps)\n",
    "        obb_points = np.array(obb.points)\n",
    "        obb_points_dict = {'BP{}'.format(idx): p for idx, p in enumerate(obb_points)}\n",
    "        \n",
    "        normalized_label = label * 1e-4 if label else None\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'kp_input': obb_points_dict,\n",
    "            'modified_wkps': modified_wkps,\n",
    "            'label': normalized_label,\n",
    "            'stereo_pair_id': stereo_pair_id,\n",
    "            'single_point_inference': sample.get('single_point_inference')\n",
    "        }\n",
    "        \n",
    "        return transformed_sample\n",
    "        \n",
    "        \n",
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        kp_input, label, stereo_pair_id = \\\n",
    "            sample['kp_input'], sample.get('label'), sample.get('stereo_pair_id')\n",
    "        \n",
    "        x = []\n",
    "        for bp in kp_input.keys():\n",
    "            kp_data = kp_input[bp]\n",
    "            x.append(kp_data)\n",
    "        if sample.get('single_point_inference'):\n",
    "            x = np.array([x])\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        \n",
    "        kp_input_tensor = torch.from_numpy(x).float()\n",
    "        \n",
    "        tensorized_sample = {\n",
    "            'kp_input': kp_input_tensor\n",
    "        }\n",
    "\n",
    "        if label:\n",
    "            label_tensor = torch.from_numpy(np.array([label])).float() if label else None\n",
    "            tensorized_sample['label'] = label_tensor\n",
    "\n",
    "        if stereo_pair_id:\n",
    "            tensorized_sample['stereo_pair_id'] = stereo_pair_id\n",
    "\n",
    "\n",
    "        \n",
    "        return tensorized_sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                                  NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                                  WorldKeypointTransform(),\n",
    "                                                  PrismTransform(),\n",
    "                                                  ToTensor()\n",
    "                                              ]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    'keypoints': df.keypoints.iloc[0],\n",
    "    'stereo_pair_id': 0,\n",
    "    'cm': df.camera_metadata.iloc[0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_keypoints_list = []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    X_keypoints = np.array([[item['xFrame'], item['yFrame']] for item in row.keypoints['leftCrop']])\n",
    "    X_body = np.array(row.matches)\n",
    "    is_valid = in_hull(X_body[:, :2], X_keypoints)\n",
    "    X_body = X_body[np.where(is_valid)]\n",
    "    \n",
    "    keypoints = deepcopy(row.keypoints)\n",
    "    left_keypoints, right_keypoints = keypoints['leftCrop'], keypoints['rightCrop']\n",
    "    left_item = {\n",
    "        'keypointType': 'BODY',\n",
    "        'xFrame': X_body[:, 0],\n",
    "        'yFrame': X_body[:, 1]\n",
    "    }\n",
    "    \n",
    "    right_item = {\n",
    "        'keypointType': 'BODY',\n",
    "        'xFrame': X_body[:, 2],\n",
    "        'yFrame': X_body[:, 3]\n",
    "    }\n",
    "    \n",
    "    left_keypoints.append(left_item)\n",
    "    right_keypoints.append(right_item)\n",
    "    modified_keypoints = {\n",
    "        'leftCrop': left_keypoints,\n",
    "        'rightCrop': right_keypoints\n",
    "    }\n",
    "\n",
    "    modified_keypoints_list.append(modified_keypoints)\n",
    "\n",
    "df['old_keypoints'] = df.keypoints\n",
    "df['keypoints'] = modified_keypoints_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf_fish_identifiers = list(df.fish_id.unique())\n",
    "train_size = int(0.8 * len(gtsf_fish_identifiers))\n",
    "fish_ids = random.sample(gtsf_fish_identifiers, train_size)\n",
    "date_mask = (df.captured_at < '2019-09-10')\n",
    "train_mask = date_mask & df.fish_id.isin(fish_ids)\n",
    "test_mask = date_mask & ~df.fish_id.isin(fish_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KeypointsDataset(df[train_mask], transform=transforms.Compose([\n",
    "                                                  NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                                  WorldKeypointTransform(),\n",
    "                                                  PrismTransform(),\n",
    "                                                  ToTensor()\n",
    "                                              ]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=25, shuffle=True, num_workers=1)\n",
    "\n",
    "test_dataset = KeypointsDataset(df[test_mask], transform=transforms.Compose([\n",
    "                                                  NormalizeCentered2D(lo=0.3, hi=2.0, jitter=10),\n",
    "                                                  WorldKeypointTransform(),\n",
    "                                                  PrismTransform(),\n",
    "                                                  ToTensor()\n",
    "                                              ]))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=25, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_outputs = False\n",
    "\n",
    "# establish output directory where model .pb files will be written\n",
    "if write_outputs:\n",
    "    dt_now = dt.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    output_base = '/root/data/alok/biomass_estimation/results/neural_network'\n",
    "    output_dir = os.path.join(output_base, dt_now)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "# instantiate neural network\n",
    "network = Network()\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.00001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# track train and test losses\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "seed = 0\n",
    "for epoch in range(epochs):\n",
    "    network.train()\n",
    "    np.random.seed(seed)\n",
    "    seed += 1\n",
    "    running_loss = 0.0\n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch, kpid_batch = \\\n",
    "            data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "        y_pred = network(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print(running_loss / i)\n",
    "            \n",
    "#     # run on test set\n",
    "#     else:\n",
    "#         test_running_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             network.eval()\n",
    "#             for i, data_batch in enumerate(test_dataloader):\n",
    "#                 X_batch, y_batch, kpid_batch = \\\n",
    "#                     data_batch['kp_input'], data_batch['label'], data_batch['stereo_pair_id']\n",
    "#                 y_pred = network(X_batch)\n",
    "#                 loss = criterion(y_pred, y_batch)\n",
    "#                 test_running_loss += loss.item()\n",
    "\n",
    "    train_loss_for_epoch = running_loss / len(train_dataloader)\n",
    "#     test_loss_for_epoch = test_running_loss / len(test_dataloader)\n",
    "#     train_losses.append(train_loss_for_epoch)\n",
    "#     test_losses.append(test_loss_for_epoch)\n",
    "    \n",
    "#     # save current state of network\n",
    "#     if write_outputs:\n",
    "#         f_name = 'nn_epoch_{}.pb'.format(str(epoch).zfill(3))\n",
    "#         f_path = os.path.join(output_dir, f_name)\n",
    "#         torch.save(network, f_path)\n",
    "    \n",
    "#     # print current loss values\n",
    "#     print('-'*20)\n",
    "#     print('Epoch: {}'.format(epoch))\n",
    "    print('Train Loss: {}'.format(train_loss_for_epoch))\n",
    "#     print('Test Loss: {}'.format(test_loss_for_epoch))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
