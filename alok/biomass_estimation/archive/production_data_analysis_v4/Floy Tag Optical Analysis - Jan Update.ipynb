{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Kjeppevikholmen Optical Analysis -- June Growth Trend </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "# from aquabyte.visualize import Visualizer\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "# from aquabyte.biomass_estimator import NormalizeCentered2D, NormalizedStabilityTransform, ToTensor, Network\n",
    "# from aquabyte.optics import pixel2world, euclidean_distance\n",
    "# from aquabyte.akpd_scorer import generate_confidence_score\n",
    "from keras.models import load_model\n",
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "import pytz\n",
    "from PIL import Image\n",
    "import datetime as dt\n",
    "import dateutil\n",
    "from collections import defaultdict\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    select * from keypoint_annotations\n",
    "    where pen_id=64\n",
    "    and keypoints -> 'leftCrop' is not null\n",
    "    and keypoints -> 'rightCrop' is not null\n",
    "    and captured_at between '2019-12-10' and '2019-12-12'\n",
    "    and is_qa=FALSE;\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AKPD(object):\n",
    "\n",
    "    def __init__(self, aws_credentials):\n",
    "        self.client = boto3.client(\n",
    "            \"sagemaker-runtime\", \n",
    "            region_name=\"eu-west-1\", \n",
    "            aws_access_key_id=aws_credentials['aws_access_key_id'], \n",
    "            aws_secret_access_key=aws_credentials['aws_secret_access_key']\n",
    "        \n",
    "        )\n",
    "\n",
    "    def predict_keypoints(self, left_crop_url, right_crop_url, left_crop_metadata, right_crop_metadata, camera_metadata):\n",
    "        body = [{\n",
    "            'leftCropUrl': left_crop_url,\n",
    "            'rightCropUrl': right_crop_url,\n",
    "            'leftCropMetadata': left_crop_metadata,\n",
    "            'rightCropMetadata': right_crop_metadata,\n",
    "            'cameraMetadata': camera_metadata,\n",
    "            'id': 1\n",
    "        }]\n",
    "\n",
    "        body_str = json.dumps(body).replace(\"'\", '\"')\n",
    "\n",
    "        resp = self.client.invoke_endpoint(EndpointName='auto-keypoints', ContentType='application/json', Body=body_str)\n",
    "        akpd_keypoints_str = resp['Body'].read()\n",
    "        akpd_keypoints = json.loads(akpd_keypoints_str.decode(\"utf-8\"))\n",
    "        return akpd_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_centered_2D_transform_biomass = NormalizeCentered2D()\n",
    "normalized_stability_transform = NormalizedStabilityTransform()\n",
    "to_tensor_transform = ToTensor()\n",
    "\n",
    "aws_credentials = json.load(open(os.environ['AWS_CREDENTIALS']))\n",
    "akpd = AKPD(aws_credentials)\n",
    "\n",
    "# load neural network weights\n",
    "akpd_scorer_network = load_model('/root/data/alok/biomass_estimation/playground/akpd_scorer_model_TF.h5') # make this better\n",
    "biomass_network = torch.load('/root/data/alok/biomass_estimation/results/neural_network/2019-11-08T00:13:09/nn_epoch_798.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight(row_id, keypoints, cm):\n",
    "    \n",
    "    # run AKPD scoring network on manual data\n",
    "    input_sample = {\n",
    "        'keypoints': keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    manual_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "    \n",
    "    # run biomass estimation on manual data\n",
    "    input_sample = {\n",
    "        'keypoints': keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform_biomass.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    manual_weight_prediction = biomass_network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    \n",
    "    akpd_keypoints = akpd.predict_keypoints(\n",
    "        row.left_image_url, \n",
    "        row.right_image_url, \n",
    "        row.left_crop_metadata, \n",
    "        row.right_crop_metadata, \n",
    "        cm\n",
    "    )[0]\n",
    "    \n",
    "    # run AKPD scoring network on AKPD data\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row.id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    akpd_score = generate_confidence_score(input_sample, akpd_scorer_network)\n",
    "    \n",
    "    # run biomass estimation on AKPD data\n",
    "    input_sample = {\n",
    "        'keypoints': akpd_keypoints,\n",
    "        'cm': cm,\n",
    "        'stereo_pair_id': row_id,\n",
    "        'single_point_inference': True\n",
    "    }\n",
    "    nomralized_centered_2D_kps = \\\n",
    "        normalize_centered_2D_transform_biomass.__call__(input_sample)\n",
    "\n",
    "    normalized_stability_kps = normalized_stability_transform.__call__(nomralized_centered_2D_kps)\n",
    "    tensorized_kps = to_tensor_transform.__call__(normalized_stability_kps)\n",
    "    akpd_weight_prediction = biomass_network(tensorized_kps['kp_input']).item() * 1e4\n",
    "    \n",
    "    return manual_weight_prediction, manual_score, akpd_weight_prediction, akpd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_weights, manual_scores, akpd_weights, akpd_scores = [], [], [], []\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    manual_weight, manual_score, akpd_weight, akpd_score = generate_weight(row.id, row.keypoints, row.camera_metadata)\n",
    "    manual_weights.append(manual_weight)\n",
    "    manual_scores.append(manual_score)\n",
    "    akpd_weights.append(akpd_weight)\n",
    "    akpd_scores.append(akpd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['manual_weight'] = manual_weights\n",
    "df['manual_score'] = manual_scores\n",
    "df['akpd_weight'] = akpd_weights\n",
    "df['akpd_score'] = akpd_scores\n",
    "df['left_floy_tag'] = df.left_crop_metadata.apply(lambda x: x.get('floyTag'))\n",
    "df['right_floy_tag'] = df.right_crop_metadata.apply(lambda x: x.get('floyTag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    'WWBW': 1085,\n",
    "    'WWBB': 730,\n",
    "    'BBBB': 2060,\n",
    "    'BBBW': 1590,\n",
    "    'WWWB': 1880,\n",
    "    'BBWB': 1500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_pcts = []\n",
    "for idx, row in df[df.score > 0.9].iterrows():\n",
    "    tag = row.left_floy_tag\n",
    "    gt_weight = ground_truth[tag]\n",
    "    pred_weight = row.manual_weight\n",
    "    err_pct = (pred_weight - gt_weight) / gt_weight\n",
    "    err_pcts.append(err_pct)\n",
    "\n",
    "print(np.median(np.abs(err_pcts)))\n",
    "print(np.mean(np.abs(err_pcts)))\n",
    "print(np.mean(err_pcts))\n",
    "print(np.median(err_pcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_pcts = []\n",
    "for idx, row in df[df.akpd_score > 0.9].iterrows():\n",
    "    tag = row.left_floy_tag\n",
    "    gt_weight = ground_truth[tag]\n",
    "    pred_weight = row.akpd_weight\n",
    "    err_pct = (pred_weight - gt_weight) / gt_weight\n",
    "    err_pcts.append(err_pct)\n",
    "\n",
    "print(np.median(np.abs(err_pcts)))\n",
    "print(np.mean(np.abs(err_pcts)))\n",
    "print(np.mean(err_pcts))\n",
    "print(np.median(err_pcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = defaultdict(list)\n",
    "for tag in df[df.score > 0.9].left_floy_tag.unique().tolist():\n",
    "    mask = (df.left_floy_tag == df.right_floy_tag) & (df.left_floy_tag.notnull())\n",
    "    tag_mask = df.left_floy_tag == tag\n",
    "    mean_prediction = df[mask & tag_mask].weight.mean()\n",
    "    median_prediction = df[mask & tag_mask].weight.median()\n",
    "    mean_err_pct = np.mean((df[mask & tag_mask].weight - ground_truth[tag]) / ground_truth[tag])\n",
    "    median_err_pct = np.median((df[mask & tag_mask].weight - ground_truth[tag]) / ground_truth[tag])\n",
    "    mean_abs_err_pct = np.mean(np.abs((df[mask & tag_mask].weight - ground_truth[tag]) / ground_truth[tag]))\n",
    "    median_abs_err_pct = np.median(np.abs((df[mask & tag_mask].weight - ground_truth[tag]) / ground_truth[tag]))\n",
    "    num_samples = df[mask & tag_mask].shape[0]\n",
    "    analysis_data['tag'].append(tag)\n",
    "    analysis_data['num_samples'].append(num_samples)\n",
    "    analysis_data['ground_truth_weight'].append(ground_truth[tag])\n",
    "    analysis_data['mean_prediction'].append(mean_prediction)\n",
    "    analysis_data['median_prediction'].append(median_prediction)\n",
    "    analysis_data['mean_err_pct'].append(mean_err_pct)\n",
    "    analysis_data['median_err_pct'].append(median_err_pct)\n",
    "    analysis_data['mean_abs_err_pct'].append(mean_abs_err_pct)\n",
    "    analysis_data['median_abs_err_pct'].append(median_abs_err_pct)\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_data)\n",
    "analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = defaultdict(list)\n",
    "for tag in df[df.akpd_score > 0.9].left_floy_tag.unique().tolist():\n",
    "    mask = (df.left_floy_tag == df.right_floy_tag) & (df.left_floy_tag.notnull())\n",
    "    tag_mask = df.left_floy_tag == tag\n",
    "    mean_prediction = df[mask & tag_mask].akpd_weight.mean()\n",
    "    median_prediction = df[mask & tag_mask].akpd_weight.median()\n",
    "    mean_err_pct = np.mean((df[mask & tag_mask].akpd_weight - ground_truth[tag]) / ground_truth[tag])\n",
    "    median_err_pct = np.median((df[mask & tag_mask].akpd_weight - ground_truth[tag]) / ground_truth[tag])\n",
    "    mean_abs_err_pct = np.mean(np.abs((df[mask & tag_mask].akpd_weight - ground_truth[tag]) / ground_truth[tag]))\n",
    "    median_abs_err_pct = np.median(np.abs((df[mask & tag_mask].akpd_weight - ground_truth[tag]) / ground_truth[tag]))\n",
    "    num_samples = df[mask & tag_mask].shape[0]\n",
    "    analysis_data['tag'].append(tag)\n",
    "    analysis_data['num_samples'].append(num_samples)\n",
    "    analysis_data['ground_truth_weight'].append(ground_truth[tag])\n",
    "    analysis_data['mean_prediction'].append(mean_prediction)\n",
    "    analysis_data['median_prediction'].append(median_prediction)\n",
    "    analysis_data['mean_err_pct'].append(mean_err_pct)\n",
    "    analysis_data['median_err_pct'].append(median_err_pct)\n",
    "    analysis_data['mean_abs_err_pct'].append(mean_abs_err_pct)\n",
    "    analysis_data['median_abs_err_pct'].append(median_abs_err_pct)\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_data)\n",
    "analysis_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[mask & tag_mask].weight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Visualizer(s3_access_utils, rds_access_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "kpid = df[mask & tag_mask].id.iloc[i]\n",
    "print(df[mask & tag_mask].weight.iloc[i])\n",
    "v.load_data(int(kpid))\n",
    "v.display_crops()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 16\n",
    "kpid = df[mask & tag_mask].id.iloc[i]\n",
    "print(df[mask & tag_mask].weight.iloc[i])\n",
    "v.load_data(int(kpid))\n",
    "v.display_crops()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = defaultdict(list)\n",
    "for i in range(df[mask & tag_mask].shape[0]):\n",
    "    kps = df[mask & tag_mask].keypoints.iloc[i]\n",
    "    cm = df[mask & tag_mask].camera_metadata.iloc[i]\n",
    "    weight = df[mask & tag_mask].weight.iloc[i]\n",
    "    kpid = df[mask & tag_mask].id.iloc[i]\n",
    "    left_kps, right_kps = kps['leftCrop'], kps['rightCrop']\n",
    "    wkps = pixel2world(left_kps, right_kps, cm)\n",
    "    for bp1 in sorted(list(wkps.keys())):\n",
    "        for bp2 in sorted(list(wkps.keys())):\n",
    "            if bp1 == bp2:\n",
    "                continue\n",
    "            analysis_data['{}-{}'.format(bp1, bp2)].append(euclidean_distance(wkps[bp1], wkps[bp2]))\n",
    "    analysis_data['weight'].append(weight)\n",
    "    analysis_data['kpid'].append(kpid)\n",
    "    \n",
    "analysis_df = pd.DataFrame(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpid1, kpid2 = 1043682, 1043530\n",
    "filter1, filter2 = analysis_df[analysis_df.kpid == kpid1], analysis_df[analysis_df.kpid == kpid2]\n",
    "filter1.T[filter1.index[0]] - filter2.T[filter2.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == filter1.kpid.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.load_data(kpid1)\n",
    "v.display_crops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.load_data(kpid2)\n",
    "v.display_crops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_keypoints(row):\n",
    "    if 'leftCrop' in row.keypoints and 'rightCrop' in row.keypoints:\n",
    "        return pixel2world(row.keypoints['leftCrop'], row.keypoints['rightCrop'], row.camera_metadata)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df['world_keypoints'] = df.apply(\n",
    "    lambda x: get_world_keypoints(x), axis=1\n",
    ")\n",
    "\n",
    "df['depth'] = df.world_keypoints.apply(lambda wkp: np.median([x[1] for x in wkp.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[mask & tag_mask].depth, df[mask & tag_mask].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(analysis_df['DORSAL_FIN-PELVIC_FIN'], analysis_df.weight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[analysis_df['DORSAL_FIN-PELVIC_FIN'] > 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[analysis_df['DORSAL_FIN-PELVIC_FIN'] < 0.12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
