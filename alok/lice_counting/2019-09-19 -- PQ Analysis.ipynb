{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from wpca import WPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aquabyte.accuracy_metrics import AccuracyMetricsGenerator\n",
    "from aquabyte.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from aquabyte.optics import euclidean_distance, pixel2world\n",
    "from aquabyte.visualize import Visualizer\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageDraw\n",
    "from urllib.parse import urlparse\n",
    "import datetime as dt\n",
    "import heapq \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get lice annotation data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['PROD_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_annotation_performance(pen_id, start_date, end_date):\n",
    "    query = \"select * from lati_fish_detections_lice_annotations where pen_id = {0} and captured_at >= '{1}' and captured_at < '{2}'\".format(pen_id, start_date, end_date)\n",
    "    cogito_df = rds_access_utils.extract_from_database(query)\n",
    "    cogito_df['is_submitted'] = ~cogito_df.is_skipped\n",
    "    cogito_df['quality_score'] = cogito_df.metadata.apply(lambda x: x['quality_score'])\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(range(cogito_df.shape[0]), np.cumsum(cogito_df.sort_values('captured_at')['is_submitted']), color='b', s=3, alpha=0.6, label='Without PQ no delay')\n",
    "    plt.scatter(range(cogito_df.shape[0]), np.cumsum(cogito_df.sort_values('completed_at')['is_submitted']), color='r', s=3, alpha=0.6, label='With PQ no delay')\n",
    "    \n",
    "    # simulate 4 hour delay\n",
    "    cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=4)\n",
    "    queue, submits = [], []\n",
    "    last_ts = None\n",
    "    i = 0\n",
    "    for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "        if not last_ts:\n",
    "            additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "        else:\n",
    "            additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "        last_ts = row.simulated_completed_at\n",
    "        additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "                                        cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "        queue.extend(additional_scores_and_submits)\n",
    "        queue.sort(key=lambda x: x[0], reverse=True)\n",
    "        _, submit = queue.pop(0)\n",
    "        submits.append(submit)\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='black', s=3, alpha=0.6, label='With PQ 4 hour delay')\n",
    "    \n",
    "    \n",
    "    # simulate 2 hour delay\n",
    "    cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=2)\n",
    "    queue, submits = [], []\n",
    "    last_ts = None\n",
    "    i = 0\n",
    "    for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "        if not last_ts:\n",
    "            additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "        else:\n",
    "            additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "        last_ts = row.simulated_completed_at\n",
    "        additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "                                        cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "        queue.extend(additional_scores_and_submits)\n",
    "        queue.sort(key=lambda x: x[0], reverse=True)\n",
    "        _, submit = queue.pop(0)\n",
    "        submits.append(submit)\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='orange', s=3, alpha=0.6, label='With PQ 2 hour delay')\n",
    "    \n",
    "    # simulate 1 hour delay\n",
    "    cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=1)\n",
    "    queue, submits = [], []\n",
    "    last_ts = None\n",
    "    i = 0\n",
    "    for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "        if not last_ts:\n",
    "            additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "        else:\n",
    "            additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "        last_ts = row.simulated_completed_at\n",
    "        additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "                                        cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "        queue.extend(additional_scores_and_submits)\n",
    "        queue.sort(key=lambda x: x[0], reverse=True)\n",
    "        _, submit = queue.pop(0)\n",
    "        submits.append(submit)\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='purple', s=3, alpha=0.6, label='With PQ 1 hour delay')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # simulate 0 delay with perfect PQ\n",
    "    cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=0)\n",
    "    queue, submits = [], []\n",
    "    last_ts = None\n",
    "    i = 0\n",
    "    for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "        if not last_ts:\n",
    "            additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "        else:\n",
    "            additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "        last_ts = row.simulated_completed_at\n",
    "        additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "                                        cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "        queue.extend(additional_scores_and_submits)\n",
    "        queue.sort(key=lambda x: x[1], reverse=True)\n",
    "        _, submit = queue.pop(0)\n",
    "        submits.append(submit)\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "#     plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='purple', s=3, alpha=0.6, label='With perfect PQ zero delay')\n",
    "    \n",
    "#     # simulate 12 hour delay\n",
    "#     cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=12)\n",
    "#     queue, submits = [], []\n",
    "#     last_ts = None\n",
    "#     i = 0\n",
    "#     for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "#         if not last_ts:\n",
    "#             additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "#         else:\n",
    "#             additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "#         last_ts = row.simulated_completed_at\n",
    "#         additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "#                                         cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "#         queue.extend(additional_scores_and_submits)\n",
    "#         queue.sort(key=lambda x: x[0], reverse=True)\n",
    "#         _, submit = queue.pop(0)\n",
    "#         submits.append(submit)\n",
    "#         if i % 100 == 0:\n",
    "#             print(i)\n",
    "#         i += 1\n",
    "        \n",
    "    \n",
    "#     plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='purple', s=3, alpha=0.6, label='With PQ 12 hour delay')\n",
    "    \n",
    "#     # simulate 16 hour delay\n",
    "#     cogito_df['simulated_completed_at'] = cogito_df.completed_at + dt.timedelta(hours=12)\n",
    "#     queue, submits = [], []\n",
    "#     last_ts = None\n",
    "#     i = 0\n",
    "#     for idx, row in cogito_df.sort_values('simulated_completed_at').iterrows():\n",
    "#         if not last_ts:\n",
    "#             additional_captures_mask = (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "#         else:\n",
    "#             additional_captures_mask = (cogito_df.captured_at > last_ts) & (cogito_df.captured_at <= row.simulated_completed_at)\n",
    "\n",
    "#         last_ts = row.simulated_completed_at\n",
    "#         additional_scores_and_submits = list(zip(cogito_df[additional_captures_mask].quality_score.tolist(), \n",
    "#                                         cogito_df[additional_captures_mask].is_submitted.tolist()))\n",
    "    \n",
    "#         queue.extend(additional_scores_and_submits)\n",
    "#         queue.sort(key=lambda x: x[0], reverse=True)\n",
    "#         _, submit = queue.pop(0)\n",
    "#         submits.append(submit)\n",
    "#         if i % 100 == 0:\n",
    "#             print(i)\n",
    "#         i += 1\n",
    "        \n",
    "    \n",
    "#     plt.scatter(range(cogito_df.shape[0]), np.cumsum(np.array(submits)), color='green', s=3, alpha=0.6, label='With PQ 16 hour delay')\n",
    "\n",
    "    \n",
    "    plt.axhline(50, linestyle='dashed', label='KPI')\n",
    "    plt.xlabel('Number of images analyzed by Cogito')\n",
    "    plt.ylabel('Number of images submitted to QA')\n",
    "    plt.title('PQ Performance for PEN ID = {0} on {1}'.format(pen_id, start_date))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return cogito_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id=37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_df = generate_annotation_performance(pen_id, '2019-09-22', '2019-09-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_df = generate_annotation_performance(pen_id, '2019-09-21', '2019-09-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_df = generate_annotation_performance(37, '2019-09-23', '2019-09-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(3, 6), (2, 7), (9, 3), (4, 1), (1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sort(key=lambda x: x[0], reverse=True)\n",
    "a, b = x.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate_df.to_csv('/root/data/alok/aggregate_df_bremnes_tittelsnes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FISH_WIDTH_M = 0.065\n",
    "FISH_LENGTH_M = 0.294\n",
    "FOCAL_LENGTH = 4015\n",
    "\n",
    "def depth_fn(x):\n",
    "    w, h = x['width'], x['height']\n",
    "    theta = np.arctan(h / w) * (180.0 / np.pi)\n",
    "    phi = np.arctan(FISH_WIDTH_M / FISH_LENGTH_M) * (180.0 / np.pi)\n",
    "    if theta < phi:\n",
    "        return w\n",
    "    elif theta > 90.0 - phi:\n",
    "        return h\n",
    "    else:\n",
    "        return (h**2 + w**2)**0.5\n",
    "\n",
    "def process_data_df(df):\n",
    "    df = df[df.is_cleaner_fish != True]\n",
    "    df['image_width'] = df.metadata.apply(lambda x: x['width'])\n",
    "    df['image_height'] = df.metadata.apply(lambda x: x['height'])\n",
    "    df['length_px'] = df.metadata.apply(lambda x: depth_fn(x))\n",
    "    df['single_image_depth_m'] = FOCAL_LENGTH * FISH_LENGTH_M / df.length_px\n",
    "    df['stereo_depth_m'] = df.metadata.apply(lambda x: x.get('depth_m'))\n",
    "    return df\n",
    "\n",
    "df = process_data_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/aggregate_vikane_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_focus_mask = (df.stereo_depth_m > 0.8) & (df.stereo_depth_m < 1.0)\n",
    "accept_mask = ~df.is_skipped\n",
    "skip_masks = {}\n",
    "skip_reasons = [\n",
    "    'is_accepted_in_qa', \n",
    "    'is_blurry', \n",
    "    'is_bad_crop', \n",
    "    'is_too_dark', \n",
    "    'is_bad_crop_many_fish', \n",
    "    'is_bad_orientation', \n",
    "    'is_bad_crop_cut_off', \n",
    "    'is_obstructed'\n",
    "]\n",
    "for skip_reason in skip_reasons:\n",
    "    skip_masks[skip_reason] = df[skip_reason] == True\n",
    "\n",
    "n = df.shape[0]\n",
    "n_in_focus = df[in_focus_mask].shape[0]\n",
    "n_in_focus_accepted = df[in_focus_mask & accept_mask].shape[0]\n",
    "n_not_in_focus_accepted = df[~in_focus_mask & accept_mask].shape[0]\n",
    "\n",
    "\n",
    "print('Total number of images inspected by Cogito over the weekend: {}'.format(n))\n",
    "print('Total number of these images within in-focus range (45 cm - 55 cm): {}'.format(n_in_focus))\n",
    "print('Total number of in-focus images accepted in QA: {}'.format(n_in_focus_accepted))\n",
    "print('Total number of not-in-focus images accepted by Cogito: {}'.format(n_not_in_focus_accepted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "valid_depth_mask = (df.stereo_depth_m > 0.0) & (df.stereo_depth_m < 2.0)\n",
    "plt.hist(df[valid_depth_mask].stereo_depth_m, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.is_skipped == True].sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, skip_reason):\n",
    "    depth_m = row[depth_field]\n",
    "    line_segment_length_px = object_length_m * FOCAL_LENGTH / depth_m\n",
    "    image_url = row.image_url\n",
    "    bucket, key = 'aquabyte-crops', urlparse(image_url, allow_fragments=False).path.lstrip('/')\n",
    "    image_f = s3_access_utils.download_from_s3(bucket, key)\n",
    "\n",
    "    im = Image.open(image_f)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.line((100, 100, 100+line_segment_length_px, 100))\n",
    "\n",
    "    f_name = os.path.basename(key)\n",
    "    f = os.path.join(modified_images_dir, '{}_{}'.format(lo, hi), skip_reason, f_name)\n",
    "    if not os.path.exists(os.path.dirname(f)):\n",
    "        os.makedirs(os.path.dirname(f))\n",
    "    im.save(f)\n",
    "\n",
    "\n",
    "modified_images_dir = '/root/data/alok/lice_counting/bremnes_tittelsnes_image_breakdown'\n",
    "object_length_m = 0.01\n",
    "N = 20\n",
    "\n",
    "cogito_accept_mask = ~df.is_skipped\n",
    "qa_accept_mask = ~reconciled_df.is_skipped\n",
    "depth_values = [round(x, 1) for x in np.arange(0.2, 1.5, 0.1)]\n",
    "\n",
    "depth_field = 'stereo_depth_m'\n",
    "for i in range(len(depth_values)-1):\n",
    "    print(i)\n",
    "    lo, hi = depth_values[i], depth_values[i+1]\n",
    "    depth_mask = (df[depth_field] >= lo) & (df[depth_field] <= hi)\n",
    "    \n",
    "    # accepted images\n",
    "    for idx, row in df[depth_mask & accept_mask].head(N).iterrows():\n",
    "        process_row(row, 'accepted')\n",
    "    \n",
    "    # rejected images due to blurriness\n",
    "    for idx, row in df[depth_mask & is_blurry_mask & (~is_bad_crop_mask) & (~is_too_dark_mask) & (~is_bad_orientation_mask)].head(N).iterrows():\n",
    "        process_row(row, 'is_blurry')\n",
    "        \n",
    "    # rejected images due to darkness\n",
    "    for idx, row in df[depth_mask & (~is_blurry_mask) & (~is_bad_crop_mask) & is_too_dark_mask & (~is_bad_orientation_mask)].head(N).iterrows():\n",
    "        process_row(row, 'is_too_dark')\n",
    "        \n",
    "    # rejected images due to bad crop\n",
    "    for idx, row in df[depth_mask & (~is_blurry_mask) & is_bad_crop_mask & (~is_too_dark_mask) & (~is_bad_orientation_mask)].head(N).iterrows():\n",
    "        process_row(row, 'is_bad_crop')\n",
    "        \n",
    "    # rejected images due to bad orientation\n",
    "    for idx, row in df[depth_mask & (~is_blurry_mask) & (~is_bad_crop_mask) & (~is_too_dark_mask) & (is_bad_orientation_mask)].head(N).iterrows():\n",
    "        process_row(row, 'is_bad_orientation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, skip_reason):\n",
    "    depth_m = row[depth_field]\n",
    "    line_segment_length_px = object_length_m * FOCAL_LENGTH / depth_m\n",
    "    image_url = row.image_url\n",
    "    if 'aquabyte-crops-lati' not in image_url:\n",
    "        bucket, key = 'aquabyte-crops', urlparse(image_url, allow_fragments=False).path.lstrip('/')\n",
    "    else:\n",
    "        components = urlparse(image_url, allow_fragments=False).path.lstrip('/').split('/')\n",
    "        bucket, key = components[0], os.path.join(*components[1:])\n",
    "    print(bucket, key)\n",
    "    image_f = s3_access_utils.download_from_s3(bucket, key)\n",
    "\n",
    "    im = Image.open(image_f)\n",
    "#     draw = ImageDraw.Draw(im)\n",
    "#     draw.line((100, 100, 100+line_segment_length_px, 100))\n",
    "\n",
    "    f_name = os.path.basename(key)\n",
    "    f = os.path.join(modified_images_dir, skip_reason, f_name)\n",
    "    if not os.path.exists(os.path.dirname(f)):\n",
    "        os.makedirs(os.path.dirname(f))\n",
    "    im.save(f)\n",
    "\n",
    "\n",
    "modified_images_dir = '/root/data/alok/lice_counting/bremnes_tittelsnes_breakdown_depth_independent'\n",
    "object_length_m = 0.01\n",
    "N = 50\n",
    "\n",
    "\n",
    "# rejected images due to skip reason\n",
    "for target_skip_reason in skip_reasons:\n",
    "    mask = skip_masks[target_skip_reason]\n",
    "    for skip_reason, skip_mask in skip_masks.items():\n",
    "        if skip_reason != target_skip_reason:\n",
    "            mask = mask & ~skip_mask\n",
    "        for idx, row in df[mask].head(N).iterrows():\n",
    "            process_row(row, skip_reason)\n",
    "\n",
    "# # rejected images due to darkness\n",
    "# for idx, row in df[(~is_blurry_mask) & (~is_bad_crop_mask) & is_too_dark_mask & (~is_bad_orientation_mask)].head(N).iterrows():\n",
    "#     process_row(row, 'is_too_dark')\n",
    "\n",
    "# # rejected images due to bad crop\n",
    "# for idx, row in df[(~is_blurry_mask) & is_bad_crop_mask & (~is_too_dark_mask) & (~is_bad_orientation_mask)].head(N).iterrows():\n",
    "#     process_row(row, 'is_bad_crop')\n",
    "\n",
    "# # rejected images due to bad orientation\n",
    "# for idx, row in df[(~is_blurry_mask) & (~is_bad_crop_mask) & (~is_too_dark_mask) & (is_bad_orientation_mask)].head(N).iterrows():\n",
    "#     process_row(row, 'is_bad_orientation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image_url.str.contains('702_1953_3290_3000')].stereo_depth_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image_url.str.contains('366_1350_2442_2229')].stereo_depth_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.image_url.str.contains('0_1127_2674_2012')].stereo_depth_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate depth values </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_field = 'stereo_depth_m'\n",
    "valid_mask = (reconciled_df[depth_field] > 0.2) & (reconciled_df[depth_field] < 0.7)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(reconciled_df.loc[valid_mask & reconciled_accept_mask, depth_field], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_field = 'single_image_depth_m'\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(reconciled_df.loc[reconciled_accept_mask, depth_field], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
