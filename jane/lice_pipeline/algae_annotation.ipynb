{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from urllib.parse import urlparse\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "OUTPUT_BASE_DIR = 'generated_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refresh_directory(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bucket_key(url):\n",
    "    parsed_url = urlparse(url, allow_fragments=False)\n",
    "    if parsed_url.netloc.startswith('s3'):\n",
    "        url_components = parsed_url.path.lstrip('/').split('/')\n",
    "        bucket, key = url_components[0], os.path.join(*url_components[1:])\n",
    "    else:\n",
    "        bucket = parsed_url.netloc.split('.')[0]\n",
    "        key = parsed_url.path.lstrip('/')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def _captured_in_hour_range(key, start_hour, end_hour):\n",
    "    hour = int([component for component in key.split('/') if component.startswith('hour=')][0].split('=')[-1])\n",
    "    return start_hour <= hour <= end_hour\n",
    "\n",
    "\n",
    "\n",
    "def extract_s3_keys(pen_id, date, start_hour, end_hour, inbound_bucket='aquabyte-frames-resized-inbound'):\n",
    "    query = \"\"\"\n",
    "        SELECT captured_at, left_crop_url\n",
    "        FROM prod.crop_annotation ca\n",
    "        WHERE ca.pen_id={} AND ca.service_id = 2\n",
    "        AND to_char(ca.captured_at, 'YYYY-MM-DD') IN ('{}')\n",
    "        LIMIT 1;\n",
    "    \"\"\".format(pen_id, date)\n",
    "\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "    image_url = df.left_crop_url.iloc[0]\n",
    "    bucket, key = _get_bucket_key(image_url)\n",
    "    s3_folder = os.path.join(key[:key.index('date')], 'date={}'.format(date))\n",
    "    generator = s3_access_utils.get_matching_s3_keys(inbound_bucket, s3_folder, suffixes=['capture.json'])\n",
    "    keys = [key for key in generator if _captured_in_hour_range(key, start_hour, end_hour)]\n",
    "    s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))\n",
    "    return s3_key_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_resize_url(s3_key_dir):\n",
    "    base_url = \"https://aquabyte-frames-resized-inbound.s3-eu-west-1.amazonaws.com/\"\n",
    "    left_f = os.path.join(base_url, s3_key_dir,'left_frame.resize_512_512.jpg')\n",
    "    right_f = os.path.join(base_url, s3_key_dir,'right_frame.resize_512_512.jpg')\n",
    "    crop_metadata_f = os.path.join(base_url, s3_key_dir, 'crops.json')\n",
    "\n",
    "    return left_f, right_f, crop_metadata_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe(pen_id, date, start_hour, end_hour, has_algae, num_processes=12):\n",
    "    _refresh_directory(OUTPUT_BASE_DIR)\n",
    "    # extract s3 keys\n",
    "    print('Extracting s3 keys...')\n",
    "    s3_key_dirs = extract_s3_keys(pen_id, date, start_hour, end_hour)\n",
    "    print('S3 keys extraction complete!')\n",
    "\n",
    "    print('extract s3 keys..')\n",
    "    pool = Pool(num_processes)\n",
    "    results = pool.map(get_resize_url, s3_key_dirs)\n",
    "    print('s3 keys complete!')\n",
    "    \n",
    "    print(\"convert to dataframe\")\n",
    "    df = pd.DataFrame(results,columns=['left_frame_resized_url', 'right_frame_resized_url', 'crop_metadata_url'])\n",
    "    df['base_key'] = np.array(s3_key_dirs)\n",
    "    df['pen_id'] = pen_id\n",
    "    df['date'] = date\n",
    "    df['has_algae'] = has_algae\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2020_04_22 = generate_dataframe(56, \"2020-04-22\", 10, 12, False, num_processes = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_04_22.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_04_22.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_04_22.right_frame_resized_url.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2020_05_04 = generate_dataframe(56, \"2020-05-04\", 10, 12, True, num_processes = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_05_04.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([df_2020_04_22, df_2020_05_04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bucket='aquabyte-images-adhoc' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_refresh_directory(OUTPUT_BASE_DIR)\n",
    "output_path = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}.csv'.format(str(56)))\n",
    "output.to_csv(output_path, index=False)\n",
    "output_key = os.path.join(\"water_turbidity/algae_binary_adhoc\", os.path.basename(output_path))\n",
    "s3_access_utils.s3_client.upload_file(output_path, output_bucket, output_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
