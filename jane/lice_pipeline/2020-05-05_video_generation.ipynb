{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from urllib.parse import urlparse\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THUMBNAIL_WIDTH = 512\n",
    "PIXEL_COUNT_WIDTH = 4096\n",
    "PIXEL_COUNT_HEIGHT = 3000\n",
    "X_PADDING_FULLRES = 190\n",
    "Y_PADDING_FULLRES = 140\n",
    "X_PADDING = X_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_WIDTH)\n",
    "Y_PADDING = Y_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_HEIGHT)\n",
    "ROOT_DIR = '/root/data/s3'\n",
    "OUTPUT_BASE_DIR = 'generated_video'\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data')\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refresh_directory(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "def _get_bucket_key(url):\n",
    "    parsed_url = urlparse(url, allow_fragments=False)\n",
    "    if parsed_url.netloc.startswith('s3'):\n",
    "        url_components = parsed_url.path.lstrip('/').split('/')\n",
    "        bucket, key = url_components[0], os.path.join(*url_components[1:])\n",
    "    else:\n",
    "        bucket = parsed_url.netloc.split('.')[0]\n",
    "        key = parsed_url.path.lstrip('/')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def process_s3_key_dir(s3_key_dir, inbound_bucket='aquabyte-frames-resized-inbound'):\n",
    "    try:\n",
    "        left_f = s3_access_utils.download_from_s3(inbound_bucket, os.path.join(s3_key_dir,\n",
    "                                                                               'left_frame.resize_512_512.jpg'))\n",
    "        right_f = s3_access_utils.download_from_s3(inbound_bucket, os.path.join(s3_key_dir,\n",
    "                                                                                'right_frame.resize_512_512.jpg'))\n",
    "        crop_metadata_f = s3_access_utils.download_from_s3(inbound_bucket, os.path.join(s3_key_dir,\n",
    "                                                                                        'crops.json'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # open images and metadata files\n",
    "    left_im = Image.open(left_f)\n",
    "    right_im = Image.open(right_f)\n",
    "    crop_metadata = json.load(open(crop_metadata_f))\n",
    "    try:\n",
    "        depth = crop_metadata['capture']['sensors'].get('aquabyte_depth_meters')\n",
    "    except Exception as e:\n",
    "        depth = 'Depth not found'\n",
    "\n",
    "    # draw boxes on images\n",
    "    left_draw = ImageDraw.Draw(left_im)\n",
    "    right_draw = ImageDraw.Draw(right_im)\n",
    "    anns = crop_metadata['annotations']\n",
    "    if anns:\n",
    "        for ann in anns:\n",
    "            c1 = max(ann['bbox'][0] - X_PADDING, 0)\n",
    "            c2 = max(ann['bbox'][1] - Y_PADDING, 0)\n",
    "            c3 = min(ann['bbox'][0] + ann['bbox'][2] + X_PADDING, THUMBNAIL_WIDTH)\n",
    "            c4 = min(ann['bbox'][1] + ann['bbox'][3] + Y_PADDING, THUMBNAIL_WIDTH)\n",
    "            if ann['image_id'] == 1:\n",
    "                left_draw.rectangle([(c1, c2), (c3, c4)])\n",
    "            elif ann['image_id'] == 2:\n",
    "                right_draw.rectangle([(c1, c2), (c3, c4)])\n",
    "\n",
    "    # stitch images\n",
    "    result = Image.new('RGB', (2 * THUMBNAIL_WIDTH, THUMBNAIL_WIDTH))\n",
    "    result.paste(im=left_im, box=(0, 0))\n",
    "    result.paste(im=right_im, box=(THUMBNAIL_WIDTH, 0))\n",
    "\n",
    "    # write timestamp on stitched image\n",
    "    result_draw = ImageDraw.Draw(result)\n",
    "    ts = [c for c in left_f.split('/') if c.startswith('at=')][0]\n",
    "    display_ts = 'UTC Time: {}'.format(ts.replace('at=', ''))\n",
    "    display_depth = 'Depth: {}m'.format(depth)\n",
    "    result_draw.text((0, 0), display_ts, (255, 255, 255))\n",
    "    result_draw.text((0, 10), display_depth, (255, 255, 255))\n",
    "\n",
    "    output_f = left_f.replace(ROOT_DIR, OUTPUT_BASE_DIR).replace('left_', 'stereo_')\n",
    "    if not os.path.exists(os.path.dirname(output_f)):\n",
    "        os.makedirs(os.path.dirname(output_f))\n",
    "    result.save(output_f)\n",
    "\n",
    "\n",
    "def stitch_frames_into_video(image_fs, video_f):\n",
    "    im = cv2.imread(image_fs[0])\n",
    "    height, width, layers = im.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(video_f, fourcc, 4, (width, height), True)\n",
    "    for idx, image_f in enumerate(image_fs):\n",
    "        if idx % 1000 == 0:\n",
    "            print(idx)\n",
    "        im = cv2.imread(image_f, cv2.IMREAD_COLOR)\n",
    "        video.write(im)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    print('Video generation complete!')\n",
    "\n",
    "\n",
    "def _captured_in_hour_range(key, start_hour, end_hour):\n",
    "    hour = int([component for component in key.split('/') if component.startswith('hour=')][0].split('=')[-1])\n",
    "    return start_hour <= hour <= end_hour\n",
    "\n",
    "\n",
    "def extract_s3_keys(pen_id, date, start_hour, end_hour, inbound_bucket='aquabyte-frames-resized-inbound'):\n",
    "    query = \"\"\"\n",
    "        SELECT captured_at, left_crop_url\n",
    "        FROM prod.crop_annotation ca\n",
    "        WHERE ca.pen_id={} AND ca.service_id = 2\n",
    "        AND to_char(ca.captured_at, 'YYYY-MM-DD') IN ('{}')\n",
    "        LIMIT 1;\n",
    "    \"\"\".format(pen_id, date)\n",
    "\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "    image_url = df.left_crop_url.iloc[0].replace(\"parallel\", \"production\")\n",
    "    bucket, key = _get_bucket_key(image_url)\n",
    "    s3_folder = os.path.join(key[:key.index('date')], 'date={}'.format(date))\n",
    "    generator = s3_access_utils.get_matching_s3_keys(inbound_bucket, s3_folder, suffixes=['capture.json'])\n",
    "    keys = [key for key in generator if _captured_in_hour_range(key, start_hour, end_hour)]\n",
    "    s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))\n",
    "    return s3_key_dirs\n",
    "\n",
    "\n",
    "def generate_video(pen_id, date, start_hour, end_hour, upload_to_s3=True, video_bucket='aquabyte-images-adhoc',\n",
    "                   num_processes=20):\n",
    "\n",
    "    # refresh output directory (i.e. clean out its contents)\n",
    "    _refresh_directory(OUTPUT_BASE_DIR)\n",
    "\n",
    "    # extract s3 keys\n",
    "    print('Extracting s3 keys...')\n",
    "    s3_key_dirs = extract_s3_keys(pen_id, date, start_hour, end_hour)\n",
    "    print('S3 keys extraction complete!')\n",
    "\n",
    "    print('Generating frames...')\n",
    "    pool = Pool(num_processes)\n",
    "    pool.map(process_s3_key_dir, s3_key_dirs)\n",
    "    print('Frame generation complete!')\n",
    "\n",
    "    print('Generating video...')\n",
    "    image_fs = sorted(\n",
    "        filter(lambda path: 'stereo' in path, glob.glob(os.path.join(OUTPUT_BASE_DIR, '**', '*.jpg'), recursive=True)))\n",
    "    video_f = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}_date_{}_video.avi'.format(str(pen_id), date))\n",
    "    stitch_frames_into_video(image_fs, video_f)\n",
    "    print('Video generation complete!')\n",
    "\n",
    "    if upload_to_s3:\n",
    "        print('Uploading to S3...')\n",
    "        video_key = os.path.join('videos', str(pen_id), os.path.basename(video_f))\n",
    "        s3_access_utils.s3_client.upload_file(video_f, video_bucket, video_key)\n",
    "        print('Upload complete! Result available here: {}'.format(os.path.join(video_bucket, video_key)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    pen_id_switch = '--pen_id'\n",
    "    date_switch = '--date'\n",
    "    start_hour_switch = '--start_hour'\n",
    "    end_hour_switch = '--end_hour'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(pen_id_switch, type=int, help='Pen ID for this video generation', required=True)\n",
    "    parser.add_argument(date_switch, type=str, help='Date for this video generation', required=True)\n",
    "    parser.add_argument(start_hour_switch, type=int, help='Start hour (full hour included)', required=False, default=24)\n",
    "    parser.add_argument(end_hour_switch, type=int, help='End hour (full hour included)', required=False, default=24)\n",
    "    args = parser.parse_args()\n",
    "    pen_id, date, start_hour, end_hour = args.pen_id, args.date, args.start_hour, args.end_hour\n",
    "    print(f'Building video for {pen_id} on {date} from {start_hour} to {end_hour} (inclusive)')\n",
    "    generate_video(pen_id, date, start_hour, end_hour)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, date, start_hour, end_hour, upload_to_s3 = 56, '2020-04-22', 10, 12, False\n",
    "num_processes = 12\n",
    "inbound_bucket='aquabyte-frames-resized-inbound'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT captured_at, left_crop_url\n",
    "    FROM prod.crop_annotation ca\n",
    "    WHERE ca.pen_id={} AND ca.service_id = 2\n",
    "    AND to_char(ca.captured_at, 'YYYY-MM-DD') IN ('{}')\n",
    "    LIMIT 1;\n",
    "\"\"\".format(pen_id, date)\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "image_url = df.left_crop_url.iloc[0]\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = image_url.replace(\"parallel\", \"production\")\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, key = _get_bucket_key(image_url)\n",
    "s3_folder = os.path.join(key[:key.index('date')], 'date={}'.format(date))\n",
    "generator = s3_access_utils.get_matching_s3_keys(inbound_bucket, s3_folder, suffixes=['capture.json'])\n",
    "keys = [key for key in generator if _captured_in_hour_range(key, start_hour, end_hour)]\n",
    "s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # refresh output directory (i.e. clean out its contents)\n",
    "# _refresh_directory(OUTPUT_BASE_DIR)\n",
    "\n",
    "# # extract s3 keys\n",
    "# print('Extracting s3 keys...')\n",
    "# s3_key_dirs = extract_s3_keys(pen_id, date, start_hour, end_hour)\n",
    "# print('S3 keys extraction complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating frames...')\n",
    "pool = Pool(num_processes)\n",
    "pool.map(process_s3_key_dir, s3_key_dirs)\n",
    "print('Frame generation complete!')\n",
    "\n",
    "print('Generating video...')\n",
    "image_fs = sorted(\n",
    "    filter(lambda path: 'stereo' in path, glob.glob(os.path.join(OUTPUT_BASE_DIR, '**', '*.jpg'), recursive=True)))\n",
    "video_f = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}_date_{}_video.avi'.format(str(pen_id), date))\n",
    "stitch_frames_into_video(image_fs, video_f)\n",
    "print('Video generation complete!')\n",
    "\n",
    "if upload_to_s3:\n",
    "    print('Uploading to S3...')\n",
    "    video_key = os.path.join('videos', str(pen_id), os.path.basename(video_f))\n",
    "    s3_access_utils.s3_client.upload_file(video_f, video_bucket, video_key)\n",
    "    print('Upload complete! Result available here: {}'.format(os.path.join(video_bucket, video_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_s3_key_dir(s3_key_dirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(key[:key.index('date')], 'date={}'.format(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
