{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from urllib.parse import urlparse\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "\n",
    "THUMBNAIL_WIDTH = 512\n",
    "PIXEL_COUNT_WIDTH = 4096\n",
    "PIXEL_COUNT_HEIGHT = 3000\n",
    "X_PADDING_FULLRES = 190\n",
    "Y_PADDING_FULLRES = 140\n",
    "X_PADDING = X_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_WIDTH)\n",
    "Y_PADDING = Y_PADDING_FULLRES * float(THUMBNAIL_WIDTH / PIXEL_COUNT_HEIGHT)\n",
    "ROOT_DIR = '/root/data/s3'\n",
    "OUTPUT_BASE_DIR = 'generated_optics_experiment_video'\n",
    "\n",
    "s3_access_utils = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _refresh_directory(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "\n",
    "def _get_bucket_key(url):\n",
    "    parsed_url = urlparse(url, allow_fragments=False)\n",
    "    if parsed_url.netloc.startswith('s3'):\n",
    "        url_components = parsed_url.path.lstrip('/').split('/')\n",
    "        bucket, key = url_components[0], os.path.join(*url_components[1:])\n",
    "    else:\n",
    "        bucket = parsed_url.netloc.split('.')[0]\n",
    "        key = parsed_url.path.lstrip('/')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def process_s3_key_dir(s3_key_dir, inbound_bucket='aquabyte-images-raw'):\n",
    "    try:\n",
    "        left_f = s3_access_utils.download_from_s3(inbound_bucket, os.path.join(s3_key_dir,\n",
    "                                                                               'left_frame.resize_512_512.jpg'))\n",
    "        right_f = s3_access_utils.download_from_s3(inbound_bucket, os.path.join(s3_key_dir,\n",
    "                                                                                'right_frame.resize_512_512.jpg'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # open images and metadata files\n",
    "    left_im = Image.open(left_f)\n",
    "    right_im = Image.open(right_f)\n",
    "#     crop_metadata = json.load(open(crop_metadata_f))\n",
    "\n",
    "\n",
    "    # draw boxes on images\n",
    "    left_draw = ImageDraw.Draw(left_im)\n",
    "    right_draw = ImageDraw.Draw(right_im)\n",
    "\n",
    "\n",
    "    # stitch images\n",
    "    result = Image.new('RGB', (2 * THUMBNAIL_WIDTH, THUMBNAIL_WIDTH))\n",
    "    result.paste(im=left_im, box=(0, 0))\n",
    "    result.paste(im=right_im, box=(THUMBNAIL_WIDTH, 0))\n",
    "\n",
    "    # write timestamp on stitched image\n",
    "    result_draw = ImageDraw.Draw(result)\n",
    "    ts = [c for c in left_f.split('/') if c.startswith('at=')][0]\n",
    "    display_ts = 'UTC Time: {}'.format(ts.replace('at=', ''))\n",
    "#     display_depth = 'Depth: {}m'.format(depth)\n",
    "    result_draw.text((0, 0), display_ts, (255, 255, 255))\n",
    "#     result_draw.text((0, 10), display_depth, (255, 255, 255))\n",
    "\n",
    "    output_f = left_f.replace(ROOT_DIR, OUTPUT_BASE_DIR).replace('left_', 'stereo_')\n",
    "    if not os.path.exists(os.path.dirname(output_f)):\n",
    "        os.makedirs(os.path.dirname(output_f))\n",
    "    result.save(output_f)\n",
    "\n",
    "\n",
    "def stitch_frames_into_video(image_fs, video_f):\n",
    "    im = cv2.imread(image_fs[0])\n",
    "    height, width, layers = im.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(video_f, fourcc, 4, (width, height), True)\n",
    "    for idx, image_f in enumerate(image_fs):\n",
    "        if idx % 1000 == 0:\n",
    "            print(idx)\n",
    "        im = cv2.imread(image_f, cv2.IMREAD_COLOR)\n",
    "        video.write(im)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    print('Video generation complete!')\n",
    "\n",
    "\n",
    "# def _captured_in_hour_range(key, start_hour, end_hour):\n",
    "#     hour = int([component for component in key.split('/') if component.startswith('hour=')][0].split('=')[-1])\n",
    "#     return start_hour <= hour <= end_hour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_video(pen_id, date, start_hour, end_hour, upload_to_s3=True, video_bucket='aquabyte-images-adhoc',\n",
    "                   num_processes=20):\n",
    "\n",
    "    # refresh output directory (i.e. clean out its contents)\n",
    "    _refresh_directory(OUTPUT_BASE_DIR)\n",
    "\n",
    "    # extract s3 keys\n",
    "    print('Extracting s3 keys...')\n",
    "    s3_key_dirs = extract_s3_keys(pen_id, date, start_hour, end_hour)\n",
    "    print('S3 keys extraction complete!')\n",
    "\n",
    "    print('Generating frames...')\n",
    "    pool = Pool(num_processes)\n",
    "    pool.map(process_s3_key_dir, s3_key_dirs)\n",
    "    print('Frame generation complete!')\n",
    "\n",
    "    print('Generating video...')\n",
    "    image_fs = sorted(\n",
    "        filter(lambda path: 'stereo' in path, glob.glob(os.path.join(OUTPUT_BASE_DIR, '**', '*.jpg'), recursive=True)))\n",
    "    video_f = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}_date_{}_video.avi'.format(str(pen_id), date))\n",
    "    stitch_frames_into_video(image_fs, video_f)\n",
    "    print('Video generation complete!')\n",
    "\n",
    "    if upload_to_s3:\n",
    "        print('Uploading to S3...')\n",
    "        video_key = os.path.join('videos', str(pen_id), os.path.basename(video_f))\n",
    "        s3_access_utils.s3_client.upload_file(video_f, video_bucket, video_key)\n",
    "        print('Upload complete! Result available here: {}'.format(os.path.join(video_bucket, video_key)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def extract_s3_keys(pen_id, date, start_hour, end_hour, subsample, inbound_bucket='aquabyte-images-raw'):\n",
    "    key = \"environment=production/site-id=40/pen-id=61\"\n",
    "#     s3_folder = os.path.join(key, 'date={}'.format(date))\n",
    "#     generator = s3_access_utils.get_matching_s3_keys(inbound_bucket, s3_folder, suffixes=['capture.json'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     keys = [key for key in generator if _captured_in_hour_range(key, start_hour, end_hour)]\n",
    "#     s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))\n",
    "#     return s3_key_dirs\n",
    "\n",
    "\n",
    "    hours = np.arange(start_hour, end_hour + 1, 1)\n",
    "    keys = []\n",
    "    for hour in hours:\n",
    "        s3_folder = os.path.join(key, 'date={}'.format(date), 'hour={}'.format(str(hour).zfill(2)))\n",
    "        generator = s3_access_utils.get_matching_s3_keys(inbound_bucket, prefix=s3_folder, subsample=subsample,\n",
    "                                                         suffixes=['capture.json'])\n",
    "        \n",
    "        these_keys = [key for key in generator]\n",
    "        print(len(these_keys))\n",
    "        keys.extend(these_keys)\n",
    "    s3_key_dirs = sorted(list(set([os.path.dirname(f) for f in keys])))\n",
    "    return s3_key_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id, start_hour, end_hour, subsample, upload_to_s3 = 61, 0, 24, 0.1, True\n",
    "num_processes = 12\n",
    "dates = ['2020-05-04', \n",
    "         '2020-05-05', \n",
    "         '2020-05-06', \n",
    "         '2020-05-07',\n",
    "         '2020-05-08',\n",
    "         '2020-05-09'\n",
    "        ]\n",
    "video_bucket = 'aquabyte-images-adhoc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    _refresh_directory(OUTPUT_BASE_DIR)\n",
    "    s3_key_dirs = extract_s3_keys(pen_id, date, start_hour, end_hour, subsample)\n",
    "\n",
    "    len(s3_key_dirs)\n",
    "\n",
    "\n",
    "    pool = Pool(num_processes)\n",
    "    pool.map(process_s3_key_dir, s3_key_dirs)\n",
    "\n",
    "\n",
    "    print('Generating video...')\n",
    "    image_fs = sorted(\n",
    "        filter(lambda path: 'stereo' in path, glob.glob(os.path.join(OUTPUT_BASE_DIR, '**', '*.jpg'), recursive=True)))\n",
    "\n",
    "    video_f = os.path.join(OUTPUT_BASE_DIR, 'pen_id_{}_date_{}_start_{}_end_{}_subsample_{}_video.avi'.format(\n",
    "            str(pen_id), \n",
    "            date,\n",
    "            str(start_hour),\n",
    "            str(end_hour),\n",
    "            str(subsample)))\n",
    "    stitch_frames_into_video(image_fs, video_f)\n",
    "    print('Video generation complete!')\n",
    "\n",
    "\n",
    "\n",
    "    if upload_to_s3:\n",
    "        print('Uploading to S3...')\n",
    "        video_key = os.path.join(\"optics_experiment\", str(pen_id), os.path.basename(video_f))\n",
    "        s3_access_utils.s3_client.upload_file(video_f, video_bucket, video_key)\n",
    "        print('Upload complete! Result available here: {}'.format(os.path.join(video_bucket, video_key)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
