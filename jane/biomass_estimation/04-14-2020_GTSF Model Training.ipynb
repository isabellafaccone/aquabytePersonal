{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from research.weight_estimation.gtsf_data.gtsf_dataset import GTSFDataset\n",
    "from research.weight_estimation.gtsf_data.gtsf_augmentation import GTSFAugmentation\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras as keras\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load GTSF Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "gtsf_dataset = GTSFDataset('2019-03-01', '2020-02-10', akpd_scorer_url)\n",
    "df = gtsf_dataset.get_prepared_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perform Augmentation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.captured_at < '2019-09-20') & (df.median_depth < 1.0) & (df.akpd_score > 0.9)]\n",
    "gtsf_augmentation = GTSFAugmentation(df)\n",
    "y_bounds, max_jitter_std, trials = (0.5, 3.0), 10, 40\n",
    "augmented_df = gtsf_augmentation.generate_augmented_dataset(y_bounds, max_jitter_std, trials, random_seed=0)\n",
    "print(augmented_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.iloc[0][\"original_ann\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Train / Test Split </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_stabilized_input(augmented_df, mask=None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        X = augmented_df[mask].wkps.values\n",
    "        y = 1e-4 * augmented_df[mask].weight.values\n",
    "    else:\n",
    "        X = augmented_df.wkps.values\n",
    "        y = 1e-4 * augmented_df.weight.values\n",
    "    X = np.concatenate(X).reshape(X.shape[0], 8, 3)\n",
    "    \n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, :, 0] = 0.5 * X[:, :, 0] / X[:, :, 1]\n",
    "    X_new[:, :, 1] = 0.5 * X[:, :, 2] / X[:, :, 1]\n",
    "    X_new[:, :, 2] = 0.05 / X[:, :, 1]\n",
    "    X_new = X_new.reshape(-1, 24)\n",
    "    return X_new, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fish_ids = augmented_df.fish_id.unique()\n",
    "train_pct, val_pct, test_pct = 0.8, 0.1, 0.1\n",
    "train_cnt, val_cnt, test_cnt = np.random.multinomial(len(fish_ids), [train_pct, val_pct, test_pct])\n",
    "assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "np.random.shuffle(assignments)\n",
    "train_fish_ids = fish_ids[np.where(assignments == 0)]\n",
    "val_fish_ids = fish_ids[np.where(assignments == 1)]\n",
    "test_fish_ids = fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "train_mask = augmented_df.fish_id.isin(train_fish_ids)\n",
    "val_mask = augmented_df.fish_id.isin(val_fish_ids)\n",
    "test_mask = augmented_df.fish_id.isin(test_fish_ids)\n",
    "\n",
    "X_train, y_train = generate_stabilized_input(augmented_df, train_mask)\n",
    "X_val, y_val = generate_stabilized_input(augmented_df, val_mask)\n",
    "X_test, y_test = generate_stabilized_input(augmented_df, test_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train Neural Network in Keras </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_error_pct(y_pred, y_gt):\n",
    "    return np.mean(np.abs((y_pred - y_gt) / y_gt))\n",
    "\n",
    "def get_pct_diff(y_pred, y_gt):\n",
    "    return (np.mean(y_pred) - np.mean(y_gt)) / np.mean(y_gt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=24, activation='relu', name='layer_1'))\n",
    "\n",
    "model.add(Dense(128, activation='relu', name='layer_2'))\n",
    "\n",
    "model.add(Dense(64, activation='relu', name='layer_3'))\n",
    "\n",
    "model.add(Dense(1, activation='linear', name='output_layer'))\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0,\n",
    "                               patience=10,\n",
    "                               verbose=0, \n",
    "                               mode='auto')\n",
    "\n",
    "log_dir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "logger = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',\n",
    "    write_graph=True,\n",
    "    histogram_freq=5\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error',  metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                    callbacks=[earlystopping, tensorboard_callback], batch_size=64, epochs=500)\n",
    "y_pred = model.predict(X_test).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"abs error percentage is {}\".format(get_abs_error_pct(y_pred, y_test)))\n",
    "print(\"Percentage difference between predicted average and ground truth average {}\".format(get_pct_diff(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_original.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_original.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = np.array([[0.1, 0.3, 0.5], [0.2, 0.3], [0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, d1 in enumerate(dropout_rate[0]):\n",
    "    for j, d2 in enumerate(dropout_rate[1]):\n",
    "        for k, d3 in enumerate(dropout_rate[2]):\n",
    "            s = \"d1_{}_d2_{}_d3_{}\".format(d1, d2, d3)\n",
    "            print(s)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(256, input_dim=24, activation='relu', name='layer_1'))\n",
    "            model.add(Dropout(d1))\n",
    "            model.add(Dense(128, activation='relu', name='layer_2'))\n",
    "            model.add(Dropout(d2))\n",
    "            model.add(Dense(64, activation='relu', name='layer_3'))\n",
    "            model.add(Dropout(d3))\n",
    "            model.add(Dense(1, activation='linear', name='output_layer'))\n",
    "            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                           min_delta=0,\n",
    "                                           patience=10,\n",
    "                                           verbose=0, \n",
    "                                           mode='auto')]\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error',  metrics=['accuracy'])\n",
    "            history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                                callbacks=callbacks, batch_size=64, epochs=500)\n",
    "            y_pred = model.predict(X_test).squeeze()\n",
    "            \n",
    "            l1 = get_abs_error_pct(y_pred, y_test)\n",
    "            l2 = get_pct_diff(y_pred, y_test)\n",
    "            print(\"abs error percentage is {}\".format(l1))\n",
    "            print(\"Pct diff between predicted avg and ground truth avg {}\".format(l2))\n",
    "            \n",
    "\n",
    "            L1[(d1, d2, d3)] = l1\n",
    "            L2[(d1, d2, d3)] = l2\n",
    "            model_json = model.to_json()\n",
    "            with open(\"model_{}.json\".format(s), \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"model_{}.h5\".format(s))\n",
    "            print(\"Saved model to disk\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "logger = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',\n",
    "    write_graph=True,\n",
    "    histogram_freq=5\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, \n",
    "          1e-4 * y_train, \n",
    "          validation_data=(X_test, 1e-4 * y_test), \n",
    "          batch_size=25, \n",
    "          epochs=100, \n",
    "          callbacks=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras.losses.MSE( y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --host localhost --port 6006 --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
