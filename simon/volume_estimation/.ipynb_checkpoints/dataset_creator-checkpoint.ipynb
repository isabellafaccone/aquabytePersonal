{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import os \n",
    "from os.path import dirname\n",
    "import re\n",
    "import json \n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Create csv containing files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(dataset_name,\n",
    "               datas_list=['stereo_images', 'mask', 'depth_map', 'annotations']):\n",
    "    \"\"\"\n",
    "    Creates a dataframe containing all files names for the dataset\n",
    "    Input:\n",
    "       - datas_list: list datas from the dataset to include in the\n",
    "       dataframe\n",
    "       - dataset_name: name of the dataset\n",
    "    Output:\n",
    "        - dataframe object\n",
    "    \"\"\"\n",
    "    dataset_path = dirname(os.getcwd()) + '/data/' + str(dataset_name) + '/'\n",
    "    regex = re.compile(r'\\d+')\n",
    "    datas = {}\n",
    "    # Getting the files\n",
    "    for data_type in datas_list:\n",
    "        files = os.listdir(dataset_path + data_type)\n",
    "        if data_type in ['mask', 'stereo_images']:\n",
    "            right_files = [f for f in files if 'right' in f]\n",
    "            if 'right' in right_files:\n",
    "                right_files.remove('right')\n",
    "            right_id = [int(regex.findall(f)[0]) for f in right_files]\n",
    "            left_files = [f for f in files if 'left' in f]\n",
    "            if 'left' in left_files:\n",
    "                left_files.remove('left')\n",
    "            left_id = [int(regex.findall(f)[0]) for f in left_files] \n",
    "            datas[data_type + '_left'] = (left_files, left_id)\n",
    "            datas[data_type + '_right'] = (right_files, right_id)\n",
    "        else:\n",
    "            files_id = [int(regex.findall(f)[0]) for f in files]\n",
    "            datas[data_type] = ((files, files_id))\n",
    "    size = len(datas[datas.keys()[0]][0])\n",
    "    dataset = pd.DataFrame(index=range(size), columns=datas.keys())\n",
    "    \n",
    "    # Let's fill the dataframe now\n",
    "    for key in datas.keys():\n",
    "        for ix in range(size):\n",
    "            dataset[key][datas[key][1][ix]] = datas[key][0][ix]\n",
    "    dataset.to_csv(dataset_name + '.csv')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'blender_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = create_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_split):\n",
    "    \"\"\"\n",
    "    Split a dataframe randomly into train & test set\n",
    "    \"\"\"\n",
    "    train_ix = random.sample(range(len(df)), int(train_split * len(df)))\n",
    "    test_ix = list(set(df.index) - set(train_ix))\n",
    "    train_df = df.iloc[train_ix, :].reset_index(drop=True)\n",
    "    test_df = df.iloc[test_ix, :].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(datas, train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Wrap everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load a dataset for Volume estimation.\n",
    "    \n",
    "    Args:\n",
    "       - dataset_name\n",
    "       - datas_list: datas to keep in the dataset\n",
    "       - size: float between 0 & 1 (ex 0.7)\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, dataset_name,\n",
    "                 datas_list, size, target='volume'):\n",
    "        self.csv_file = csv_file\n",
    "        self.dataset_name = dataset_name\n",
    "        self.datas_list = datas_list\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        \n",
    "        # Create the csv file needed\n",
    "        if '.csv' not in csv_file:\n",
    "            csv_file + '.csv'\n",
    "        self.dataset = pd.read_csv(csv_file, index_col=0)\n",
    "        self.dataset = self.dataset[datas_list]\n",
    "        self.subsample_dataset()\n",
    "    \n",
    "    def subsample_dataset(self):\n",
    "        \"\"\"\n",
    "        Subsample dataset to a give size ratio of the whole dataset\n",
    "        \"\"\"\n",
    "        num_examples = int(self.size * len(self.dataset))\n",
    "        self.dataset = self.dataset.sample(num_examples).reset_index(drop=True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        input = {}\n",
    "        dataset_dir = '../data/' + self.dataset_name \n",
    "        for data in self.dataset.columns:\n",
    "            file_name = self.dataset.loc[index, data]\n",
    "            if '_right' in data : \n",
    "                file_path = dataset_dir + '/' + data.replace('_right', '') + '/' + file_name\n",
    "            elif '_left' in data:\n",
    "                file_path = dataset_dir + '/' + data.replace('_left', '') + '/' + file_name\n",
    "            else: \n",
    "                file_path = dataset_dir + '/' + data + '/' + file_name\n",
    "            if 'npy' in file_name:\n",
    "                if data == 'depth_map':\n",
    "                    input[data] = np.load(file_path).T\n",
    "                else:\n",
    "                    input[data] = np.load(file_path)\n",
    "            elif 'png' in file_name:\n",
    "                input[data] = cv2.imread(file_path)\n",
    "            elif 'json' in file_name:\n",
    "                with open(file_path) as f:\n",
    "                    label = json.load(f)[self.target]\n",
    "            \n",
    "        return input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'blender_v3.csv'\n",
    "dataset_name = 'blender_v3'\n",
    "datas_list = ['depth_map', 'mask_right', 'annotations']\n",
    "size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VolumeDataset(csv_file=csv_file,\n",
    "                              dataset_name=dataset_name, \n",
    "                              datas_list=datas_list, \n",
    "                              size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = train_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
