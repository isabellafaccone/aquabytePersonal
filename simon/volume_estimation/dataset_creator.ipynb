{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import os \n",
    "from os.path import dirname\n",
    "import re\n",
    "import json \n",
    "import time\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom modules\n",
    "from data_utils import create_csv, split_train_test\n",
    "from data_loader import DataGenerator, CroperNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Create csv containing files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'blender_v3'\n",
    "datas = create_csv(dataset_name=dataset_name,datas_list=['depth_map', 'annotations', 'mask'])\n",
    "datas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Creating a DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating this class allows us to load data per data instead of the whole dataset in order to avoid to fill up RAM. These files should be saved in a python module instead of being defined in the notebook (same for all functions defined in these notebooks), since I am coding from my mac I cannot mount the root/ on my computer and develop in sublimetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = DataGenerator(dataframe=datas,\n",
    "                              dataset_name=dataset_name,\n",
    "                              datas_list=['depth_map', 'mask_left', 'annotations'],\n",
    "                              target_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Depth map masked & normalized dataset creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could integrate this function to the DataLoader but this would take time during training and at inference, therefore we create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_dataset:\n",
    "    processor = CroperNormalizer()\n",
    "    dataset_name = 'blender_v3'\n",
    "    datas = pd.read_csv(dataset_name + '.csv')\n",
    "    whole_dataset = DataGenerator(dataframe=datas,\n",
    "                                  dataset_name=dataset_name,\n",
    "                                  datas_list=['depth_map', 'mask_left', 'annotations'],\n",
    "                                  target_list=None)\n",
    "    for ix in range(len(whole_dataset)):\n",
    "        data = whole_dataset[ix]\n",
    "        input_data, label_data = processor.process_dmap(data)\n",
    "        np.save('../data/blender_v3_croped_normalized_dmap_300_100/depth_map/depth_map_' + str(ix) + '.npy',\n",
    "            input_data.T)\n",
    "        with open( '../data/blender_v3_croped_normalized_dmap_300_100/annotations/annot_' + str(ix) + '.json', 'w') as fp:\n",
    "            json.dump(label_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Loading the dataset & visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'blender_v3_croped_normalized_dmap_300_100'\n",
    "datas_list = ['depth_map', 'annotations']\n",
    "datas = pd.read_csv(dataset_name + '.csv', index_col=0)\n",
    "dataset = DataGenerator(dataframe=datas,\n",
    "                        dataset_name=dataset_name,\n",
    "                        datas_list=datas_list,\n",
    "                        target_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = random.randint(0, len(dataset))\n",
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[img_id][0]['depth_map'], cmap='hot')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[img_id][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
