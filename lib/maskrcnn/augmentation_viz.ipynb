{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '../libraries')\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "import mrcnn.model as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "from mrcnn.model import log\n",
    "import mcoco.coco as coco\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "BASE_DIR = '/root/data/models/erko/mask_rcnn_instance_segmentation'\n",
    "DATA_DIR = '/root/data/aquabyte-images/erko/'\n",
    "WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #0. Get the datasets ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib.patches as patches\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train  = coco.CocoDataset()\n",
    "dataset_train.load_coco('/root/data/aquabyte-images/cocofiles/coco_clear_full_curved_2018-09-07.json', class_ids=[0])\n",
    "dataset_train.load_coco('/root/data/aquabyte-images/cocofiles/coco_clear_full_curved_2018-09-08.json', class_ids=[0])\n",
    "# dataset_train.load_coco('/root/data/aquabyte-images/cocofiles/coco_clear_full_curved_2018-09-10.json', class_ids=[0])\n",
    "# dataset_train.load_coco('/root/data/aquabyte-images/cocofiles/coco_clear_full_curved_2018-09-11.json', class_ids=[0])\n",
    "dataset_train.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_id = np.random.choice(dataset_train.image_ids)\n",
    "image = dataset_train.load_image(random_image_id)\n",
    "mask = dataset_train.load_mask(random_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f ,ax = plt.subplots(1, figsize=(20, 20))\n",
    "ax.imshow(image); \n",
    "for k in range(mask[0].shape[-1]):\n",
    "    # draw mask\n",
    "    m = mask[0][...,k]\n",
    "    x, y = np.nonzero(m)\n",
    "    img = np.ones( (m.shape[0], m.shape[1], 3) )\n",
    "    color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "    for i in range(3):\n",
    "        img[:,:,i] = color_mask[i]\n",
    "    ax.imshow(np.dstack( (img, m*0.3) ))\n",
    "    \n",
    "    # draw bbox\n",
    "    bbox = [min(y), min(x), max(y)-min(y), max(x)-min(x)]\n",
    "    rec = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                            edgecolor='w', facecolor=None, fill=False, linestyle='--', linewidth=2)\n",
    "    ax.add_patch(rec)\n",
    "    \n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 1024\n",
    "rpn_anchor_template = (1, 2, 4, 8, 16) # anchor sizes in pixels\n",
    "rpn_anchor_scales = tuple(i * (image_size // 16) for i in rpn_anchor_template)\n",
    "\n",
    "class FishConfig(Config):\n",
    "    \"\"\"Configuration for training on the shapes dataset.\n",
    "    \"\"\"\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 2 images per GPU. Put multiple images on each\n",
    "    # GPU if the images are small. Batch size is 2 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 3 shapes (triangles, circles, and squares)\n",
    "\n",
    "    # Use smaller images for faster training. \n",
    "    IMAGE_MAX_DIM = image_size\n",
    "    IMAGE_MIN_DIM = image_size\n",
    "    \n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = rpn_anchor_scales\n",
    "\n",
    "    # Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    STEPS_PER_EPOCH = 400\n",
    "\n",
    "    VALIDATION_STEPS = STEPS_PER_EPOCH / 20\n",
    "    USE_MINI_MASK= False\n",
    "config = FishConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [\n",
    "    OneOf([RandomSizedCrop(min_max_height=(50, 101), height=512, width=512, p=0.5),\n",
    "          PadIfNeeded(min_height=512, min_width=512, p=0.5)], p=1),    \n",
    "    VerticalFlip(p=0.5),              \n",
    "    RandomRotate90(p=0.5),\n",
    "    OneOf([\n",
    "        ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "        GridDistortion(p=0.5),\n",
    "        OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n",
    "        ], p=0.8),\n",
    "    CLAHE(p=0.8),\n",
    "    RandomContrast(p=0.8),\n",
    "    RandomBrightness(p=0.8),\n",
    "    RandomGamma(p=0.8)], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [   \n",
    "    RandomContrast(p=0.0),\n",
    "    CLAHE(p=0.0),\n",
    "], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, image_meta, gt_class_ids, gt_boxes, gt_masks = modellib.load_image_gt(dataset_train, config, 1, \\\n",
    "                  augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(gt_masks.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [   \n",
    "    RandomContrast(p=0.6),\n",
    "    CLAHE(p=0.6),\n",
    "], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, image_meta, gt_class_ids, gt_boxes, gt_masks = modellib.load_image_gt(dataset_train, config, 1, \\\n",
    "                  augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(gt_masks.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Random crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [   \n",
    "    OneOf([RandomSizedCrop(min_max_height=(300, 300), height=1024, width=1024, p=1.0),\n",
    "          PadIfNeeded(min_height=1024, min_width=1024, p=0.5)], p=1.0)], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, image_meta, gt_class_ids, gt_boxes, gt_masks = modellib.load_image_gt(dataset_train, config, 30, \\\n",
    "                  augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(gt_masks[..., 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [   \n",
    "   RandomBrightness(p=1.0),\n",
    "    RandomGamma(p=1.0)], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, image_meta, gt_class_ids, gt_boxes, gt_masks = modellib.load_image_gt(dataset_train, config, 1, \\\n",
    "                  augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(gt_masks[..., 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [   \n",
    "   OneOf([\n",
    "        OpticalDistortion(p=1, distort_limit=0.6, shift_limit=0.2)                  \n",
    "        ], p=1),], 'imgaug': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, image_meta, gt_class_ids, gt_boxes, gt_masks = modellib.load_image_gt(dataset_train, config, 1, \\\n",
    "                  augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "plt.imshow(gt_masks[..., 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since datas are rectified, not really relevant. We will start with basic things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {'albumentations': [\n",
    "    OneOf([RandomSizedCrop(min_max_height=(300, 300), height=1024, width=1024, p=0.5),\n",
    "          PadIfNeeded(min_height=1024, min_width=1024, p=0.5)], p=1),\n",
    "    CLAHE(p=0.6),\n",
    "    RandomContrast(p=0.6),\n",
    "    RandomBrightness(p=0.8),\n",
    "    RandomGamma(p=0.8)], 'imgaug': None}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
