{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from concurrent import futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_credentials = json.load(open('/root/thomas/aws_credentials.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', aws_access_key_id=aws_credentials[\"aws_access_key_id\"],\n",
    "                         aws_secret_access_key=aws_credentials[\"aws_secret_access_key\"],\n",
    "                         region_name=\"eu-west-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/root/data/fish_detections/aquabyte-crops/environment=production/site-id=23/pen-id=4/*/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todownload = []\n",
    "for f in files:\n",
    "    prefix = '/'.join(f.split('/')[5:])\n",
    "    for s3file in ['crops.json', 'left_frame.resize_512_512.jpg', 'right_frame.resize_512_512.jpg']:\n",
    "        todownload.append(os.path.join(prefix, s3file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_object_keys = []\n",
    "# for url in renamed_urls:\n",
    "#     key = '/'.join(url.split('/')[4:])\n",
    "#     s3_object_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/root/data/fish_detections/aquabyte-frames-resized-inbound/'\n",
    "bucket_name = 'aquabyte-frames-resized-inbound'\n",
    "s3_object_keys = todownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 30\n",
    "\n",
    "# abs_path = os.path.abspath(relative_path)\n",
    "\n",
    "def fetch(key):\n",
    "    file = '{}/{}'.format(path, key)\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)  \n",
    "    with open(file, 'wb') as data:\n",
    "        s3.download_fileobj(bucket_name, key, data)\n",
    "    return file\n",
    "\n",
    "\n",
    "def fetch_all(keys):\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_key = {executor.submit(fetch, key): key for key in keys}\n",
    "\n",
    "        print(\"All URLs submitted.\")\n",
    "\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "\n",
    "\n",
    "for key, result in fetch_all(s3_object_keys):\n",
    "    print('key: {}  result: {}'.format(key, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sql credentials stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sql_credentials = json.load(open('/root/thomas/sqlcredentials.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data to use for conv pose machine training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table, select, func, and_, insert, delete, update, or_\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.automap import automap_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\n",
    "    \"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], sql_credentials[\"password\"],\n",
    "                                         sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "                                         sql_credentials[\"database\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_class = sessionmaker(bind=engine)\n",
    "session = session_class()\n",
    "\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query all keypoints annotations done on BATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = Base.classes.keypoint_annotations\n",
    "detections = Base.classes.fish_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = session.query(detections) \\\n",
    "                 .filter(detections.created_at >= '2019-04-27') \\\n",
    "                 .filter(detections.pen_id == 1) \\\n",
    "                 .filter(detections.site_id == 23) \\\n",
    "                 .all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of fish detections: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in results:\n",
    "    tmp = {'left_image_url': d.left_image_url,\n",
    "           'right_image_url': d.right_image_url,\n",
    "           'site_id': d.site_id,\n",
    "           'pen_id': d.pen_id,\n",
    "            }\n",
    "    data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 freeze | grep req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "urls = []\n",
    "for d in tqdm(data):\n",
    "    # DOWNLOAD LEFT\n",
    "    url = d['left_image_url']  \n",
    "    urls.append(url)\n",
    "    s = url.split('at=')[1].split('/')[0]\n",
    "    ts = time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S.%f000Z\").timetuple())\n",
    "    date = str(datetime.datetime.utcfromtimestamp(ts).date())\n",
    "    folder = '/root/data/fish_detections/images/{}/{}/{}/'.format(d['site_id'], d['pen_id'], date)\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "    image_name = os.path.basename(url).replace('.jpg', str(int(ts)) + '.jpg')\n",
    "    destination = os.path.join(folder, image_name)\n",
    "    d['local_path'] = destination\n",
    "    d['date'] = date\n",
    "#     if os.path.isfile(destination):\n",
    "#         continue\n",
    "#     print(destination)\n",
    "#     count += 1\n",
    "#     f = open(destination,'wb')\n",
    "#     f.write(requests.get(url).content)\n",
    "#     f.close()\n",
    "    \n",
    "    # DOWNLOAD RIGHT\n",
    "    url = d['right_image_url']   \n",
    "    urls.append(url)\n",
    "    s = url.split('at=')[1].split('/')[0]\n",
    "    ts = time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S.%f000Z\").timetuple())\n",
    "    date = str(datetime.datetime.utcfromtimestamp(ts).date())\n",
    "    folder = '/root/data/fish_detections/images/{}/{}/{}/'.format(d['site_id'], d['pen_id'], date)\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "    image_name = os.path.basename(url).replace('.jpg', str(int(ts)) + '.jpg')\n",
    "    destination = os.path.join(folder, image_name)\n",
    "    d['local_path'] = destination\n",
    "    d['date'] = date\n",
    "#     if os.path.isfile(destination):\n",
    "#         continue\n",
    "#     print(destination)\n",
    "#     count += 1\n",
    "#     f = open(destination,'wb')\n",
    "#     f.write(requests.get(url).content)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_urls = []\n",
    "for url in urls:\n",
    "    url = url.replace('aquabyte-crops', 'aquabyte-frames-resized-inbound')\n",
    "    renamed_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(renamed_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open('/root/data/fish_detections/urls_{}.txt'.format(i), 'w') as f:\n",
    "        for url in subset:\n",
    "            f.write(\"%s\\n\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.arange(0, len(renamed_urls)+50000, 50000)\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for j in range(1, len(intervals)):\n",
    "    print(intervals[j])\n",
    "    if j == len(intervals) -1:\n",
    "        subset = renamed_urls[intervals[j-1]: ]\n",
    "#         print('lol')\n",
    "    else:\n",
    "        subset = renamed_urls[intervals[j-1]: intervals[j]]\n",
    "#     print(intervals[j-1], intervals[j])\n",
    "    with open('/root/data/fish_detections/urls_{}.txt'.format(i), 'w') as f:\n",
    "        for url in subset:\n",
    "            f.write(\"%s\\n\" % url)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labelbox format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    path = d['local_path']\n",
    "    side = os.path.basename(path).split('_')[0]\n",
    "    if side == 'left':\n",
    "        lab_kp = d['left_keypoints']\n",
    "    else:\n",
    "        lab_kp = d['right_keypoints']\n",
    "    \n",
    "    labels = {}\n",
    "    for kpp in lab_kp:\n",
    "        name = kpp['keypointType']\n",
    "        labels[name] = [{'geometry': {'x': kpp['xCrop'], 'y': kpp['yCrop']}}]\n",
    "    d['Label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(set([d['date'] for d in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-05-06',\n",
    " '2019-04-30',\n",
    " '2019-05-03',\n",
    " '2019-05-02',\n",
    " '2019-05-07',\n",
    " '2019-05-05',\n",
    " '2019-05-04',\n",
    " '2019-04-29',\n",
    " '2019-05-01',\n",
    " '2019-05-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in dates:\n",
    "    data_points = [d for d in data if d['date'] == dat]\n",
    "    print('{}: {}'.format(dat, len(data_points)))\n",
    "    with open('/root/data/bati/labels/labels_{}.json'.format(dat), 'w') as f:\n",
    "        json.dump(data_points, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('/root/data/bati/labels/labels.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_image_keypoints, DataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    \"\"\" \"\"\"\n",
    "    \"\"\"\n",
    "    General settings\n",
    "    \"\"\"\n",
    "    input_size = (512, 512)\n",
    "    keypoints_order = [\"TAIL_NOTCH\",\n",
    "                        \"ADIPOSE_FIN\",\n",
    "                        \"UPPER_LIP\",\n",
    "                        \"ANAL_FIN\",\n",
    "                        \"PELVIC_FIN\",\n",
    "                        \"EYE\",\n",
    "                        \"PECTORAL_FIN\",\n",
    "                        \"DORSAL_FIN\"]\n",
    "    augmentation = None\n",
    "    crop = False\n",
    "    batch_size = 8\n",
    "    input_size = (512, 512)\n",
    "    heatmap_size = 64\n",
    "    cpm_stages = 3\n",
    "    joint_gaussian_variance = 1.0\n",
    "    center_radius = 21\n",
    "    num_of_joints = 8\n",
    "    color_channel = 'RGB'\n",
    "    normalize = True\n",
    "    use_gpu = True\n",
    "    gpu_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(data[0]['local_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, kps = load_image_keypoints(data[0], FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(im)\n",
    "plt.scatter(kps[:, 0], kps[:, 1])\n",
    "for i in range(8):\n",
    "    plt.text(kps[i, 0], kps[i,1], FLAGS.keypoints_order[i], color='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(data, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, heatmaps = train_generator[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(xb[1, ...])\n",
    "i = 0\n",
    "for kpp in yb[1]:\n",
    "    plt.scatter(kpp[0], kpp[1], c=\"r\")\n",
    "    plt.text(kpp[0], kpp[1], FLAGS.keypoints_order[i], {\"color\": \"w\"})\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    n = (len(list(d['Label'].keys())))\n",
    "    if n != 8:\n",
    "        print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
