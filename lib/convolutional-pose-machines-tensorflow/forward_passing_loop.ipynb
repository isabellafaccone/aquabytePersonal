{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/root/data/models/keypoints_detection/2019_05_13_22_13_59//config.json' \n",
    "checkpoint_path = '/root/data/models/keypoints_detection/2019_05_13_22_13_59/weights/fish_test-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(config_path))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    \"\"\" \"\"\"\n",
    "    \"\"\"\n",
    "    General settings\n",
    "    \"\"\"\n",
    "    input_size = tuple(config[\"input_size\"])\n",
    "    stages = 3\n",
    "    joints = 8\n",
    "    model_path = checkpoint_path\n",
    "    cmap_radius = 21\n",
    "    batch_size = 1\n",
    "    keypoints_order = [\"TAIL_NOTCH\",\n",
    "                    \"ADIPOSE_FIN\",\n",
    "                    \"UPPER_LIP\",\n",
    "                    \"ANAL_FIN\",\n",
    "                    \"PELVIC_FIN\",\n",
    "                    \"EYE\",\n",
    "                    \"PECTORAL_FIN\",\n",
    "                    \"DORSAL_FIN\"]\n",
    "    normalize = True\n",
    "    heatmap_size = 64\n",
    "    joint_gaussian_variance = 1.0\n",
    "    augmentation = None\n",
    "    crop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import importlib\n",
    "from models.nets import fish_test\n",
    "from utils import cpm_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import pywrap_tensorflow\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_tensors_in_checkpoint_file(checkpoint_path, \"\", False,\n",
    "#                                      all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "# var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "# for key in sorted(var_to_shape_map):\n",
    "#     print(\"tensor_name: \", key, reader.get_tensor(key).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "tf_device = '/gpu:0'\n",
    "with tf.device(tf_device):\n",
    "    model = fish_test.CPM_Model(FLAGS.input_size, 2, FLAGS.stages, FLAGS.joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cmap_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, FLAGS.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_center_map = cpm_utils.gaussian_img(FLAGS.input_size[0],\n",
    "                                         FLAGS.input_size[0], \n",
    "                                         FLAGS.input_size[0] / 2,\n",
    "                                         FLAGS.input_size[0] / 2,\n",
    "                                         FLAGS.cmap_radius)\n",
    "test_center_map = np.reshape(test_center_map, [1, FLAGS.input_size[0], FLAGS.input_size[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from utils.utils import DataGenerator, load_image_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = glob.glob(\"/root/data/bati/labels/labels_*\")\n",
    "# annotations = ['/root/data/gtsf_phase_I/2019-05-02/2019-05-02_cogito_annotations.json']\n",
    "print(\"Total number of gtsf sessions: {}\".format(len(annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(len(annotations)*0.8)\n",
    "random.shuffle(annotations)\n",
    "train_files = annotations[:cutoff]\n",
    "val_files = annotations[cutoff:]\n",
    "print(\"Number of train files: {}\".format(len(train_files)))\n",
    "print(\"Number of validation files: {}\".format(len(val_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotations = []\n",
    "for jpath in val_files:\n",
    "    val_annotations += json.load(open(jpath))\n",
    "print(\"Number of validation data: {}\".format(len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Froward pass loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rez = {}\n",
    "high_rez = {}\n",
    "\n",
    "for ann in tqdm(val_annotations):\n",
    "    ann[\"predictions\"] = []\n",
    "    img_input, kp_resized = load_image_keypoints(ann, FLAGS)\n",
    "\n",
    "    img_input  = img_input / 255.0 - 0.5\n",
    "    img_input = img_input[np.newaxis, ...]\n",
    "    image, keypoints = load_image_keypoints(ann, FLAGS, reshape=False)\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "#     xs = keypoints[:, 0]\n",
    "#     min_x = np.max([np.min(xs) - 100, 0])\n",
    "#     ys = keypoints[:, 1]\n",
    "#     min_y = np.max([np.min(ys) - 100, 0])\n",
    "    \n",
    "    with tf.device(tf_device):   \n",
    "        predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap,\n",
    "                                                  model.stage_heatmap,\n",
    "                                                  ],\n",
    "                                                 feed_dict={'input_placeholder:0': img_input,\n",
    "                                                            'cmap_placeholder:0': test_center_map})\n",
    "    final_stage_heatmap = predict_heatmap.squeeze()  \n",
    "    todelete = []\n",
    "#     # manhattan distance\n",
    "#     for c in range(8):\n",
    "#         hm = cv2.resize(final_stage_heatmap[..., c], FLAGS.input_size)\n",
    "#         hm_max = np.where(hm == hm.max())\n",
    "#         pred_kp = np.array([hm_max[1][0], hm_max[0][0]])\n",
    "#         todelete.append(pred_kp)\n",
    "#         gt_kp = np.array(kp_resized[c, :], dtype=np.uint32)\n",
    "#         man_dist = np.sum(np.abs(pred_kp - gt_kp))\n",
    "#         if FLAGS.keypoints_order[c] not in low_rez:\n",
    "#             low_rez[FLAGS.keypoints_order[c]] = []\n",
    "#         low_rez[FLAGS.keypoints_order[c]].append(man_dist)\n",
    "#         ann[\"predictions\"].append(pred_kp.tolist())\n",
    "#         # print(\"Manhattan distance between pred and gt {} for {}\".format(man_dist, FLAGS.keypoints_order[c]))\n",
    "\n",
    "    #  # manhattan distance\n",
    "    for c in range(8):\n",
    "        hm = cv2.resize(final_stage_heatmap[..., c], (width, height))\n",
    "        hm_max = np.where(hm == hm.max())\n",
    "        pred_kp = np.array([hm_max[1][0], hm_max[0][0]])\n",
    "        # pred_kp += np.array([min_x, min_y])\n",
    "        gt_kp = np.array(keypoints[c, :], dtype=np.uint32)\n",
    "        man_dist = np.sum(np.abs(pred_kp - gt_kp))\n",
    "        if FLAGS.keypoints_order[c] not in high_rez:\n",
    "            high_rez[FLAGS.keypoints_order[c]] = []\n",
    "        high_rez[FLAGS.keypoints_order[c]].append(man_dist)\n",
    "        ann[\"predictions\"].append(pred_kp.tolist())\n",
    "    \n",
    "    \n",
    "#     full = cv2.imread(ann['local_path'])\n",
    "#     predictions = np.array(ann[\"predictions\"])\n",
    "#     plt.imshow(full)\n",
    "#     plt.scatter(predictions[:, 0], predictions[:, 1], c='b')\n",
    "#     plt.scatter(keypoints[:, 0], keypoints[:, 1], c='r', alpha=0.5)\n",
    "#     plt.show()\n",
    "#     break\n",
    "        # print(\"Manhattan distance between pred and gt {} for {}\".format(man_dist, FLAGS.keypoints_order[c]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = {k: [] for k in FLAGS.keypoints_order}\n",
    "for ann in tqdm(val_annotations):\n",
    "    if 'predictions' not in ann:\n",
    "        continue\n",
    "    if len(ann['predictions']) == 0:\n",
    "        continue\n",
    "    \n",
    "    keypoints = []\n",
    "    for kp_name in FLAGS.keypoints_order:\n",
    "        value = ann[\"Label\"][kp_name]\n",
    "        keypoints.append([int(value[0][\"geometry\"][\"x\"]), \n",
    "                          int(value[0][\"geometry\"][\"y\"])])\n",
    "    keypoints = np.array(keypoints)\n",
    "    predictions = np.array(ann['predictions'])\n",
    "    \n",
    "    for i, k in enumerate(FLAGS.keypoints_order):\n",
    "        man_dist = np.sum(np.abs(keypoints[i, :] - predictions[i, :]))\n",
    "        distances[k].append(man_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in distances.items():\n",
    "    print('Average error for {} is {}'.format(k, np.mean(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total average man dist: {}'.format(np.array(list(distances.values())).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = np.random.choice(val_annotations)\n",
    "full = cv2.imread(ann['local_path'])\n",
    "img_input, kp_resized = load_image_keypoints(ann, FLAGS)\n",
    "predictions = np.array(ann[\"predictions\"])\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img_input)\n",
    "plt.scatter(predictions[:, 0], predictions[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/thomas/biomass_kp_predictions_val.json', 'w') as f:\n",
    "    json.dump(val_annotations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k,v) in low_rez.items():\n",
    "    print(\"Average error on {}x{} images for keypoint {} is {}\".format(FLAGS.input_size[0],\n",
    "                                                                    FLAGS.input_size[0],k, np.median(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k,v) in high_rez.items():\n",
    "    print(\"Average error on full rez images for keypoint {} is {}\".format( k, np.median(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = '190313010007'\n",
    "debug = '190301010006'\n",
    "debug = '190313010009'\n",
    "debug = '190301010014'\n",
    "debug = '190313010011'\n",
    "debug = '190313010009'\n",
    "# bug = '190313010008'\n",
    "\n",
    "debug_ann = [ann for ann in val_annotations if ann[\"timestamp\"] == debug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, keypoints = load_image_keypoints(debug_ann[0], FLAGS, reshape=False)\n",
    "preds = np.array(debug_ann[0][\"predictions\"])\n",
    "f, ax = plt.subplots(1, figsize=(20,10))\n",
    "ax.imshow(image)\n",
    "ax.scatter(keypoints[:, 0], keypoints[:, 1], c=\"r\")\n",
    "ax.scatter(preds[:, 0], preds[:, 1], c=\"y\")\n",
    "for (i, t) in enumerate(FLAGS.keypoints_order):\n",
    "    ax.text(preds[i, 0], preds[i, 1], t, {\"color\": \"w\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, keypoints = load_image_keypoints(debug_ann[1], FLAGS, reshape=False)\n",
    "preds = np.array(debug_ann[1][\"predictions\"])\n",
    "f, ax = plt.subplots(1, figsize=(20,10))\n",
    "ax.imshow(image)\n",
    "ax.scatter(keypoints[:, 0], keypoints[:, 1], c=\"r\")\n",
    "ax.scatter(preds[:, 0], preds[:, 1], c=\"y\")\n",
    "for (i, t) in enumerate(FLAGS.keypoints_order):\n",
    "    ax.text(preds[i, 0], preds[i, 1], t, {\"color\": \"w\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disparity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_ann_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs per timestamp\n",
    "pairs = {}\n",
    "for ann in val_annotations:\n",
    "    if ann[\"species\"] != \"salmon\":\n",
    "        continue\n",
    "    if ann[\"kfactor\"] < 0.3:\n",
    "        continue\n",
    "    timestamp = ann[\"timestamp\"]\n",
    "    side = os.path.basename(ann[\"local_path\"]).split(\"_\")[0]\n",
    "    ann[\"side\"] = side\n",
    "    if timestamp not in pairs:\n",
    "        pairs[timestamp] = {}\n",
    "    pairs[timestamp][side] = ann\n",
    "\n",
    "full_pairs = [k for (k, v)in pairs.items() if \"left\" in v and \"right\" in v]\n",
    "print(\"Number of full pairs: {}\".format(len(full_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_disparities = []\n",
    "pred_disparities= []\n",
    "\n",
    "for ts in full_pairs:\n",
    "    left_ann = pairs[ts][\"left\"]\n",
    "    right_ann = pairs[ts][\"right\"]\n",
    "    \n",
    "    left_keypoints = load_ann_keypoints(left_ann, FLAGS.keypoints_order)\n",
    "    xmin = left_keypoints[:, 1].min()\n",
    "    ymin = left_keypoints[:, 0].min()\n",
    "    left_predictions = np.array(left_ann[\"predictions\"])\n",
    "    left_predictions[:, 0] += ymin-buffer\n",
    "    left_predictions[:, 1] += xmin-buffer\n",
    "    \n",
    "    right_keypoints = load_ann_keypoints(right_ann, FLAGS.keypoints_order)\n",
    "    xmin = right_keypoints[:, 1].min()\n",
    "    ymin = right_keypoints[:, 0].min()\n",
    "    right_predictions = np.array(right_ann[\"predictions\"])\n",
    "    right_predictions[:, 0] += ymin-buffer\n",
    "    right_predictions[:, 1] += xmin-buffer\n",
    "    \n",
    "    ground_truth_disparities.append(left_keypoints[:, 0] - right_keypoints[:, 0])\n",
    "    pred_disparities.append(left_predictions[:, 0] - right_predictions[:, 0])\n",
    "    \n",
    "ground_truth_disparities = np.array(ground_truth_disparities)\n",
    "pred_disparities = np.array(pred_disparities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pairs[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = np.mean(ground_truth_disparities - pred_disparities)\n",
    "print(\"Mean disparity error: {}\".format(mean_error))\n",
    "relative_error = np.mean((ground_truth_disparities - pred_disparities)*100 / ground_truth_disparities)\n",
    "print(\"Mean relative disparity error: {}\".format(relative_error))\n",
    "abs_relative_error = np.mean(np.abs((ground_truth_disparities - pred_disparities)*100 / ground_truth_disparities))\n",
    "print(\"Mean absolute relative disparity error: {}\".format(abs_relative_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = ground_truth_disparities - pred_disparities\n",
    "np.where(error > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pairs[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
