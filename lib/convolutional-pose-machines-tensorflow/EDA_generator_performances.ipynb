{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    \"\"\" \"\"\"\n",
    "    \"\"\"\n",
    "    General settings\n",
    "    \"\"\"\n",
    "    input_size = (1024, 1024)\n",
    "    heatmap_size = 128\n",
    "    cpm_stages = 4\n",
    "    joint_gaussian_variance = 1.0\n",
    "    center_radius = 21\n",
    "    num_of_joints = 8\n",
    "    color_channel = 'RGB'\n",
    "    normalize = True\n",
    "    use_gpu = True\n",
    "    gpu_id = 0\n",
    "    \n",
    "    gradient_clipping = True # gradient clipping\n",
    "\n",
    "    \"\"\"\n",
    "    Demo settings\n",
    "    \"\"\"\n",
    "    # 'MULTI': show multiple stage heatmaps\n",
    "    # 'SINGLE': show last stage heatmap\n",
    "    # 'Joint_HM': show last stage heatmap for each joint\n",
    "    # 'image or video path': show detection on single image or video\n",
    "    DEMO_TYPE = 'SINGLE'\n",
    "\n",
    "    model_path = 'cpm_hand'\n",
    "    cam_id = 0\n",
    "\n",
    "    use_kalman = True\n",
    "    kalman_noise = 0.03\n",
    "    keypoints_order = [\"TAIL_NOTCH\",\n",
    "                        \"ADIPOSE_FIN\",\n",
    "                        \"UPPER_LIP\",\n",
    "                        \"ANAL_FIN\",\n",
    "                        \"PELVIC_FIN\",\n",
    "                        \"EYE\",\n",
    "                        \"PECTORAL_FIN\",\n",
    "                        \"DORSAL_FIN\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Training settings\n",
    "    \"\"\"\n",
    "    network_def = 'fish_test'\n",
    "    train_img_dir = ''\n",
    "    val_img_dir = ''\n",
    "    bg_img_dir = ''\n",
    "    pretrained_model = 'fish_test'\n",
    "    batch_size = 4\n",
    "    init_lr = 0.001\n",
    "    lr_decay_rate = 0.45\n",
    "    lr_decay_step = 8000\n",
    "    augmentation = None\n",
    "    buffer_range = [int(n) for n in np.arange(100, 600, 100)] # useless if crop = False\n",
    "    crop = False # crop input image based on keypoints - for GTSF only\n",
    "#     augmentation = albu.Compose([albu.HorizontalFlip(p=0.5),\n",
    "# #                                  albu.Rotate(limit=10, p=1.0)\n",
    "#                                 ], \n",
    "#                                  p=1.0,\n",
    "#                                  keypoint_params={'format': 'xy'})\n",
    "    \n",
    "    epochs=200\n",
    "\n",
    "    hnm = True  # Make sure generate hnm files first\n",
    "    do_cropping = True\n",
    "\n",
    "    \"\"\"\n",
    "    For Freeze graphs\n",
    "    \"\"\"\n",
    "    output_node_names = 'stage_3/mid_conv7/BiasAdd:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import DataGenerator, data_generator\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open('/root/data/bati/labels/labels_2019-05-10.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann in annotations:\n",
    "    ann['local_path'] = ann['local_path'].replace('/app', '/root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_generator(annotations, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    img, kp, hm = next(gen)\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @background(max_prefetch=10)\n",
    "# def data_generator_async(annotations, FLAGS, nsteps=1000):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         xbatch = []\n",
    "#         ybatch = []\n",
    "#         start = index*FLAGS.batch_size\n",
    "#         for i in range(FLAGS.batch_size):\n",
    "#             buffer = np.random.choice(FLAGS.buffer_range)\n",
    "#             image, keypoints = load_image_keypoints(annotations[start + i], \n",
    "#                                                     FLAGS,\n",
    "#                                                     reshape=True,\n",
    "#                                                     buffer=buffer)\n",
    "#             xbatch.append(image)\n",
    "#             ybatch.append(keypoints)\n",
    "#         xbatch = np.array(xbatch)\n",
    "#         if FLAGS.normalize:\n",
    "#             xbatch = xbatch / 255.0 - 0.5\n",
    "#         else:\n",
    "#             xbatch -= 128.0\n",
    "#         ybatch = np.array(ybatch)\n",
    "#         batch_gt_heatmap_np = make_heatmaps_from_joints(FLAGS.input_size[0],\n",
    "#                                                         FLAGS.heatmap_size,\n",
    "#                                                         FLAGS.joint_gaussian_variance,\n",
    "#                                                         ybatch)\n",
    "#         index += 1\n",
    "#         if index >= nsteps:\n",
    "#             index = 0\n",
    "#             random.shuffle(annotations)\n",
    "#         yield xbatch, ybatch, batch_gt_heatmap_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async_gen = data_generator_async(annotations, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# for _ in range(100):\n",
    "#     img, kp, hm = next(async_gen)\n",
    "# end = time.time()\n",
    "# print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_image_keypoints, make_heatmaps_from_joints\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# xbatch = []\n",
    "# ybatch = []\n",
    "# for i in range(4):\n",
    "#     buffer = np.random.choice(FLAGS.buffer_range)\n",
    "#     image, keypoints = load_image_keypoints(annotations[i], \n",
    "#                                             FLAGS,\n",
    "#                                             reshape=True,\n",
    "#                                             buffer=buffer)\n",
    "#     xbatch.append(image)\n",
    "#     ybatch.append(keypoints)   \n",
    "# end = time.time()\n",
    "# print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# pool = Pool(4)\n",
    "# pool.apply()\n",
    "# end = time.time()\n",
    "# print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = DataGenerator(annotations, FLAGS)\n",
    "# start = time.time()\n",
    "# for i in range(100):\n",
    "#     output = train_generator[i]\n",
    "# end = time.time()\n",
    "# print(\"Loading time: {}\".format((end - start)/100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_async(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, annotations_set, FLAGS):\n",
    "        'Initialization'\n",
    "        self.annotations = annotations_set\n",
    "        self.on_epoch_end()\n",
    "        self.FLAGS = FLAGS\n",
    "        self.batches = []\n",
    "        self._preload_batches()\n",
    "        \n",
    "    def _log_result(self, result):\n",
    "        self.batches.append(result)\n",
    "        \n",
    "    def _load_batch(self, anns):\n",
    "        xbatch = []\n",
    "        ybatch = []\n",
    "        for ann in anns:\n",
    "            buffer = np.random.choice(self.FLAGS.buffer_range)\n",
    "            image, keypoints = load_image_keypoints(ann, \n",
    "                                                    self.FLAGS,\n",
    "                                                    reshape=True,\n",
    "                                                    buffer=buffer)\n",
    "            xbatch.append(image)\n",
    "            ybatch.append(keypoints)\n",
    "        \n",
    "        xbatch = np.array(xbatch)\n",
    "        if self.FLAGS.normalize:\n",
    "            xbatch = xbatch / 255.0 - 0.5\n",
    "        else:\n",
    "            xbatch -= 128.0\n",
    "        ybatch = np.array(ybatch)\n",
    "        batch_gt_heatmap_np = make_heatmaps_from_joints(self.FLAGS.input_size[0],\n",
    "                                                        self.FLAGS.heatmap_size,\n",
    "                                                        self.FLAGS.joint_gaussian_variance,\n",
    "                                                        ybatch)   \n",
    "        return xbatch, ybatch, batch_gt_heatmap_np\n",
    "            \n",
    "    \n",
    "    def _preload_batches(self):\n",
    "#         num_batches = int(math.ceil(len(self.annotations) / 4))\n",
    "        num_batches = 1000\n",
    "        pool = Pool(12)\n",
    "        for i in range(num_batches):\n",
    "            pool.apply_async(self._load_batch, \n",
    "                             args = (self.annotations[i*4: (i+1)*4],)\n",
    "                             , callback = self._log_result)\n",
    "        pool.close()\n",
    "           \n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return len(self.annotations) // self.FLAGS.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        return self.batches[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_generator_async = DataGenerator_async(annotations, FLAGS)\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_generator_async.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    start = time.time()\n",
    "    img, kp, hm = train_generator_async[i]\n",
    "    train_generator_async._preload_batches()\n",
    "    time.sleep(0.5)\n",
    "    end = time.time()\n",
    "    print(\"Loading time: {}\".format(end - start))\n",
    "    \n",
    "    \n",
    "    print(img.shape, kp.shape, hm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cpm_utils import make_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmaps_from_joints(input_size, heatmap_size, gaussian_variance, batch_joints):\n",
    "    # Generate ground-truth heatmaps from ground-truth 2d joints\n",
    "    scale_factor = input_size // heatmap_size\n",
    "    batch_gt_heatmap_np = []\n",
    "    for i in range(batch_joints.shape[0]):\n",
    "        gt_heatmap_np = []\n",
    "        invert_heatmap_np = np.ones(shape=(heatmap_size, heatmap_size))\n",
    "        for j in range(batch_joints.shape[1]):\n",
    "            cur_joint_heatmap = make_gaussian(heatmap_size,\n",
    "                                              gaussian_variance,\n",
    "                                              center=(batch_joints[i][j] // scale_factor))\n",
    "            gt_heatmap_np.append(cur_joint_heatmap)\n",
    "            invert_heatmap_np -= cur_joint_heatmap\n",
    "        gt_heatmap_np.append(invert_heatmap_np)\n",
    "        batch_gt_heatmap_np.append(gt_heatmap_np)\n",
    "    batch_gt_heatmap_np = np.asarray(batch_gt_heatmap_np)\n",
    "    print(batch_gt_heatmap_np.shape)\n",
    "    batch_gt_heatmap_np = np.transpose(batch_gt_heatmap_np, (0, 2, 3, 1))\n",
    "    \n",
    "    return batch_gt_heatmap_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap_single(input_size, heatmap_size, gaussian_variance, batch_joints):\n",
    "    scale_factor = input_size // heatmap_size\n",
    "    gt_heatmap_np = []\n",
    "    invert_heatmap_np = np.ones(shape=(heatmap_size, heatmap_size))\n",
    "    for j in range(batch_joints.shape[0]):\n",
    "        cur_joint_heatmap = make_gaussian(heatmap_size,\n",
    "                                          gaussian_variance,\n",
    "                                          center=(batch_joints[j] // scale_factor))\n",
    "        gt_heatmap_np.append(cur_joint_heatmap)\n",
    "        invert_heatmap_np -= cur_joint_heatmap\n",
    "    gt_heatmap_np.append(invert_heatmap_np)\n",
    "    gt_heatmap_np = np.array(gt_heatmap_np)\n",
    "    gt_heatmap_np = np.transpose(gt_heatmap_np, (1, 2, 0))\n",
    "    return gt_heatmap_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_keypoints_gaussian(annotation):\n",
    "    image, keypoints = load_image_keypoints(annotation, FLAGS)\n",
    "#     print(keypoints.shape)\n",
    "    heatmap = make_heatmap_single(FLAGS.input_size[0], \n",
    "                                FLAGS.heatmap_size, \n",
    "                                FLAGS.joint_gaussian_variance,\n",
    "                                keypoints)\n",
    "    return image, keypoints, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, kps, hm = load_image_keypoints_gaussian(annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pool = Pool(8)\n",
    "data = pool.map_async(load_image_keypoints_gaussian, annotations[:10000])\n",
    "pool.close()\n",
    "pool.join()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_async(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, annotations_set, FLAGS):\n",
    "        'Initialization'\n",
    "        self.annotations = annotations_set\n",
    "        self.on_epoch_end()\n",
    "        self.FLAGS = FLAGS\n",
    "        self.batch = []\n",
    "        \n",
    "        self.images = []\n",
    "        self.keypoints = []\n",
    "        self.heatmaps = []\n",
    "        \n",
    "        # get the first batch\n",
    "        self._prefetch(0)\n",
    "        \n",
    "    def _log_result(self, result):\n",
    "        self.images.append(result[0])\n",
    "        self.keypoints.append(result[1])\n",
    "        self.heatmaps.append(result[2])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return len(self.annotations) // self.FLAGS.batch_size\n",
    "\n",
    "    \n",
    "    def _prefetch(self, index):\n",
    "        print('Fetching')      \n",
    "        \n",
    "        pool = Pool(8)\n",
    "        pool.map(load_image_keypoints_gaussian, \n",
    "                 args = (self.annotations[:125],)\n",
    "                 , callback = self._log_result)\n",
    "        pool.close()       \n",
    "#         pool.join()      \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "#         self._prefetch(index+1)\n",
    "        \n",
    "        images = np.array(self.images[:4])\n",
    "        keypoints = np.array(self.keypoints[:4])\n",
    "        heatmaps = np.array(self.heatmaps[:4])\n",
    "        \n",
    "        return images, keypoints, heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(8)\n",
    "pool.apply_async(load_image_keypoints_gaussian, \n",
    "                     args = (annotations[:125]))\n",
    "pool.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         num_batches = 1000\n",
    "#         pool = Pool(12)\n",
    "#         for i in range(num_batches):\n",
    "#             pool.apply_async(self._load_batch, \n",
    "#                              args = (self.annotations[i*4: (i+1)*4],)\n",
    "#                              , callback = self._log_result)\n",
    "#         pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_generator_async = DataGenerator_async(annotations, FLAGS)\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_generator_async.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "img, kp, hm = train_generator_async[0]\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    start = time.time()\n",
    "    img, kp, hm = train_generator_async[i]\n",
    "    train_generator_async._prefetch(i+1)\n",
    "    time.sleep(0.5)\n",
    "    end = time.time()\n",
    "    print(\"Loading time: {}\".format(end - start))\n",
    "    \n",
    "    \n",
    "    print(img.shape, kp.shape, hm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pool = Pool(4)\n",
    "pool.apply()\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,hm = train_generator_async.batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_generator_async.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = train_generator_async.batches[0].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(100):\n",
    "    output = train_generator_async[i]\n",
    "end = time.time()\n",
    "print(\"Loading time: {}\".format((end - start)/5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_async.batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = DataGenerator(train_annotations, FLAGS)\n",
    "generator = data_generator(train_annotations, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch, ybatch, batch_gt_heatmap_np = next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 \n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('/root/data/aquabyte-images/erko-rotoy/2018-08-30/1/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stupid generator\n",
    "# def generator():\n",
    "#     for path in paths:\n",
    "#         yield cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(data_generator,\n",
    "                                         output_types=tf.int64).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(lambda: data_generator(train_annotations, FLAGS), \n",
    "                                                  output_types=(tf.int64, tf.int64, tf.float64))\n",
    "iter = training_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset = tf.data.Dataset.from_generator(lambda: raw_data_gen(train_val_or_test=1), (tf.float32, tf.uint8), ([None, 1], [None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter = dataset.make_initializable_iterator()\n",
    "# el = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(iter.initializer)\n",
    "#     image = sess.run(el)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     image = sess.run(el)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     image = sess.run(el)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this:\n",
    "\n",
    "https://github.com/cs230-stanford/cs230-code-examples/blob/master/tensorflow/vision/model/input_fn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_dataset = tf.data.Dataset.from_generator(lambda: data_generator(train_annotations, FLAGS), \n",
    "                                                      output_types=(tf.int64, tf.int64, tf.float64))\n",
    "    iter = training_dataset.make_initializable_iterator()\n",
    "    el = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    start = time.time()\n",
    "    sess.run(iter.initializer)\n",
    "    xbatch, ybatch, batch_gt_heatmap_np = sess.run(el)\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this is better https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_image_keypoints\n",
    "from utils.cpm_utils import make_heatmaps_from_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parallel_calls = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_gen():\n",
    "    i = 0\n",
    "    nsteps = 1000\n",
    "    while True:\n",
    "        i += 1 \n",
    "        if i >= nsteps:\n",
    "            i = 0\n",
    "        yield train_annotations[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_fn = lambda ann:\n",
    "load_fn = lambda ann: load_image_keypoints(ann, FLAGS)\n",
    "expand_fn = lambda x,y : (x, np.expand_dims(y, axis=0))\n",
    "heatmap_fn = lambda x, y: (x, y, make_heatmaps_from_joints(FLAGS.input_size[0],\n",
    "                                                           FLAGS.heatmap_size,\n",
    "                                                           FLAGS.joint_gaussian_variance,\n",
    "                                                           y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_fn(train_annotations[0])\n",
    "x, y  = expand_fn(x,y)\n",
    "xa, ya, ga = heatmap_fn(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/cpu:0'):\n",
    "#     dataset = tf.data.Dataset.from_generator(new_gen, tf.string)\n",
    "# #     dataset = dataset.map(load_fn, num_parallel_calls=num_parallel_calls)\n",
    "# #     dataset = dataset.map(expand_fn, num_parallel_calls=num_parallel_calls)\n",
    "# #     dataset = dataset.map(heatmap_fn, num_parallel_calls=num_parallel_calls)\n",
    "    \n",
    "#     iter = dataset.make_initializable_iterator()\n",
    "#     el = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# #     start = time.time()\n",
    "#     sess.run(iter.initializer)\n",
    "#     print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(new_gen, tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    dataset = (tf.data.Dataset.from_generator(new_gen, tf.string)\n",
    "        .shuffle(buffer_size=1)  # whole dataset into the buffer ensures good shuffling\n",
    "        .map(load_fn, num_parallel_calls=num_parallel_calls)\n",
    "        .map(expand_fn, num_parallel_calls=num_parallel_calls)\n",
    "        .map(heatmap_fn, num_parallel_calls=num_parallel_calls)\n",
    "        .batch(4)\n",
    "        .prefetch(1)  # make sure you always have one batch ready to serve\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
