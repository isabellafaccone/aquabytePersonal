{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import albumentations as albu\n",
    "\n",
    "from utils import cpm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    \"\"\" \"\"\"\n",
    "    \"\"\"\n",
    "    General settings\n",
    "    \"\"\"\n",
    "    input_size = (512, 512)\n",
    "    heatmap_size = 64\n",
    "    cpm_stages = 3\n",
    "    joint_gaussian_variance = 4.0\n",
    "    center_radius = 21\n",
    "    num_of_joints = 8\n",
    "    color_channel = 'RGB'\n",
    "    normalize = True\n",
    "    use_gpu = True\n",
    "    gpu_id = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Demo settings\n",
    "    \"\"\"\n",
    "    # 'MULTI': show multiple stage heatmaps\n",
    "    # 'SINGLE': show last stage heatmap\n",
    "    # 'Joint_HM': show last stage heatmap for each joint\n",
    "    # 'image or video path': show detection on single image or video\n",
    "    DEMO_TYPE = 'SINGLE'\n",
    "\n",
    "    model_path = 'cpm_hand'\n",
    "    cam_id = 0\n",
    "\n",
    "    webcam_height = 480\n",
    "    webcam_width = 640\n",
    "\n",
    "    use_kalman = True\n",
    "    kalman_noise = 0.03\n",
    "    keypoints_order = [\"TAIL_NOTCH\",\n",
    "                        \"ADIPOSE_FIN\",\n",
    "                        \"UPPER_LIP\",\n",
    "                        \"ANAL_FIN\",\n",
    "                        \"PELVIC_FIN\",\n",
    "                        \"EYE\",\n",
    "                        \"PECTORAL_FIN\",\n",
    "                        \"DORSAL_FIN\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Training settings\n",
    "    \"\"\"\n",
    "    network_def = 'fish_test'\n",
    "    train_img_dir = ''\n",
    "    val_img_dir = ''\n",
    "    bg_img_dir = ''\n",
    "    pretrained_model = 'fish_test'\n",
    "    batch_size = 8\n",
    "    init_lr = 0.001\n",
    "    lr_decay_rate = 0.5\n",
    "    lr_decay_step = 10000\n",
    "    augmentation = None\n",
    "    augmentation = albu.Compose([#albu.RandomContrast(limit=0.3, p=0.3),\n",
    "                                 #albu.RandomBrightness(limit=0.4, p=0.3),\n",
    "                                 albu.Rotate(limit=10, p=1.0)], \n",
    "                                 p=1.0,\n",
    "                                 keypoint_params={'format': 'xy'})\n",
    "    \n",
    "    epochs=300\n",
    "#     augmentation_config = {'hue_shift_limit': (-5, 5),\n",
    "#                            'sat_shift_limit': (-10, 10),\n",
    "#                            'val_shift_limit': (-15, 15),\n",
    "#                            'translation_limit': (-0.15, 0.15),\n",
    "#                            'scale_limit': (-0.3, 0.5),\n",
    "#                            'rotate_limit': (-90, 90)}\n",
    "    hnm = True  # Make sure generate hnm files first\n",
    "    do_cropping = True\n",
    "\n",
    "    \"\"\"\n",
    "    For Freeze graphs\n",
    "    \"\"\"\n",
    "    output_node_names = 'stage_3/mid_conv7/BiasAdd:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_model = importlib.import_module('models.nets.' + FLAGS.network_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bunch of folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datenow = str(datetime.now()).split(\".\")[0].replace(\" \",\"_\").replace(\"-\",\"_\").replace(\":\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/root/data/models/keypoints_detection/{}\".format(datenow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "model_path_suffix = os.path.join(FLAGS.network_def,\n",
    "                                 'input_{}_output_{}'.format(FLAGS.input_size, FLAGS.heatmap_size),\n",
    "                                 'joints_{}'.format(FLAGS.num_of_joints),\n",
    "                                 'stages_{}'.format(FLAGS.cpm_stages),\n",
    "                                 'init_{}_rate_{}_step_{}'.format(FLAGS.init_lr, FLAGS.lr_decay_rate,\n",
    "                                                                  FLAGS.lr_decay_step)\n",
    "                                 )\n",
    "model_save_dir = os.path.join(base_dir,\n",
    "                              'weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cpm_model.CPM_Model(input_size=FLAGS.input_size,\n",
    "                            heatmap_size=FLAGS.heatmap_size,\n",
    "                            stages=FLAGS.cpm_stages,\n",
    "                            joints=FLAGS.num_of_joints,\n",
    "                            img_type=FLAGS.color_channel,\n",
    "                            is_training=True)\n",
    "model.build_loss(FLAGS.init_lr, FLAGS.lr_decay_rate, FLAGS.lr_decay_step, optimizer='RMSProp')\n",
    "print('=====Model Build=====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating data generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from utils.utils import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the gtsf session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = glob.glob(\"/root/data/gtsf_phase_I/*/*_cogito_annotations.json\")\n",
    "print(\"Total number of gtsf sessions: {}\".format(len(annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Val split. Let's split by experiment. Better practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(len(annotations)*0.8)\n",
    "random.shuffle(annotations)\n",
    "train_files = annotations[:cutoff]\n",
    "val_files = annotations[cutoff:]\n",
    "print(\"Number of train files: {}\".format(len(train_files)))\n",
    "print(\"Number of validation files: {}\".format(len(val_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = []\n",
    "for jpath in train_files:\n",
    "    train_annotations += json.load(open(jpath))\n",
    "print(\"Number of training data: {}\".format(len(train_annotations)))\n",
    "train_annotations= [ann for ann in train_annotations if ann[\"species\"] == \"salmon\"]\n",
    "print(\"Number of training data: {}\".format(len(train_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotations = []\n",
    "for jpath in val_files:\n",
    "    val_annotations += json.load(open(jpath))\n",
    "print(\"Number of validation data: {}\".format(len(val_annotations)))\n",
    "val_annotations = [ann for ann in val_annotations if ann[\"species\"] == \"salmon\"]\n",
    "print(\"Number of validation data: {}\".format(len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ann in val_annotations + train_annotations:\n",
    "#     local_path = os.path.join(\"/root/data/gtsf_phase_I/\", \n",
    "#           \"/\".join(ann[\"Labeled Data\"].split(\"/\")[7:]))\n",
    "#     if not os.path.isfile(local_path):\n",
    "#         print(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.utils import load_image_keypoints, DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, kps = load_image_keypoints(np.random.choice(val_annotations), FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(im)\n",
    "plt.scatter(kps[:, 0], kps[:, 1])\n",
    "for i in range(8):\n",
    "    plt.text(kps[i, 0], kps[i,1], FLAGS.keypoints_order[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generator itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(train_annotations, FLAGS)\n",
    "val_generator = DataGenerator(train_annotations, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, heatmaps = val_generator[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(xb[1, ...])\n",
    "i = 0\n",
    "for kpp in yb[1]:\n",
    "    plt.scatter(kpp[0], kpp[1], c=\"r\")\n",
    "    plt.text(kpp[0], kpp[1], FLAGS.keypoints_order[i], {\"color\": \"w\"})\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(im)\n",
    "# plt.show()\n",
    "# results = FLAGS.augmentation(image=im, keypoints=list(kps.values()))\n",
    "# nkps = np.array(results['keypoints'])\n",
    "# plt.imshow(results[\"image\"])\n",
    "# plt.scatter(nkps[:, 0], nkps[:, 1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config\n",
    "with open(os.path.join(base_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump({k:v for (k,v) in FLAGS.__dict__.items() if k not in  [\"__dict__\", '__weakref__', 'augmentation']}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_current_training_stats(global_step, cur_lr, stage_losses, total_loss, time_elapsed):\n",
    "    stats = 'Step: {}/{} ----- Cur_lr: {:1.7f} ----- Time: {:>2.2f} sec.'.format(global_step, train_generator.__len__() * FLAGS.epochs,\n",
    "                                                                                 cur_lr, time_elapsed)\n",
    "    losses = ' | '.join(\n",
    "        ['S{} loss: {:>7.2f}'.format(stage_num + 1, stage_losses[stage_num]) for stage_num in range(FLAGS.cpm_stages)])\n",
    "    losses += ' | Total loss: {}'.format(total_loss)\n",
    "    print(stats)\n",
    "    print(losses + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "device_count = {'GPU': 1} if FLAGS.use_gpu else {'GPU': 0}\n",
    "\n",
    "# cause fuck tensorboard\n",
    "history = {\"train_stages_loss\":[],\n",
    "           \"train_total_loss\": [],\n",
    "           \"val_total_loss\": []}\n",
    "with tf.Session(config=tf.ConfigProto(device_count=device_count,\n",
    "                                      allow_soft_placement=True)) as sess:\n",
    "#     # Create tensorboard\n",
    "#     train_writer = tf.summary.FileWriter(train_log_save_dir, sess.graph)\n",
    "#     test_writer = tf.summary.FileWriter(test_log_save_dir, sess.graph)\n",
    "    \n",
    "    # Create model saver\n",
    "    saver = tf.train.Saver(max_to_keep=None) #max_to_keep=None)\n",
    "\n",
    "    # Init all vars\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # saver.restore(sess, '/root/data/models/keypoints_detection/2019_03_27_01_52_17/weights/fish_test-39')\n",
    "    train_generator = DataGenerator(train_annotations, FLAGS)\n",
    "    val_generator = DataGenerator(train_annotations, FLAGS)\n",
    "    \n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        print(\"Epoch {} starts\".format(epoch))\n",
    "        \n",
    "        training_itr = 0\n",
    "        \n",
    "        while training_itr < train_generator.__len__():\n",
    "            t1 = time.time()\n",
    "            # load input + labels\n",
    "            batch_x_np, _, batch_gt_heatmap_np = train_generator[training_itr]\n",
    "            training_itr += 1\n",
    "            \n",
    "            # Forward and update weights\n",
    "            stage_losses_np, total_loss_np, _, summaries, current_lr, \\\n",
    "            stage_heatmap_np, global_step = sess.run([model.stage_loss,\n",
    "                                                  model.total_loss,\n",
    "                                                  model.train_op,\n",
    "                                                  merged_summary,\n",
    "                                                  model.cur_lr,\n",
    "                                                  model.stage_heatmap,\n",
    "                                                  model.global_step\n",
    "                                                  ],\n",
    "                                                 feed_dict={model.input_images: batch_x_np,\n",
    "                                                            model.gt_hmap_placeholder: batch_gt_heatmap_np})\n",
    "            \n",
    "            history[\"train_stages_loss\"].append([float(s) for s in stage_losses_np])\n",
    "            history[\"train_total_loss\"].append(float(total_loss_np))\n",
    "            # Show training info\n",
    "            print_current_training_stats(global_step, current_lr, stage_losses_np, total_loss_np, time.time() - t1)\n",
    "            \n",
    "            # Write logs\n",
    "            # train_writer.add_summary(summaries, epoch*training_itr)\n",
    "        \n",
    "        # shuffle on epoch end\n",
    "        train_generator.on_epoch_end()\n",
    "        \n",
    "        saver.save(sess=sess, save_path=model_save_dir + '/' + FLAGS.network_def.split('.py')[0], \n",
    "                   global_step=epoch)\n",
    "        print('\\nModel checkpoint saved...\\n')\n",
    "        \n",
    "        # now validation stuff\n",
    "        mean_val_loss = 0\n",
    "        val_itr = 0\n",
    "        while val_itr < val_generator.__len__():\n",
    "            # load input + labels\n",
    "            batch_x_np, _, batch_gt_heatmap_np = val_generator[val_itr]\n",
    "            val_itr += 1\n",
    "\n",
    "            val_total_loss, summaries = sess.run([model.total_loss, merged_summary],\n",
    "                                                 feed_dict={model.input_images: batch_x_np,\n",
    "                                                               model.gt_hmap_placeholder: batch_gt_heatmap_np})\n",
    "            mean_val_loss += val_total_loss\n",
    "        val_mean_loss = mean_val_loss / val_generator.__len__()\n",
    "        history[\"val_total_loss\"].append(float(val_mean_loss))\n",
    "        print('\\nValidation loss: {:>7.2f}\\n'.format(val_mean_loss))\n",
    "        # test_writer.add_summary(summaries, global_step)\n",
    "        # save history\n",
    "        with open(os.path.join(base_dir, \"history.json\"), \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "        \n",
    "        print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stage_heatmap = stage_heatmap_np[-1][0, ...]\n",
    "print(final_stage_heatmap.shape)\n",
    "f, ax = plt.subplots(5, 2, figsize=(20, 30))\n",
    "c = 0\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        if c == 9:\n",
    "            continue\n",
    "        hm = cv2.resize(final_stage_heatmap[..., c], FLAGS.input_size)\n",
    "        hm_max = np.where(hm == hm.max())\n",
    "        ax[i,j].imshow(xb[0, ...])\n",
    "        ax[i,j].imshow(hm, alpha=0.5)\n",
    "        ax[i,j].scatter(hm_max[1], hm_max[0], c=\"r\")\n",
    "        ax[i,j].axis(\"off\")\n",
    "        c+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
