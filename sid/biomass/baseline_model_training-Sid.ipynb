{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'weight_estimation.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e82dff562883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mresearch_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_access_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3AccessUtils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDSAccessUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mweight_estimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_gtsf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_akpd_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mweight_estimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_data_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'weight_estimation.dataset'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load Raw GTSF Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded!\n",
      "Data preprocessed!\n",
      "Percentage complete: 0.0%\n",
      "Percentage complete: 6.39%\n",
      "Percentage complete: 12.78%\n",
      "Percentage complete: 19.17%\n",
      "Percentage complete: 25.56%\n",
      "Percentage complete: 31.94%\n",
      "Percentage complete: 38.33%\n",
      "Percentage complete: 44.72%\n",
      "Percentage complete: 51.11%\n",
      "Percentage complete: 57.5%\n",
      "Percentage complete: 63.89%\n",
      "Percentage complete: 70.28%\n",
      "Percentage complete: 76.67%\n",
      "Percentage complete: 83.06%\n",
      "Percentage complete: 89.45%\n",
      "Percentage complete: 95.83%\n"
     ]
    }
   ],
   "source": [
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded!\n",
      "Data preprocessed!\n",
      "Percentage complete: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/alok/repos/production_algo/weight_estimation/src/weight_estimation/utils.py:104: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (X_left[:, 0] - X_right[:, 0])\n",
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Augment the Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_estimation.utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_ann_from_keypoint_arrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ca5af32d6ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mweight_estimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_left_right_keypoint_arrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_ann_from_keypoint_arrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconvert_to_nn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCameraMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mweight_estimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_left_right_keypoint_arrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_world_point_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_ann_from_keypoint_arrs'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, get_ann_from_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, convert_to_world_point_arr\n",
    "\n",
    "\n",
    "def augment(df: pd.DataFrame, augmentation_config: Dict) -> pd.DataFrame:\n",
    "    print('hello')\n",
    "    \n",
    "    counts, edges = np.histogram(df.weight, bins=np.arange(0, 10000, 1000))\n",
    "    trial_values = (5.0 / (counts / np.max(counts))).astype(int)\n",
    "    max_jitter_std = augmentation_config['max_jitter_std']\n",
    "    min_depth = augmentation_config['min_depth']\n",
    "    max_depth = augmentation_config['max_depth']\n",
    "\n",
    "    augmented_data = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        camera_metadata = row.camera_metadata\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        \n",
    "        weight = row.weight\n",
    "        trials = trial_values[min(int(weight / 1000), len(trial_values) - 1)]\n",
    "        for _ in range(trials):\n",
    "            \n",
    "            ann = row.keypoints\n",
    "            X_left, X_right = get_left_right_keypoint_arrs(ann)\n",
    "            wkps = convert_to_world_point_arr(X_left, X_right, cm)\n",
    "            original_depth = np.median(wkps[:, 1])\n",
    "            \n",
    "            depth = np.random.uniform(min_depth, max_depth)\n",
    "            scaling_factor = float(original_depth) / depth\n",
    "            jitter_std = np.random.uniform(0, max_jitter_std)\n",
    "            \n",
    "\n",
    "            # rescale\n",
    "            X_left = X_left * scaling_factor\n",
    "            X_right = X_right * scaling_factor\n",
    "\n",
    "            # add jitter\n",
    "            X_left[:, 0] += np.random.normal(0, jitter_std, X_left.shape[0])\n",
    "            X_right[:, 0] += np.random.normal(0, jitter_std, X_right.shape[0])\n",
    "\n",
    "            # reconstruct annotation\n",
    "            ann = get_ann_from_keypoint_arrs(X_left, X_right)\n",
    "            augmented_data['annotation'].append(ann)\n",
    "            augmented_data['fish_id'].append(row.fish_id)\n",
    "            augmented_data['weight'].append(row.weight)\n",
    "            augmented_data['kf'].append(row.k_factor)\n",
    "            augmented_data['camera_metadata'].append(row.camera_metadata)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "augmentation_config = dict(\n",
    "    trials=10,\n",
    "    max_jitter_std=10,\n",
    "    min_depth=0.5,\n",
    "    max_depth=2.5\n",
    ")\n",
    "\n",
    "augmented_df = augment(df, augmentation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_csv('/root/data/sid/augmented_df_depth_weight_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.read_csv('/root/data/sid/augmented_df_depth_weight_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Note: cell below takes about 1.5 hrs to run. To load cached version, run the cell below this one </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# akpd_scores = []\n",
    "# akpd_scorer_network = load_model(akpd_scorer_f)\n",
    "# for idx, row in augmented_df.iterrows():\n",
    "#     if count % 1000 == 0:\n",
    "#         print('Percentage complete: {}%'.format(round(100 * count / augmented_df.shape[0], 2)))\n",
    "#     count += 1\n",
    "#     akpd_score = compute_akpd_score(akpd_scorer_network, row.annotation, row.camera_metadata)\n",
    "#     akpd_scores.append(akpd_score)\n",
    "\n",
    "\n",
    "# augmented_df['akpd_score'] = akpd_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> If you ran the cell above, do not run the one here </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180746it [00:39, 4539.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "augmented_df = pd.read_csv('/root/data/alok/biomass_estimation/playground/gtsf_augmented_dataset.csv')\n",
    "\n",
    "new_anns, new_cms = [], []\n",
    "for idx, row in tqdm(augmented_df.iterrows()):\n",
    "    cm = row.camera_metadata\n",
    "    new_cm = json.loads(cm.replace(\"'\", '\"'))\n",
    "    new_cms.append(new_cm)\n",
    "    \n",
    "    ann = row.annotation\n",
    "    new_ann = json.loads(ann.replace(\"'\", '\"'))\n",
    "    new_anns.append(new_ann)\n",
    "    \n",
    "augmented_df['annotation'] = new_anns\n",
    "augmented_df['camera_metadata'] = new_cms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interpn\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs,\\\n",
    "    convert_to_nn_input, CameraMetadata\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def normalize(anns: List, camera_metadatas: List) -> np.ndarray:\n",
    "    norm_anns = []\n",
    "    for ann, camera_metadata in zip(anns, camera_metadatas):\n",
    "\n",
    "        cm = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        norm_ann = convert_to_nn_input(ann, cm)\n",
    "        norm_anns.append(norm_ann)\n",
    "    return np.array(norm_anns)\n",
    "\n",
    "\n",
    "def get_data_split(X: np.ndarray, y: np.ndarray, fish_ids: np.ndarray, train_pct: float,\n",
    "                   val_pct: float) -> Tuple:\n",
    "    # select train / test sets such that there are no overlapping fish IDs\n",
    "\n",
    "    test_pct = 1.0 - train_pct - val_pct\n",
    "    unique_fish_ids = np.array(list(set(fish_ids)))\n",
    "    train_cnt, val_cnt, test_cnt = np.random.multinomial(len(unique_fish_ids),\n",
    "                                                         [train_pct, val_pct, test_pct])\n",
    "\n",
    "    assignments = np.array([0] * train_cnt + [1] * val_cnt + [2] * test_cnt)\n",
    "    np.random.shuffle(assignments)\n",
    "    train_fish_ids = unique_fish_ids[np.where(assignments == 0)]\n",
    "    val_fish_ids = unique_fish_ids[np.where(assignments == 1)]\n",
    "    test_fish_ids = unique_fish_ids[np.where(assignments == 2)]\n",
    "\n",
    "    train_mask = np.isin(fish_ids, train_fish_ids)\n",
    "    val_mask = np.isin(fish_ids, val_fish_ids)\n",
    "    test_mask = np.isin(fish_ids, test_fish_ids)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, train_config):\n",
    "    inputs = Input(shape=(24,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(1)(x)\n",
    "    model = Model(inputs, pred)\n",
    "\n",
    "    epochs = train_config['epochs']\n",
    "    batch_size = train_config['batch_size']\n",
    "    lr = train_config['learning_rate']\n",
    "    patience = train_config['patience']\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0,\n",
    "                                               patience=patience,\n",
    "                                               verbose=0,\n",
    "                                               mode='auto')]\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def density_scatter(x, y, bins=20, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:]+y_e[:-1])), data, np.vstack([x, y]).T,\n",
    "                method=\"splinef2d\", bounds_error=False)\n",
    "\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "    norm = Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm), ax=ax)\n",
    "    cbar.ax.set_ylabel('Density')\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    ax.grid()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_accuracy_details(model, X_train, y_train, X_test, y_test, x_train_pred, y_train_pred):\n",
    "\n",
    "    train_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_train_pred - y_train) / y_train)),\n",
    "        'mean_error_pct': 100 * np.mean(y_train_pred - y_train) / np.mean(y_train)\n",
    "    }\n",
    "    test_stats = {\n",
    "        'mean_absolute_error_pct': 100 * np.mean(np.abs((y_test_pred - y_test) / y_test)),\n",
    "        'mean_error_pct': 100 * np.mean(y_test_pred - y_test) / np.mean(y_test)\n",
    "    }\n",
    "\n",
    "    return train_stats, test_stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180746it [00:30, 5986.14it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d9295fb86398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnorm_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_nn_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnorm_anns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_ann\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_anns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "norm_anns = []\n",
    "for ann, camera_metadata in tqdm(zip(anns, cms)):\n",
    "\n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "\n",
    "    norm_ann = convert_to_nn_input(ann, cm)\n",
    "    norm_anns.append(norm_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180746, 8, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = np.array()\n",
    "XX = np.array([x.numpy()[0] for x in norm_anns])\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180746,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping...\n",
      "normalizing annotations to nn input...\n",
      "Splitting...\n"
     ]
    }
   ],
   "source": [
    "print('Prepping...')\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# anns = augmented_df.annotation.values.tolist()\n",
    "# cms = augmented_df.camera_metadata.values.tolist()\n",
    "print('normalizing annotations to nn input...')\n",
    "# X = normalize(anns, cms)\n",
    "\n",
    "train_config = dict(\n",
    "    train_pct=0.8,\n",
    "    val_pct=0.1,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "y = 1e-4 * augmented_df.weight.values\n",
    "fish_ids = augmented_df.fish_id.values\n",
    "print('Splitting...')\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, train_mask, val_mask, test_mask = get_data_split(XX, y, fish_ids,\n",
    "                                                                train_config['train_pct'],\n",
    "                                                                train_config['val_pct'])\n",
    "\n",
    "augmented_df['train_mask'] = train_mask\n",
    "augmented_df['val_mask'] = val_mask\n",
    "augmented_df['test_mask'] = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm traintest_weight_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_df.to_csv('/root/data/sid/traintest_weight_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153487, 8, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 24) for input Tensor(\"input_2:0\", shape=(None, 24), dtype=float32), but it was called on an input with incompatible shape (None, 8, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_4 is incompatible with the layer: expected axis -1 of input shape to have value 24 but received input with shape [None, 8, 3]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c29ad59b4d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-6001d13ad83e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_val, y_val, train_config)\u001b[0m\n\u001b[1;32m     89\u001b[0m                   metrics=['accuracy'])\n\u001b[1;32m     90\u001b[0m     model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callbacks,\n\u001b[0;32m---> 91\u001b[0;31m               batch_size=batch_size, epochs=epochs)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_4 is incompatible with the layer: expected axis -1 of input shape to have value 24 but received input with shape [None, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "print('Training....')\n",
    "model = train_model(X_train, y_train, X_val, y_val, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180746,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-aebaa76fccff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/data/sid/gtsf_model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/root/data/sid/gtsf_model.pt', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of urllib3.connectionpool failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 11, in <module>\n",
      "    from .connection import (\n",
      "ImportError: cannot import name 'BrokenPipeError'\n",
      "]\n",
      "[autoreload of urllib3.packages.six failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 287, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/packages/six.py\", line 92, in __get__\n",
      "    setattr(obj, self.name, result)  # Invokes __set__.\n",
      "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
      "]\n",
      "[autoreload of urllib3.connection failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py\", line 55, in <module>\n",
      "    from .util import SKIP_HEADER, SKIPPABLE_HEADERS, connection\n",
      "ImportError: cannot import name 'SKIP_HEADER'\n",
      "]\n",
      "[autoreload of urllib3.util failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/util/__init__.py\", line 5, in <module>\n",
      "    from .request import SKIP_HEADER, SKIPPABLE_HEADERS, make_headers\n",
      "ImportError: cannot import name 'SKIP_HEADER'\n",
      "]\n",
      "[autoreload of urllib3.util.ssl_ failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/util/ssl_.py\", line 17, in <module>\n",
      "    from .url import BRACELESS_IPV6_ADDRZ_RE, IPV4_RE\n",
      "ImportError: cannot import name 'BRACELESS_IPV6_ADDRZ_RE'\n",
      "]\n",
      "[autoreload of urllib3.contrib.pyopenssl failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\", line 87, in <module>\n",
      "    util.PROTOCOL_TLS: OpenSSL.SSL.SSLv23_METHOD,\n",
      "AttributeError: module 'urllib3.util' has no attribute 'PROTOCOL_TLS'\n",
      "]\n",
      "[autoreload of botocore.session failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/botocore/session.py\", line 53, in <module>\n",
      "    from botocore.utils import EVENT_ALIASES, validate_region_name\n",
      "ImportError: cannot import name 'validate_region_name'\n",
      "]\n",
      "[autoreload of botocore.vendored.six failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 287, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/botocore/vendored/six.py\", line 93, in __get__\n",
      "    setattr(obj, self.name, result)  # Invokes __set__.\n",
      "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
      "]\n",
      "[autoreload of botocore.credentials failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/botocore/credentials.py\", line 47, in <module>\n",
      "    from botocore.utils import SSOTokenLoader\n",
      "ImportError: cannot import name 'SSOTokenLoader'\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of botocore.hooks failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
      "    ref.__class__ = new\n",
      "TypeError: __class__ assignment: 'NodeList' object layout differs from 'NodeList'\n",
      "]\n",
      "[autoreload of botocore.docs.service failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/botocore/docs/service.py\", line 16, in <module>\n",
      "    from botocore.docs.client import ClientExceptionsDocumenter\n",
      "ImportError: cannot import name 'ClientExceptionsDocumenter'\n",
      "]\n",
      "[autoreload of botocore.auth failed: Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fd9c0ea02e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.91591514e-02,  1.15923751e-02,  3.75006979e-02, ...,\n",
       "          1.22998057e-01,  9.40281811e-20,  3.61096629e-02]],\n",
       "\n",
       "       [[-5.66725699e-02,  1.02486739e-02,  2.73871426e-02, ...,\n",
       "          1.04916664e-01,  2.65612392e-19,  2.62406236e-02]],\n",
       "\n",
       "       [[-2.31413973e-01,  4.19746665e-02,  1.20768736e-01, ...,\n",
       "          4.40347414e-01, -6.14641661e-19,  1.25326297e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.05079472e-02,  1.19912729e-02,  3.60919683e-02, ...,\n",
       "          7.43890939e-02,  3.87695079e-19,  3.48870558e-02]],\n",
       "\n",
       "       [[-8.06035287e-02,  2.65521287e-02,  8.80695432e-02, ...,\n",
       "          1.76905119e-01,  2.01692632e-18,  8.80023801e-02]],\n",
       "\n",
       "       [[-9.61349860e-02,  3.09448508e-02,  1.01042869e-01, ...,\n",
       "          2.04336608e-01, -8.49254895e-19,  1.00289347e-01]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 24) for input Tensor(\"input_1:0\", shape=(None, 24), dtype=float32), but it was called on an input with incompatible shape (None, 1, 24).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (19171,) (139087,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3ccaee7f0d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgenerate_accuracy_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-6b4fa5d35f61>\u001b[0m in \u001b[0;36mgenerate_accuracy_details\u001b[0;34m(model, X_train, y_train, X_test, y_test, x_train_pred, y_train_pred)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     train_stats = {\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;34m'mean_absolute_error_pct'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;34m'mean_error_pct'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (19171,) (139087,) "
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train).squeeze().astype(float)\n",
    "y_test_pred = model.predict(X_test).squeeze().astype(float)\n",
    "\n",
    "train_stats, test_stats = \\\n",
    "    generate_accuracy_details(model, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df['is_train'] = train_mask.astype(int)\n",
    "augmented_df['is_val'] = val_mask.astype(int)\n",
    "augmented_df['is_test'] = test_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_bucket_error(X, y):\n",
    "    y_pred = model.predict(X).squeeze().astype(float)\n",
    "\n",
    "    buckets = np.arange(0, 10000, 1000) * 1e-4\n",
    "    bucket_strs = []\n",
    "    mean_errs = []\n",
    "    for low, high in zip(buckets, buckets[1:]):\n",
    "        bucket_str = '{}-{}'.format(round(1e4 * low), round(1e4 * high))\n",
    "        mask = (y >= low) & (y < high)\n",
    "        mean_err = np.mean((y_pred[mask] - y[mask]) / y[mask])\n",
    "        mean_errs.append(mean_err)\n",
    "        bucket_strs.append(bucket_str)\n",
    "    \n",
    "    return pd.DataFrame({'bucket': bucket_strs, 'mean_err': mean_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0-1000.0</td>\n",
       "      <td>0.021373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0-2000.0</td>\n",
       "      <td>0.006779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0-3000.0</td>\n",
       "      <td>-0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0-4000.0</td>\n",
       "      <td>0.035327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4000.0-5000.0</td>\n",
       "      <td>0.009389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0-6000.0</td>\n",
       "      <td>0.002183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6000.0-7000.0</td>\n",
       "      <td>0.006544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000.0-8000.0</td>\n",
       "      <td>0.026875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8000.0-9000.0</td>\n",
       "      <td>0.025561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bucket  mean_err\n",
       "0     0.0-1000.0  0.021373\n",
       "1  1000.0-2000.0  0.006779\n",
       "2  2000.0-3000.0 -0.001876\n",
       "3  3000.0-4000.0  0.035327\n",
       "4  4000.0-5000.0  0.009389\n",
       "5  5000.0-6000.0  0.002183\n",
       "6  6000.0-7000.0  0.006544\n",
       "7  7000.0-8000.0  0.026875\n",
       "8  8000.0-9000.0  0.025561"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_per_bucket_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0-1000.0</td>\n",
       "      <td>0.009211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0-2000.0</td>\n",
       "      <td>0.024160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0-3000.0</td>\n",
       "      <td>0.019169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0-4000.0</td>\n",
       "      <td>0.019690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4000.0-5000.0</td>\n",
       "      <td>0.023237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0-6000.0</td>\n",
       "      <td>0.008424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6000.0-7000.0</td>\n",
       "      <td>0.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000.0-8000.0</td>\n",
       "      <td>0.007415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8000.0-9000.0</td>\n",
       "      <td>0.008148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bucket  mean_err\n",
       "0     0.0-1000.0  0.009211\n",
       "1  1000.0-2000.0  0.024160\n",
       "2  2000.0-3000.0  0.019169\n",
       "3  3000.0-4000.0  0.019690\n",
       "4  4000.0-5000.0  0.023237\n",
       "5  5000.0-6000.0  0.008424\n",
       "6  6000.0-7000.0  0.013830\n",
       "7  7000.0-8000.0  0.007415\n",
       "8  8000.0-9000.0  0.008148"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_per_bucket_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0-1000.0</td>\n",
       "      <td>0.030109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0-2000.0</td>\n",
       "      <td>-0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0-3000.0</td>\n",
       "      <td>-0.027920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0-4000.0</td>\n",
       "      <td>0.022288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4000.0-5000.0</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0-6000.0</td>\n",
       "      <td>-0.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6000.0-7000.0</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000.0-8000.0</td>\n",
       "      <td>0.010087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8000.0-9000.0</td>\n",
       "      <td>0.017081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bucket  mean_err\n",
       "0     0.0-1000.0  0.030109\n",
       "1  1000.0-2000.0 -0.010025\n",
       "2  2000.0-3000.0 -0.027920\n",
       "3  3000.0-4000.0  0.022288\n",
       "4  4000.0-5000.0  0.000550\n",
       "5  5000.0-6000.0 -0.007879\n",
       "6  6000.0-7000.0 -0.000133\n",
       "7  7000.0-8000.0  0.010087\n",
       "8  8000.0-9000.0  0.017081"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_per_bucket_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f249a131ba8>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save Pytorch version of this model - the resulting output file can be then used to backtest </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from weight_estimation.utils import CameraMetadata, convert_to_nn_input\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_intermediate(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor and get final hiddel layer weights.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = Network()\n",
    "weights = model.get_weights()\n",
    "\n",
    "pytorch_model.fc1.weight.data = torch.from_numpy(np.transpose(weights[0]))\n",
    "pytorch_model.fc1.bias.data = torch.from_numpy(np.transpose(weights[1]))\n",
    "pytorch_model.fc2.weight.data = torch.from_numpy(np.transpose(weights[2]))\n",
    "pytorch_model.fc2.bias.data = torch.from_numpy(np.transpose(weights[3]))\n",
    "pytorch_model.fc3.weight.data = torch.from_numpy(np.transpose(weights[4]))\n",
    "pytorch_model.fc3.bias.data = torch.from_numpy(np.transpose(weights[5]))\n",
    "pytorch_model.output.weight.data = torch.from_numpy(np.transpose(weights[6]))\n",
    "pytorch_model.output.bias.data = torch.from_numpy(np.transpose(weights[7]))\n",
    "\n",
    "f = '/root/data/sid/output_model.pb'\n",
    "torch.save(pytorch_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Network()\n",
    "model2.load_state_dict(torch.load('/root/data/sid/output_model.pb'))\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate errors with respect to depth </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get depth array and add as column to augmented data-frame\n",
    "\n",
    "from weight_estimation.utils import get_left_right_keypoint_arrs, convert_to_world_point_arr\n",
    "\n",
    "depths = []\n",
    "for idx, row in augmented_df.iterrows():\n",
    "    ann, camera_metadata = row.annotation, row.camera_metadata\n",
    "    cm = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "    \n",
    "    X = convert_to_world_point_arr(*get_left_right_keypoint_arrs(ann), cm)\n",
    "    median_depth = np.median(X[:, 1])\n",
    "    depths.append(median_depth)\n",
    "    \n",
    "augmented_df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth_bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2-0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3-0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4-0.5</td>\n",
       "      <td>0.002595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5-0.6</td>\n",
       "      <td>0.011358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6-0.7</td>\n",
       "      <td>0.012337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7-0.8</td>\n",
       "      <td>0.008366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8-0.9</td>\n",
       "      <td>0.011369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.014150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0-1.1</td>\n",
       "      <td>0.013687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.1-1.2</td>\n",
       "      <td>0.011476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2-1.3</td>\n",
       "      <td>0.013095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.3-1.4</td>\n",
       "      <td>0.013562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.4-1.5</td>\n",
       "      <td>0.015699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.5-1.6</td>\n",
       "      <td>0.015318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.6-1.7</td>\n",
       "      <td>0.014057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.7-1.8</td>\n",
       "      <td>0.012911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.8-1.9</td>\n",
       "      <td>0.010762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.9-2.0</td>\n",
       "      <td>0.014444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0-2.1</td>\n",
       "      <td>0.013860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.1-2.2</td>\n",
       "      <td>0.012717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.2-2.3</td>\n",
       "      <td>0.013874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.3-2.4</td>\n",
       "      <td>0.011372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.4-2.5</td>\n",
       "      <td>0.018263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.5-2.6</td>\n",
       "      <td>0.062465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth_bucket  mean_err\n",
       "0       0.2-0.3       NaN\n",
       "1       0.3-0.4       NaN\n",
       "2       0.4-0.5  0.002595\n",
       "3       0.5-0.6  0.011358\n",
       "4       0.6-0.7  0.012337\n",
       "5       0.7-0.8  0.008366\n",
       "6       0.8-0.9  0.011369\n",
       "7       0.9-1.0  0.014150\n",
       "8       1.0-1.1  0.013687\n",
       "9       1.1-1.2  0.011476\n",
       "10      1.2-1.3  0.013095\n",
       "11      1.3-1.4  0.013562\n",
       "12      1.4-1.5  0.015699\n",
       "13      1.5-1.6  0.015318\n",
       "14      1.6-1.7  0.014057\n",
       "15      1.7-1.8  0.012911\n",
       "16      1.8-1.9  0.010762\n",
       "17      1.9-2.0  0.014444\n",
       "18      2.0-2.1  0.013860\n",
       "19      2.1-2.2  0.012717\n",
       "20      2.2-2.3  0.013874\n",
       "21      2.3-2.4  0.011372\n",
       "22      2.4-2.5  0.018263\n",
       "23      2.5-2.6  0.062465"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X)\n",
    "augmented_df['y_pred'] = np.array(predictions).squeeze()\n",
    "augmented_df['y'] = y\n",
    "\n",
    "depths = np.arange(0.2, 2.7, 0.1)\n",
    "mean_pct_errs = []\n",
    "depth_buckets = []\n",
    "for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "    depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "    depth_buckets.append(depth_bucket)\n",
    "    mask = (augmented_df.depth >= low_depth) & (augmented_df.depth <= high_depth)\n",
    "    mean_pct_err = np.mean((augmented_df[mask].y_pred - augmented_df[mask].y) / augmented_df[mask].y)\n",
    "    mean_pct_errs.append(mean_pct_err)\n",
    "    \n",
    "\n",
    "pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perform OLS on final layer </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ols = pytorch_model.forward_intermediate(torch.from_numpy(X).float()).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_ols, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06786713e-01,  3.19070697e-01, -2.83521295e-01, -5.30951321e-01,\n",
       "       -1.11107302e+00, -8.43273699e-01,  1.16229057e-06,  1.06622875e-01,\n",
       "        3.63074899e-01,  5.58793545e-08, -2.98023224e-08,  9.99934673e-02,\n",
       "       -3.50177288e-07, -4.76837158e-07, -5.44536293e-01,  2.83486307e-01,\n",
       "       -4.48852181e-02, -1.55996531e-08, -2.39884675e-01,  6.79491282e-01,\n",
       "       -3.02723646e-02,  2.08616257e-07, -6.81084037e-01,  0.00000000e+00,\n",
       "        3.99387747e-01,  2.37562209e-01, -4.95155334e-01, -1.19209290e-07,\n",
       "       -1.63912773e-07,  8.03550333e-02,  2.08616257e-07,  4.14938122e-01,\n",
       "        5.87636232e-01,  2.57033288e-01,  2.18202889e-01,  1.63912773e-07,\n",
       "        1.62759155e-01,  1.92945451e-01, -1.06310844e-01, -1.78813934e-07,\n",
       "       -1.79884911e-01,  3.16589445e-01,  2.08835244e-01,  9.00914490e-01,\n",
       "       -2.98023224e-08,  1.83933765e-01, -6.90452993e-01,  0.00000000e+00,\n",
       "       -8.14925209e-02, -4.02680814e-01, -2.69617587e-02,  6.67871892e-01,\n",
       "        0.00000000e+00,  7.06496239e-01,  0.00000000e+00,  2.57771671e-01,\n",
       "        0.00000000e+00,  1.69338822e-01, -8.57449323e-02,  8.42009783e-02,\n",
       "        3.64438295e-02, -5.63630104e-01, -2.90542006e-01, -6.57351315e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0679e-01,  3.1907e-01, -2.8352e-01, -5.3095e-01, -1.1111e+00,\n",
       "         -8.4327e-01,  1.1623e-06,  1.0662e-01,  3.6307e-01,  5.5879e-08,\n",
       "         -2.9802e-08,  9.9993e-02, -3.5018e-07, -4.7684e-07, -5.4454e-01,\n",
       "          2.8349e-01, -4.4885e-02, -1.5600e-08, -2.3988e-01,  6.7949e-01,\n",
       "         -3.0272e-02,  2.0862e-07, -6.8108e-01,  0.0000e+00,  3.9939e-01,\n",
       "          2.3756e-01, -4.9516e-01, -1.1921e-07, -1.6391e-07,  8.0355e-02,\n",
       "          2.0862e-07,  4.1494e-01,  5.8764e-01,  2.5703e-01,  2.1820e-01,\n",
       "          1.6391e-07,  1.6276e-01,  1.9295e-01, -1.0631e-01, -1.7881e-07,\n",
       "         -1.7988e-01,  3.1659e-01,  2.0884e-01,  9.0091e-01, -2.9802e-08,\n",
       "          1.8393e-01, -6.9045e-01,  0.0000e+00, -8.1493e-02, -4.0268e-01,\n",
       "         -2.6962e-02,  6.6787e-01,  0.0000e+00,  7.0650e-01,  0.0000e+00,\n",
       "          2.5777e-01,  0.0000e+00,  1.6934e-01, -8.5745e-02,  8.4201e-02,\n",
       "          3.6444e-02, -5.6363e-01, -2.9054e-01, -6.5735e-03]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07646975"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2024647 ,  0.09605896,  0.3325458 ,  0.01092559, -0.2536363 ,\n",
       "        -0.36302453,  0.26512384,  0.1708132 ,  0.24816908,  0.13684621,\n",
       "         0.2518013 ,  0.08240502,  0.0305814 , -0.19304208, -0.4585975 ,\n",
       "         0.22320633, -0.47782862,  0.29728243, -0.16720478,  0.27830702,\n",
       "        -0.3700971 , -0.28571743, -0.11325362, -0.0921092 ,  0.30040807,\n",
       "         0.33768034, -0.31778136,  0.18218729, -0.2121274 ,  0.2199305 ,\n",
       "        -0.1901273 ,  0.3289497 , -0.23294154, -0.2563552 ,  0.0983988 ,\n",
       "         0.16808727,  0.217962  ,  0.25226253, -0.28155154,  0.1247516 ,\n",
       "        -0.262387  , -0.17155552,  0.30622014,  0.2477939 , -0.1691536 ,\n",
       "         0.2733551 , -0.2318182 ,  0.1123949 ,  0.12816449, -0.30265012,\n",
       "         0.26815662,  0.2097252 , -0.06624337,  0.1236477 ,  0.2950945 ,\n",
       "         0.3244635 , -0.29667875, -0.33519307,  0.32149637,  0.14746365,\n",
       "         0.30879143, -0.2418651 , -0.42719656,  0.30562183]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(weights[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.output.weight.data = torch.from_numpy(np.array(lr.coef_).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.output.bias.data = torch.from_numpy(np.array([lr.intercept_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pytorch_model(torch.from_numpy(X).float()).detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error_pct': 7.438082437849622,\n",
       " 'mean_error_pct': 1.6617988080696113}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009314873877741148"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((pytorch_model(torch.from_numpy(X_test).float()).detach().numpy().squeeze() - y_test) / y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_bucket_error_2(y_pred, y):\n",
    "\n",
    "    buckets = np.arange(0, 10000, 1000) * 1e-4\n",
    "    bucket_strs = []\n",
    "    mean_errs = []\n",
    "    for low, high in zip(buckets, buckets[1:]):\n",
    "        bucket_str = '{}-{}'.format(round(1e4 * low), round(1e4 * high))\n",
    "        mask = (y >= low) & (y < high)\n",
    "        mean_err = np.mean((y_pred[mask] - y[mask]) / y[mask])\n",
    "        mean_errs.append(mean_err)\n",
    "        bucket_strs.append(bucket_str)\n",
    "    \n",
    "    return pd.DataFrame({'bucket': bucket_strs, 'mean_err': mean_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0-1000.0</td>\n",
       "      <td>0.035925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0-2000.0</td>\n",
       "      <td>0.011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0-3000.0</td>\n",
       "      <td>0.009951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0-4000.0</td>\n",
       "      <td>0.013634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4000.0-5000.0</td>\n",
       "      <td>0.013093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0-6000.0</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6000.0-7000.0</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000.0-8000.0</td>\n",
       "      <td>-0.004707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8000.0-9000.0</td>\n",
       "      <td>0.002903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bucket  mean_err\n",
       "0     0.0-1000.0  0.035925\n",
       "1  1000.0-2000.0  0.011479\n",
       "2  2000.0-3000.0  0.009951\n",
       "3  3000.0-4000.0  0.013634\n",
       "4  4000.0-5000.0  0.013093\n",
       "5  5000.0-6000.0  0.000516\n",
       "6  6000.0-7000.0  0.004813\n",
       "7  7000.0-8000.0 -0.004707\n",
       "8  8000.0-9000.0  0.002903"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_per_bucket_error_2(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth_bucket</th>\n",
       "      <th>mean_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2-0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3-0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4-0.5</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5-0.6</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6-0.7</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7-0.8</td>\n",
       "      <td>-0.003573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8-0.9</td>\n",
       "      <td>0.003206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9-1.0</td>\n",
       "      <td>0.005842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0-1.1</td>\n",
       "      <td>0.007641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.1-1.2</td>\n",
       "      <td>0.007154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2-1.3</td>\n",
       "      <td>0.008091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.3-1.4</td>\n",
       "      <td>0.008337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.4-1.5</td>\n",
       "      <td>0.010820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.5-1.6</td>\n",
       "      <td>0.010672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.6-1.7</td>\n",
       "      <td>0.010434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.7-1.8</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.8-1.9</td>\n",
       "      <td>0.007328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.9-2.0</td>\n",
       "      <td>0.009741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0-2.1</td>\n",
       "      <td>0.008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.1-2.2</td>\n",
       "      <td>0.006636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.2-2.3</td>\n",
       "      <td>0.006953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.3-2.4</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.4-2.5</td>\n",
       "      <td>0.007756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.5-2.6</td>\n",
       "      <td>0.050319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth_bucket  mean_err\n",
       "0       0.2-0.3       NaN\n",
       "1       0.3-0.4       NaN\n",
       "2       0.4-0.5  0.001599\n",
       "3       0.5-0.6  0.001967\n",
       "4       0.6-0.7  0.000028\n",
       "5       0.7-0.8 -0.003573\n",
       "6       0.8-0.9  0.003206\n",
       "7       0.9-1.0  0.005842\n",
       "8       1.0-1.1  0.007641\n",
       "9       1.1-1.2  0.007154\n",
       "10      1.2-1.3  0.008091\n",
       "11      1.3-1.4  0.008337\n",
       "12      1.4-1.5  0.010820\n",
       "13      1.5-1.6  0.010672\n",
       "14      1.6-1.7  0.010434\n",
       "15      1.7-1.8  0.010600\n",
       "16      1.8-1.9  0.007328\n",
       "17      1.9-2.0  0.009741\n",
       "18      2.0-2.1  0.008529\n",
       "19      2.1-2.2  0.006636\n",
       "20      2.2-2.3  0.006953\n",
       "21      2.3-2.4  0.002933\n",
       "22      2.4-2.5  0.007756\n",
       "23      2.5-2.6  0.050319"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X)\n",
    "augmented_df['y_pred_2'] = preds\n",
    "augmented_df['y'] = y\n",
    "\n",
    "depths = np.arange(0.2, 2.7, 0.1)\n",
    "mean_pct_errs = []\n",
    "depth_buckets = []\n",
    "for low_depth, high_depth in zip(depths, depths[1:]):\n",
    "    depth_bucket = '{}-{}'.format(round(low_depth, 2), round(high_depth, 2))\n",
    "    depth_buckets.append(depth_bucket)\n",
    "    mask = (augmented_df.depth >= low_depth) & (augmented_df.depth <= high_depth)\n",
    "    mean_pct_err = np.mean((augmented_df[mask].y_pred_2 - augmented_df[mask].y) / augmented_df[mask].y)\n",
    "    mean_pct_errs.append(mean_pct_err)\n",
    "    \n",
    "\n",
    "pd.DataFrame({'depth_bucket': depth_buckets, 'mean_err': mean_pct_errs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/root/data/alok/biomass_estimation/playground/output_model_v2.pb'\n",
    "torch.save(pytorch_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
