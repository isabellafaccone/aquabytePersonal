{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aquabyte-models/skip-classifier/model.pt current_production_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RDSAccessUtils(object):\n",
    "\n",
    "    def __init__(self, sql_credentials):\n",
    "        self.sql_engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(sql_credentials[\"user\"], \n",
    "         sql_credentials[\"password\"],\n",
    "         sql_credentials[\"host\"], sql_credentials[\"port\"],\n",
    "         sql_credentials[\"database\"]))\n",
    "        self.db_connection = self.sql_engine.connect()\n",
    "\n",
    "    def extract_from_database(self, sql_query):\n",
    "        results = self.db_connection.execute(sql_query)\n",
    "        df = pd.DataFrame(results.fetchall())\n",
    "        df.columns = results.keys()\n",
    "        return df\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client = RDSAccessUtils(json.load(open('/root/sid/credentials/data_warehouse_sql_credentials.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM prod.crop_annotation WHERE (service_id=1) \n",
    " AND (annotation_state_id IN (3, 4, 6, 7)) AND captured_at>'2020-01-16'\"\"\"\n",
    "\n",
    "production_data = client.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data['site_id'] = production_data['base_key'].str.split('/').apply(lambda ps: ps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# production_eval_img_dir = '/root/data/sid/needed_data/skip_classifier_datasets/production_evaluation/images/'\n",
    "# already_downloaded = set(os.listdir(production_eval_img_dir))\n",
    "# len(already_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(already_downloaded)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the metadata to only include analyzed images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2state = {\n",
    "    3:'QA',\n",
    "    4:'SKIPPED_ANN',\n",
    "    6:'SKIPPED_QA',\n",
    "    7:'VERIFIED'\n",
    "}\n",
    "\n",
    "production_data['state'] = production_data['annotation_state_id'].apply(lambda id: id2state[id] if id in id2state else None)\n",
    "production_data = production_data[production_data['state'].notnull()]\n",
    "production_data['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_accepts = production_data[production_data['state'] == 'VERIFIED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_counts = qa_accepts.site_id.value_counts()\n",
    "pen_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_pens = list(production_data.site_id.unique())\n",
    "naccepts_per_pen = 200\n",
    "sampled_accepts = pd.DataFrame([], columns=qa_accepts.columns)\n",
    "\n",
    "for s in all_pens:\n",
    "    this_pen_accepts = qa_accepts[qa_accepts['site_id'] == s]\n",
    "    this_pen_count = 0 if p not in pen_counts else pen_counts[p]\n",
    "    this_pen_sample = this_pen_accepts.sample(min(naccepts_per_pen, len(this_pen_accepts)))\n",
    "    sampled_accepts = pd.concat([sampled_accepts, this_pen_sample])\n",
    "sampled_accepts.site_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_accepts.site_id.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_counts = sampled_accepts['site_id'].value_counts()\n",
    "pen_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_counts[137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_skips = production_data[production_data['state'] == 'SKIPPED_ANN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_pens = list(production_data.site_id.unique())\n",
    "nskips_per_pen = int(round((len(qa_accepts)*2)/len(all_pens), 0))\n",
    "sampled_skips = pd.DataFrame([], columns=cogito_skips.columns)\n",
    "\n",
    "for p in all_pens:\n",
    "    this_pen_skips = cogito_skips[cogito_skips['site_id'] == p]\n",
    "    this_pen_count = 0 if p not in pen_counts else pen_counts[p]\n",
    "    this_pen_sample = this_pen_skips.sample(min(this_pen_count, len(this_pen_skips)))\n",
    "    sampled_skips = pd.concat([sampled_skips, this_pen_sample])\n",
    "sampled_skips.pen_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#eval_data = pd.concat([sampled_accepts, sampled_skips])\n",
    "eval_data = pd.concat([sampled_accepts, sampled_skips])\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(row):\n",
    "    if isinstance(row['left_crop_url'], str):\n",
    "        return row['left_crop_url']\n",
    "    elif isinstance(row['right_crop_url'], str):\n",
    "        return row['right_crop_url']\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "\n",
    "eval_data['url'] = eval_data.apply(get_url, axis=1)\n",
    "eval_data['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /root/data/sid/needed_data/skip_classifier_datasets/production_evaluation/may15-may20_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "production_eval_img_dir = '/root/data/sid/needed_data/skip_classifier_datasets/production_evaluation/images/'\n",
    "\n",
    "def get_local_path(url):\n",
    "    name = '_PATHSEP_'.join(url.split('/')[3:])\n",
    "    return os.path.join(production_eval_img_dir, name)\n",
    "eval_data['local_path'] = eval_data.url.apply(get_local_path)\n",
    "eval_data.local_path.iloc[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(already_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "already_downloaded = os.listdir('/root/data/sid/needed_data/skip_classifier_datasets/production_evaluation/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_downloaded = ['/root/data/sid/needed_data/skip_classifier_datasets/production_evaluation/images/' + url\n",
    "                      for url in already_downloaded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(already_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def download_image(row, exclude_images=[]):\n",
    "    url, local_path = row['url'], row['local_path']\n",
    "    if local_path not in exclude_images:\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(local_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    return local_path \n",
    "\n",
    "download_image(eval_data.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data_images = eval_data[~eval_data.url.duplicated()]\n",
    "production_data_images.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_downloaded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_download = production_data_images[~production_data_images['local_path'].isin(already_downloaded)]\n",
    "len(need_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "need_to_download.progress_apply(partial(download_image), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data = production_data_images#[production_data_images['local_path'].isin(already_downloaded)]\n",
    "downloaded_production_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classifier on these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/testing123__2021-01-19__08-57-40/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL_NAME = 'testing123__2021-01-19__08-57-40'\n",
    "NEW_MODEL_PATH = os.path.join('/root/data/sid/needed_datasets/skip_classifier_checkpoints/', NEW_MODEL_NAME)\n",
    "#SPLITS_NAME = '07-14-2020_stratify_hour_partialfish.json'\n",
    "#SPLIT_PATH = os.path.join('/root/data/sid/needed_data/skip_classifier_datasets/splits', SPLITS_NAME)\n",
    "BEST_EPOCH = 'epoch_1'\n",
    "metric_path = os.path.join(NEW_MODEL_PATH, BEST_EPOCH, 'train', 'metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open(metric_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/sid/repos/cv_research/sid/lice_counting/skip_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/qa_accept_cogito_skips_05-15-2020_recentsample_stratified__2020-05-17__13-15-57/epoch_14/val/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'torch==1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'torch==1.3.1'\n",
    "# !pip install 'torchvision==0.4.2'\n",
    "# !pip install 'albumentations==0.4.5'\n",
    "# !pip install 'opencv-python==4.2.0.32'\n",
    "# !pip install --upgrade 'numpy==1.15.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/data/sid/needed_datasets/skip_classifier_checkpoints/\n",
    "#  /root/data/sid/needed_datasets/skip_classifier_checkpoints/testing123__2020-07-15__08-51-12/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/sid/repos/cv_research/sid/lice_counting/skip_classifier/')\n",
    "\n",
    "from model import MultilabelClassifier\n",
    "help(MultilabelClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import ACCEPT_LABEL, SKIP_LABEL\n",
    "import torch\n",
    "\n",
    "device = 0\n",
    "metric_path = os.path.join(NEW_MODEL_PATH, BEST_EPOCH, 'train', 'metrics.json')\n",
    "print(json.load(open(metric_path)))\n",
    "path = os.path.join(NEW_MODEL_PATH, BEST_EPOCH, 'train', 'model.pt')\n",
    "new_model = MultilabelClassifier(savename=None, num_labels=5)\n",
    "new_model.load_state_dict(torch.load(path))\n",
    "new_model.to(device)\n",
    "new_model.cuda()\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = [ACCEPT_LABEL, SKIP_LABEL]\n",
    "# eval_set['paths'] = production_data['local_path']\n",
    "# eval_set['labels'] = production_data['skip_reasons'].notnull().apply(int)\n",
    "# samples = [(path, label) for path, label in zip(\n",
    "#             eval_set['paths'], eval_set['labels'])]\n",
    "# len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from loader import TRANSFORMS\n",
    "import cv2\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "def image_to_array(file_path):\n",
    "    # Read an image with OpenCV\n",
    "    image = cv2.imread(file_path)\n",
    "\n",
    "    # By default OpenCV uses BGR color space for color images,\n",
    "    # so we need to convert the image to RGB color space.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = TRANSFORMS['pad'](image=image)['image']\n",
    "    return image \n",
    "\n",
    "def multilabel_preds(cuda_inputs, model):\n",
    "    outputs = model(cuda_inputs)\n",
    "    preds = sigmoid(outputs)\n",
    "    cpu_outputs = preds.detach().cpu().numpy()\n",
    "    return cpu_outputs[0]\n",
    "\n",
    "def regular_preds(cuda_inputs, model):\n",
    "    preds = model(cuda_inputs)\n",
    "    cpu_outputs = preds.detach().cpu().numpy()\n",
    "    return cpu_outputs[0][0]\n",
    "\n",
    "def get_predictions(image, model, pred_fn):\n",
    "    cuda_inputs = torch.unsqueeze(image.to(device), dim=0)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        return pred_fn(cuda_inputs, model)\n",
    "    \n",
    "def path2newmodelpredictions(file_path):\n",
    "    return get_predictions(image_to_array(file_path), new_model, multilabel_preds)\n",
    "\n",
    "downloaded_production_data['local_path'].iloc[:5].apply(path2newmodelpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_MODEL_NAME = 'bodypart_model_multi__2020-09-28__00-36-53'\n",
    "OLD_MODEL_PATH = os.path.join('/root/data/sid/needed_datasets/skip_classifier_checkpoints/', OLD_MODEL_NAME)\n",
    "#SPLITS_NAME = '07-14-2020_stratify_hour_partialfish.json'\n",
    "#SPLIT_PATH = os.path.join('/root/data/sid/needed_data/skip_classifier_datasets/splits', SPLITS_NAME)\n",
    "BEST_EPOCH = 'epoch_0'\n",
    "metric_path = os.path.join(OLD_MODEL_PATH, BEST_EPOCH, 'train', 'metrics.json')\n",
    "open(metric_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import ACCEPT_LABEL, SKIP_LABEL\n",
    "import torch\n",
    "from model import ImageClassifier\n",
    "\n",
    "device = 0\n",
    "metric_path = os.path.join(OLD_MODEL_PATH, BEST_EPOCH, 'train', 'metrics.json')\n",
    "print(json.load(open(metric_path)))\n",
    "path = os.path.join(OLD_MODEL_PATH, BEST_EPOCH, 'train', 'model.pt')\n",
    "old_model = MultilabelClassifier(savename=None, num_labels=5)\n",
    "old_model.load_state_dict(torch.load(path))\n",
    "old_model.to(device)\n",
    "old_model.cuda()\n",
    "old_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2oldmodelpredictions(file_path):\n",
    "    return get_predictions(image_to_array(file_path), old_model, multilabel_preds)\n",
    "\n",
    "downloaded_production_data['local_path'].iloc[:5].apply(path2oldmodelpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data = downloaded_production_data.sort_values('captured_at', ascending=True).head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['production_predicted_accept_prob'] = downloaded_production_data.progress_apply(lambda row:\n",
    "    row['left_crop_metadata']['quality_score'] if row['left_crop_metadata'] \n",
    "    else row['right_crop_metadata']['quality_score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['old_model_predicted_accept_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "downloaded_production_data['new_model_predicted_accept_prob'] = downloaded_production_data['local_path'].progress_apply(\n",
    "    path2newmodelpredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['old_model_predicted_accept_prob'] = downloaded_production_data['local_path'].progress_apply(path2oldmodelpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['new_model_cleaned'] = downloaded_production_data['new_model_predicted_accept_prob'].apply(sum)\n",
    "downloaded_production_data['old_model_cleaned'] = downloaded_production_data['old_model_predicted_accept_prob'].apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data.annotation.apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data2 = downloaded_production_data[downloaded_production_data['annotation'].apply(type)!=list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "len(downloaded_production_data2)\n",
    "\n",
    "def get_sections(row):\n",
    "    ann = row['annotation']\n",
    "\n",
    "    if row['annotation_state_id'] == 7:\n",
    "        if ann is None:\n",
    "            return ['VENTRAL_POSTERIOR',\n",
    "                     'VENTRAL_ANTERIOR',\n",
    "                     'DORSAL_POSTERIOR',\n",
    "                     'DORSAL_ANTERIOR',\n",
    "                     'HEAD']\n",
    "        elif ann.get('isPartial', None):\n",
    "            return ann.get('visibleBodySections', None)\n",
    "        else:\n",
    "            return ['VENTRAL_POSTERIOR',\n",
    "                     'VENTRAL_ANTERIOR',\n",
    "                     'DORSAL_POSTERIOR',\n",
    "                     'DORSAL_ANTERIOR',\n",
    "                     'HEAD']\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "            \n",
    "\n",
    "downloaded_production_data2['visibleBodySections'] = downloaded_production_data2.apply(get_sections, axis=1)\n",
    "print(downloaded_production_data2['visibleBodySections'].isnull().sum())\n",
    "downloaded_production_data2 = downloaded_production_data2[downloaded_production_data2['visibleBodySections'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data2\n",
    "\n",
    "for col in BODYPART_COLS:\n",
    "    downloaded_production_data2[col] = downloaded_production_data2['visibleBodySections'].apply(\n",
    "        lambda l: col[4:] in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['site_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new = downloaded_production_data2.sort_values('new_model_cleaned', ascending=False)\n",
    "new[BODYPART_COLS] = new[BODYPART_COLS].cumsum()\n",
    "new['kpi'] = new[BODYPART_COLS].apply(min, axis=1)\n",
    "\n",
    "old = downloaded_production_data2.sort_values('old_model_cleaned', ascending=False)\n",
    "old[BODYPART_COLS] = old[BODYPART_COLS].cumsum()\n",
    "old['kpi'] = old[BODYPART_COLS].apply(min, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(new)), new['kpi'])\n",
    "ax.plot(range(len(old)), old['kpi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new = downloaded_production_data2.sort_values('new_model_cleaned', ascending=False)\n",
    "new[BODYPART_COLS] = new[BODYPART_COLS].cumsum()\n",
    "new['kpi'] = new[BODYPART_COLS].apply(min, axis=1)\n",
    "\n",
    "old = downloaded_production_data2.sort_values('old_model_cleaned', ascending=False)\n",
    "old[BODYPART_COLS] = old[BODYPART_COLS].cumsum()\n",
    "old['kpi'] = old[BODYPART_COLS].apply(min, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(new)), new['kpi'])\n",
    "ax.plot(range(len(old)), old['kpi'])\n",
    "ax.set_ylim((0, 500))\n",
    "ax.set_xlim((0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def path2oldmodelpredictions(file_path):\n",
    "#     return get_predictions(image_to_array(file_path), old_model)\n",
    "\n",
    "# downloaded_production_data['old_model_predicted_accept_prob'] = downloaded_production_data['local_path'].progress_apply(path2oldmodelpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(labels, outputs, title, pen_id=None, skip_reason=None, ax=None):\n",
    "    if len(labels.unique()) > 1:\n",
    "        fpr, tpr, thresholds = roc_curve(labels, outputs)\n",
    "        auc = roc_auc_score(labels, outputs)\n",
    "        lw = 2\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.plot(fpr, tpr, color='darkorange',\n",
    "                lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate (skiprate)')\n",
    "        ax.set_ylabel('Recall (KPI)')\n",
    "        ax.set_title(title, size=20)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import BODYPART_COLS\n",
    "\n",
    "BODYPART_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['annotation'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_sections(row):\n",
    "    ann = row['annotation']\n",
    "\n",
    "    if row['annotation_state_id'] == 7:\n",
    "        if ann is None:\n",
    "            return ['VENTRAL_POSTERIOR',\n",
    "                     'VENTRAL_ANTERIOR',\n",
    "                     'DORSAL_POSTERIOR',\n",
    "                     'DORSAL_ANTERIOR',\n",
    "                     'HEAD']\n",
    "        elif isinstance(ann, list):\n",
    "            print(row)\n",
    "        elif (not ann['isPartial']):\n",
    "            return ['VENTRAL_POSTERIOR',\n",
    "                     'VENTRAL_ANTERIOR',\n",
    "                     'DORSAL_POSTERIOR',\n",
    "                     'DORSAL_ANTERIOR',\n",
    "                     'HEAD']\n",
    "        else:\n",
    "            return ann['visibleBodySections']\n",
    "    else:\n",
    "        return []\n",
    "            \n",
    "\n",
    "downloaded_production_data['visibleBodySections'] = downloaded_production_data.apply(get_sections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bp_col in BODYPART_COLS:\n",
    "    downloaded_production_data[bp_col] = downloaded_production_data.visibleBodySections.progress_apply(\n",
    "        lambda sections: bp_col[4:] in sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodypart_weights = 1/downloaded_production_data[BODYPART_COLS].mean()\n",
    "bodypart_weights /= bodypart_weights.sum()\n",
    "bodypart_weights.index = bodypart_weights.index.map(lambda s: s[4:])\n",
    "normalize_params = {'overall': bodypart_weights.to_dict()}\n",
    "normalize_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(normalize_params, open('oct5_bodypart_normalize_params.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODYPART_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, bp in enumerate(BODYPART_COLS):\n",
    "    pred_col = bp.replace('HAS_', 'PRED_')\n",
    "    downloaded_production_data[pred_col] = downloaded_production_data['new_model_predicted_accept_prob'].apply(\n",
    "        lambda arr: arr[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cogito accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(state):\n",
    "    if state == 'VERIFIED':\n",
    "        return 1 \n",
    "    elif state == 'SKIPPED_ANN':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "downloaded_production_data['label'] = downloaded_production_data['state'].apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_production_data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cogito_data = downloaded_production_data[downloaded_production_data['cogito_label'].notnull()]\n",
    "\n",
    "all_pens = ['overall'] + list(cogito_data.pen_id.unique())\n",
    "fig, axes = plt.subplots(nrows=len(all_pens), ncols=7, figsize=(30, 5*len(all_pens)))\n",
    "cogito_data = downloaded_production_data[downloaded_production_data['label'].notnull()]\n",
    "\n",
    "for pen, ax in zip(all_pens, axes):\n",
    "    if pen != 'overall':\n",
    "        this_pen = cogito_data[cogito_data['pen_id'] == pen]\n",
    "    else:\n",
    "        this_pen = cogito_data\n",
    "    if pen == 'overall':\n",
    "        title1 = 'production'\n",
    "        title2 = 'fullbody'\n",
    "    ax[0].set_xlabel(f'pen:{pen}')\n",
    "    plot_roc(this_pen['label'], this_pen['production_predicted_accept_prob'], ax=ax[0], title=title1)\n",
    "    plot_roc(this_pen['label'], this_pen['old_model_predicted_accept_prob'], ax=ax[1], title=title2)\n",
    "    for i, col in enumerate(BODYPART_COLS):\n",
    "        if pen == 'overall':\n",
    "            title = col\n",
    "        else:\n",
    "            title = ''\n",
    "        try:\n",
    "            plot_roc(this_pen[col], this_pen[col.replace('HAS_', 'PRED_')], ax=ax[i+2], pen_id=pen, title=title)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_data['production_predicted_accept_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "res = dict()\n",
    "pen = 'overall'\n",
    "for pen in all_pens:\n",
    "    res[pen] = dict()\n",
    "    if pen != 'overall':\n",
    "        this_pen = cogito_data[cogito_data['pen_id'] == pen]\n",
    "    else:\n",
    "        this_pen = cogito_data\n",
    "    pred_cols = ['production_predicted_accept_prob', 'old_model_predicted_accept_prob'] + [col.replace('HAS_', 'PRED_') for col in BODYPART_COLS]\n",
    "    lab_cols = ['label', 'label'] + BODYPART_COLS\n",
    "    for lab_col, pred_col in zip(lab_cols, pred_cols):\n",
    "        if len(this_pen[lab_col].unique()) != 1:\n",
    "            res[pen][pred_col] = roc_auc_score(this_pen[lab_col], this_pen[pred_col])\n",
    "pd.DataFrame(res).apply(pd.Series, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cogito_accept_rate(states):\n",
    "    if states.isin([3,4]).sum() > 20:\n",
    "        return (states == 3).sum() / states.isin([3,4]).sum()\n",
    "    \n",
    "def cogito_samples(states):\n",
    "    return states.isin([3, 4]).sum()\n",
    "\n",
    "def qa_samples(states):\n",
    "    return states.isin([4, 6, 7]).sum()\n",
    "\n",
    "def qa_accept_rate(states):\n",
    "    if states.isin([4,6,7]).sum()> 20:\n",
    "        return (states == 7).sum() / states.isin([4,6,7]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nbins = 20\n",
    "bins = [0] + [cogito_data['new_model_predicted_accept_prob'].quantile(b/nbins) for b in range(nbins+1)]\n",
    "cogito_data['score_bucket'] = pd.cut(cogito_data['new_model_predicted_accept_prob'], bins=bins, labels=False)\n",
    "expected_bins = set(range(nbins+1))\n",
    "\n",
    "def acceptrate_to_score_hist(data, bins, binmethod='pctile'):\n",
    "        \n",
    "    \n",
    "    scorebin_acceptrates = data.groupby('score_bucket')['annotation_state_id'].aggregate(\n",
    "        [cogito_accept_rate, qa_accept_rate, cogito_samples, qa_samples])\n",
    "    missing_bins = set(range(len(bins)-1)) - set(list(scorebin_acceptrates.index))\n",
    "    #missing_bins = scorebin_acceptrates[scorebin_acceptrates['cogito_accept_rate'].isnull()].index\n",
    "    for b in missing_bins:\n",
    "        scorebin_acceptrates.loc[b] = None\n",
    "        #next_bin = b\n",
    "        #while (next_bin in missing_bins) and (next_bin <= nbins):\n",
    "        #    next_bin += 1\n",
    "        #if next_bin <= nbins:\n",
    "        #    scorebin_acceptrates.loc[b] = scorebin_acceptrates.loc[next_bin]\n",
    "        #else:\n",
    "        #    scorebin_acceptrates.loc[b] = scorebin_acceptrates.loc[max(found_bins)]\n",
    "    scorebin_acceptrates.sort_index(inplace=True)\n",
    "    scorebin_acceptrates['cutoff'] = bins[:-1]\n",
    "    return scorebin_acceptrates\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "acceptrate_to_score_hist(cogito_data, bins).plot.bar(y=['cogito_accept_rate'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pen_scores = dict()\n",
    "pen_counts = dict()\n",
    "all_pens = ['overall'] + list(cogito_data['pen_id'].unique())\n",
    "fig, axes = plt.subplots(nrows=len(all_pens), figsize=(5, len(all_pens)*5))\n",
    "\n",
    "for pen_id, ax in zip(all_pens, axes):\n",
    "    if pen_id != 'overall':\n",
    "        print(pen_id)\n",
    "        this_pen = cogito_data[cogito_data['pen_id'] == pen_id]\n",
    "        print(this_pen)\n",
    "        rate2scores = acceptrate_to_score_hist(this_pen, bins)\n",
    "        rate2scores = rate2scores[rate2scores['cogito_samples'] > 20] \n",
    "        pen_scores[pen_id] = rate2scores\n",
    "        if len(pen_scores):\n",
    "            pen_scores[pen_id].plot.bar(y=['cogito_accept_rate'], ax=ax)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f'Pen:{pen_id}')\n",
    "        ax.set_ylim((0, 1.0))\n",
    "    else:\n",
    "        pen_scores[pen_id] = acceptrate_to_score_hist(cogito_data, bins)\n",
    "        pen_scores[pen_id].plot.bar(y=['cogito_accept_rate'], ax=ax)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f'Pen:{pen_id}')\n",
    "        ax.set_ylim((0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_counts = cogito_data.groupby(['pen_id', 'score_bucket'])['score_bucket'].aggregate('count')\n",
    "for pen_id in cogito_data['pen_id'].unique():\n",
    "    print(pen_id)\n",
    "    print(pen_counts.loc[(pen_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "transform_data = dict()\n",
    "\n",
    "for pen_id in pen_scores:\n",
    "    cutoffs = pen_scores[pen_id]['cutoff']\n",
    "    rates = pen_scores[pen_id]['cogito_accept_rate']\n",
    "    bins = list(sorted([int(bin) for bin in cutoffs.keys()]))\n",
    "    print(rates)\n",
    "    transform_data[pen_id] = [(cutoffs[b], rates[b]) for b in bins\n",
    "                              if not np.isnan(rates[b])]\n",
    "\n",
    "transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_normscore(data):\n",
    "    new_data = dict()\n",
    "    for pen in data:\n",
    "        if len(data[pen]) < 5:\n",
    "            continue\n",
    "        new_data[str(pen)] = []\n",
    "        cutoffs = [x[0] for x in data[pen]]\n",
    "        assert cutoffs == sorted(cutoffs)\n",
    "        rates = [x[1] for x in data[pen]]\n",
    "        for idx, (cutoff, rate) in enumerate(zip(cutoffs, rates)):\n",
    "            if idx != 0:\n",
    "                these_rates = [x[1] for x in new_data[str(pen)]]\n",
    "                biggest_rate_sofar = max(these_rates[:idx])\n",
    "            else:\n",
    "                biggest_rate_sofar = 0.0\n",
    "            \n",
    "            if rate is None:      \n",
    "                rate = biggest_rate_sofar\n",
    "            if  rate < biggest_rate_sofar:\n",
    "                rate = biggest_rate_sofar\n",
    "            \n",
    "            new_data[str(pen)].append((cutoff, rate))\n",
    "            \n",
    "        if new_data[str(pen)][0][0] != 0.0:\n",
    "            new_data[str(pen)] = [(0, new_data[str(pen)][0][1])] + new_data[str(pen)]\n",
    "        if new_data[str(pen)][-1][0] != 1.0:\n",
    "            new_data[str(pen)] = new_data[str(pen)] + [(1.0, new_data[str(pen)][-1][1])]\n",
    "        if max([x[1] for x in new_data[str(pen)]]) == 0:\n",
    "               del new_data[str(pen)]\n",
    "    return new_data\n",
    "new_data = postprocess_normscore(transform_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['119']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(new_data, open('pen_normalization_aug3_model.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://aquabyte-research/sid/production_models/skip_classifier/08-03-2020/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp /root/data/sid/needed_datasets/skip_classifier_checkpoints/08-03-2020_stratify_hour_partialfish_justlice__2020-08-03__01-47-17/epoch_2/val/model.pt s3://aquabyte-research/sid/production_models/skip_classifier/08-03-2020/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp pen_normalization_aug3_model.json s3://aquabyte-research/sid/production_models/skip_classifier/08-03-2020/norm_params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
