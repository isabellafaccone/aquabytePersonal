{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'qa_accept_cogito_skips_03-04-2020_stratified'\n",
    "MODEL_PATH = os.path.join('/root/data/sid/skip_classifier_checkpoints/', MODEL_NAME)\n",
    "SPLIT_PATH = os.path.join('/root/data/sid/skip_classifier_datasets/splits', MODEL_NAME + '_splits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Best Model Using Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "best_epoch ={'precision':(0, None, None), 'recall': (0, None, None), 'auc': (0, None, None)}\n",
    "\n",
    "for epoch in os.listdir(MODEL_PATH):\n",
    "    metrics_path = os.path.join(MODEL_PATH, epoch, 'val', 'metrics.json')\n",
    "    metrics = json.load(open(metrics_path))['acc']\n",
    "    for m in best_epoch:\n",
    "        if metrics[m] > best_epoch[m][0]:\n",
    "            best_epoch[m] = (metrics[m], metrics, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EPOCH = best_epoch['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "splits = json.load(open(SPLIT_PATH))\n",
    "splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [splits['original'][i] for i in splits['test_indices']]\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_files = [x[0] for x in test_set]\n",
    "all_metadata_files = [s.replace('_crop.jpg', '_metadata.json') for s in all_image_files]\n",
    "all_metadata_data = []\n",
    "for i, (metadata_path, image_path) in enumerate(zip(all_metadata_files, all_image_files)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    assert metadata_path.split('/')[-1].split('_')[0] == image_path.split('/')[-1].split('_')[0], (metadata_path, image_path)\n",
    "    metadata = json.load(open(metadata_path))\n",
    "    metadata['local_image_path'] = image_path\n",
    "    all_metadata_data.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_dataset = pd.DataFrame.from_dict(all_metadata_data)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.drop_duplicates('left_crop_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepts = eval_dataset[eval_dataset['skip_reasons'].isnull()]\n",
    "accepts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepts.pen_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips = eval_dataset[eval_dataset['skip_reasons'].notnull()]\n",
    "skips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips = skips.groupby('pen_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000)))\n",
    "skips.pen_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = pd.concat([skips, accepts])\n",
    "eval_set = eval_set.sample(frac=1)\n",
    "print(eval_set.pen_id.value_counts())\n",
    "eval_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "useful_labels = [\n",
    "    'BLURRY',\n",
    "    'BAD_CROP',\n",
    "    'BAD_ORIENTATION',\n",
    "    'OBSTRUCTION',\n",
    "    'TOO_DARK'\n",
    "]\n",
    "\n",
    "eval_dataset['skip_reasons'] = eval_dataset['skip_reasons'].str.replace(\"'\", \"\\\"\")\n",
    "eval_dataset['skip_reasons'] = eval_dataset['skip_reasons'].apply(lambda l: l if l is None else json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(useful_labels):\n",
    "    eval_set[f'{label}'] = eval_set['skip_reasons'].apply(lambda l: False if l is None else (label in l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ImageClassifier\n",
    "from train import ACCEPT_LABEL, SKIP_LABEL\n",
    "\n",
    "path = os.path.join(MODEL_PATH, BEST_EPOCH[2], 'val', 'model.pt')\n",
    "model = ImageClassifier([ACCEPT_LABEL, SKIP_LABEL], device=0, savename=None)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import get_image_backend\n",
    "\n",
    "get_image_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "def albumentations_loader(file_path):\n",
    "    # Read an image with OpenCV\n",
    "    image = cv2.imread(file_path)\n",
    "\n",
    "    # By default OpenCV uses BGR color space for color images,\n",
    "    # so we need to convert the image to RGB color space.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, classes, samples, loader=albumentations_loader, extensions=None, transform=None,\n",
    "                 target_transform=None, is_valid_file=None):\n",
    "        if len(samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {c: classes.index(c) for c in classes}\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = albumentations_loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(image=sample)['image']\n",
    "\n",
    "        return sample, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [ACCEPT_LABEL, SKIP_LABEL]\n",
    "eval_set['paths'] = eval_set['local_image_path']\n",
    "eval_set['labels'] = eval_set['skip_reasons'].notnull().apply(int)\n",
    "samples = [(path, label) for path, label in zip(\n",
    "            eval_set['paths'], eval_set['labels'])]\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import TRANSFORMS\n",
    "\n",
    "dataset = ImageDataset(classes, samples, transform=TRANSFORMS['pad'])\n",
    "example = dataset[0]\n",
    "print(example)\n",
    "print(example[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=1)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = None\n",
    "all_outputs = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, tgts) in enumerate(loader):\n",
    "        cuda_inputs = inputs.to(device)\n",
    "        outputs = model(cuda_inputs)\n",
    "        outputs = outputs.cpu()\n",
    "        if all_outputs is None:\n",
    "            all_outputs = outputs\n",
    "            all_labels = tgts\n",
    "        else:\n",
    "            all_outputs = torch.cat([all_outputs, outputs])\n",
    "            all_labels = torch.cat([all_labels, tgts])\n",
    "        print(f'batch:{i}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_outputs.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = all_outputs.detach().numpy()\n",
    "all_labels = all_labels.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = eval_set.iloc[:all_outputs.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(eval_set) == all_outputs.shape[0]\n",
    "\n",
    "eval_set['model_outputs'] = all_outputs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set['loaded_labels'] = (all_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert (eval_set['labels'] == eval_set['loaded_labels']).sum() == len(eval_set), eval_set[['labels', 'loaded_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_set['model_preds'] = eval_set['model_outputs'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_roc(fpr, tpr, auc, pen_id, skip_reason, ax):\n",
    "    lw = 2\n",
    "    ax.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate (skiprate)')\n",
    "    ax.set_ylabel('Recall (KPI)')\n",
    "    ax.set_title(f'ROC Curve Pen:{pen_id} SkipReason:{skip_reason}', size=20)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def evaluate(eval_set, pen_id, skip_reason):\n",
    "    results = dict()\n",
    "    results['pen_id'] = pen_id\n",
    "    results['skip_reason'] = skip_reason\n",
    "    results['n'] = len(eval_set)\n",
    "    if eval_set['model_preds'].sum():\n",
    "        results['prec'] = precision_score(eval_set['loaded_labels'], eval_set['model_preds'])\n",
    "    else:\n",
    "        results['prec'] = None\n",
    "    if eval_set['loaded_labels'].sum():\n",
    "        results['rec'] = recall_score(eval_set['loaded_labels'], eval_set['model_preds'])\n",
    "    else:\n",
    "        results['rec'] = None\n",
    "    try:\n",
    "        results['auc'] = roc_auc_score(eval_set['loaded_labels'], eval_set['model_outputs'])\n",
    "        fpr, tpr, thresholds = roc_curve(eval_set['loaded_labels'], eval_set['model_outputs'])\n",
    "    except:\n",
    "        results['auc'] = None\n",
    "        fpr, tpr, thresholds = None, None, None\n",
    "    return results,  (fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "all_pens = ['overall']\n",
    "all_labels = ['overall']\n",
    "nrows = len(all_pens)\n",
    "ncols = len(all_labels)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10*ncols, 10*nrows))\n",
    "\n",
    "pen_id, skip_reason='overall', 'overall'\n",
    "if pen_id != 'overall':\n",
    "    this_eval_set = eval_set[eval_set['pen_id'] == pen_id]\n",
    "else:\n",
    "    this_eval_set = eval_set\n",
    "    \n",
    "if skip_reason != 'overall' :\n",
    "    skipped_with_this_reason = this_eval_set[skip_reason]\n",
    "    accepted = this_eval_set['loaded_labels']\n",
    "    #print(skipped_with_this_reason)\n",
    "    #print(accepted)\n",
    "    this_eval_set = this_eval_set[skipped_with_this_reason | accepted]\n",
    "else:\n",
    "    this_eval_set = this_eval_set\n",
    "    \n",
    "result, (fpr, tpr, thresholds) = evaluate(this_eval_set, pen_id, skip_reason)\n",
    "results.append(result)\n",
    "if fpr is not None:\n",
    "    plot_roc(fpr, tpr, result['auc'], pen_id, skip_reason, axes)\n",
    "    \n",
    "out = pd.DataFrame.from_dict(results)\n",
    "out.set_index(['pen_id', 'skip_reason'], inplace=True)\n",
    "out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "all_pens = ['overall'] + list(eval_set['pen_id'].unique())\n",
    "all_labels = ['overall'] + useful_labels\n",
    "nrows = len(all_pens)\n",
    "ncols = 1 + len(useful_labels)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10*ncols, 10*nrows))\n",
    "\n",
    "\n",
    "for i, pen_id in enumerate(all_pens):\n",
    "    for j, skip_reason in enumerate(all_labels):\n",
    "        if pen_id != 'overall':\n",
    "            this_eval_set = eval_set[eval_set['pen_id'] == pen_id]\n",
    "        else:\n",
    "            this_eval_set = eval_set\n",
    "            \n",
    "        if skip_reason != 'overall' :\n",
    "            skipped_with_this_reason = this_eval_set[skip_reason]\n",
    "            accepted = this_eval_set['loaded_labels']\n",
    "            #print(skipped_with_this_reason)\n",
    "            #print(accepted)\n",
    "            this_eval_set = this_eval_set[skipped_with_this_reason | accepted]\n",
    "        else:\n",
    "            this_eval_set = this_eval_set\n",
    "            \n",
    "        result, (fpr, tpr, thresholds) = evaluate(this_eval_set, pen_id, skip_reason)\n",
    "        results.append(result)\n",
    "        if fpr is not None:\n",
    "            plot_roc(fpr, tpr, result['auc'], pen_id, skip_reason, axes[i][j])\n",
    "    \n",
    "out = pd.DataFrame.from_dict(results)\n",
    "out.set_index(['pen_id', 'skip_reason'], inplace=True)\n",
    "out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
