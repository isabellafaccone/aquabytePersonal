{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/root/data/sid/skip_classifier_datasets/raw/production_skips_accepts/annotations.csv'\n",
    "annotations = pd.read_csv(path)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations[annotations['left_crop_url'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['annotation_state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's a couple ways we can slice and dice this skip dataset to improve the quality, which depends on how what we include to be skips, and what we include to be accepts.\n",
    "\n",
    "* Skips\n",
    "  1. Use all cogito skips\n",
    "  2. Use all QA skips\n",
    "  3. Use confident cogito skips\n",
    "  4. Use confident QA skips.\n",
    "  5. Break out skips into different skip reasons.\n",
    "    \n",
    "* Accepts\n",
    "  1. Use all cogito accepts\n",
    "  2. Use all QA accepts\n",
    "  3. Use confident cogito skips.\n",
    "  4. Use confident cogito accepts.\n",
    "    \n",
    "##### To start, let's do 2 options:\n",
    "\n",
    "### all cogito skips and all cogito accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SAMPLE_SIZE = 100000\n",
    "SAMPLE_RATIO = 0.7\n",
    "\n",
    "cogito_skips = annotations[annotations['annotation_state_id'] == 4]\n",
    "cogito_skips = cogito_skips[~cogito_skips.left_crop_url.duplicated()]\n",
    "cogito_skips = cogito_skips.sample(int(SAMPLE_RATIO * SAMPLE_SIZE))\n",
    "cogito_accepts = annotations[annotations['annotation_state_id'] == 3]\n",
    "cogito_accepts = cogito_accepts[~cogito_accepts.left_crop_url.duplicated()]\n",
    "cogito_accepts = cogito_accepts.sample(int((1-SAMPLE_RATIO) * SAMPLE_SIZE))\n",
    "all_cogito_data = pd.concat([cogito_skips, cogito_accepts])\n",
    "all_cogito_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confident cogito skips and confident QA accepts, just be to sure the labels have clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_skips = annotations[annotations['annotation_state_id'] == 4]\n",
    "cogito_skips = cogito_skips[~cogito_skips.left_crop_url.duplicated()]\n",
    "cogito_skips = cogito_skips.sample(int(SAMPLE_RATIO * SAMPLE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_accepts = annotations[annotations['annotation_state_id'] == 7]\n",
    "qa_accepts = qa_accepts[~qa_accepts.left_crop_url.duplicated()]\n",
    "qa_accepts = qa_accepts.sample(int((1-SAMPLE_RATIO) * SAMPLE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset = pd.concat([cogito_skips, qa_accepts])\n",
    "skip_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(state_id):\n",
    "    if state_id == 4:\n",
    "        return 'SKIP'\n",
    "    elif state_id == 7:\n",
    "        return 'ACCEPT'\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "skip_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset.to_csv('/root/data/sid/skip_classifier_datasets/sampled_datasets/qa_accept_cogito_skips_03-04-2020_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down binary datasets by skip reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "annotations['skip_reasons'] = annotations['skip_reasons'].apply(lambda l: l if isinstance(l, float) else json.loads(l))\n",
    "annotations['skip_reasons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skip_reasons = annotations['skip_reasons'].explode().unique()\n",
    "all_skip_reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(all_skip_reasons), figsize=(5, len(all_skip_reasons)*5))\n",
    "\n",
    "skips = annotations[annotations['skip_reasons'].notnull()]\n",
    "\n",
    "reason_ratios = {col: [] for col in ['label', 'ratio']\n",
    "for i, label in enumerate(all_skip_reasons):\n",
    "    skips[f'{label}'] = skips['skip_reasons'].apply(lambda l: (label in l))\n",
    "    ratio = skips[f'{label}'].value_counts(normalize=True)[True]\n",
    "    reason_ratios['label'].append(label)\n",
    "    reason_ratios['ratio'].append(ratio)\n",
    "pd.Series(reason_ratios, index='label').plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_labels = [\n",
    "    'BLURRY',\n",
    "    'BAD_CROP',\n",
    "    'BAD_ORIENTATION',\n",
    "    'OBSTRUCTION',\n",
    "    'TOO_DARK'\n",
    "]\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "SAMPLE_RATIO = 0.7\n",
    "\n",
    "for lab in useful_labels:\n",
    "    label_skips = skips[skips[lab] & (skips['annotation_state_id'] == 4)]\n",
    "    label_skips = label_skips[~label_skips.left_crop_url.duplicated()]\n",
    "    label_skips = label_skips.sample(int(SAMPLE_RATIO * SAMPLE_SIZE)) \n",
    "    \n",
    "    qa_accepts = annotations[annotations['annotation_state_id'] == 7]\n",
    "    qa_accepts = qa_accepts[~qa_accepts.left_crop_url.duplicated()]\n",
    "    qa_accepts = qa_accepts.sample(int((1-SAMPLE_RATIO) * SAMPLE_SIZE))\n",
    "    \n",
    "    skip_dataset = pd.concat([label_skips, qa_accepts])\n",
    "    print(skip_dataset.skip_reasons.apply(lambda s: (lab in str(s))).value_counts())\n",
    "    print(skip_dataset['annotation_state_id'].value_counts())\n",
    "    skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "    out_path = f'/root/data/sid/skip_classifier_datasets/sampled_datasets/qa_accept_{lab}_skips_03-04-2020.csv'\n",
    "    skip_dataset.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_labels = [\n",
    "    'BLURRY',\n",
    "    'BAD_CROP',\n",
    "    'BAD_ORIENTATION',\n",
    "    'OBSTRUCTION',\n",
    "    'TOO_DARK'\n",
    "]\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "SAMPLE_RATIO = 0.7\n",
    "\n",
    "for lab in useful_labels:\n",
    "    label_skips = skips[skips[lab] & (skips['annotation_state_id'] == 4)]\n",
    "    label_skips = label_skips[~label_skips.left_crop_url.duplicated()]\n",
    "    label_skips = label_skips.sample(int(SAMPLE_RATIO * SAMPLE_SIZE)) \n",
    "    \n",
    "    qa_accepts = annotations[annotations['annotation_state_id'] == 7]\n",
    "    qa_accepts = qa_accepts[~qa_accepts.left_crop_url.duplicated()]\n",
    "    qa_accepts = qa_accepts.sample(int((1-SAMPLE_RATIO) * SAMPLE_SIZE))\n",
    "    \n",
    "    skip_dataset = pd.concat([label_skips, qa_accepts])\n",
    "    print(skip_dataset.skip_reasons.apply(lambda s: (lab in str(s))).value_counts())\n",
    "    print(skip_dataset['annotation_state_id'].value_counts())\n",
    "    skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "    out_path = f'/root/data/sid/skip_classifier_datasets/sampled_datasets/qa_accept_{lab}_skips_03-04-2020.csv'\n",
    "    skip_dataset.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in useful_labels:\n",
    "\n",
    "    out_path = f'/root/data/sid/skip_classifier_datasets/sampled_datasets/qa_accept_{lab}_skips_03-04-2020.csv'\n",
    "    print(len(pd.read_csv(out_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
