{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"SELECT service_id, pen_id, annotation_state_id, left_crop_url, left_crop_metadata, right_crop_url,\"\n",
    "         \"right_crop_metadata, camera_metadata, captured_at, skip_reasons, annotation FROM prod.crop_annotation \"\n",
    "         \"WHERE pen_id IN (56, 60, 37, 85, 86, 66, 83, 84, 95, 100, 61, 1, 4, 126, 128, 129, 133, 122, 123,\"\n",
    "         \"137, 114, 119, 116, 131, 132, 5, 145, 171, 173, 138, 149, 159, 210, 211, 67, 193, 140, 142, 216) AND captured_at >= '2021-01-01'\"\n",
    "         \" AND annotation_state_id IN (3, 4, 7) AND (service_id=1)\")\n",
    "annotations = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations[(annotations['left_crop_url'].notnull()) | (annotations['right_crop_url'].notnull())]\n",
    "\n",
    "annotations['time_bucket'] = annotations['captured_at'].apply(lambda c: c.hour // 2)\n",
    "annotations['is_qa_accept'] = annotations['annotation_state_id'] == 7\n",
    "\n",
    "accepts = annotations[annotations['is_qa_accept']]\n",
    "\n",
    "accepts['visibleBodySections'] = accepts['annotation'].apply(\n",
    "    lambda ann: ann['visibleBodySections'] if 'visibleBodySections' in ann else None)\n",
    "\n",
    "accepts['isPartial'] = accepts['annotation'].apply(\n",
    "    lambda ann: ann['isPartial'] if 'isPartial' in ann else None)\n",
    "\n",
    "accepts = accepts[~(accepts['isPartial'] & accepts['visibleBodySections'].isnull())]\n",
    "len(accepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confident cogito skips and confident QA accepts, just be to sure the labels have clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = list(accepts['visibleBodySections'].explode().unique())\n",
    "\n",
    "for part in bodyparts:\n",
    "    if part is not None:\n",
    "        accepts['HAS_' + part] = accepts['visibleBodySections'].apply(\n",
    "            lambda sections: part in sections if sections is not None else True)\n",
    "        print(part, accepts['HAS_' + part].value_counts(normalize=True).loc[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_PEN = 5000\n",
    "SAMPLE_SIZE = SAMPLES_PER_PEN // len(annotations['time_bucket'].unique())\n",
    "print(SAMPLE_SIZE)\n",
    "\n",
    "def sample_from_pen(pen_rows, sample_strat='random', sample_size=SAMPLE_SIZE):\n",
    "    if sample_strat == 'random':\n",
    "        return pen_rows.sample(min(len(pen_rows), int(sample_size)))\n",
    "    elif sample_strat == 'recent':\n",
    "        sorted_rows = pen_rows.sort_values(['is_qa_accept', 'captured_at'], ascending=False)\n",
    "        sorted_rows.drop_duplicates(subset='left_crop_url', inplace=True)\n",
    "        sorted_rows.drop_duplicates(subset='right_crop_url', inplace=True)\n",
    "        return sorted_rows.head(sample_size)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepts = annotations[annotations['annotation_state_id'].isin([7])]\n",
    "accepts = accepts.groupby(['pen_id', 'time_bucket'], group_keys=False).apply(sample_from_pen)\n",
    "len(accepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sample_sizes = accepts[['pen_id', 'time_bucket']].apply(lambda row: tuple(row), axis=1).value_counts()\n",
    "bucket_sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_skips_to_accepts(pen_rows):\n",
    "    sample_size = pen_rows.sample_size.unique()\n",
    "    assert len(sample_size) == 1\n",
    "    sample_size = sample_size[0]\n",
    "    return sample_from_pen(pen_rows, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "cogito_skips = annotations[annotations['annotation_state_id'] == 4]\n",
    "cogito_skips.drop_duplicates('left_crop_url', inplace=True)\n",
    "cogito_skips.drop_duplicates('right_crop_url', inplace=True)\n",
    "def get_sample_size(row):\n",
    "    try:\n",
    "        return bucket_sample_sizes[(row['pen_id'], row['time_bucket'])]\n",
    "    except:\n",
    "        return 0\n",
    "cogito_skips['sample_size'] = cogito_skips.progress_apply(get_sample_size , axis=1)\n",
    "chosen_cogito_skips = cogito_skips.groupby(['pen_id', 'time_bucket'], group_keys=False).apply(sample_equal_skips_to_accepts)\n",
    "len(chosen_cogito_skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_need = accepts.pen_id.value_counts() - chosen_cogito_skips.pen_id.value_counts()\n",
    "print(still_need.sum() + len(chosen_cogito_skips))\n",
    "still_need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftover_skips = cogito_skips[~cogito_skips['left_crop_url'].isin(chosen_cogito_skips['left_crop_url'])]\n",
    "leftover_skips = leftover_skips[~cogito_skips['right_crop_url'].isin(chosen_cogito_skips['right_crop_url'])]\n",
    "\n",
    "leftover_skips['sample_size'] = leftover_skips['pen_id'].progress_apply(\n",
    "    lambda p: still_need[p])\n",
    "chosen_cogito_skips2 = leftover_skips.groupby(['pen_id'], group_keys=False).apply(sample_equal_skips_to_accepts)\n",
    "chosen_cogito_skips = pd.concat([chosen_cogito_skips, chosen_cogito_skips2])\n",
    "len(chosen_cogito_skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_need = accepts.time_bucket.value_counts() - chosen_cogito_skips.time_bucket.value_counts()\n",
    "extras = still_need[still_need<0].sum() * -1\n",
    "still_need = still_need[still_need>0]\n",
    "still_need = ((still_need / still_need.sum()) * extras).apply(int)\n",
    "still_need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftover_skips = cogito_skips[~cogito_skips['left_crop_url'].isin(chosen_cogito_skips['left_crop_url'])]\n",
    "leftover_skips = leftover_skips[~cogito_skips['right_crop_url'].isin(chosen_cogito_skips['right_crop_url'])]\n",
    "\n",
    "leftover_skips['sample_size'] = leftover_skips['time_bucket'].progress_apply(\n",
    "    lambda p: 0 if p not in still_need else still_need[p])\n",
    "chosen_cogito_skips2 = leftover_skips.groupby(['time_bucket'], group_keys=False).apply(sample_equal_skips_to_accepts)\n",
    "chosen_cogito_skips = pd.concat([chosen_cogito_skips, chosen_cogito_skips2])\n",
    "len(chosen_cogito_skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset = pd.concat([chosen_cogito_skips, accepts.sample(len(chosen_cogito_skips))])\n",
    "skip_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset['is_qa_accept'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(state_id):\n",
    "    if state_id == 4:\n",
    "        return 'SKIP'\n",
    "    elif state_id in [3,7]:\n",
    "        return 'ACCEPT'\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "skip_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(skip_dataset.groupby(['pen_id', 'label'])['left_crop_url'].aggregate('count')).unstack().plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(skip_dataset.groupby(['time_bucket', 'label'])['left_crop_url'].aggregate('count')).unstack().plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset[skip_dataset['right_crop_metadata'].isnull()]['right_crop_metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(row):\n",
    "    if row['left_crop_url'] is None:\n",
    "        return row['right_crop_url']\n",
    "    if row['right_crop_url'] is None:\n",
    "        return row['left_crop_url']\n",
    "    else:\n",
    "        if row['left_crop_metadata']['quality_score'] > row['left_crop_metadata']['quality_score']:\n",
    "            return row['left_crop_url']\n",
    "        else:\n",
    "            return row['right_crop_url']\n",
    "        \n",
    "skip_dataset['url'] = skip_dataset.apply(get_url, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skip_dataset['url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skip_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset = skip_dataset[~skip_dataset.url.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(skip_dataset) == len(skip_dataset['url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset.to_csv('/root/data/sid/needed_data/skip_classifier_datasets/sampled_datasets/03102021_bodyparts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "skip_dataset = pd.read_csv('/root/data/sid/needed_data/skip_classifier_datasets/sampled_datasets/03102021_bodyparts.csv')\n",
    "skip_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_dict(a):\n",
    "    if isinstance(a, str):\n",
    "        a = a.replace(\"'\", \"\\\"\").replace('True', 'true').replace('False', 'false')\n",
    "    try:\n",
    "        return json.loads(a) if isinstance(a, str) else None\n",
    "    except:\n",
    "        return json.loads(a) if isinstance(a, str) else None\n",
    "\n",
    "        \n",
    "skip_dataset['annotation'] = skip_dataset['annotation'].apply(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sections(row):\n",
    "    ann = row['annotation']\n",
    "    if row['is_qa_accept']:\n",
    "        if (ann is None or ('isPartial' not in ann) or (not ann['isPartial'])):\n",
    "            return ['VENTRAL_POSTERIOR', 'VENTRAL_ANTERIOR', 'DORSAL_POSTERIOR',\n",
    "                'DORSAL_ANTERIOR', 'HEAD']\n",
    "        else:\n",
    "            if 'visibleBodySections' in ann:\n",
    "                if np.nan in ann['visibleBodySections']:\n",
    "                    print(ann)\n",
    "                return ann['visibleBodySections'] \n",
    "            else:\n",
    "                return []\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "skip_dataset['visibleBodySections'] = skip_dataset.apply(get_sections, axis=1\n",
    ")\n",
    "skip_dataset['visibleBodySections'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset['visibleBodySections'].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = list(skip_dataset['visibleBodySections'].explode().unique())\n",
    "\n",
    "for part in bodyparts:\n",
    "    if part is not np.nan:\n",
    "        skip_dataset['HAS_' + part] = skip_dataset['visibleBodySections'].apply(\n",
    "            lambda sections: part in sections if sections is not None else False)\n",
    "        print(skip_dataset['HAS_' + part].value_counts(normalize=True).loc[True])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://aquabyte-crops.s3.eu-west-1.amazonaws.com/environment=production/site-id=90/pen-id=145/date=2020-09-10/hour=21/at=2020-09-10T21:56:14.391265000Z/right_frame_crop_92_1263_4096_3000.jpg' in skip_dataset['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def json_loader(json_path):\n",
    "    with open(json_path) as inp:\n",
    "        return json.load(inp)\n",
    "pre = '/root/data/sid/needed_data/skip_classifier_datasets/images/01152020_bodyparts/images/ACCEPT/'\n",
    "ps = os.listdir(pre)\n",
    "ps = [p for p in ps if p.endswith('metadata.json')]\n",
    "urls = []\n",
    "for p in tqdm(ps):\n",
    "    path = os.path.join(pre, p)\n",
    "    urls.append(json_loader(path)['url'])\n",
    "    path = os.path.join(pre, p)\n",
    "    d = json_loader(path)\n",
    "    this = skip_dataset[skip_dataset['url'] == d['url']]\n",
    "    assert len(this) == 1\n",
    "    row = this.iloc[0]\n",
    "    for col in [\n",
    "        'visibleBodySections', \n",
    "        'HAS_VENTRAL_POSTERIOR', \n",
    "        'HAS_VENTRAL_ANTERIOR', \n",
    "        'HAS_HEAD', \n",
    "        'HAS_DORSAL_ANTERIOR', \n",
    "        'HAS_DORSAL_POSTERIOR'\n",
    "    ]:\n",
    "        d[col] = bool(row[col])\n",
    "    with open(path.replace('metadata', 'metadata2'), 'w') as out:\n",
    "        json.dump(d, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in tqdm(ps):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset['captured_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 4000\n",
    "\n",
    "def sample_from_pen(pen_rows, sample_strat='recent'):\n",
    "    if sample_strat == 'random':\n",
    "        return pen_rows.sample(min(len(pen_rows), SAMPLE_SIZE))\n",
    "    elif sample_strat == 'recent':\n",
    "        return pen_rows.sort_values('captured_at', ascending=False).head(SAMPLE_SIZE)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_accepts = annotations[annotations['annotation_state_id'] == 7]\n",
    "qa_accepts = qa_accepts[~qa_accepts.left_crop_url.duplicated()]\n",
    "qa_accepts = qa_accepts.groupby('pen_id', group_keys=False).apply(sample_from_pen)\n",
    "len(qa_accepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_accepts.pen_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_skips = annotations[annotations['annotation_state_id'] == 4]\n",
    "cogito_skips = cogito_skips[~cogito_skips.left_crop_url.duplicated()]\n",
    "cogito_skips = cogito_skips.groupby('pen_id', group_keys=False).apply(sample_from_pen)\n",
    "len(cogito_skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogito_skips.pen_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset = pd.concat([cogito_skips, qa_accepts])\n",
    "skip_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(state_id):\n",
    "    if state_id == 4:\n",
    "        return 'SKIP'\n",
    "    elif state_id == 7:\n",
    "        return 'ACCEPT'\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "skip_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(skip_dataset.groupby(['pen_id', 'label'])['left_crop_url'].aggregate('count')).unstack().plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(row):\n",
    "    if isinstance(row['left_crop_url'], str):\n",
    "        return row['left_crop_url']\n",
    "    elif isinstance(row['right_crop_url'], str):\n",
    "        return row['right_crop_url']\n",
    "    else:\n",
    "        assert False, row\n",
    "        \n",
    "skip_dataset['url'] = skip_dataset.apply(get_url, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset.to_csv('/root/data/sid/needed_data/skip_classifier_datasets/sampled_datasets/qa_accept_cogito_skips_05-04-2020_recentsample_stratified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "skip_dataset = pd.read_csv('/root/data/sid/needed_data/skip_classifier_datasets/sampled_datasets/qa_accept_cogito_skips_05-04-2020_recentsample_stratified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dataset['captured_at'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down binary datasets by skip reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "annotations['skip_reasons'] = annotations['skip_reasons'].apply(lambda l: l if isinstance(l, float) else json.loads(l))\n",
    "annotations['skip_reasons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skip_reasons = annotations['skip_reasons'].explode().unique()\n",
    "all_skip_reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(all_skip_reasons), figsize=(5, len(all_skip_reasons)*5))\n",
    "\n",
    "skips = annotations[annotations['skip_reasons'].notnull()]\n",
    "\n",
    "for i, label in enumerate(all_skip_reasons):\n",
    "    skips[f'{label}'] = skips['skip_reasons'].apply(lambda l: (label in l))\n",
    "    skips[f'{label}'].value_counts(normalize=True).plot.bar(ax=axes[i])\n",
    "    axes[i].set_title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_labels = [\n",
    "    'BLURRY',\n",
    "    'BAD_CROP',\n",
    "    'BAD_ORIENTATION',\n",
    "    'OBSTRUCTION',\n",
    "    'TOO_DARK'\n",
    "]\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "SAMPLE_RATIO = 0.7\n",
    "\n",
    "for lab in useful_labels:\n",
    "    label_skips = skips[skips[lab] & (skips['annotation_state_id'] == 4)]\n",
    "    label_skips = label_skips[~label_skips.left_crop_url.duplicated()]\n",
    "    label_skips = label_skips.groupby('pen_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 2000)))\n",
    "    \n",
    "    qa_accepts = annotations[annotations['annotation_state_id'] == 7]\n",
    "    qa_accepts = qa_accepts[~qa_accepts.left_crop_url.duplicated()]\n",
    "    qa_accepts = qa_accepts.groupby('pen_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 2000)))\n",
    "    \n",
    "    skip_dataset = pd.concat([label_skips, qa_accepts])\n",
    "    \n",
    "    print(skip_dataset.pen_id.value_counts)\n",
    "    print(skip_dataset.skip_reasons.apply(lambda s: (lab in str(s))).value_counts())\n",
    "    print(skip_dataset['annotation_state_id'].value_counts())\n",
    "    skip_dataset['label'] = skip_dataset['annotation_state_id'].apply(get_label)\n",
    "    out_path = f'/root/data/sid/skip_classifier_datasets/sampled_datasets/qa_accept_{lab}_skips_03-04-2020-stratified.csv'\n",
    "    skip_dataset.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
