{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn import cross_validation  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/root/data/small_pen_data_collection/freckles.csv', names=['id', 'file', 'eye_coordinate', 'freckle_coordinates'])\n",
    "df = pd.read_csv('/root/data/reidentification/freckles.csv', names=['id', 'file', 'eye_coordinate', 'freckle_coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180921010001, 29, 2, 58\n",
      "181001010001, 2, 1, 4\n",
      "181001010002, 31, 4, 67\n",
      "181001010003, 11, 3, 23\n",
      "181001010004, 12, 1, 27\n",
      "181001010005, 12, 1, 27\n",
      "181001010006, 7, 1, 11\n",
      "181001010007, 6, 1, 14\n",
      "181001010008, 4, 1, 9\n",
      "181001010009, 19, 3, 39\n",
      "181001010010, 23, 2, 54\n",
      "181008010001, 109, 32, 223\n",
      "181010010001, 80, 29, 123\n",
      "181010010002, 98, 31, 177\n",
      "181010010003, 102, 28, 161\n",
      "181012010001, 126, 49, 236\n",
      "181012010002, 41, 5, 103\n",
      "181012010003, 42, 3, 85\n",
      "181012010004, 60, 20, 103\n",
      "181012010005, 168, 105, 270\n",
      "181012010007, 43, 3, 78\n",
      "181012010008, 14, 3, 32\n",
      "181012010009, 42, 2, 122\n",
      "181012010010, 16, 2, 33\n",
      "181012010011, 123, 63, 158\n",
      "181012010012, 89, 50, 134\n",
      "181012010013, 12, 3, 23\n",
      "181012010014, 137, 71, 197\n",
      "181015010001, 130, 44, 258\n",
      "181015010002, 145, 40, 204\n",
      "181015010003, 93, 32, 164\n",
      "181015010004, 152, 35, 243\n",
      "181015010005, 122, 42, 203\n",
      "181015010006, 128, 51, 190\n",
      "181015010007, 103, 23, 158\n",
      "181016010001, 131, 67, 214\n",
      "181016010002, 140, 47, 229\n",
      "181016010003, 53, 10, 97\n",
      "181016010004, 176, 68, 253\n",
      "181016010005, 111, 22, 166\n",
      "181016010006, 143, 47, 275\n",
      "181016010007, 71, 26, 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{180921010001: (48, 29.38095238095238, 2.200000000000001, 58.19999999999999),\n",
       " 181001010001: (36, 2.4166666666666665, 1.0, 4.9),\n",
       " 181001010002: (145, 31.412587412587413, 4.0, 67.8),\n",
       " 181001010003: (191, 11.155172413793103, 3.0, 23.400000000000034),\n",
       " 181001010004: (240, 12.182222222222222, 1.0, 27.0),\n",
       " 181001010005: (93, 12.194029850746269, 1.0, 27.0),\n",
       " 181001010006: (33, 7.222222222222222, 1.0, 11.600000000000001),\n",
       " 181001010007: (121, 6.1265822784810124, 1.0, 14.400000000000006),\n",
       " 181001010008: (124, 4.142857142857143, 1.0, 9.799999999999997),\n",
       " 181001010009: (183, 19.064327485380115, 3.0, 39.0),\n",
       " 181001010010: (174, 23.158940397350992, 2.0, 54.0),\n",
       " 181008010001: (16, 109.5, 32.0, 223.0),\n",
       " 181010010001: (54, 80.75925925925925, 29.100000000000005, 123.10000000000001),\n",
       " 181010010002: (44, 98.68181818181819, 31.099999999999998, 177.20000000000005),\n",
       " 181010010003: (43, 102.97674418604652, 28.6, 161.40000000000003),\n",
       " 181012010001: (59, 126.44067796610169, 49.80000000000001, 236.00000000000003),\n",
       " 181012010002: (37, 41.108108108108105, 5.2, 103.59999999999998),\n",
       " 181012010003: (24, 42.27272727272727, 3.3000000000000003, 85.8),\n",
       " 181012010004: (34, 60.76470588235294, 20.0, 103.8),\n",
       " 181012010005: (51, 168.7843137254902, 105.0, 270.0),\n",
       " 181012010007: (15, 43.5, 3.9, 78.30000000000001),\n",
       " 181012010008: (44, 14.095238095238095, 3.0, 32.49999999999999),\n",
       " 181012010009: (37,\n",
       "  42.470588235294116,\n",
       "  2.3000000000000003,\n",
       "  122.19999999999997),\n",
       " 181012010010: (34, 16.323529411764707, 2.0, 33.5),\n",
       " 181012010011: (37, 123.78378378378379, 63.400000000000006, 158.2),\n",
       " 181012010012: (62, 89.6774193548387, 50.1, 134.5),\n",
       " 181012010013: (44, 12.704545454545455, 3.0, 23.400000000000006),\n",
       " 181012010014: (50, 137.12, 71.9, 197.8),\n",
       " 181015010001: (36, 130.88888888888889, 44.0, 258.0),\n",
       " 181015010002: (61, 145.52459016393442, 40.0, 204.0),\n",
       " 181015010003: (78, 93.73076923076923, 32.0, 164.6),\n",
       " 181015010004: (83, 152.32530120481928, 35.60000000000001, 243.0),\n",
       " 181015010005: (58, 122.06896551724138, 42.7, 203.3),\n",
       " 181015010006: (81, 128.11111111111111, 51.0, 190.0),\n",
       " 181015010007: (63, 103.04761904761905, 23.2, 158.0),\n",
       " 181016010001: (32, 131.53125, 67.10000000000001, 214.6),\n",
       " 181016010002: (64, 140.40625, 47.3, 229.20000000000005),\n",
       " 181016010003: (43, 53.44186046511628, 10.600000000000001, 97.20000000000006),\n",
       " 181016010004: (40, 176.275, 68.50000000000001, 253.10000000000002),\n",
       " 181016010005: (55, 111.47272727272727, 22.4, 166.20000000000002),\n",
       " 181016010006: (57, 143.28070175438597, 47.0, 275.6),\n",
       " 181016010007: (57, 71.7719298245614, 26.0, 123.19999999999999)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_id_counts():\n",
    "    fish_ids = np.unique(df['id'])\n",
    "\n",
    "    id_count = {}\n",
    "\n",
    "    for id in fish_ids:\n",
    "        coordinates = df[df['id'] == id]['freckle_coordinates']\n",
    "        non_zero_coordinates = [coordinate for coordinate in coordinates if len(json.loads(coordinate)) > 0]\n",
    "        avg_coordinate_length = np.mean([ len(json.loads(coordinate)) for coordinate in non_zero_coordinates])\n",
    "        coordinate_10 = np.percentile([ len(json.loads(coordinate)) for coordinate in non_zero_coordinates], 10)\n",
    "        coordinate_90 = np.percentile([ len(json.loads(coordinate)) for coordinate in non_zero_coordinates], 90)\n",
    "        id_count[id] = (len(df[df['id'] == id]), avg_coordinate_length, coordinate_10, coordinate_90)\n",
    "        \n",
    "        print('%i, %i, %i, %i' % (id, avg_coordinate_length, coordinate_10, coordinate_90))\n",
    "\n",
    "    return id_count\n",
    "\n",
    "get_id_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_fish = df[df['id'] == 181016010007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for datum in same_fish:\n",
    "\n",
    "datum = same_fish.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_coordinate = json.loads(datum['eye_coordinate'])\n",
    "freckle_coordinates = json.loads(datum['freckle_coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(freckle_coordinates)\n",
    "\n",
    "components = pca.components_\n",
    "\n",
    "newEyeCoordinates = np.dot(eye_coordinate, components.T)\n",
    "newFreckleCoordinates = np.dot(freckle_coordinates, components.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([378.31528493, 130.78816913])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newEyeCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relativeCoordinates = newFreckleCoordinates - newEyeCoordinates\n",
    "\n",
    "mean = np.mean(relativeCoordinates, axis=0)\n",
    "stdev = np.std(relativeCoordinates, axis=0)\n",
    "\n",
    "normalizedCoordinates = (relativeCoordinates - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(normalizedCoordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75984356, -0.04161388, -0.30634032,  1.57817731,  2.02299628,\n",
       "       -0.76895179, -1.04236539, -1.47883421,  0.46099082,  0.17914607])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;\n",
    "\n",
    "def generate_data(n_clusters, lower_thresh, higher_thresh):\n",
    "    data = None\n",
    "    ids = []\n",
    "\n",
    "    for index, datum in df.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print('Processing %i out of %i' % (index, len(df)))\n",
    "            \n",
    "        eye_coordinate = json.loads(datum['eye_coordinate'])\n",
    "        freckle_coordinates = json.loads(datum['freckle_coordinates'])\n",
    "\n",
    "        if len(freckle_coordinates) < lower_thresh or len(freckle_coordinates) > higher_thresh:\n",
    "            continue\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "\n",
    "        pca.fit(freckle_coordinates)\n",
    "\n",
    "        components = pca.components_\n",
    "\n",
    "        newEyeCoordinates = np.dot(eye_coordinate, components.T)\n",
    "        newFreckleCoordinates = np.dot(freckle_coordinates, components.T)\n",
    "\n",
    "        relativeCoordinates = newFreckleCoordinates - newEyeCoordinates\n",
    "\n",
    "        mean = np.mean(relativeCoordinates, axis=0)\n",
    "        stdev = np.std(relativeCoordinates, axis=0)\n",
    "\n",
    "        normalizedCoordinates = (relativeCoordinates - mean) / stdev\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters).fit(normalizedCoordinates)\n",
    "\n",
    "        if data is None:\n",
    "            data = kmeans.cluster_centers_.flatten()\n",
    "        else:\n",
    "            data = np.vstack((data, kmeans.cluster_centers_.flatten()))\n",
    "\n",
    "        ids.append((datum['id'], len(freckle_coordinates)))\n",
    "    \n",
    "    unique_ids = np.unique([id[1] for id in ids])\n",
    "    max_id_count = 0\n",
    "    for id in unique_ids:\n",
    "        id_counts = np.sum(ids == id)\n",
    "        if id_counts > max_id_count:\n",
    "            max_id_count = id_counts\n",
    "            \n",
    "    for id in unique_ids:\n",
    "        print('Augmenting id %i' % (id, ))\n",
    "\n",
    "        id_indices = ids == id\n",
    "        found_ids = [ myId for myId in ids if myId[1] == id ]\n",
    "        \n",
    "        id_counts = np.sum(ids == id)\n",
    "        multiplier = max_id_count * 1.0 / id_counts #(1.0 / len(unique_ids)) / (id_counts * 1.0 / len(df))\n",
    "        \n",
    "        for i in range(1, int(multiplier)):\n",
    "            data = np.vstack((data, data[np.where(id_indices), :]))\n",
    "            ids = np.concatenate((ids, found_ids))\n",
    "    \n",
    "    return (data, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(4, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_freckle_detection(n_clusters, max_n_neighbors):\n",
    "    print('Generating dataset...')\n",
    "    \n",
    "    X, y = generate_data(n_clusters, 20, 100)\n",
    "    \n",
    "    error = []\n",
    "    \n",
    "    for i in range(1, max_n_neighbors): \n",
    "        print('Running for %i neighbors' % (i, ))\n",
    "        \n",
    "        total_error = []\n",
    "        \n",
    "        for j in range(1, 50):\n",
    "            X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "            y_train_id = np.array([ y[0] for y in y_train ])\n",
    "            y_test_id = np.array([ y[0] for y in y_test ])\n",
    "            y_test_len = np.array([ y[1] for y in y_test ])\n",
    "\n",
    "            # Calculating error for K values between 1 and 40\n",
    "\n",
    "            knn = KNeighborsClassifier(n_neighbors=i)\n",
    "            knn.fit(X_train, y_train_id)\n",
    "            pred_i = knn.predict(X_test)\n",
    "            total_error.append(np.mean(pred_i != y_test_id))\n",
    "        \n",
    "        error.append(np.mean(total_error))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))  \n",
    "    plt.plot(range(1, max_n_neighbors), error, color='red', linestyle='dashed', marker='o',  \n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('Error Rate K Value')  \n",
    "    plt.xlabel('K Value')  \n",
    "    plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_freckle_detection(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(4, 20, 100)\n",
    "\n",
    "all_y_test_id = []\n",
    "all_y_pred = []\n",
    "\n",
    "#for i in range(1, 40):\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "y_train_id = np.array([ y[0] for y in y_train ])\n",
    "y_test_id = np.array([ y[0] for y in y_test ])\n",
    "y_test_len = np.array([ y[1] for y in y_test ])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train_id)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "all_y_test_id = np.concatenate((all_y_test_id, y_test_id))\n",
    "all_y_pred = np.concatenate((all_y_pred, y_pred))\n",
    "    \n",
    "    #y_pred == y_test_id\n",
    "\n",
    "    #success_failure = y_pred == y_test_id\n",
    "    #success_lens = y_test_len[success_failure == True]\n",
    "    #failure_lens = y_test_len[success_failure == False]\n",
    "\n",
    "    #np.vstack((y_pred, y_test_id, success_failure, y_test_len)).T\n",
    "\n",
    "#print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(all_y_test_id, all_y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(all_y_test_id == all_y_pred)\n",
    "len(all_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_count = {}\n",
    "\n",
    "for myY in y:\n",
    "    id = myY[0]\n",
    "    \n",
    "    if id in id_count:\n",
    "        id_count[id] = id_count[id] + 1\n",
    "    else:\n",
    "        id_count[id] = 1\n",
    "        \n",
    "pp.pprint(id_count)\n",
    "\n",
    "pp.pprint(get_id_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_len, y_pred == y_test_id)\n",
    "plt.show()\n",
    "\n",
    "percentiles = []\n",
    "success_percentiles = []\n",
    "failure_percentiles = []\n",
    "\n",
    "for i in range(0, 100, 5):\n",
    "    p1 = np.percentile(success_lens, i) # return 50th percentile, e.g median.\n",
    "    p2 = np.percentile(failure_lens, i) # return 50th percentile, e.g median.\n",
    "    \n",
    "    percentiles.append(i)\n",
    "    success_percentiles.append(p1)\n",
    "    failure_percentiles.append(p2)\n",
    "    \n",
    "    print('%0.2f: %0.2f, %0.2f' % (i, p1, p2))\n",
    "    \n",
    "plt.plot(percentiles, success_percentiles)\n",
    "plt.plot(percentiles, failure_percentiles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
