{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df = pd.read_csv('/root/data/small_pen_data_collection/dataset_13k_pairs.csv')"
=======
    "df = pd.read_csv('/root/thomas/github/cv_research/thomas/full_pipeline/small_pen/dataset_predictions_complete.csv')\n",
    "# df = pd.read_csv('/root/data/small_pen_data_collection/dataset_13k_pairs.csv')"
>>>>>>> 33e5ec51185c8b566d45c6b387a8532dcac03aa2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "idx = np.array(df[\"ground_truth\"]) == 6020\n",
    "hc = np.array(df[\"23\"])[idx]"
   ]
=======
   "source": []
>>>>>>> 33e5ec51185c8b566d45c6b387a8532dcac03aa2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hc[~np.isnan(hc)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.iqr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "#print(df.isna().sum())\n",
    "col = [str(1)+str(x) for x in range(2, 10)]\n",
    "col2 = [str(x)+str(9) for x in range(2, 9)]\n",
    "new_df = df.drop(columns=col+col2)\n",
    "#new_df = df.drop(columns=['14', '24', '34', '45', '46', '47']) \n",
    "#new_df = new_df.drop(columns=['16', '26', '36', '56', '67']) # '46', \n",
    "\n",
    "# print(new_df.isna().sum())\n",
    "\n",
    "new_df = new_df.dropna(subset=new_df.columns[1:-2])\n",
    "\n",
    "idx = np.array(new_df[\"ground_truth\"]) == 6020\n",
    "plt.hist(np.array(new_df[\"23\"])[idx])\n",
    "plt.show()\n",
    "df_save = new_df.copy()\n",
    "print(new_df.shape)\n",
    "\n",
    "\n",
    "my_mean = new_df.mean()\n",
    "#my_sd = np.std(new_df, axis=0)\n",
    "\n",
    "#print(my_mean)\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "my_columns = new_df.columns[1:-2]\n",
    "\n",
    "print(my_columns)\n",
    "\n",
    "norm_df = new_df.copy()\n",
    "\n",
    "iqrs = {}\n",
    "for x in my_columns:\n",
    "    my_mean = new_df[x].median()\n",
    "    my_std = new_df[x].std()\n",
    "    my_iqr = stats.iqr(new_df[x])\n",
    "    \n",
    "    means.append(my_mean)\n",
    "    stds.append(my_std)\n",
    "    \n",
    "    #print(my_mean)\n",
    "    #print(my_std)\n",
    "    #print((new_df[x] - my_mean) / my_std)\n",
    "    \n",
    "    my_row = new_df[x].copy()\n",
    "    new_df[x] = my_row / my_iqr\n",
    "    iqrs[x] = my_iqr\n",
    "    norm_df[x] = (my_row - my_mean) / my_std\n",
    "    \n",
    "#norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "idx = np.array(new_df[\"ground_truth\"]) == 6020\n",
    "plt.hist(np.array(new_df[\"23\"])[idx])\n",
    "plt.show()\n",
    "print(new_df.shape)"
   ]
=======
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(iqrs, open('/root/data/models/biomass/iqrs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 33e5ec51185c8b566d45c6b387a8532dcac03aa2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = new_df.loc[:, new_df.columns[1]:new_df.columns[-3]]\n",
    "my_norm_df = norm_df.loc[:, new_df.columns[1]:new_df.columns[-3]]\n",
    "\n",
    "array_subset = (np.abs(my_norm_df) > 1.5).any(axis=1) == False\n",
    "\n",
    "print('Keeping %i of %i' % (np.sum(array_subset), my_norm_df.shape[0]))\n",
    "\n",
    "my_df = my_df.loc[array_subset, :]                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = new_df['ground_truth']\n",
    "Y = new_df['ground_truth'][array_subset]\n",
    "\n",
    "#my_df_X = np.hstack((my_df, my_df ** 2, my_df ** 3))\n",
    "pidx = np.indices((my_df.shape[1], my_df.shape[1])).reshape(2, -1)\n",
    "lcol = pd.MultiIndex.from_product([my_df.columns, my_df.columns],\n",
    "                                  names=[my_df.columns.name, my_df.columns.name])\n",
    "my_df_X = pd.DataFrame(my_df.values[:, pidx[0]] * my_df.values[:, pidx[1]],\n",
    "             columns=lcol)\n",
    "\n",
    "#my_df_X = np.hstack(( my_df_X))\n",
    "\n",
    "my_df_X = sm.add_constant(my_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "\n",
    "pca.fit(my_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_ratio_\n",
    "print(np.sum(eigenvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "\n",
    "newX = np.dot(my_df_X, components.T)\n",
    "\n",
    "print(components.shape)\n",
    "\n",
    "newX\n",
    "\n",
    "#outlierIndices = np.where(newX[:,0] > 10)\n",
    "#my_indices = my_df.index[outlierIndices[0]]\n",
    "\n",
    "\n",
    "#my_df.loc[my_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(newX[:,0], Y)#plt.scatter(np.log(newX[:,0]),np.log(Y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myX = newX[:, 0]\n",
    "print(newX.shape)\n",
    "myX = newX\n",
    "myY = Y\n",
    "\n",
    "plt.scatter(newX[:, 0], Y)\n",
    "plt.show()\n",
    "\n",
    "myX = sm.add_constant(myX)\n",
    "\n",
    "print(myX.shape)\n",
    "#print(myY)\n",
    "\n",
    "model = sm.OLS(myY, myX).fit()\n",
    "predictions = model.predict(myX) # make the predictions by the model\n",
    "\n",
    "print(model.params.shape)\n",
    "#print(model.summary())\n",
    "\n",
    "predY = predictions\n",
    "#predY = np.exp(predictions)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "error = predY - myY\n",
    "\n",
    "plt.scatter(Y, predY)\n",
    "\n",
    "res = model.resid\n",
    "fig = sm.qqplot(res)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Error: %0.2f' % (np.median(np.abs(error)), ))\n",
    "print('Pct Error: %0.2f' % (np.median(np.abs(error) / myY * 100), ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT predictions versus features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [c[0] for c in Counter(myY).most_common() if c[1] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in weights:\n",
    "    print(\"Ground truth weight: {}\".format(w))\n",
    "    idx = myY == w\n",
    "    one_point = np.random.choice(np.nonzero(idx)[0])\n",
    "    idx2 = np.zeros_like(idx)\n",
    "    idx2[one_point] = 1\n",
    "    idx2 = idx\n",
    "    head_caudal_y = myY[idx2]\n",
    "    head_caudal_predy = predictions[idx2]\n",
    "    # df_save = new_df\n",
    "    error = np.abs(head_caudal_y - head_caudal_predy)*100 / head_caudal_y\n",
    "    print(np.mean(error))\n",
    "    \n",
    "    head_caudal = np.array(df_save[\"23\"])[array_subset][idx2]\n",
    "    head_dorsal = np.array(df_save[\"24\"])[array_subset][idx2]\n",
    "    dorsal_caudal = np.array(df_save[\"34\"])[array_subset][idx2]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    ax[0].scatter(head_caudal, head_caudal_y)\n",
    "    ax[0].scatter(head_caudal, head_caudal_predy)\n",
    "    ax[0].legend([\"ground truth\", \"predictions\"])\n",
    "    ax[0].set_xlabel(\"Head - Caudal length\")\n",
    "    ax[0].set_ylabel(\"Weights\")\n",
    "    \n",
    "    ax[1].scatter(head_dorsal, head_caudal_y)\n",
    "    ax[1].scatter(head_dorsal, head_caudal_predy)\n",
    "    ax[1].legend([\"ground truth\", \"predictions\"])\n",
    "    ax[1].set_xlabel(\"Head - Dorsal length\")\n",
    "    ax[1].set_ylabel(\"Weights\")\n",
    "    \n",
    "    ax[2].scatter(dorsal_caudal, head_caudal_y)\n",
    "    ax[2].scatter(dorsal_caudal, head_caudal_predy)\n",
    "    ax[2].legend([\"ground truth\", \"predictions\"])\n",
    "    ax[2].set_xlabel(\"Caudal - Dorsal length\")\n",
    "    ax[2].set_ylabel(\"Weights\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(n_components, k):\n",
    "    errors = []\n",
    "    avg_errors = []\n",
    "    avg_errors_raw = []\n",
    "    error_pcts = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        pca = PCA(n_components=n_components)\n",
    "\n",
    "        pca.fit(my_df_X)\n",
    "\n",
    "        components = pca.components_\n",
    "\n",
    "        newX = np.dot(my_df_X, components.T)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(newX, Y, test_size=0.4)\n",
    "\n",
    "        myX = X_train\n",
    "        myY = y_train\n",
    "        model = sm.OLS(myY, myX).fit()\n",
    "        \n",
    "        predY = model.predict(X_test) # make the predictions by the model\n",
    "\n",
    "        error = predY - y_test\n",
    "\n",
    "        errors.append(np.median(np.abs(error)))\n",
    "        avg_errors.append(np.abs(np.mean(error)) / np.mean(y_test) * 100)\n",
    "        avg_errors_raw.append(np.mean(error) / np.mean(y_test) * 100)\n",
    "        error_pcts.append(np.median(np.abs(error) / y_test * 100))\n",
    "    \n",
    "    return (np.mean(errors), np.mean(avg_errors), avg_errors_raw, np.mean(error_pcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_eigens = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "n_eigens = [1, 2, 3]\n",
    "\n",
    "errors = []\n",
    "avg_errors_raws = []\n",
    "\n",
    "for n_eigen in n_eigens:\n",
    "    myError = getError(n_eigen, 10)\n",
    "    errors.append(myError[3])\n",
    "    avg_errors_raws.append(myError[2])    \n",
    "    #print(myError)\n",
    "    print('Achieve %0.2f with %i eigenvectors' % (myError[1], n_eigen))\n",
    "    \n",
    "#print(avg_errors_raws[1])\n",
    "plt.plot(avg_errors_raws[1])\n",
    "plt.show()\n",
    "\n",
    "print(.4 * newX.shape[0])\n",
    "\n",
    "# plt.plot(n_eigens, errors)\n",
    "# plt.xlabel('Number of regressors')\n",
    "# plt.ylabel('Median error %')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extensions to this\n",
    "- Try different filtering based off of total norm of covariance of eigenvectors\n",
    "- More data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
