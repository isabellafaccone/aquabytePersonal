{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from filter_optimization.filter_optimization_task import extract_biomass_data, _add_date_hour_columns\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "from report_generation.report_generator import generate_ts_data, SamplingFilter, gen_pm_base\n",
    "from research.utils.datetime_utils import add_days\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, ValidationError\n",
    "\n",
    "from population_metrics.population_metrics_base import PopulationMetricsBase, ValidationError\n",
    "from population_metrics.raw_metrics import get_raw_sample_size, get_raw_weight_values, get_raw_kf_values\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.confidence_metrics import generate_trend_stability, get_raw_and_historical_weights\n",
    "\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from research.utils.datetime_utils import add_days, day_difference, get_dates_in_range\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_names = [\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13',\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17',\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24',\n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26',\n",
    "    'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02',\n",
    "    'silda_pen_id_86_2020-06-19_2020-07-19',\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30',\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06',\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/root/data/alok/biomass_estimation/playground'\n",
    "batch_name = 'simulation'\n",
    "dfs, gt_metadatas = {}, {}\n",
    "for cohort_name in cohort_names:\n",
    "    s3_dir = os.path.join(\n",
    "        'https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/alok/production_datasets',\n",
    "        cohort_name\n",
    "    )\n",
    "\n",
    "    ground_truth_metadata_url = os.path.join(s3_dir, 'ground_truth_metadata.json')\n",
    "    ground_truth_key_base = os.path.join(batch_name, cohort_name, 'ground_truth_metadata.json')\n",
    "    ground_truth_f = os.path.join(ROOT_DIR, ground_truth_key_base)\n",
    "    s3.download_from_url(ground_truth_metadata_url, custom_location=ground_truth_f)\n",
    "    gt_metadata = json.load(open(ground_truth_f))\n",
    "    gt_metadatas[cohort_name] = gt_metadata\n",
    "    \n",
    "    data_url = os.path.join(s3_dir, 'annotation_dataset.csv')\n",
    "    data_f, _, _= s3.download_from_url(data_url)\n",
    "    df = pd.read_csv(data_f)\n",
    "    df = _add_date_hour_columns(df)\n",
    "    dfs[cohort_name] = df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "def get_included_dates(pm_base: PopulationMetricsBase, date: str,\n",
    "                       max_day_difference: int, incorporate_future: bool) -> List:\n",
    "    \"\"\"\n",
    "    Gets list of dates that fall into window corresponding to max_day_difference. Window\n",
    "    is affected by whether or not incorporate_future is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    start = add_days(date, -max_day_difference)\n",
    "    end = add_days(date, max_day_difference if incorporate_future else 0)\n",
    "    included_dates = sorted([date for date in get_dates_in_range(start, end) if date in pm_base.unique_dates])\n",
    "    if not included_dates:\n",
    "        raise ValidationError('No raw biomass data found in window!')\n",
    "    return included_dates\n",
    "\n",
    "def get_smart_growth_rate(pm_base: PopulationMetricsBase, date: str,\n",
    "                          incorporate_future: bool = True, apply_growth_rate: bool = True,\n",
    "                          trend_stability_threshold: float = 0.9) -> float:\n",
    "    \"\"\"Get local growth rate adjustment to use for smart average computation.\"\"\"\n",
    "\n",
    "    raw_sample_size = get_raw_sample_size(pm_base, date)\n",
    "    growth_rate_for_smart_metrics = 0.0\n",
    "    if apply_growth_rate:\n",
    "        try:\n",
    "            growth_rate = compute_local_growth_rate(pm_base, date, incorporate_future=incorporate_future)\n",
    "            trend_stability = generate_trend_stability(pm_base, date, incorporate_future=incorporate_future)\n",
    "            if raw_sample_size and trend_stability and trend_stability > trend_stability_threshold:\n",
    "                growth_rate_for_smart_metrics = growth_rate\n",
    "        except ValidationError as err:\n",
    "            print(str(err))\n",
    "    return growth_rate_for_smart_metrics\n",
    "\n",
    "def generate_smart_individual_values(pm_base: PopulationMetricsBase, date: str, max_day_difference: int,\n",
    "                                     incorporate_future: bool, apply_growth_rate: bool,\n",
    "                                     trend_stability_threshold: float) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate smart individual values for weight and k-factor on given date.\n",
    "    Args:\n",
    "        pm_base: PopulationMetricsBase instance\n",
    "        date: the date to compute smart individual values for\n",
    "        max_day_difference: what is the maximum day difference of dates in the window?\n",
    "        incorporate_future: should future data be incorporated?\n",
    "        apply_growth_rate: should we apply a growth rate adjustment?\n",
    "        trend_stability_threshold: if apply_growth_rate is True, what minimum trend_stability_threshold\n",
    "                                   should we mandate for growth rate adjustment?\n",
    "    Returns:\n",
    "        adj_weights: growth rate adjusted individual weights in window\n",
    "        kfs: individual k-factor values in window\n",
    "    \"\"\"\n",
    "    \n",
    "    # validate data\n",
    "    included_dates = get_included_dates(pm_base, date, max_day_difference, incorporate_future)\n",
    "\n",
    "    # compute local growth rate to use for smart average\n",
    "    growth_rate_for_smart_metrics = get_smart_growth_rate(pm_base, date, incorporate_future=incorporate_future,\n",
    "                                                          apply_growth_rate=apply_growth_rate,\n",
    "                                                          trend_stability_threshold=trend_stability_threshold)\n",
    "\n",
    "    # get adjusted weights and kfs for smart metrics\n",
    "    all_weights = []\n",
    "    adj_weights, kfs = [], []\n",
    "    for d in included_dates:\n",
    "\n",
    "        # extend adjusted weights list for this date\n",
    "        weights_for_date = get_raw_weight_values(pm_base, d)\n",
    "        day_diff = day_difference(d, date)\n",
    "        adj_weights_for_date = np.array(weights_for_date) * np.exp(-day_diff * growth_rate_for_smart_metrics)\n",
    "        adj_weights.extend(adj_weights_for_date)\n",
    "\n",
    "        # extend k-factor list for this date\n",
    "        kfs_for_date = get_raw_kf_values(pm_base, d)\n",
    "        kfs.extend(kfs_for_date)\n",
    "        all_weights.extend(weights_for_date)\n",
    "\n",
    "    weights = np.array(adj_weights)\n",
    "#     new_reflection_point = reflection_point * np.median(adj_weights) / np.median(all_weights)\n",
    "#     weights = np.array(list(weights[weights < new_reflection_point]) + list(new_reflection_point + (new_reflection_point - weights[weights < new_reflection_point])))\n",
    "    return weights\n",
    "\n",
    "def generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding, ):\n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    date = add_days(last_feeding_date, days_post_feeding)\n",
    "    weights = generate_smart_individual_values(pm_base, date, max_day_diff, True, apply_growth_rate, 0.9)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/21100716/fast-arbitrary-distribution-random-sampling-inverse-transform-sampling\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def inverse_sample_decorator(dist):\n",
    "    \n",
    "    def wrapper(pnts, x_min=0, x_max=20000, n=1e5, **kwargs):\n",
    "        \n",
    "        x = np.linspace(x_min, x_max, int(n))\n",
    "        cumulative = np.cumsum(dist(x, **kwargs))\n",
    "        cumulative -= cumulative.min()\n",
    "        f = interp1d(cumulative/cumulative.max(), x)\n",
    "        return f(np.random.random(pnts))\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_from_weight(weight):\n",
    "    return weight ** (1/3) / 23.6068\n",
    "\n",
    "def get_simulated_weights(degrees, density, pdf, max_iter):\n",
    "    sample_pdf = inverse_sample_decorator(pdf)\n",
    "    \n",
    "    fov = degrees * np.pi / 180\n",
    "    params_depth = 2\n",
    "    camera_location = 5\n",
    "    total_length = 10\n",
    "\n",
    "    all_weights = []\n",
    "\n",
    "    num_samples = int(total_length * density)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while(len(all_weights) < 3000 and count < max_iter):\n",
    "        count = count + 1\n",
    "#         if count % 5000 == 0:\n",
    "#             print(count)\n",
    "            \n",
    "        sampled_weights = sample_pdf(num_samples)\n",
    "\n",
    "        x = []\n",
    "\n",
    "        for weight in sampled_weights:\n",
    "            location = np.random.uniform(0, total_length)\n",
    "            length = get_length_from_weight(weight)\n",
    "            depth = np.random.uniform(0, params_depth)\n",
    "\n",
    "            x.append([location, length, depth, weight])\n",
    "\n",
    "        a = np.array(x)\n",
    "        b = a[np.argsort(a[:, 2])]\n",
    "\n",
    "        all_segments = []\n",
    "        curr_segments = []\n",
    "        curr_depth = 0\n",
    "\n",
    "        for row in b:\n",
    "            curr_depth = row[2]\n",
    "\n",
    "            band = np.tan(fov / 2) * curr_depth\n",
    "\n",
    "            lower_bound = camera_location - band\n",
    "            upper_bound = camera_location + band\n",
    "            \n",
    "            if not ((row[0] > lower_bound) and (row[0] + row[1] < upper_bound)):\n",
    "                if (row[0] > lower_bound) and (row[0] < upper_bound):\n",
    "                    all_segments.append(row)\n",
    "                elif ((row[0] + row[1]) > lower_bound) and ((row[0] + row[1]) < upper_bound):\n",
    "                    all_segments.append(row)\n",
    "                continue\n",
    "\n",
    "            is_occluded = False\n",
    "\n",
    "            for seg in all_segments:\n",
    "                lower_adj_segment = camera_location + (row[0] - camera_location) * curr_depth / seg[2]\n",
    "                upper_adj_segment = camera_location + ((row[0] + row[1]) - camera_location) * curr_depth / seg[2]\n",
    "\n",
    "                if not ((row[0] + row[1]) < lower_adj_segment or row[0] > upper_adj_segment):\n",
    "                    is_occluded = True\n",
    "\n",
    "            if not is_occluded:\n",
    "                all_weights.append(row[3])\n",
    "                \n",
    "            all_segments.append(row)\n",
    "\n",
    "    return all_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This implements a Gram-Charlier expansion of the normal distribution where the first 2 moments coincide with those of the normal distribution but skew and kurtosis can deviate from it.\n",
    "'''\n",
    "# https://www.statsmodels.org/devel/generated/statsmodels.sandbox.distributions.extras.pdf_mvsk.html#statsmodels.sandbox.distributions.extras.pdf_mvsk\n",
    "# Not used: https://github.com/gregversteeg/gaussianize\n",
    "# Not used: https://stats.stackexchange.com/questions/20445/how-to-transform-data-to-normality\n",
    "# Inspiration: https://stats.stackexchange.com/questions/43482/transformation-to-increase-kurtosis-and-skewness-of-normal-r-v\n",
    "\n",
    "from statsmodels.sandbox.distributions.extras import pdf_mvsk\n",
    "\n",
    "def simulate_with_params(mean_factor, sd_factor, density, max_iter):\n",
    "    fov = 55\n",
    "    \n",
    "    print('Simulation: mean_factor: %0.2f, sd_factor: %0.2f, density: %i, max_iter: %i' % (mean_factor, sd_factor, density, max_iter))\n",
    "    \n",
    "    estimated_initial_weight = np.mean(original_weights) / mean_factor\n",
    "    estimated_initial_sd = np.std(original_weights) / sd_factor\n",
    "\n",
    "    new_pdf = pdf_mvsk([estimated_initial_weight, estimated_initial_sd ** 2, 0, 1])\n",
    "\n",
    "    simulated_weights = get_simulated_weights(fov, density, new_pdf, max_iter)\n",
    "    \n",
    "    return simulated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(simulated_weights, original_weights, verbose):\n",
    "    if verbose:\n",
    "        print(np.mean(simulated_weights), np.std(simulated_weights), len(simulated_weights))\n",
    "        print(np.mean(original_weights), np.std(original_weights))\n",
    "\n",
    "    weight_error = (np.mean(simulated_weights) - np.mean(original_weights)) / np.mean(original_weights) * 100\n",
    "    sd_error = (np.std(simulated_weights) - np.std(original_weights)) / np.std(original_weights) * 100\n",
    "    \n",
    "    if verbose:\n",
    "        plt.hist(simulated_weights, alpha = 0.5, color = 'blue', density = True)\n",
    "        plt.hist(original_weights, alpha = 0.5, color = 'red', density = True)\n",
    "\n",
    "    buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    x_buckets = np.array(buckets[:-1])\n",
    "\n",
    "    d1 = np.array(original_weights)\n",
    "    d2 = np.array(simulated_weights)\n",
    "\n",
    "    pcts1 = []\n",
    "    pcts2 = []\n",
    "\n",
    "    errors1 = []\n",
    "    errors2 = []\n",
    "\n",
    "    for i in range(len(buckets) - 1):\n",
    "        mask1 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "        mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "        pct1 = np.sum(mask1) / len(mask1)\n",
    "        pcts1.append(pct1)\n",
    "        pct2 = np.sum(mask2) / len(mask2)\n",
    "        pcts2.append(pct2)\n",
    "\n",
    "        errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "        if verbose:\n",
    "            print('%i: %0.2f%%' % (buckets[i], 100 * (pct1 - pct2)))\n",
    "\n",
    "    if verbose:\n",
    "        print(np.max(errors1))\n",
    "        print(np.mean(errors1))\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "        plt.bar(x_buckets, pcts2, color = 'blue', width = 150, label = 'Dedup')\n",
    "    \n",
    "    return weight_error, sd_error, np.max(errors1), np.mean(errors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scratchwork\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(weights), np.std(original_weights)\n",
    "# print(estimated_initial_weight, estimated_initial_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_pdf = inverse_sample_decorator(new_pdf)\n",
    "\n",
    "# plt.hist(sample_pdf(10000), density = True, bins = 50)\n",
    "\n",
    "# vec = np.arange(0, 10000, 1)\n",
    "# example_pdf = new_pdf(vec)\n",
    "\n",
    "# plt.plot(vec, example_pdf, lw = 4, color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Calculate empirical SD, mean\n",
    "2. Initialize Gram-Charlier distribution to empirical SD * 1.06, empirical mean * 0.965, density to 5\n",
    "3. Run gradient search to find minimum distribution deviation via simulation\n",
    "4. Resulting distribution is the ground truth distribution\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'vikane_pen_id_60_2020-08-05_2020-08-30'\n",
    "start_hour = 5\n",
    "end_hour = 15\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.17\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .005):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.column_stack((mean_factors, sd_factors, weight_errors, sd_errors, max_dist_errors, avg_dist_errors)), columns = ['mean_factors', 'sd_factors', 'weight_errors', 'sd_errors', 'max_dist_errors', 'avg_dist_errors']) \n",
    "\n",
    "pivot1 = output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'weight_errors')\n",
    "pivot2 = output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'sd_errors')\n",
    "pivot3 = output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'max_dist_errors')\n",
    "pivot4 = output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'avg_dist_errors')\n",
    "\n",
    "pivot_all = 0.25 * (np.abs(pivot1) + np.abs(pivot2) + np.abs(pivot3) + np.abs(pivot4))\n",
    "\n",
    "pivot_interp = 0.25 * (pivot_all.values[1:,1:] + pivot_all.values[:-1,1:] + pivot_all.values[1:,:-1] + pivot_all.values[:-1,:-1])\n",
    "\n",
    "print(pivot_all)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((pivot_all), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((pivot_interp), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "print(np.mean(pivot_all, 0))\n",
    "print(np.mean(pivot_all, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vikane Pen 5 Review\n",
    "\n",
    "selected_mean_factor = .96\n",
    "selected_sd_factor = 1.02\n",
    "\n",
    "exact_mean_factor = .9625\n",
    "exact_sd_factor = 1.015\n",
    "\n",
    "count = 0\n",
    "index = None\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .005):\n",
    "    for sd_factor in np.arange(1, 1.08, .02):\n",
    "        if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "            index = count\n",
    "        count = count + 1\n",
    "\n",
    "count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "estimated_initial_weight = np.mean(original_weights) / exact_mean_factor\n",
    "estimated_initial_sd = np.std(original_weights) / exact_sd_factor\n",
    "\n",
    "gt_mean = 4234.9465174614315\n",
    "gt_sd = 820.6951915326154\n",
    "\n",
    "print('Sim', sim_avg_weights[index], sim_sds[index])\n",
    "print('Orig', np.mean(original_weights), np.std(original_weights))\n",
    "print('Estimate', estimated_initial_weight, estimated_initial_sd)\n",
    "print('GT', gt_mean, gt_sd)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Estimate', '%0.2f, %0.2f%%' % (estimated_initial_weight - gt_mean, 100 * (estimated_initial_weight - gt_mean) / gt_mean))\n",
    "print('Orig', '%0.2f, %0.2f%%' % (np.mean(original_weights) - gt_mean, 100 * (np.mean(original_weights) - gt_mean) / gt_mean))\n",
    "\n",
    "print()\n",
    "\n",
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.012362459546925567,\n",
    "    0.05272923408845739,\n",
    "    0.31016181229773465,\n",
    "    0.4608414239482201,\n",
    "    0.14977346278317152,\n",
    "    0.013398058252427184,\n",
    "    0.0006040992448759439,\n",
    "    0.00012944983818770226,\n",
    "    0.0\n",
    "]\n",
    "\n",
    "d1 = np.array(original_weights)\n",
    "# d2 = np.array(sim_raw_weights[index]) / exact_mean_factor\n",
    "\n",
    "sim_vec = np.arange(start=0, stop = 10000, step = 1)\n",
    "\n",
    "orig_pdf = pdf_mvsk([np.mean(original_weights), np.std(original_weights) ** 2, 0, 1])\n",
    "orig_dist = orig_pdf(sim_vec)\n",
    "\n",
    "sim_pdf = pdf_mvsk([estimated_initial_weight, estimated_initial_sd ** 2, 0, 1])\n",
    "sim_dist = sim_pdf(sim_vec)\n",
    "\n",
    "avg_pdf = pdf_mvsk([0.5 * (estimated_initial_weight + np.mean(original_weights)), (0.5 * (estimated_initial_sd + np.std(original_weights))) ** 2, 0, 1])\n",
    "avg_dist = avg_pdf(sim_vec)\n",
    "\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "#     mask1 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "    pct1 = np.sum(orig_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(orig_dist)\n",
    "#     mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "#     pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "#     pct2 = np.sum(mask2) / len(mask2)\n",
    "    pct2 = np.sum(sim_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(sim_dist)\n",
    "    pcts2.append(pct2)\n",
    "    \n",
    "    pct3 = np.sum(avg_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(avg_dist)\n",
    "    pcts3.append(pct3)\n",
    "\n",
    "#     errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "#     if verbose:\n",
    "    gt_pct = gt_pcts[i]\n",
    "    print('%i: %0.2f%% vs %0.2f%% vs %0.2f%%' % (buckets[i], 100 * (pct1 - gt_pct), 100 * (pct2 - gt_pct), 100 * (pct3 - gt_pct)))\n",
    "\n",
    "#     print(np.max(errors1))\n",
    "#     print(np.mean(errors1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'GT')\n",
    "plt.bar(x_buckets + 150, pcts2, color = 'blue', width = 150, label = 'Sim')\n",
    "plt.bar(x_buckets + 300, pcts3, color = 'purple', width = 150, label = 'Orig + Sim Avg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_mean_factor = .97\n",
    "# selected_sd_factor = 1.04\n",
    "\n",
    "# count = 0\n",
    "# index = None\n",
    "\n",
    "# for mean_factor in np.arange(0.95, .98, .005):\n",
    "#     for sd_factor in np.arange(1, 1.08, .02):\n",
    "#         if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "#             index = count\n",
    "#         count = count + 1\n",
    "        \n",
    "# print(sim_avg_weights[index], sim_sds[index])\n",
    "# print(np.mean(original_weights), np.std(original_weights))\n",
    "\n",
    "# count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "# plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "# estimated_initial_weight = np.mean(original_weights) / selected_mean_factor\n",
    "# estimated_initial_sd = np.std(original_weights) / selected_sd_factor\n",
    "\n",
    "# print(estimated_initial_weight, estimated_initial_sd)\n",
    "# print(estimated_initial_weight - 4234.9465174614315)\n",
    "# print(np.mean(original_weights) - 4234.9465174614315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'aplavika_pen_id_95_2020-06-26_2020-07-26'\n",
    "start_hour = 6\n",
    "end_hour = 16\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.1753\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output = pd.DataFrame(np.column_stack((mean_factors, sd_factors, weight_errors, sd_errors, max_dist_errors, avg_dist_errors)), columns = ['mean_factors', 'sd_factors', 'weight_errors', 'sd_errors', 'max_dist_errors', 'avg_dist_errors']) \n",
    "\n",
    "a_pivot1 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'weight_errors')\n",
    "a_pivot2 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'sd_errors')\n",
    "a_pivot3 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'max_dist_errors')\n",
    "a_pivot4 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'avg_dist_errors')\n",
    "\n",
    "a_pivot_all = 0.25 * (np.abs(a_pivot1) + np.abs(a_pivot2) + np.abs(a_pivot3) + np.abs(a_pivot4))\n",
    "\n",
    "a_pivot_interp = 0.25 * (a_pivot_all.values[1:,1:] + a_pivot_all.values[:-1,1:] + a_pivot_all.values[1:,:-1] + a_pivot_all.values[:-1,:-1])\n",
    "\n",
    "print(a_pivot_all)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_all), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_interp), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "print(np.mean(a_pivot_all, 0))\n",
    "print(np.mean(a_pivot_all, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplavika Review\n",
    "\n",
    "selected_mean_factor = .96\n",
    "selected_sd_factor = 1.02\n",
    "\n",
    "exact_mean_factor = .965\n",
    "exact_sd_factor = 1.015\n",
    "\n",
    "count = 0\n",
    "index = None\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .02):\n",
    "        if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "            index = count\n",
    "        count = count + 1\n",
    "\n",
    "count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "loss_factor = 0.1753\n",
    "\n",
    "estimated_initial_weight = np.mean(original_weights) / exact_mean_factor\n",
    "estimated_initial_sd = np.std(original_weights) / exact_sd_factor\n",
    "\n",
    "gt_mean = 4944 / (1 - loss_factor)\n",
    "\n",
    "print('Sim', sim_avg_weights[index], sim_sds[index])\n",
    "print('Orig', np.mean(original_weights), np.std(original_weights))\n",
    "print('Estimate', estimated_initial_weight, estimated_initial_sd)\n",
    "print('GT', gt_mean)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Estimate', '%0.2f, %0.2f%%' % (estimated_initial_weight - gt_mean, 100 * (estimated_initial_weight - gt_mean) / gt_mean))\n",
    "print('Orig', '%0.2f, %0.2f%%' % (np.mean(original_weights) - gt_mean, 100 * (np.mean(original_weights) - gt_mean) / gt_mean))\n",
    "\n",
    "print()\n",
    "\n",
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.0,\n",
    "    0.0036,\n",
    "    0.1060,\n",
    "    0.3990,\n",
    "    0.3576,\n",
    "    .1147,\n",
    "    .0180,\n",
    "    .0011,\n",
    "    0.0\n",
    "]\n",
    "\n",
    "d1 = np.array(original_weights) * (1 - loss_factor)\n",
    "# d2 = np.array(sim_raw_weights[index]) / exact_mean_factor\n",
    "\n",
    "sim_vec = np.arange(start=0, stop = 10000, step = 1)\n",
    "\n",
    "orig_pdf = pdf_mvsk([np.mean(original_weights * (1 - loss_factor)), np.std(original_weights * (1 - loss_factor)) ** 2, 0, 1])\n",
    "orig_dist = orig_pdf(sim_vec)\n",
    "\n",
    "sim_pdf = pdf_mvsk([estimated_initial_weight * (1 - loss_factor), (estimated_initial_sd  * np.sqrt(1 - loss_factor)) ** 2, 0, 1])\n",
    "sim_dist = sim_pdf(sim_vec)\n",
    "\n",
    "avg_pdf = pdf_mvsk([0.5 * (estimated_initial_weight + np.mean(original_weights)) * (1 - loss_factor), (0.5 * (estimated_initial_sd  * np.sqrt(1 - loss_factor) + np.std(original_weights * (1 - loss_factor)))) ** 2, 0, 1])\n",
    "avg_dist = avg_pdf(sim_vec)\n",
    "\n",
    "pcts0 = []\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask0 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "    pct0 = np.sum(mask0) / len(mask0)\n",
    "    pcts0.append(pct0)\n",
    "\n",
    "    pct1 = np.sum(orig_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(orig_dist)\n",
    "#     mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "#     pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "#     pct2 = np.sum(mask2) / len(mask2)\n",
    "    pct2 = np.sum(sim_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(sim_dist)\n",
    "    pcts2.append(pct2)\n",
    "    \n",
    "    pct3 = np.sum(avg_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(avg_dist)\n",
    "    pcts3.append(pct3)\n",
    "\n",
    "#     errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "#     if verbose:\n",
    "    gt_pct = gt_pcts[i]\n",
    "    print('%i: %0.2f%% vs %0.2f%% vs %0.2f%% vs %0.2f%%' % (buckets[i], 100 * (pct0 - gt_pct), 100 * (pct1 - gt_pct), 100 * (pct2 - gt_pct), 100 * (pct3 - gt_pct)))\n",
    "\n",
    "#     print(np.max(errors1))\n",
    "#     print(np.mean(errors1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'GT')\n",
    "plt.bar(x_buckets + 150, pcts2, color = 'blue', width = 150, label = 'Sim')\n",
    "plt.bar(x_buckets + 300, pcts3, color = 'purple', width = 150, label = 'Orig + Sim Avg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'tittelsnes_pen_id_37_2020-05-23_2020-06-24'\n",
    "start_hour = 6\n",
    "end_hour = 16\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.16\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output = pd.DataFrame(np.column_stack((mean_factors, sd_factors, weight_errors, sd_errors, max_dist_errors, avg_dist_errors)), columns = ['mean_factors', 'sd_factors', 'weight_errors', 'sd_errors', 'max_dist_errors', 'avg_dist_errors']) \n",
    "\n",
    "a_pivot1 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'weight_errors')\n",
    "a_pivot2 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'sd_errors')\n",
    "a_pivot3 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'max_dist_errors')\n",
    "a_pivot4 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'avg_dist_errors')\n",
    "\n",
    "a_pivot_all = 0.25 * (np.abs(a_pivot1) + np.abs(a_pivot2) + np.abs(a_pivot3) + np.abs(a_pivot4))\n",
    "\n",
    "a_pivot_interp = 0.25 * (a_pivot_all.values[1:,1:] + a_pivot_all.values[:-1,1:] + a_pivot_all.values[1:,:-1] + a_pivot_all.values[:-1,:-1])\n",
    "\n",
    "# print(a_pivot_all)\n",
    "\n",
    "print(a_pivot1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot1), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "# print(a_pivot2)\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow((a_pivot2), cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "\n",
    "# print(a_pivot3)\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow((a_pivot3), cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "\n",
    "# print(a_pivot4)\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow((a_pivot4), cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow((a_pivot_all), cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow((a_pivot_interp), cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "\n",
    "print(np.mean(a_pivot_all, 0))\n",
    "print(np.mean(a_pivot_all, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplavika Review\n",
    "\n",
    "selected_mean_factor = .95\n",
    "selected_sd_factor = 1.08\n",
    "\n",
    "exact_mean_factor = .95\n",
    "exact_sd_factor = 1.08\n",
    "\n",
    "count = 0\n",
    "index = None\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .02):\n",
    "        if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "            index = count\n",
    "        count = count + 1\n",
    "\n",
    "count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "loss_factor = 0.16\n",
    "\n",
    "estimated_initial_weight = np.mean(original_weights) / exact_mean_factor\n",
    "estimated_initial_sd = np.std(original_weights) / exact_sd_factor\n",
    "\n",
    "gt_mean = 3894.77 / (1 - loss_factor)\n",
    "\n",
    "print('Sim', sim_avg_weights[index], sim_sds[index])\n",
    "print('Orig', np.mean(original_weights), np.std(original_weights))\n",
    "print('Estimate', estimated_initial_weight, estimated_initial_sd)\n",
    "print('GT', gt_mean)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Estimate', '%0.2f, %0.2f%%' % (estimated_initial_weight - gt_mean, 100 * (estimated_initial_weight - gt_mean) / gt_mean))\n",
    "print('Orig', '%0.2f, %0.2f%%' % (np.mean(original_weights) - gt_mean, 100 * (np.mean(original_weights) - gt_mean) / gt_mean))\n",
    "print('New', '%0.2f, %0.2f%%' % (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean, 100 * (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean) / gt_mean))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.0365,\n",
    "#     .2114,\n",
    "#     .3232,\n",
    "#     .2538,\n",
    "#     .1254,\n",
    "#     .0399,\n",
    "#     .0087,\n",
    "#     .0010,\n",
    "#     0.0001\n",
    "# ]\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.0174,\n",
    "    .1711,\n",
    "    .3285,\n",
    "    .2777,\n",
    "    .1459,\n",
    "    .0477,\n",
    "    .0104,\n",
    "    .0013,\n",
    "    0.0001\n",
    "]\n",
    "\n",
    "d1 = np.array(original_weights) * (1 - loss_factor)\n",
    "# d2 = np.array(sim_raw_weights[index]) / exact_mean_factor\n",
    "\n",
    "sim_vec = np.arange(start=0, stop = 10000, step = 1)\n",
    "\n",
    "orig_pdf = pdf_mvsk([np.mean(original_weights * (1 - loss_factor)), np.std(original_weights * (1 - loss_factor)) ** 2, 0, 1])\n",
    "orig_dist = orig_pdf(sim_vec)\n",
    "\n",
    "sim_pdf = pdf_mvsk([estimated_initial_weight * (1 - loss_factor), (estimated_initial_sd  * np.sqrt(1 - loss_factor)) ** 2, 0, 1])\n",
    "sim_dist = sim_pdf(sim_vec)\n",
    "\n",
    "avg_pdf = pdf_mvsk([0.5 * (estimated_initial_weight + sim_avg_weights[index]) * (1 - loss_factor), (0.5 * (estimated_initial_sd  * np.sqrt(1 - loss_factor) + sim_sds[index] * np.sqrt(1 - loss_factor))) ** 2, 0, 1])\n",
    "avg_dist = avg_pdf(sim_vec)\n",
    "\n",
    "pcts0 = []\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask0 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "    pct0 = np.sum(mask0) / len(mask0)\n",
    "    pcts0.append(pct0)\n",
    "\n",
    "    pct1 = np.sum(orig_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(orig_dist)\n",
    "#     mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "#     pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "#     pct2 = np.sum(mask2) / len(mask2)\n",
    "    pct2 = np.sum(sim_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(sim_dist)\n",
    "    pcts2.append(pct2)\n",
    "    \n",
    "    pct3 = np.sum(avg_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(avg_dist)\n",
    "    pcts3.append(pct3)\n",
    "\n",
    "#     errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "#     if verbose:\n",
    "    gt_pct = gt_pcts[i]\n",
    "    print('%i: %0.2f%% vs %0.2f%% vs %0.2f%% vs %0.2f%%' % (buckets[i], 100 * (pct0 - gt_pct), 100 * (pct1 - gt_pct), 100 * (pct2 - gt_pct), 100 * (pct3 - gt_pct)))\n",
    "\n",
    "#     print(np.max(errors1))\n",
    "#     print(np.mean(errors1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'GT')\n",
    "plt.bar(x_buckets + 150, pcts2, color = 'blue', width = 150, label = 'Sim')\n",
    "plt.bar(x_buckets + 300, pcts3, color = 'purple', width = 150, label = 'Orig + Sim Avg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort_name = 'langoy_pen_id_108_2020-05-07_2020-05-17'\n",
    "# start_hour = 6\n",
    "# end_hour = 12\n",
    "# apply_growth_rate = True\n",
    "# max_day_diff = 3\n",
    "# final_days_post_feeding = 1\n",
    "# max_final_days_post_feeding = 1\n",
    "# loss_factor = 0.16\n",
    "\n",
    "cohort_name = 'eldviktaren_pen_id_164_2020-09-06_2020-10-06'\n",
    "start_hour = 7\n",
    "end_hour = 15\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.16\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output = pd.DataFrame(np.column_stack((mean_factors, sd_factors, weight_errors, sd_errors, max_dist_errors, avg_dist_errors)), columns = ['mean_factors', 'sd_factors', 'weight_errors', 'sd_errors', 'max_dist_errors', 'avg_dist_errors']) \n",
    "\n",
    "a_pivot1 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'weight_errors')\n",
    "a_pivot2 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'sd_errors')\n",
    "a_pivot3 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'max_dist_errors')\n",
    "a_pivot4 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'avg_dist_errors')\n",
    "\n",
    "a_pivot_all = 0.25 * (np.abs(a_pivot1) + np.abs(a_pivot2) + np.abs(a_pivot3) + np.abs(a_pivot4))\n",
    "\n",
    "a_pivot_interp = 0.25 * (a_pivot_all.values[1:,1:] + a_pivot_all.values[:-1,1:] + a_pivot_all.values[1:,:-1] + a_pivot_all.values[:-1,:-1])\n",
    "\n",
    "print(a_pivot_all)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_all), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_interp), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "print(np.mean(a_pivot_all, 0))\n",
    "print(np.mean(a_pivot_all, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eldviktaren Review\n",
    "\n",
    "selected_mean_factor = .98\n",
    "selected_sd_factor = 1.01\n",
    "\n",
    "exact_mean_factor = .98\n",
    "exact_sd_factor = 1.01\n",
    "\n",
    "count = 0\n",
    "index = None\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "            index = count\n",
    "        count = count + 1\n",
    "\n",
    "count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "loss_factor = 0.18\n",
    "\n",
    "estimated_initial_weight = np.mean(original_weights) / exact_mean_factor\n",
    "estimated_initial_sd = np.std(original_weights) / exact_sd_factor\n",
    "\n",
    "gt_mean = 3365.32 / (1 - loss_factor)\n",
    "\n",
    "print('Sim', sim_avg_weights[index], sim_sds[index])\n",
    "print('Orig', np.mean(original_weights), np.std(original_weights))\n",
    "print('Estimate', estimated_initial_weight, estimated_initial_sd)\n",
    "print('GT', gt_mean)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Estimate', '%0.2f, %0.2f%%' % (estimated_initial_weight - gt_mean, 100 * (estimated_initial_weight - gt_mean) / gt_mean))\n",
    "print('Orig', '%0.2f, %0.2f%%' % (np.mean(original_weights) - gt_mean, 100 * (np.mean(original_weights) - gt_mean) / gt_mean))\n",
    "print('New', '%0.2f, %0.2f%%' % (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean, 100 * (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean) / gt_mean))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.0365,\n",
    "#     .2114,\n",
    "#     .3232,\n",
    "#     .2538,\n",
    "#     .1254,\n",
    "#     .0399,\n",
    "#     .0087,\n",
    "#     .0010,\n",
    "#     0.0001\n",
    "# ]\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.0062,\n",
    "    .2281,\n",
    "    .6490,\n",
    "    .1143,\n",
    "    .0023,\n",
    "    .0001,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "]\n",
    "\n",
    "d1 = np.array(original_weights) * (1 - loss_factor)\n",
    "# d2 = np.array(sim_raw_weights[index]) / exact_mean_factor\n",
    "\n",
    "sim_vec = np.arange(start=0, stop = 10000, step = 1)\n",
    "\n",
    "orig_pdf = pdf_mvsk([np.mean(original_weights * (1 - loss_factor)), np.std(original_weights * (1 - loss_factor)) ** 2, 0, 1])\n",
    "orig_dist = orig_pdf(sim_vec)\n",
    "\n",
    "sim_pdf = pdf_mvsk([estimated_initial_weight * (1 - loss_factor), (estimated_initial_sd  * np.sqrt(1 - loss_factor)) ** 2, 0, 1])\n",
    "sim_dist = sim_pdf(sim_vec)\n",
    "\n",
    "avg_pdf = pdf_mvsk([0.5 * (estimated_initial_weight + sim_avg_weights[index]) * (1 - loss_factor), (0.5 * (estimated_initial_sd  * np.sqrt(1 - loss_factor) + sim_sds[index] * np.sqrt(1 - loss_factor))) ** 2, 0, 1])\n",
    "avg_dist = avg_pdf(sim_vec)\n",
    "\n",
    "pcts0 = []\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask0 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "    pct0 = np.sum(mask0) / len(mask0)\n",
    "    pcts0.append(pct0)\n",
    "\n",
    "    pct1 = np.sum(orig_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(orig_dist)\n",
    "#     mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "#     pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "#     pct2 = np.sum(mask2) / len(mask2)\n",
    "    pct2 = np.sum(sim_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(sim_dist)\n",
    "    pcts2.append(pct2)\n",
    "    \n",
    "    pct3 = np.sum(avg_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(avg_dist)\n",
    "    pcts3.append(pct3)\n",
    "\n",
    "#     errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "#     if verbose:\n",
    "    gt_pct = gt_pcts[i]\n",
    "    print('%i: %0.2f%% vs %0.2f%% vs %0.2f%% vs %0.2f%%' % (buckets[i], 100 * (pct0 - gt_pct), 100 * (pct1 - gt_pct), 100 * (pct2 - gt_pct), 100 * (pct3 - gt_pct)))\n",
    "\n",
    "#     print(np.max(errors1))\n",
    "#     print(np.mean(errors1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'GT')\n",
    "plt.bar(x_buckets + 150, pcts2, color = 'blue', width = 150, label = 'Sim')\n",
    "plt.bar(x_buckets + 300, pcts3, color = 'purple', width = 150, label = 'Orig + Sim Avg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'langoy_pen_id_108_2020-05-07_2020-05-17'\n",
    "start_hour = 6\n",
    "end_hour = 12\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.16\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output = pd.DataFrame(np.column_stack((mean_factors, sd_factors, weight_errors, sd_errors, max_dist_errors, avg_dist_errors)), columns = ['mean_factors', 'sd_factors', 'weight_errors', 'sd_errors', 'max_dist_errors', 'avg_dist_errors']) \n",
    "\n",
    "a_pivot1 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'weight_errors')\n",
    "a_pivot2 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'sd_errors')\n",
    "a_pivot3 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'max_dist_errors')\n",
    "a_pivot4 = a_output.pivot(index = 'mean_factors', columns = 'sd_factors', values = 'avg_dist_errors')\n",
    "\n",
    "a_pivot_all = 0.25 * (np.abs(a_pivot1) + np.abs(a_pivot2) + np.abs(a_pivot3) + np.abs(a_pivot4))\n",
    "\n",
    "a_pivot_interp = 0.25 * (a_pivot_all.values[1:,1:] + a_pivot_all.values[:-1,1:] + a_pivot_all.values[1:,:-1] + a_pivot_all.values[:-1,:-1])\n",
    "\n",
    "print(a_pivot_all)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_all), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow((a_pivot_interp), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "print(np.mean(a_pivot_all, 0))\n",
    "print(np.mean(a_pivot_all, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langoy Review\n",
    "\n",
    "selected_mean_factor = .96\n",
    "selected_sd_factor = 1.07\n",
    "\n",
    "exact_mean_factor = .96\n",
    "exact_sd_factor = 1.07\n",
    "\n",
    "count = 0\n",
    "index = None\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        if np.abs(mean_factor - selected_mean_factor) < .001 and np.abs(sd_factor - selected_sd_factor) < .001:\n",
    "            index = count\n",
    "        count = count + 1\n",
    "\n",
    "count, bins, _ = plt.hist(sim_raw_weights[index], alpha = 0.5, density = True, color = 'red', bins = 30)\n",
    "plt.hist(original_weights, alpha = 0.5, density = True, color = 'blue', bins = bins)\n",
    "\n",
    "loss_factor = 0.16\n",
    "\n",
    "estimated_initial_weight = np.mean(original_weights) / exact_mean_factor\n",
    "estimated_initial_sd = np.std(original_weights) / exact_sd_factor\n",
    "\n",
    "gt_mean = 4628.51 / (1 - loss_factor)\n",
    "\n",
    "print('Sim', sim_avg_weights[index], sim_sds[index])\n",
    "print('Orig', np.mean(original_weights), np.std(original_weights))\n",
    "print('Estimate', estimated_initial_weight, estimated_initial_sd)\n",
    "print('GT', gt_mean)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Estimate', '%0.2f, %0.2f%%' % (estimated_initial_weight - gt_mean, 100 * (estimated_initial_weight - gt_mean) / gt_mean))\n",
    "print('Orig', '%0.2f, %0.2f%%' % (np.mean(original_weights) - gt_mean, 100 * (np.mean(original_weights) - gt_mean) / gt_mean))\n",
    "print('New', '%0.2f, %0.2f%%' % (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean, 100 * (0.5 * (estimated_initial_weight + sim_avg_weights[index]) - gt_mean) / gt_mean))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.0365,\n",
    "#     .2114,\n",
    "#     .3232,\n",
    "#     .2538,\n",
    "#     .1254,\n",
    "#     .0399,\n",
    "#     .0087,\n",
    "#     .0010,\n",
    "#     0.0001\n",
    "# ]\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.0104,\n",
    "    0.0953,\n",
    "    .2740,\n",
    "    .2716,\n",
    "    .1702,\n",
    "    .1014,\n",
    "    .0511,\n",
    "    .0229,\n",
    "    .0031\n",
    "]\n",
    "\n",
    "d1 = np.array(original_weights) * (1 - loss_factor)\n",
    "# d2 = np.array(sim_raw_weights[index]) / exact_mean_factor\n",
    "\n",
    "sim_vec = np.arange(start=0, stop = 10000, step = 1)\n",
    "\n",
    "orig_pdf = pdf_mvsk([np.mean(original_weights * (1 - loss_factor)), np.std(original_weights * (1 - loss_factor)) ** 2, 0, 1])\n",
    "orig_dist = orig_pdf(sim_vec)\n",
    "\n",
    "sim_pdf = pdf_mvsk([estimated_initial_weight * (1 - loss_factor), (estimated_initial_sd  * np.sqrt(1 - loss_factor)) ** 2, 0, 1])\n",
    "sim_dist = sim_pdf(sim_vec)\n",
    "\n",
    "avg_pdf = pdf_mvsk([0.5 * (estimated_initial_weight + sim_avg_weights[index]) * (1 - loss_factor), (0.5 * (estimated_initial_sd  * np.sqrt(1 - loss_factor) + sim_sds[index] * np.sqrt(1 - loss_factor))) ** 2, 0, 1])\n",
    "avg_dist = avg_pdf(sim_vec)\n",
    "\n",
    "pcts0 = []\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "# errors1 = []\n",
    "# errors2 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask0 = (d1 > buckets[i]) & (d1 <= buckets[i + 1])\n",
    "    pct0 = np.sum(mask0) / len(mask0)\n",
    "    pcts0.append(pct0)\n",
    "\n",
    "    pct1 = np.sum(orig_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(orig_dist)\n",
    "#     mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "\n",
    "#     pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "#     pct2 = np.sum(mask2) / len(mask2)\n",
    "    pct2 = np.sum(sim_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(sim_dist)\n",
    "    pcts2.append(pct2)\n",
    "    \n",
    "    pct3 = np.sum(avg_dist[(sim_vec > buckets[i]) & (sim_vec <= buckets[i + 1])]) / np.sum(avg_dist)\n",
    "    pcts3.append(pct3)\n",
    "\n",
    "#     errors1.append(np.abs(100 * (pct1 - pct2)))\n",
    "\n",
    "#     if verbose:\n",
    "    gt_pct = gt_pcts[i]\n",
    "    print('%i: %0.2f%% vs %0.2f%% vs %0.2f%% vs %0.2f%%' % (buckets[i], 100 * (pct0 - gt_pct), 100 * (pct1 - gt_pct), 100 * (pct2 - gt_pct), 100 * (pct3 - gt_pct)))\n",
    "\n",
    "#     print(np.max(errors1))\n",
    "#     print(np.mean(errors1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'GT')\n",
    "plt.bar(x_buckets + 150, pcts2, color = 'blue', width = 150, label = 'Sim')\n",
    "plt.bar(x_buckets + 300, pcts3, color = 'purple', width = 150, label = 'Orig + Sim Avg')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'bolaks_pen_id_88_2020-02-10_2020-03-10'\n",
    "start_hour = 7\n",
    "end_hour = 15\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "final_days_post_feeding = 1\n",
    "max_final_days_post_feeding = 1\n",
    "loss_factor = 0.17\n",
    "\n",
    "gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=start_hour,\n",
    "    end_hour=end_hour,\n",
    "    kf_cutoff=0.0,\n",
    "    akpd_score_cutoff=0.01\n",
    ")\n",
    "df = dfs[cohort_name]\n",
    "final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "original_weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "\n",
    "mean_factors = []\n",
    "sd_factors = []\n",
    "\n",
    "sim_avg_weights = []\n",
    "sim_sds = []\n",
    "sim_len = []\n",
    "sim_raw_weights = []\n",
    "\n",
    "weight_errors = []\n",
    "sd_errors = []\n",
    "max_dist_errors = []\n",
    "avg_dist_errors = []\n",
    "\n",
    "density = 5\n",
    "max_iter = 10000\n",
    "\n",
    "for mean_factor in np.arange(0.95, .98, .01):\n",
    "    for sd_factor in np.arange(1, 1.08, .01):\n",
    "        simulated_weights = simulate_with_params(mean_factor, sd_factor, density, max_iter)\n",
    "        weight_error, sd_error, max_dist_error, avg_dist_error = get_error(simulated_weights, original_weights, False)\n",
    "        print('weight_error: %0.2f, sd_error: %0.2f, max_dist_error: %0.2f, avg_dist_error: %0.2f' % (weight_error, sd_error, max_dist_error, avg_dist_error))\n",
    "        \n",
    "        mean_factors.append(mean_factor)\n",
    "        sd_factors.append(sd_factor)\n",
    "        \n",
    "        sim_avg_weights.append(np.mean(simulated_weights))\n",
    "        sim_raw_weights.append(simulated_weights)\n",
    "        sim_sds.append(np.std(simulated_weights))\n",
    "        sim_len.append(len(simulated_weights))\n",
    "        \n",
    "        weight_errors.append(weight_error)\n",
    "        sd_errors.append(sd_error)\n",
    "        max_dist_errors.append(max_dist_error)\n",
    "        avg_dist_errors.append(avg_dist_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vikane\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.012362459546925567,\n",
    "#     0.05272923408845739,\n",
    "#     0.31016181229773465,\n",
    "#     0.4608414239482201,\n",
    "#     0.14977346278317152,\n",
    "#     0.013398058252427184,\n",
    "#     0.0006040992448759439,\n",
    "#     0.00012944983818770226,\n",
    "#     0.0\n",
    "# ]\n",
    "\n",
    "# Aplavika\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.0,\n",
    "#     0.0036,\n",
    "#     0.1060,\n",
    "#     0.3990,\n",
    "#     0.3576,\n",
    "#     .1147,\n",
    "#     .0180,\n",
    "#     .0011,\n",
    "#     0.0\n",
    "# ]\n",
    "\n",
    "# Tittelsnes\n",
    "# gt_pcts = [\n",
    "#     0.0,\n",
    "#     0.0365,\n",
    "#     .2114,\n",
    "#     .3232,\n",
    "#     .2538,\n",
    "#     .1254,\n",
    "#     .0399,\n",
    "#     .0087,\n",
    "#     .0010,\n",
    "#     0.0001\n",
    "# ]\n",
    "\n",
    "# Eldviktaren\n",
    "gt_pcts = [\n",
    "    0.0,\n",
    "    0.0062,\n",
    "    .2281,\n",
    "    .6490,\n",
    "    .1143,\n",
    "    .0023,\n",
    "    .0001,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "]\n",
    "\n",
    "a = np.cumsum(gt_pcts)\n",
    "mask = (a > 0) & (a < 0.999)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(x_buckets[mask], stats.norm.ppf(a[mask]))\n",
    "\n",
    "X = x_buckets[mask]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(stats.norm.ppf(a[mask]), X)\n",
    "results = model.fit()\n",
    "\n",
    "plt.plot(x_buckets[mask], results.predict(X), color = 'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf(a[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
