{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "from report_generation.report_generator import generate_ts_data, SamplingFilter\n",
    "from research.utils.datetime_utils import add_days\n",
    "from report_generation.report_generator import gen_pm_base\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, ValidationError\n",
    "from filter_optimization.filter_optimization_task import _add_date_hour_columns\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_names = [\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13',\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17',\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24',\n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26',\n",
    "    'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02',\n",
    "    'silda_pen_id_86_2020-06-19_2020-07-19',\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30',\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06',\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/root/data/alok/biomass_estimation/playground'\n",
    "batch_name = 'mirror'\n",
    "dfs, gt_metadatas = {}, {}\n",
    "for cohort_name in cohort_names:\n",
    "    s3_dir = os.path.join(\n",
    "        'https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/alok/production_datasets',\n",
    "        cohort_name\n",
    "    )\n",
    "\n",
    "    ground_truth_metadata_url = os.path.join(s3_dir, 'ground_truth_metadata.json')\n",
    "    ground_truth_key_base = os.path.join(batch_name, cohort_name, 'ground_truth_metadata.json')\n",
    "    ground_truth_f = os.path.join(ROOT_DIR, ground_truth_key_base)\n",
    "    s3.download_from_url(ground_truth_metadata_url, custom_location=ground_truth_f)\n",
    "    gt_metadata = json.load(open(ground_truth_f))\n",
    "    gt_metadatas[cohort_name] = gt_metadata\n",
    "    \n",
    "    data_url = os.path.join(s3_dir, 'annotation_dataset.csv')\n",
    "    data_f, _, _= s3.download_from_url(data_url)\n",
    "    df = pd.read_csv(data_f)\n",
    "    df = _add_date_hour_columns(df)\n",
    "    dfs[cohort_name] = df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate average weight accuracy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from population_metrics.population_metrics_base import PopulationMetricsBase, ValidationError\n",
    "from population_metrics.raw_metrics import get_raw_sample_size, get_raw_weight_values, get_raw_kf_values\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.confidence_metrics import generate_trend_stability, get_raw_and_historical_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "import numpy as np\n",
    "from population_metrics.population_metrics_base import PopulationMetricsBase, ValidationError\n",
    "from population_metrics.raw_metrics import get_raw_sample_size, get_raw_weight_values, get_raw_kf_values\n",
    "from population_metrics.growth_rate import compute_local_growth_rate\n",
    "from population_metrics.confidence_metrics import generate_trend_stability, get_raw_and_historical_weights\n",
    "from research.utils.datetime_utils import add_days, day_difference, get_dates_in_range\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for computing daily level smart features - for example, smart growth rate,\n",
    "smart distribution, smart average, smart k-factor, smart sample-size, and smart standard deviation. \n",
    "Ask Alok for more context behind mathematical formulation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_included_dates(pm_base: PopulationMetricsBase, date: str,\n",
    "                       max_day_difference: int, incorporate_future: bool) -> List:\n",
    "    \"\"\"\n",
    "    Gets list of dates that fall into window corresponding to max_day_difference. Window\n",
    "    is affected by whether or not incorporate_future is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    start = add_days(date, -max_day_difference)\n",
    "    end = add_days(date, max_day_difference if incorporate_future else 0)\n",
    "    included_dates = sorted([date for date in get_dates_in_range(start, end) if date in pm_base.unique_dates])\n",
    "    if not included_dates:\n",
    "        raise ValidationError('No raw biomass data found in window!')\n",
    "    return included_dates\n",
    "\n",
    "\n",
    "def get_smart_growth_rate(pm_base: PopulationMetricsBase, date: str,\n",
    "                          incorporate_future: bool = True, apply_growth_rate: bool = True,\n",
    "                          trend_stability_threshold: float = 0.9) -> float:\n",
    "    \"\"\"Get local growth rate adjustment to use for smart average computation.\"\"\"\n",
    "\n",
    "    raw_sample_size = get_raw_sample_size(pm_base, date)\n",
    "    growth_rate_for_smart_metrics = 0.0\n",
    "    if apply_growth_rate:\n",
    "        try:\n",
    "            growth_rate = compute_local_growth_rate(pm_base, date, incorporate_future=incorporate_future)\n",
    "            trend_stability = generate_trend_stability(pm_base, date, incorporate_future=incorporate_future)\n",
    "            if raw_sample_size and trend_stability and trend_stability > trend_stability_threshold:\n",
    "                growth_rate_for_smart_metrics = growth_rate\n",
    "        except ValidationError as err:\n",
    "            print(str(err))\n",
    "    return growth_rate_for_smart_metrics\n",
    "\n",
    "\n",
    "def generate_smart_individual_values(pm_base: PopulationMetricsBase, date: str, max_day_difference: int,\n",
    "                                     incorporate_future: bool, apply_growth_rate: bool,\n",
    "                                     trend_stability_threshold: float, reflection_point: float) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate smart individual values for weight and k-factor on given date.\n",
    "    Args:\n",
    "        pm_base: PopulationMetricsBase instance\n",
    "        date: the date to compute smart individual values for\n",
    "        max_day_difference: what is the maximum day difference of dates in the window?\n",
    "        incorporate_future: should future data be incorporated?\n",
    "        apply_growth_rate: should we apply a growth rate adjustment?\n",
    "        trend_stability_threshold: if apply_growth_rate is True, what minimum trend_stability_threshold\n",
    "                                   should we mandate for growth rate adjustment?\n",
    "    Returns:\n",
    "        adj_weights: growth rate adjusted individual weights in window\n",
    "        kfs: individual k-factor values in window\n",
    "    \"\"\"\n",
    "    \n",
    "    # validate data\n",
    "    included_dates = get_included_dates(pm_base, date, max_day_difference, incorporate_future)\n",
    "\n",
    "    # compute local growth rate to use for smart average\n",
    "    growth_rate_for_smart_metrics = get_smart_growth_rate(pm_base, date, incorporate_future=incorporate_future,\n",
    "                                                          apply_growth_rate=apply_growth_rate,\n",
    "                                                          trend_stability_threshold=trend_stability_threshold)\n",
    "\n",
    "    # get adjusted weights and kfs for smart metrics\n",
    "    all_weights = []\n",
    "    adj_weights, kfs = [], []\n",
    "    for d in included_dates:\n",
    "\n",
    "        # extend adjusted weights list for this date\n",
    "        weights_for_date = get_raw_weight_values(pm_base, d)\n",
    "        day_diff = day_difference(d, date)\n",
    "        adj_weights_for_date = np.array(weights_for_date) * np.exp(-day_diff * growth_rate_for_smart_metrics)\n",
    "        adj_weights.extend(adj_weights_for_date)\n",
    "\n",
    "        # extend k-factor list for this date\n",
    "        kfs_for_date = get_raw_kf_values(pm_base, d)\n",
    "        kfs.extend(kfs_for_date)\n",
    "        all_weights.extend(weights_for_date)\n",
    "\n",
    "    weights = np.array(adj_weights)\n",
    "#     new_reflection_point = reflection_point * np.median(adj_weights) / np.median(all_weights)\n",
    "#     weights = np.array(list(weights[weights < new_reflection_point]) + list(new_reflection_point + (new_reflection_point - weights[weights < new_reflection_point])))\n",
    "    return weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding, \n",
    "                                  reflection_point):\n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    date = add_days(last_feeding_date, days_post_feeding)\n",
    "    weights = generate_smart_individual_values(pm_base, date, max_day_diff, True, apply_growth_rate, 0.9, reflection_point)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def generate_average_weight_accuracy(weights, gt_metadata, loss_factor):\n",
    "    avg_weight_prediction = np.mean(weights)\n",
    "    gutted_weight_prediction = avg_weight_prediction * (1.0 - loss_factor)\n",
    "    gt_weight = gt_metadata['gutted_average_weight']\n",
    "    avg_weight_err = (gutted_weight_prediction - gt_weight) / gt_weight\n",
    "    return avg_weight_err\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_depths(df):\n",
    "    depths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ann = json.loads(row.annotation.replace(\"'\", '\"'))\n",
    "        cm = json.loads(row.camera_metadata.replace(\"'\", '\"'))\n",
    "        wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "\n",
    "        depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "        depths.append(depth)\n",
    "    return depths\n",
    "\n",
    "def get_reflection_point(tdf, sampling_filter):\n",
    "    hour_mask = (tdf.hour >= sampling_filter.start_hour) & (tdf.hour <= sampling_filter.end_hour)\n",
    "    kdf = tdf[hour_mask].copy(deep=True)\n",
    "    depths = generate_depths(kdf)\n",
    "    kdf['depth'] = depths\n",
    "    far_mask = kdf.depth > np.percentile(kdf.depth.values, 75)\n",
    "    reflection_point = kdf[far_mask].estimated_weight_g.median()\n",
    "    return reflection_point\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_factors = {\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13': 0.16, # unconfirmed\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10': 0.17, # confirmed\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17': 0.16, # unknown\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24': 0.16, # confirmed \n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26': 0.1753, # confirmed\n",
    "    'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02': 0.17, # confirmed\n",
    "    'silda_pen_id_86_2020-06-19_2020-07-19': .20, # confirmed\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30': .17, # confirmed\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06': .16, # confirmed\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31': .14 #uncofirmed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starvation_times = {\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13': 7, # unconfirmed\n",
    "    'bolaks_pen_id_88_2020-02-10_2020-03-10': 9, # 7, 8, 9, 12 - avg is 9. Could do sample size weighted\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17': 12, # confirmed\n",
    "    'tittelsnes_pen_id_37_2020-05-23_2020-06-24': 7, # 6, 7, 8 - avg is 7\n",
    "    'aplavika_pen_id_95_2020-06-26_2020-07-26': 6, # confirmed\n",
    "#     'kjeppevikholmen_pen_id_5_2019-06-05_2019-07-02': 0, # exclude from the study\n",
    "#     'silda_pen_id_86_2020-06-19_2020-07-19': 0, # unknown, exclude from the study\n",
    "    'vikane_pen_id_60_2020-08-05_2020-08-30': 9, # confirmed\n",
    "    'eldviktaren_pen_id_164_2020-09-06_2020-10-06': 13.5, # 12, 13, 14, 15 - avg is 13.5\n",
    "    'habranden_pen_id_100_2020-08-10_2020-08-31': 5 #uncofirmed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [5]\n",
    "end_hours = [15]\n",
    "apply_growth_rate_list = [True]\n",
    "max_day_diff_list = [3]\n",
    "days_post_feeding_list = [1]\n",
    "max_final_days_post_feeding = 1\n",
    "# loss_factors = [0.16, 0.17]\n",
    "\n",
    "cohort_name_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "apply_growth_rate_col = []\n",
    "max_day_diff_col = []\n",
    "days_post_feeding_col = []\n",
    "final_days_post_feeding_col = []\n",
    "loss_factor_col = []\n",
    "avg_weight_error_col = []\n",
    "starvation_time_col = []\n",
    "\n",
    "vikaneData = None\n",
    "\n",
    "for cohort_name in sorted(list(dfs.keys())):\n",
    "    if cohort_name not in starvation_times:\n",
    "        continue\n",
    "    starvation_time = starvation_times[cohort_name]\n",
    "    print(cohort_name)\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "    for start_hour in start_hours:\n",
    "        for end_hour in end_hours:\n",
    "            for final_days_post_feeding in days_post_feeding_list:\n",
    "                sampling_filter = SamplingFilter(\n",
    "                    start_hour=start_hour,\n",
    "                    end_hour=end_hour,\n",
    "                    kf_cutoff=0.0,\n",
    "                    akpd_score_cutoff=0.01\n",
    "                )\n",
    "                df = dfs[cohort_name]\n",
    "                final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "                tdf = df[(df.date <= final_date_post_feeding) & (df.date >= add_days(final_date_post_feeding, -14))]\n",
    "                pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "                \n",
    "                reflection_point = get_reflection_point(tdf, sampling_filter)\n",
    "                \n",
    "                for apply_growth_rate in apply_growth_rate_list:\n",
    "                    for max_day_diff in max_day_diff_list:\n",
    "                        for days_post_feeding in [1]:#range(0, final_days_post_feeding + 1):\n",
    "#                             for loss_factor in loss_factors:\n",
    "                            loss_factor = loss_factors[cohort_name]\n",
    "                            try:\n",
    "                                weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding, \n",
    "                                                                        reflection_point)\n",
    "                                if cohort_name == 'vikane_pen_id_60_2020-08-05_2020-08-30':\n",
    "                                    vikaneData = weights\n",
    "                            except ValidationError as err:\n",
    "                                continue\n",
    "                            avg_weight_err = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "\n",
    "                            cohort_name_col.append(cohort_name)\n",
    "                            start_hour_col.append(start_hour)\n",
    "                            end_hour_col.append(end_hour)\n",
    "                            apply_growth_rate_col.append(apply_growth_rate)\n",
    "                            max_day_diff_col.append(max_day_diff)\n",
    "                            days_post_feeding_col.append(days_post_feeding)\n",
    "                            final_days_post_feeding_col.append(final_days_post_feeding)\n",
    "                            loss_factor_col.append(loss_factor)\n",
    "                            avg_weight_error_col.append(avg_weight_err)\n",
    "                            starvation_time_col.append(starvation_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask1 = (vikaneData > buckets[i]) & (vikaneData <= buckets[i + 1])\n",
    "    print(np.sum(mask1) / len(mask1), ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(vikaneData), np.std(vikaneData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame({\n",
    "    'cohort_name': cohort_name_col,\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'apply_growth_rate': apply_growth_rate_col,\n",
    "    'max_day_diff': max_day_diff_col,\n",
    "    'days_post_feeding': days_post_feeding_col,\n",
    "    'final_days_post_feeding': final_days_post_feeding_col,\n",
    "    'loss_factor': loss_factor_col,\n",
    "    'avg_weight_error': avg_weight_error_col,\n",
    "    'starvation_time_col': starvation_time_col\n",
    "})\n",
    "\n",
    "tdf['avg_weight_error_abs'] = tdf.avg_weight_error.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(tdf.starvation_time_col, tdf.avg_weight_error)\n",
    "plt.axhline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name in cohort_names:\n",
    "    mask = (tdf.cohort_name == cohort_name) & (tdf.days_post_feeding == 1)\n",
    "    print(tdf[mask].sort_values('avg_weight_error_abs', ascending=True)[['cohort_name', 'avg_weight_error', 'loss_factor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['vikane_pen_id_60_2020-08-05_2020-08-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.cohort_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.cohort_name == 'tittelsnes_pen_id_37_2020-05-23_2020-06-24') & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3) & (tdf.loss_factor == 0.17)\n",
    "tdf[mask].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.start_hour_col == 6) & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3)\n",
    "tdf[mask].avg_weight_error_abs.median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.start_hour_col == 7) & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3)\n",
    "tdf[mask].avg_weight_error_abs.median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "apply_growth_rate_col = []\n",
    "max_day_diff_col = []\n",
    "days_post_feeding_col = []\n",
    "final_days_post_feeding_col = []\n",
    "loss_factor_col = []\n",
    "std_avg_weight_error_col = []\n",
    "abs_avg_weight_error_col = []\n",
    "mean_avg_weight_error_col = []\n",
    "\n",
    "for start_hour in start_hours:\n",
    "    for end_hour in end_hours:\n",
    "        for apply_growth_rate in apply_growth_rate_list:\n",
    "            for max_day_diff in max_day_diff_list:\n",
    "                for days_post_feeding in days_post_feeding_list:\n",
    "                    for final_days_post_feeding in final_days_post_feeding_list:\n",
    "                        for loss_factor in loss_factors:\n",
    "                            mask = (tdf.start_hour_col == start_hour) & \\\n",
    "                            (tdf.end_hour_col == end_hour) & \\\n",
    "                            (tdf.apply_growth_rate == apply_growth_rate) & \\\n",
    "                            (tdf.max_day_diff == max_day_diff) & \\\n",
    "                            (tdf.days_post_feeding == days_post_feeding) & \\\n",
    "                            (tdf.final_days_post_feeding == final_days_post_feeding) & \\\n",
    "                            (tdf.loss_factor == loss_factor)\n",
    "                            \n",
    "                            start_hour_col.append(start_hour)\n",
    "                            end_hour_col.append(end_hour)\n",
    "                            apply_growth_rate_col.append(apply_growth_rate)\n",
    "                            max_day_diff_col.append(max_day_diff)\n",
    "                            days_post_feeding_col.append(days_post_feeding)\n",
    "                            final_days_post_feeding_col.append(final_days_post_feeding)\n",
    "                            loss_factor_col.append(loss_factor)\n",
    "                            std_avg_weight_error_col.append(tdf[mask].avg_weight_error.std())\n",
    "                            abs_avg_weight_error_col.append(tdf[mask].avg_weight_error_abs.mean())\n",
    "                            mean_avg_weight_error_col.append(tdf[mask].avg_weight_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame({\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'apply_growth_rate': apply_growth_rate_col,\n",
    "    'max_day_diff': max_day_diff_col,\n",
    "    'days_post_feeding': days_post_feeding_col,\n",
    "    'final_days_post_feeding': final_days_post_feeding_col,\n",
    "    'loss_factor': loss_factor_col,\n",
    "    'abs_avg_weight_error': abs_avg_weight_error_col,\n",
    "    'std_avg_weight_error': std_avg_weight_error_col,\n",
    "    'mean_avg_weight_error': mean_avg_weight_error_col,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (rdf.loss_factor == 0.16)\n",
    "rdf[mask].sort_values('abs_avg_weight_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('/root/data/alok/biomass_estimation/playground/smart_average_param_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'bolaks_pen_id_88_2020-02-10_2020-03-10')].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Vikane average weight and distribution error - explore basic parameters\n",
    "\n",
    "ground_truth_metadata = json.load(open(ground_truth_f))\n",
    "day_after_feeding_stop = add_days(ground_truth_metadata['last_feeding_date'], 1)\n",
    "start_date, end_date = add_days(day_after_feeding_stop, -2), add_days(day_after_feeding_stop, -1)\n",
    "tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=7,\n",
    "    end_hour=15,\n",
    "    akpd_score_cutoff=0.95,\n",
    "    kf_cutoff=0.0\n",
    ")\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "weights, _ = generate_smart_individual_values(pm_base, day_after_feeding_stop, 3, True, True, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
