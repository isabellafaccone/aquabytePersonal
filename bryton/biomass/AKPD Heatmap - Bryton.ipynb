{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from research.weight_estimation.keypoint_utils import body_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KP = [\"TAIL_NOTCH\", \"ADIPOSE_FIN\", \"UPPER_LIP\", \"ANAL_FIN\", \"PELVIC_FIN\", \"EYE\", \"PECTORAL_FIN\", \"DORSAL_FIN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/data/alok/biomass_estimation/playground/gtsf_keypoints_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)\n",
    "\n",
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "gt_metadata = {'pen_id': 144,\n",
    " 'gutted_average_weight': 8000,\n",
    " 'gutted_weight_distribution': None,\n",
    " 'expected_loss_factor': 0.16,\n",
    " 'last_feeding_date': '2021-01-11',\n",
    " 'harvest_date': '2021-01-15',\n",
    " 'slaughter_date': '2021-01-15'}\n",
    "\n",
    "df2 = extract_biomass_data(gt_metadata['pen_id'], '2021-01-01', '2021-01-12', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from functools import singledispatch\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def to_serializable(val):\n",
    "    \"\"\"Used by default.\"\"\"\n",
    "    return str(val)\n",
    "\n",
    "\n",
    "@to_serializable.register(np.float32)\n",
    "def ts_float32(val):\n",
    "    \"\"\"Used if *val* is an instance of numpy.float32.\"\"\"\n",
    "    return np.float64(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(keypoints_new, open('/root/data/alok/biomass_estimation/playground/keypoints_new.json', 'w'), default=to_serializable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keypoints_new_crop'] = keypoints_new_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/gtsf_keypoints_new_crop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keypoints_new_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_new_crop = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print(count, 'out of', len(df))\n",
    "\n",
    "    count = count + 1\n",
    "    \n",
    "    if count < 0:\n",
    "        continue\n",
    "        \n",
    "    oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax = get_keypoints_with_crop(row)\n",
    "    \n",
    "    newKeypoints = {\n",
    "        'leftCrop': [],\n",
    "        'rightCrop': []\n",
    "    }\n",
    "    \n",
    "    left_crop_metadata = ast.literal_eval(row.left_crop_metadata)\n",
    "    right_crop_metadata = ast.literal_eval(row.right_crop_metadata)\n",
    "    \n",
    "    for i in np.arange(0, len(KP), 1):\n",
    "        newKeypoints['leftCrop'].append({\n",
    "            'xCrop': oL[i][0],\n",
    "            'yCrop': oL[i][1],\n",
    "            'xCropNew': kpL[i][0],\n",
    "            'yCropNew': kpL[i][1],\n",
    "            'xFrame': oL[i][0] + left_crop_metadata['x_coord'],\n",
    "            'yFrame': oL[i][1] + left_crop_metadata['y_coord'],\n",
    "            'xFrameNew': kpL[i][0] + left_crop_metadata['x_coord'],\n",
    "            'yFrameNew': kpL[i][1] + left_crop_metadata['y_coord'],\n",
    "            'score': kpLScore[i],\n",
    "            'scoreAvg': kpLScoreAvg[i],\n",
    "            'scoreMax': kpLScoreMax[i],\n",
    "            'keypointType': KP[i]\n",
    "        })\n",
    "        \n",
    "        newKeypoints['rightCrop'].append({\n",
    "            'xCrop': oR[i][0],\n",
    "            'yCrop': oR[i][1],\n",
    "            'xCropNew': kpR[i][0],\n",
    "            'yCropNew': kpR[i][1],\n",
    "            'xFrame': oR[i][0] + right_crop_metadata['x_coord'],\n",
    "            'yFrame': oR[i][1] + right_crop_metadata['y_coord'],\n",
    "            'xFrameNew': kpR[i][0] + right_crop_metadata['x_coord'],\n",
    "            'yFrameNew': kpR[i][1] + right_crop_metadata['y_coord'],\n",
    "            'score': kpRScore[i],\n",
    "            'scoreAvg': kpRScoreAvg[i],\n",
    "            'scoreMax': kpRScoreMax[i],\n",
    "            'keypointType': KP[i]\n",
    "        })\n",
    "        \n",
    "    keypoints_new_crop.append(newKeypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_bak_path = '/root/data/bati/model/config.json'\n",
    "# config_bak = json.load(open(config_bak_path))\n",
    "# config_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "config_path = '/root/data/bati/model/production_config.json'\n",
    "# config_path = '/root/data/bati/model/config_4_stage.json'\n",
    "\n",
    "checkpoint_path = '/root/data/bati/model/model_499.pb'\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "class FLAGS(object):\n",
    "    input_size = tuple(config[\"input_size\"])\n",
    "    stages = config[\"cpm_stages\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    joints = config[\"num_of_joints\"]\n",
    "    model_path = checkpoint_path\n",
    "    cmap_radius = config[\"center_radius\"]\n",
    "    keypoints_order = config[\"keypoints_order\"]\n",
    "    normalize = config[\"normalize\"]\n",
    "    heatmap_size = 512#config[\"heatmap_size\"]\n",
    "    joint_gaussian_variance = config[\"joint_gaussian_variance\"]\n",
    "    crop = config[\"crop\"]\n",
    "    augmentation = None\n",
    "    \n",
    "def load_pb(path_to_pb):\n",
    "    with tf.io.gfile.GFile(path_to_pb, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, input_map=None,\n",
    "                                return_elements=None,\n",
    "                                name=\"\",\n",
    "                                op_dict=None,\n",
    "                                producer_op_list=None)\n",
    "        graph_nodes=[n for n in graph_def.node]\n",
    "#         for t in graph_nodes:\n",
    "#             print(t.name)\n",
    "        return graph\n",
    "\n",
    "mod = load_pb(checkpoint_path)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "sess.graph.as_default()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "tf_device = '/gpu:0'\n",
    "with tf.device(tf_device):\n",
    "    model = mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "\n",
    "def image_resize(image, FLAGS):\n",
    "    height, width, _ = image.shape\n",
    "    ratio_width = width / FLAGS.input_size[0]\n",
    "    ratio_height = height / FLAGS.input_size[1]\n",
    "    image = cv2.resize(image, FLAGS.input_size)\n",
    "    image  = image / 255.0 - 0.5\n",
    "    image = image[np.newaxis, ...]\n",
    "    return image\n",
    "\n",
    "def enhance(image, clip_limit=5):\n",
    "    # convert image to LAB color model\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # split the image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(image_lab)\n",
    "\n",
    "    # apply CLAHE to lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
    "    merged_channels = cv2.merge((cl, a_channel, b_channel))\n",
    "\n",
    "    # convert image from LAB color model back to RGB color model\n",
    "    final_image = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
    "    return final_image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(row):\n",
    "    imL = row['left_image_url']\n",
    "    imR = row['right_image_url']\n",
    "    lco = row['left_crop_metadata']\n",
    "    rco = row['right_crop_metadata']\n",
    "    meta = row['camera_metadata']\n",
    "\n",
    "    imageL = url_to_image(imL)\n",
    "    imageR = url_to_image(imR)\n",
    "    \n",
    "    \n",
    "\n",
    "    img1 = enhance(imageL)\n",
    "    img2 = enhance(imageR)\n",
    "\n",
    "    heightL, widthL, _ = img1.shape\n",
    "    img_input = image_resize(img1, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "    heightR, widthR, _ = img2.shape\n",
    "    img_input = image_resize(img2, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapR = predict_heatmap.squeeze()\n",
    "\n",
    "    oL = [] # original left key-points using max method\n",
    "    oR = [] # original right key-points using max method\n",
    "    kpL = [] # new left key-points using avg method\n",
    "    kpR = [] # new right key-points using avg method\n",
    "    kpLScore = []\n",
    "    kpRScore = []\n",
    "    kpLScoreAvg = []\n",
    "    kpRScoreAvg = []\n",
    "    kpLScoreMax = []\n",
    "    kpRScoreMax = []\n",
    "\n",
    "    for c in np.arange(0, len(KP), 1):\n",
    "        hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "        hm_maxL = list(np.where(hm == hm.max()))   \n",
    "        \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape) # 2D locations corresponding to highest 10000 heatmap values\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oL.append([int(hm_maxL[1][0]), int(hm_maxL[0][0])]) \n",
    "        kpL.append([int(np.rint(x)), int(np.rint(y))])\n",
    "        kpLScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpLScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpLScoreMax.append(hm.max())\n",
    "\n",
    "        hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "        hm_maxR = np.where(hm == hm.max())\n",
    "    \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oR.append([int(hm_maxR[1][0]), int(hm_maxR[0][0])])\n",
    "        kpR.append([int(np.rint(x)), int(np.rint(y))])\n",
    "        kpRScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpRScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpRScoreMax.append(hm.max())\n",
    "\n",
    "    return oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_with_crop(row):\n",
    "    imL = row['left_image_url']\n",
    "    imR = row['right_image_url']\n",
    "    lco = row['left_crop_metadata']\n",
    "    rco = row['right_crop_metadata']\n",
    "    meta = ast.literal_eval(row['camera_metadata'])\n",
    "    gt_keypoints = ast.literal_eval(row['keypoints'])\n",
    "\n",
    "    # imL = row['left_crop_url']\n",
    "    # imR = row['right_crop_url']\n",
    "    # lco = row['left_crop_metadata']\n",
    "    # rco = row['right_crop_metadata']\n",
    "    # meta = row['camera_metadata']\n",
    "\n",
    "    imageL = url_to_image(imL)\n",
    "    imageR = url_to_image(imR)\n",
    "\n",
    "    leftCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "    rightCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "\n",
    "    for keypoint in gt_keypoints['leftCrop']:\n",
    "        leftCrop[0] = min(leftCrop[0], keypoint['xCrop'])\n",
    "        leftCrop[1] = max(leftCrop[1], keypoint['xCrop'])\n",
    "        leftCrop[2] = min(leftCrop[2], keypoint['yCrop'])\n",
    "        leftCrop[3] = max(leftCrop[3], keypoint['yCrop'])\n",
    "\n",
    "    for keypoint in gt_keypoints['rightCrop']:\n",
    "        rightCrop[0] = min(rightCrop[0], keypoint['xCrop'])\n",
    "        rightCrop[1] = max(rightCrop[1], keypoint['xCrop'])\n",
    "        rightCrop[2] = min(rightCrop[2], keypoint['yCrop'])\n",
    "        rightCrop[3] = max(rightCrop[3], keypoint['yCrop'])\n",
    "\n",
    "    buffer = 100\n",
    "\n",
    "    cropL = [max(leftCrop[0] - buffer, 0), min(leftCrop[1] + buffer, meta['pixelCountWidth']), max(leftCrop[2] - buffer, 0), min(leftCrop[3] + buffer, meta['pixelCountHeight'])]\n",
    "    min_x, max_x, min_y, max_y = cropL\n",
    "\n",
    "    newImageL = imageL[min_y:(max_y + 1), min_x:(max_x+1),:]\n",
    "\n",
    "    cropR = [max(rightCrop[0] - buffer, 0), min(rightCrop[1] + buffer, meta['pixelCountWidth']), max(rightCrop[2] - buffer, 0), min(rightCrop[3] + buffer, meta['pixelCountHeight'])]\n",
    "    min_x, max_x, min_y, max_y = cropR\n",
    "\n",
    "    newImageR = imageR[min_y:(max_y + 1), min_x:(max_x+1),:]\n",
    "\n",
    "    img1 = enhance(newImageL)\n",
    "    img2 = enhance(newImageR)\n",
    "\n",
    "    heightL, widthL, _ = img1.shape\n",
    "    img_input = image_resize(img1, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "    heightR, widthR, _ = img2.shape\n",
    "    img_input = image_resize(img2, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapR = predict_heatmap.squeeze()\n",
    "\n",
    "    oL = [] # original left key-points using max method\n",
    "    oR = [] # original right key-points using max method\n",
    "    kpL = [] # new left key-points using avg method\n",
    "    kpR = [] # new right key-points using avg method\n",
    "    kpLScore = []\n",
    "    kpRScore = []\n",
    "    kpLScoreAvg = []\n",
    "    kpRScoreAvg = []\n",
    "    kpLScoreMax = []\n",
    "    kpRScoreMax = []\n",
    "\n",
    "    for c in np.arange(0, len(KP), 1):\n",
    "        hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "        hm_maxL = list(np.where(hm == hm.max()))   \n",
    "        \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape) # 2D locations corresponding to highest 10000 heatmap values\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oL.append([cropL[0] + int(hm_maxL[1][0]), cropL[2] + int(hm_maxL[0][0])]) \n",
    "        kpL.append([cropL[0] + int(np.rint(x)), cropL[2] + int(np.rint(y))])\n",
    "        kpLScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpLScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpLScoreMax.append(hm.max())\n",
    "\n",
    "        hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "        hm_maxR = np.where(hm == hm.max())\n",
    "    \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oR.append([cropR[0] + int(hm_maxR[1][0]), cropR[2] + int(hm_maxR[0][0])])\n",
    "        kpR.append([cropR[0] + int(np.rint(x)), cropR[2] + int(np.rint(y))])\n",
    "        kpRScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpRScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpRScoreMax.append(hm.max())\n",
    "\n",
    "    return oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[24]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[24]\n",
    "    \n",
    "imL = row['left_image_url']\n",
    "imR = row['right_image_url']\n",
    "lco = row['left_crop_metadata']\n",
    "rco = row['right_crop_metadata']\n",
    "meta = ast.literal_eval(row['camera_metadata'])\n",
    "\n",
    "gt_keypoints = ast.literal_eval(row['keypoints'])\n",
    "\n",
    "# imL = row['left_crop_url']\n",
    "# imR = row['right_crop_url']\n",
    "# lco = row['left_crop_metadata']\n",
    "# rco = row['right_crop_metadata']\n",
    "# meta = row['camera_metadata']\n",
    "\n",
    "imageL = url_to_image(imL)\n",
    "imageR = url_to_image(imR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "rightCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "\n",
    "for keypoint in gt_keypoints['leftCrop']:\n",
    "    leftCrop[0] = min(leftCrop[0], keypoint['xCrop'])\n",
    "    leftCrop[1] = max(leftCrop[1], keypoint['xCrop'])\n",
    "    leftCrop[2] = min(leftCrop[2], keypoint['yCrop'])\n",
    "    leftCrop[3] = max(leftCrop[3], keypoint['yCrop'])\n",
    "    \n",
    "buffer = 100\n",
    "\n",
    "min_x, max_x, min_y, max_y = [max(leftCrop[0] - buffer, 0), min(leftCrop[1] + buffer, meta['pixelCountWidth']), max(leftCrop[2] - buffer, 0), min(leftCrop[3] + buffer, meta['pixelCountHeight'])]\n",
    "print(max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.imshow(imageL[min_y:(max_y + 1), min_x:(max_x+1),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['input_name']='input_placeholder:0'\n",
    "config['output_name']='stage_3/mid_conv7/BiasAdd:0'\n",
    "\n",
    "# for row in df.iterrows():\n",
    "\n",
    "\n",
    "# def get_keypoints(row):\n",
    "    # row = df.iloc[0]\n",
    "row = df.iloc[24]\n",
    "    \n",
    "imL = row['left_image_url']\n",
    "imR = row['right_image_url']\n",
    "lco = row['left_crop_metadata']\n",
    "rco = row['right_crop_metadata']\n",
    "meta = ast.literal_eval(row['camera_metadata'])\n",
    "gt_keypoints = ast.literal_eval(row['keypoints'])\n",
    "\n",
    "# imL = row['left_crop_url']\n",
    "# imR = row['right_crop_url']\n",
    "# lco = row['left_crop_metadata']\n",
    "# rco = row['right_crop_metadata']\n",
    "# meta = row['camera_metadata']\n",
    "\n",
    "imageL = url_to_image(imL)\n",
    "imageR = url_to_image(imR)\n",
    "\n",
    "leftCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "rightCrop = [meta['pixelCountWidth'], 0, meta['pixelCountHeight'], 0]\n",
    "\n",
    "for keypoint in gt_keypoints['leftCrop']:\n",
    "    leftCrop[0] = min(leftCrop[0], keypoint['xCrop'])\n",
    "    leftCrop[1] = max(leftCrop[1], keypoint['xCrop'])\n",
    "    leftCrop[2] = min(leftCrop[2], keypoint['yCrop'])\n",
    "    leftCrop[3] = max(leftCrop[3], keypoint['yCrop'])\n",
    "\n",
    "for keypoint in gt_keypoints['rightCrop']:\n",
    "    rightCrop[0] = min(rightCrop[0], keypoint['xCrop'])\n",
    "    rightCrop[1] = max(rightCrop[1], keypoint['xCrop'])\n",
    "    rightCrop[2] = min(rightCrop[2], keypoint['yCrop'])\n",
    "    rightCrop[3] = max(rightCrop[3], keypoint['yCrop'])\n",
    "    \n",
    "buffer = 100\n",
    "\n",
    "cropL = [max(leftCrop[0] - buffer, 0), min(leftCrop[1] + buffer, meta['pixelCountWidth']), max(leftCrop[2] - buffer, 0), min(leftCrop[3] + buffer, meta['pixelCountHeight'])]\n",
    "min_x, max_x, min_y, max_y = cropL\n",
    "\n",
    "newImageL = imageL[min_y:(max_y + 1), min_x:(max_x+1),:]\n",
    "\n",
    "cropR = [max(rightCrop[0] - buffer, 0), min(rightCrop[1] + buffer, meta['pixelCountWidth']), max(rightCrop[2] - buffer, 0), min(rightCrop[3] + buffer, meta['pixelCountHeight'])]\n",
    "min_x, max_x, min_y, max_y = cropR\n",
    "\n",
    "newImageR = imageR[min_y:(max_y + 1), min_x:(max_x+1),:]\n",
    "\n",
    "img1 = enhance(newImageL)\n",
    "img2 = enhance(newImageR)\n",
    "\n",
    "heightL, widthL, _ = img1.shape\n",
    "img_input = image_resize(img1, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "heightR, widthR, _ = img2.shape\n",
    "img_input = image_resize(img2, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapR = predict_heatmap.squeeze()\n",
    "\n",
    "# SIFT matching\n",
    "# MIN_MATCH_COUNT = 10\n",
    "# GOOD_PERC = 0.7\n",
    "# sift = cv2.KAZE_create()\n",
    "# FLANN_INDEX_KDTREE = 0\n",
    "# index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "# search_params = dict(checks = 50)\n",
    "\n",
    "# kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "# kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "# flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# matches = flann.knnMatch(des1,des2,k=2)\n",
    "# good = []\n",
    "# for m,n in matches:\n",
    "#     if m.distance < GOOD_PERC*n.distance:\n",
    "#         good.append(m)\n",
    "# if len(good)>=MIN_MATCH_COUNT:\n",
    "#     src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "#     dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "#     M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "#     matchesMask = mask.ravel().tolist()\n",
    "# else:\n",
    "#     print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "#     matchesMask = None\n",
    "\n",
    "# Hx=M\n",
    "# Hy=np.linalg.inv(M)\n",
    "\n",
    "# kpL = []\n",
    "# kpR = []    \n",
    "# kpL2R = []\n",
    "# kpR2L = [] \n",
    "# im1ps = []\n",
    "# im2ps = []\n",
    "# gtL2R = []\n",
    "# gtR2L = [] \n",
    "# gt1ps = []\n",
    "# gt2ps = []\n",
    "\n",
    "# for c in np.arange(0, len(KP), 1):\n",
    "#     hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "\n",
    "#     hm_maxL = list(np.where(hm == hm.max()))   \n",
    "#     kpL.append([int(hm_maxL[1][0]), int(hm_maxL[0][0])]) \n",
    "#     ptx=np.array([kpL[c][0],kpL[c][1],1])\n",
    "#     zx=np.dot(Hx,ptx)\n",
    "#     kpL2R.append([int(zx[0]/zx[2]), int(zx[1]/zx[2])]) \n",
    "\n",
    "#     hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "#     hm_maxR = np.where(hm == hm.max())\n",
    "#     kpR.append([int(hm_maxR[1][0]), int(hm_maxR[0][0])])\n",
    "#     pty=np.array([kpR[c][0],kpR[c][1],1])\n",
    "#     zy=np.dot(Hy,pty)\n",
    "#     kpR2L.append([int(zy[0]/zy[2]), int(zy[1]/zy[2])]) \n",
    "\n",
    "#     im1ps.append([int((kpL[c][0]+kpR2L[c][0])/2), int((kpL[c][1]+kpR2L[c][1])/2)]) \n",
    "#     im2ps.append([int((kpR[c][0]+kpL2R[c][0])/2), int((kpR[c][1]+kpL2R[c][1])/2)]) \n",
    "        \n",
    "#     return kpL, kpR, im1ps, im2ps, final_stage_heatmapL, final_stage_heatmapR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_keypoints_left = []\n",
    "refined_keypoints_right = []\n",
    "\n",
    "for c in np.arange(0, len(KP), 1):\n",
    "    hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "    ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "    # list(np.where(hm == hm.max()))\n",
    "    x = cropL[0] + np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "    y = cropL[2] + np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "    \n",
    "    refined_keypoints_left.append([np.rint(x), np.rint(y)])\n",
    "\n",
    "    hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "    ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "    # list(np.where(hm == hm.max()))\n",
    "    x = cropR[0] + np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "    y = cropR[2] + np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "    \n",
    "    refined_keypoints_right.append([np.rint(x), np.rint(y)])\n",
    "    \n",
    "    #ii\n",
    "# X = x / hmL.shape[0] * row.left_crop_metadata['width']\n",
    "# Y = y / hmL.shape[1] * row.left_crop_metadata['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.exp(hm[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_keypoints_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.where(hm == hm.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm[112, 1174], hm[111, 1154], hm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.arange(0, len(KP), 1):\n",
    "    hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "    print(KP[c], hm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpL, kpR, im1ps, im2ps, final_stage_heatmapL, final_stage_heatmapR = get_keypoints(df2.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, os\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "\n",
    "def display_crops(left_image_f, right_image_f, ann, overlay_keypoints=True, show_labels=False, secondary = False, custom_kps_left = {}, custom_kps_right = {}):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    left_ann, right_ann = ann['leftCrop'], ann['rightCrop']\n",
    "    \n",
    "    if secondary is False:\n",
    "        left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in left_ann}\n",
    "        right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in right_ann}\n",
    "    else:\n",
    "        left_keypoints = {item['keypointType']: [item['xCropNew'], item['yCropNew']] for item in left_ann}\n",
    "        right_keypoints = {item['keypointType']: [item['xCropNew'], item['yCropNew']] for item in right_ann}\n",
    "        \n",
    "    if overlay_keypoints:\n",
    "        for bp, kp in left_keypoints.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in custom_kps_left.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='cyan', s=10)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='cyan')\n",
    "        for bp, kp in right_keypoints.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in custom_kps_right.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='cyan', s=10)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='cyan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[24]\n",
    "\n",
    "left_crop_url, right_crop_url = row.left_image_url, row.right_image_url\n",
    "ann, cm = row.keypoints, row.camera_metadata\n",
    "\n",
    "# left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "# ann, cm = row.annotation, row.camera_metadata\n",
    "\n",
    "left_crop_f, _, _ = s3.download_from_url(left_crop_url)\n",
    "right_crop_f, _, _ = s3.download_from_url(right_crop_url)\n",
    "wkps1 = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oDists = {}\n",
    "nDists = {}\n",
    "oldAkpdScore = {}\n",
    "akpdScore = {}\n",
    "akpdScoreAvg = {}\n",
    "akpdScoreMax = {}\n",
    "\n",
    "for kp in KP:\n",
    "    oDists[kp] = []\n",
    "    nDists[kp] = []\n",
    "    oldAkpdScore[kp] = []\n",
    "    akpdScore[kp] = []\n",
    "    akpdScoreAvg[kp] = []\n",
    "    akpdScoreMax[kp] = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if cnt % 500  == 0:\n",
    "        print(cnt)\n",
    "        \n",
    "    oldScore = row['akpd_score']\n",
    "        \n",
    "    for i in np.arange(0, len(KP)):\n",
    "        keypoints = ast.literal_eval(row['keypoints'])\n",
    "        keypoints_new = row['keypoints_new_crop']#ast.literal_eval(row['keypoints_new_crop'])\n",
    "        \n",
    "        newKeypointLeft = [kp for kp in keypoints_new['leftCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        newKeypointRight = [kp for kp in keypoints_new['rightCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "    \n",
    "        gtKeypoint = [kp for kp in keypoints['leftCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        gtX = gtKeypoint['xCrop']\n",
    "        gtY = gtKeypoint['yCrop']\n",
    "\n",
    "        oX = newKeypointLeft['xCrop']\n",
    "        oY = newKeypointLeft['yCrop']\n",
    "\n",
    "        nX = newKeypointLeft['xCropNew']\n",
    "        nY = newKeypointLeft['yCropNew']\n",
    "        \n",
    "        score = newKeypointLeft['score']\n",
    "        scoreAvg = newKeypointLeft['scoreAvg']\n",
    "        scoreMax = newKeypointLeft['scoreMax']\n",
    "        \n",
    "        oDist = np.abs(gtX - oX) + np.abs(gtY - oY)\n",
    "        nDist = np.abs(gtX - nX) + np.abs(gtY - nY)\n",
    "        \n",
    "#         if oDist > 1000:\n",
    "#             print(KP[i], index, gtX, oX, gtY, oY)\n",
    "#             stop = True\n",
    "\n",
    "        oDists[KP[i]].append(oDist)\n",
    "        nDists[KP[i]].append(nDist)\n",
    "        oldAkpdScore[KP[i]].append(oldScore)\n",
    "        akpdScore[KP[i]].append(score)\n",
    "        akpdScoreAvg[KP[i]].append(scoreAvg)\n",
    "        akpdScoreMax[KP[i]].append(scoreMax)\n",
    "\n",
    "        gtKeypoint = [kp for kp in keypoints['rightCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        gtX = gtKeypoint['xCrop']\n",
    "        gtY = gtKeypoint['yCrop']\n",
    "\n",
    "        oX = newKeypointRight['xCrop']\n",
    "        oY = newKeypointRight['yCrop']\n",
    "\n",
    "        nX = newKeypointRight['xCropNew']\n",
    "        nY = newKeypointRight['yCropNew']\n",
    "        \n",
    "        score = newKeypointRight['score']\n",
    "        scoreAvg = newKeypointRight['scoreAvg']\n",
    "        scoreMax = newKeypointRight['scoreMax']\n",
    "\n",
    "        oDist = np.abs(gtX - oX) + np.abs(gtY - oY)\n",
    "        nDist = np.abs(gtX - nX) + np.abs(gtY - nY)\n",
    "\n",
    "        oDists[KP[i]].append(oDist)\n",
    "        nDists[KP[i]].append(nDist)\n",
    "        oldAkpdScore[KP[i]].append(oldScore)\n",
    "        akpdScore[KP[i]].append(score)\n",
    "        akpdScoreAvg[KP[i]].append(scoreAvg)\n",
    "        akpdScoreMax[KP[i]].append(scoreMax)\n",
    "\n",
    "    cnt = cnt + 1\n",
    "    \n",
    "#     if cnt >= 449:\n",
    "#         break\n",
    "#     np.mean(oDists), np.mean(nDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KP[0]\n",
    "\n",
    "plt.scatter(akpdScore[kp], nDists[kp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = oldAkpdScore[kp]\n",
    "X = sm.add_constant(X)\n",
    "y = nDists[kp]\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = akpdScoreMax[kp]\n",
    "X = sm.add_constant(X)\n",
    "y = oDists[kp]\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = akpdScoreMax[kp]\n",
    "X = sm.add_constant(X)\n",
    "y = nDists[kp]\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kp in KP:\n",
    "    oData = np.array(oDists[kp])\n",
    "    nData = np.array(nDists[kp])\n",
    "    diff = oData - nData\n",
    "    \n",
    "    print(kp, np.median(oData), np.median(nData), np.median(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[24]\n",
    "\n",
    "left_crop_url, right_crop_url = row.left_image_url, row.right_image_url\n",
    "\n",
    "left_crop_f, _, _ = s3.download_from_url(left_crop_url)\n",
    "right_crop_f, _, _ = s3.download_from_url(right_crop_url)\n",
    "ann, cm = ast.literal_eval(row.keypoints), ast.literal_eval(row.camera_metadata)\n",
    "\n",
    "display_crops(left_crop_f, right_crop_f, ann, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_crop_url, right_crop_url = row.left_image_url, row.right_image_url\n",
    "\n",
    "left_crop_f, _, _ = s3.download_from_url(left_crop_url)\n",
    "right_crop_f, _, _ = s3.download_from_url(right_crop_url)\n",
    "# ann, cm = ast.literal_eval(row.keypoints_new), ast.literal_eval(row.camera_metadata)\n",
    "ann, cm = row.keypoints_new_crop, ast.literal_eval(row.camera_metadata)\n",
    "\n",
    "display_crops(left_crop_f, right_crop_f, ann, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax = get_keypoints_with_crop(row)\n",
    "\n",
    "left_crop_metadata = ast.literal_eval(row.left_crop_metadata)\n",
    "right_crop_metadata = ast.literal_eval(row.right_crop_metadata)\n",
    "\n",
    "newKeypoints = {\n",
    "    'leftCrop': [],\n",
    "    'rightCrop': []\n",
    "}\n",
    "\n",
    "for i in np.arange(0, len(KP), 1):\n",
    "    newKeypoints['leftCrop'].append({\n",
    "        'xCrop': oL[i][0],\n",
    "        'yCrop': oL[i][1],\n",
    "        'xCropNew': kpL[i][0],\n",
    "        'yCropNew': kpL[i][1],\n",
    "        'xFrame': oL[i][0] + left_crop_metadata['x_coord'],\n",
    "        'yFrame': oL[i][1] + left_crop_metadata['y_coord'],\n",
    "        'xFrameNew': kpL[i][0] + left_crop_metadata['x_coord'],\n",
    "        'yFrameNew': kpL[i][1] + left_crop_metadata['y_coord'],\n",
    "        'score': kpLScore[i],\n",
    "        'scoreAvg': kpLScoreAvg[i],\n",
    "        'scoreMax': kpLScoreMax[i],\n",
    "        'keypointType': KP[i]\n",
    "    })\n",
    "\n",
    "    newKeypoints['rightCrop'].append({\n",
    "        'xCrop': oR[i][0],\n",
    "        'yCrop': oR[i][1],\n",
    "        'xCropNew': kpR[i][0],\n",
    "        'yCropNew': kpR[i][1],\n",
    "        'xFrame': oR[i][0] + right_crop_metadata['x_coord'],\n",
    "        'yFrame': oR[i][1] + right_crop_metadata['y_coord'],\n",
    "        'xFrameNew': kpR[i][0] + right_crop_metadata['x_coord'],\n",
    "        'yFrameNew': kpR[i][1] + right_crop_metadata['y_coord'],\n",
    "        'score': kpRScore[i],\n",
    "        'scoreAvg': kpRScoreAvg[i],\n",
    "        'scoreMax': kpRScoreMax[i],\n",
    "        'keypointType': KP[i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imL = row['left_image_url']\n",
    "imR = row['right_image_url']\n",
    "lco = row['left_crop_metadata']\n",
    "rco = row['right_crop_metadata']\n",
    "meta = row['camera_metadata']\n",
    "\n",
    "imageL = url_to_image(imL)\n",
    "imageR = url_to_image(imR)\n",
    "\n",
    "img1 = enhance(imageL)\n",
    "img2 = enhance(imageR)\n",
    "\n",
    "heightL, widthL, _ = img1.shape\n",
    "img_input = image_resize(img1, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "heightR, widthR, _ = img2.shape\n",
    "img_input = image_resize(img2, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapR = predict_heatmap.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KP = [\"TAIL_NOTCH\", \"ADIPOSE_FIN\", \"UPPER_LIP\", \"ANAL_FIN\", \"PELVIC_FIN\", \"EYE\", \"PECTORAL_FIN\", \"DORSAL_FIN\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(KP), 2, figsize=(2 * 5, len(KP) * 5))\n",
    "\n",
    "for i in np.arange(0, len(KP), 1):\n",
    "    axes[i, 0].imshow(final_stage_heatmapL[:,:,i], cmap='hot', interpolation='nearest')\n",
    "    axes[i, 0].set_title('%s %s' % (KP[i], 'Left'))\n",
    "    axes[i, 1].imshow(final_stage_heatmapR[:,:,i], cmap='hot', interpolation='nearest')\n",
    "    axes[i, 1].set_title('%s %s' % (KP[i], 'Right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
