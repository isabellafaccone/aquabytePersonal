{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryCache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id = 116\n",
    "df_start_date = '2020-10-26'\n",
    "df_end_date = '2020-10-30'\n",
    "\n",
    "if pen_id in queryCache and df_start_date in queryCache[pen_id] and df_end_date in queryCache[pen_id][df_start_date]:\n",
    "    df2 = queryCache[pen_id][df_start_date][df_end_date]\n",
    "else:\n",
    "    df2 = extract_biomass_data(pen_id, df_start_date, df_end_date, 0.95)\n",
    "    # df = extract_biomass_data(pen_id, '2020-08-24', '2020-09-03', 0.99)\n",
    "\n",
    "    df2.date = pd.to_datetime(df2.date)\n",
    "#     df['week'] = df.date.apply(lambda x: x.weekofyear)\n",
    "\n",
    "    depths = []\n",
    "    new_lengths = []\n",
    "    for idx, row in df2.iterrows():\n",
    "        ann, cm = row.annotation, row.camera_metadata\n",
    "        wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "        depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "        vector = wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']\n",
    "        depths.append(depth)\n",
    "        new_lengths.append(np.linalg.norm(vector))\n",
    "    df2['depth'] = depths\n",
    "    df2['new_lengths'] = new_lengths\n",
    "    \n",
    "    queryCache[pen_id] = { df_start_date: { df_end_date: df2 } }\n",
    "df2 = df2[((df2.hour >= 7) & (df2.hour <= 15))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id = 165\n",
    "df_start_date = '2020-10-25'\n",
    "df_end_date = '2020-10-28'\n",
    "\n",
    "if pen_id in queryCache and df_start_date in queryCache[pen_id] and df_end_date in queryCache[pen_id][df_start_date]:\n",
    "    df = queryCache[pen_id][df_start_date][df_end_date]\n",
    "else:\n",
    "    df = extract_biomass_data(pen_id, df_start_date, df_end_date, 0.95)\n",
    "    # df = extract_biomass_data(pen_id, '2020-08-24', '2020-09-03', 0.99)\n",
    "\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "#     df['week'] = df.date.apply(lambda x: x.weekofyear)\n",
    "\n",
    "    depths = []\n",
    "    new_lengths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ann, cm = row.annotation, row.camera_metadata\n",
    "        wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "        depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "        vector = wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']\n",
    "        depths.append(depth)\n",
    "        new_lengths.append(np.linalg.norm(vector))\n",
    "    df['depth'] = depths\n",
    "    df['new_lengths'] = new_lengths\n",
    "    \n",
    "    queryCache[pen_id] = { df_start_date: { df_end_date: df } }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://aquabyte-crops.s3.eu-west-1.amazonaws.com/environment=production/site-id=61/pen-id=116/date=2020-10-14/hour=17/at=2020-10-14T17:50:23.086425000Z/left_frame_crop_516_1433_4096_2762.jpg'\n",
    "\n",
    "\n",
    "# query = \"SELECT * FROM prod.biomass_computations where left_crop_url='%s'\" % (url,)\n",
    "\n",
    "# df = rds_access_utils.extract_from_database(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depths = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     ann, cm = row.annotation, row.camera_metadata\n",
    "#     wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "#     depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "#     depths.append(depth)\n",
    "# df['depth'] = depths\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = df.loc[0]\n",
    "# ann1, cm1 = row.annotation, row.camera_metadata\n",
    "# wkps1 = pixel2world(ann1['leftCrop'], ann1['rightCrop'], cm1)\n",
    "# vector = wkps1['UPPER_LIP'] - wkps1['TAIL_NOTCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector\n",
    "\n",
    "# np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[((df.hour >= 7) & (df.hour <= 14))]\n",
    "# df = df[(df.depth < 1.8)]\n",
    "\n",
    "# df_7000 = df[df['estimated_weight_g'] > 7000]\n",
    "# df_5000 = df[df['estimated_weight_g'] < 5000]\n",
    "# print(np.mean(df_7000['depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "depths = np.arange(.5, 2.5, .1)\n",
    "weights = np.arange(1000, 10000, 1000)\n",
    "\n",
    "outputs = []\n",
    "outputs2 = []\n",
    "\n",
    "for index, depth in enumerate(depths):\n",
    "    mask = (df.depth > depth) & (df.depth < (depth + .1))\n",
    "#     mask = (df.estimated_weight_g > weight) & (df.estimated_weight_g < (weight + 1000))\n",
    "    outputs.append(np.mean(df[mask].estimated_weight_g))\n",
    "    outputs2.append(np.sum(mask))\n",
    "#     plt.hist(df[mask].depth, bins = 20)\n",
    "#     plt.title(weight)\n",
    "#     plt.figure(figsize=(5, 2))\n",
    "plt.bar(depths, outputs, width = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum((df.estimated_weight_g > 8000)) / len(df) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5, 2))\n",
    "# depths = np.arange(.5, 2.5, .1)\n",
    "# weights = np.arange(1000, 10000, 1000)\n",
    "\n",
    "# outputs = []\n",
    "# outputs2 = []\n",
    "\n",
    "# for index, weight in enumerate(weights):#for index, depth in enumerate(depths):\n",
    "#     #mask = (df.depth > depth) & (df.depth < (depth + .1))\n",
    "#     mask = (df.estimated_weight_g > weight) & (df.estimated_weight_g < (weight + 1000))\n",
    "# #     outputs.append(np.mean(df[mask].depth))\n",
    "# #     outputs2.append(np.sum(mask))\n",
    "#     plt.hist(df[mask].depth, bins = 20)\n",
    "#     plt.title(weight)\n",
    "#     plt.figure(figsize=(5, 2))\n",
    "# # plt.bar(depths, outputs2, width = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "count, bins, _ = plt.hist(df.estimated_weight_g * 1.1, bins = 30)\n",
    "\n",
    "bins[np.argmax(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(0, 2, 0.01)\n",
    "\n",
    "plt.plot(y, [np.mean(df.estimated_weight_g[df.depth > x]) for x in y])\n",
    "\n",
    "# np.max([np.mean(df.estimated_weight_g[df.depth > x]) for x in y])\n",
    "avg = np.mean(df.estimated_weight_g[df.depth > np.percentile(df.depth, 75)]) \n",
    "print(avg, avg * 0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 100, 1)\n",
    "output = []\n",
    "\n",
    "for y in x:\n",
    "    d1 = df['estimated_weight_g']\n",
    "    d2 = df.estimated_weight_g[df.depth > np.percentile(df.depth, y)] * (1 - loss_factor)\n",
    "    d4 = np.concatenate([d1[d1 < np.median(d2)], np.median(d2) + (np.median(d2) - d1[d1 < np.median(d2)])])\n",
    "    # d2 = dist2['estimated_weight_g'] * (1 - loss_factor)\n",
    "    # new_density_adj = new_density / np.sum(new_density)\n",
    "    output.append(np.mean(d4))\n",
    "\n",
    "plt.plot(x, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "d1 = df['estimated_weight_g'] * 0.98\n",
    "d2 = df.estimated_weight_g[df.depth > np.percentile(df.depth, 75)] \n",
    "d4 = np.concatenate([d1[d1 < np.median(d2)], np.median(d2) + (np.median(d2) - d1[d1 < np.median(d2)])])\n",
    "\n",
    "e1 = df2['estimated_weight_g']\n",
    "e2 = df2.estimated_weight_g[df2.depth > np.percentile(df2.depth, 75)]\n",
    "e4 = np.concatenate([e1[e1 < np.median(e2)], np.median(e2) + (np.median(e2) - e1[e1 < np.median(e2)])])\n",
    "\n",
    "counts, bins, _ = plt.hist(d4, alpha = 0.5, color = 'red', density = True, bins = 50, label = 'Leiv')\n",
    "plt.hist(e4, alpha = 0.5, color = 'blue', density = True, bins = bins, label = 'Slapoya')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "counts, bins, _ = plt.hist(d1, alpha = 0.5, color = 'red', density = True, bins = 50, label = 'Leiv')\n",
    "plt.hist(e1, alpha = 0.5, color = 'blue', density = True, bins = bins, label = 'Slapoya')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(d4), np.mean(e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "x_buckets = np.array(buckets[:-1])\n",
    "gt_pcts = [0, 0, .0055, .0410, .1686, .3253, .2729, .1323, .0411, .0133]\n",
    "\n",
    "loss_factor = 0.16\n",
    "\n",
    "d1 = df['estimated_weight_g'] * (1 - loss_factor)\n",
    "d2 = df.estimated_weight_g[df.depth > np.percentile(df.depth, 75)] * (1 - loss_factor)\n",
    "d3 = np.concatenate([d1[d1 < np.mean(d2)], np.mean(d2) + (np.mean(d2) - d1[d1 < np.mean(d2)])])\n",
    "d4 = np.concatenate([d1[d1 < np.median(d2)], np.median(d2) + (np.median(d2) - d1[d1 < np.median(d2)])])\n",
    "# d2 = dist2['estimated_weight_g'] * (1 - loss_factor)\n",
    "# new_density_adj = new_density / np.sum(new_density)\n",
    "\n",
    "print(np.mean(d1), np.mean(d2), np.mean(d3), np.mean(d4))\n",
    "\n",
    "# new_pcts = []\n",
    "pcts1 = []\n",
    "pcts2 = []\n",
    "pcts3 = []\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask1 = (d4 > buckets[i]) & (d4 <= buckets[i + 1])\n",
    "    mask2 = (d2 > buckets[i]) & (d2 <= buckets[i + 1])\n",
    "    mask3 = (d3 > buckets[i]) & (d3 <= buckets[i + 1])\n",
    "#     mask_new = (new_bins_adj > buckets[i]) & (new_bins_adj <= buckets[i + 1])\n",
    "    gt_pct = gt_pcts[i]\n",
    "#     dist = dist1['estimated_weight_g'][mask1]\n",
    "#     gt = gt_weights[mask2]\n",
    "\n",
    "#     new_pcts.append(np.sum(new_density_adj[mask_new]))\n",
    "    pct1 = np.sum(mask1) / len(mask1)\n",
    "    pcts1.append(pct1)\n",
    "    pct2 = np.sum(mask2) / len(mask2)\n",
    "    pcts2.append(pct2)\n",
    "    pct3 = np.sum(mask3) / len(mask3)\n",
    "    pcts3.append(pct3)\n",
    "#     print('%i: %0.3f, %0.3f vs %0.3f' % (buckets[i], np.sum(new_density_adj[mask1]) - gt_pct, np.sum(new_density_adj[mask1]), gt_pct))\n",
    "\n",
    "pcts1 = np.array(pcts1)\n",
    "# pcts2 = np.array(pcts2)\n",
    "\n",
    "# gt_avg = 4944.34\n",
    "\n",
    "# result = np.sum(new_bins_adj * new_density_adj) \n",
    "# (result - gt_avg) / gt_avg\n",
    "# print(result, gt_avg)\n",
    "# print((result - gt_avg) / gt_avg)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "# plt.bar(x_buckets - 300, new_pcts, color = 'orange', width = 150, label = 'Dedup diff')\n",
    "plt.bar(x_buckets - 150, pcts1, color = 'red', width = 150, label = 'Original')\n",
    "plt.bar(x_buckets + 300, pcts2, color = 'blue', width = 150, label = 'Dedup')\n",
    "plt.bar(x_buckets + 150, pcts3, color = 'purple', width = 150, label = 'Dedup diff')\n",
    "plt.bar(x_buckets, gt_pcts, color = 'green', width = 150, label = 'Ground truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "probplot(df.estimated_weight_g, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 100, 1)\n",
    "plt.plot(x, np.percentile(df.estimated_weight_g, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered = df.estimated_weight_g * 0.84\n",
    "a = ordered.sort_values()\n",
    "b = ordered.sort_values(ascending = False)\n",
    "plt.plot(b, np.median(a) - a)\n",
    "\n",
    "\n",
    "print(np.median(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.estimated_weight_g  < 2500)\n",
    "df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential_duplicates = []\n",
    "\n",
    "# def f():\n",
    "#     count = 0\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "#         for index2, row2 in df[df.index > index].iterrows():\n",
    "#             diff0 = row.estimated_k_factor - row2.estimated_k_factor #(index - index2).total_seconds()\n",
    "#             diff1 = row.estimated_weight_g - row2.estimated_weight_g\n",
    "#             diff2 = row.estimated_length_mm - row2.estimated_length_mm\n",
    "# #             print(diff0, diff1, diff2)\n",
    "#             if((np.abs(diff0) < 0.05) & (np.abs(diff1) < 20) & (np.abs(diff2) < 10)):\n",
    "#                 print(index, index2)\n",
    "#             count = count + 1\n",
    "#             if count % 10000 == 0:\n",
    "#                 print(count)\n",
    "            \n",
    "# f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# plt.hist(df.estimated_weight_g, bins = 50)\n",
    "plt.hist(df.new_lengths, bins = 50)\n",
    "# plt.hist(df.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(df.new_lengths, df.estimated_weight_g)\n",
    "# plt.scatter(df.estimated_length_mm, df.estimated_weight_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.new_lengths > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df.estimated_weight_g > 8000) / len(df)\n",
    "# df2 = df[df['date'] == '2020-10-27']\n",
    "# df95 = df2[df2['akpd_score'] > 0.95]\n",
    "# df99 = df2[df2['akpd_score'] > 0.99]\n",
    "# df1_5 = df2[df2['depth'] > 1.5]\n",
    "\n",
    "# print(len(df1_5), len(df2))\n",
    "# #plt.hist(df2['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "buckets = np.linspace(4000, 8000, 100)\n",
    "\n",
    "results = []\n",
    "\n",
    "def adj_weight(x):\n",
    "    return x ** (2/3)\n",
    "\n",
    "for bucket in buckets:\n",
    "    min_bucket = bucket - 1000\n",
    "    max_bucket = bucket + 1000\n",
    "#     mask = (df.estimated_weight_g > min_bucket) & (df.estimated_weight_g < max_bucket)\n",
    "    mask = (adj_weight(df.estimated_weight_g) > adj_weight(min_bucket)) & (adj_weight(df.estimated_weight_g) < adj_weight(max_bucket))\n",
    "    res = stats.weibull_min.fit(df[mask].depth, floc = 0.7)\n",
    "    results.append(res)\n",
    "    \n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = results[:,0]\n",
    "Y2 = results[:,2]\n",
    "X = buckets\n",
    "X = sm.add_constant(X)\n",
    "model0 = sm.OLS(Y0,X)\n",
    "model2 = sm.OLS(Y2,X)\n",
    "m0 = model0.fit()\n",
    "m2 = model2.fit()\n",
    "# OLSresults = model.fit()\n",
    "# OLSresults.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 5000)\n",
    "\n",
    "def get_prob(depth, weight):\n",
    "    v0 = m0.predict([1, weight])\n",
    "    v1 = 0.7\n",
    "    v2 = m2.predict([1, weight])\n",
    "\n",
    "    prob = stats.weibull_min.pdf(depth, v0, v1, v2) / max(stats.weibull_min.pdf(x, v0, v1, v2))\n",
    "#     prob = stats.weibull_min.pdf(depth, v0, v1, v2)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "weights = df.estimated_weight_g\n",
    "weights2 = df.estimated_weight_g[(df.depth < 1.8)]\n",
    "weights_weight = []\n",
    "weights_prob = []\n",
    "weights_weight2 = []\n",
    "weights_prob2 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    prob = get_prob(row['depth'], row['estimated_weight_g'])\n",
    "    \n",
    "    if prob < 0.01:\n",
    "        print(row['depth'], row['estimated_weight_g'])\n",
    "    else:\n",
    "        if row['depth'] < 1.8:\n",
    "            weights_weight2.append(row['estimated_weight_g'])\n",
    "            weights_prob2.append(prob[0])\n",
    "        weights_weight.append(row['estimated_weight_g'])\n",
    "        weights_prob.append(prob[0])\n",
    "    \n",
    "weights_weight = np.array(weights_weight)\n",
    "weights_prob = np.array(weights_prob)\n",
    "weights_weight2 = np.array(weights_weight2)\n",
    "weights_prob2 = np.array(weights_prob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.mean(weights)\n",
    "w2 = np.sum(weights_weight / weights_prob) / np.sum(1 / weights_prob)\n",
    "w3 = np.mean(weights2)\n",
    "w4 = np.sum(weights_weight2 / weights_prob2) / np.sum(1 / weights_prob2)\n",
    "\n",
    "mask1 = (weights > 8000)\n",
    "mask2 = (weights_weight > 8000)\n",
    "\n",
    "print(np.mean(weights[mask1]))\n",
    "print(np.sum(weights_weight[mask2] / weights_prob[mask2]) / np.sum(1 / weights_prob[mask2]))\n",
    "\n",
    "print(w1, w2, w3, w4)\n",
    "print((w1 - w2) / w1)\n",
    "print((w3 - w4) / w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(adj_weight(buckets), results[:,0])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(adj_weight(buckets), results[:,1])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(adj_weight(buckets), results[:,2])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(buckets, results[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (df.estimated_weight_g > 4000) & (df.estimated_weight_g < 6000)\n",
    "mask2 = (df.estimated_weight_g > 6000) & (df.estimated_weight_g < 8000)\n",
    "\n",
    "d1 = df[mask1]\n",
    "d2 = df[mask2]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = np.linspace(0, 3, 5000)\n",
    "plt.plot(x, stats.weibull_min.pdf(x, *stats.weibull_min.fit(d1['depth'])))\n",
    "plt.plot(x, stats.weibull_min.pdf(x, *stats.weibull_min.fit(d2['depth'])))\n",
    "# plt.plot(x, stats.weibull_min.pdf(x, *stats.weibull_min.fit(df_5000['depth'])))\n",
    "# plt.plot(x, stats.weibull_min.pdf(x, *stats.weibull_min.fit(df_7000['depth'])))\n",
    "# plt.hist(df['depth'], bins = 30, alpha = 0.5, density = True)\n",
    "# plt.hist(df_5000['depth'], bins = 30, alpha = 0.5, density = True)\n",
    "# plt.hist(df_7000['depth'], bins = 30, alpha = 0.5, density = True)\n",
    "plt.hist(d1['depth'], bins = 30, alpha = 0.5, color = 'green', density = True)\n",
    "plt.hist(d2['depth'], bins = 30, alpha = 0.5, color = 'red', density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.weibull_min.fit(d1['depth'], floc=0.68))\n",
    "print(stats.weibull_min.fit(d1['depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df.estimated_weight_g > 7000]['depth'], bins = 30, alpha = 0.5, density = True)\n",
    "plt.hist(df[df.estimated_weight_g < 5000]['depth'], bins = 30, alpha = 0.5, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = stats.probplot(df.depth, plot=plt)\n",
    "#res = stats.probplot(df.depth, dist=stats.chi2, sparams=(50, ), plot=plt)\n",
    "# res = stats.probplot(df.depth, dist=stats.weibull_min, sparams=(2, 0, 1.49), plot=plt)\n",
    "res = stats.probplot(df.depth, dist=stats.weibull_min, sparams=stats.weibull_min.fit(df['depth']), plot=plt)\n",
    "# res = stats.probplot(df_5000.depth, dist=stats.weibull_min, sparams=(2, ), plot=plt)\n",
    "# res = stats.probplot(df_7000.depth, dist=stats.weibull_min, sparams=(2, ), plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "density, bins, _ = plt.hist(df2.estimated_weight_g, bins = 30, alpha = 0.5, density = True, color = 'blue')\n",
    "plt.hist(df1_5.estimated_weight_g, bins = bins, alpha = 0.5, density = True, color = 'red')\n",
    "#plt.hist(df99.estimated_weight_g, bins = bins, alpha = 0.5, density = True, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(df2.estimated_weight_g))\n",
    "\n",
    "buckets = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for i in range(len(buckets) - 1):\n",
    "    mask1 = (df2['estimated_weight_g'] > buckets[i]) & (df2['estimated_weight_g'] <= buckets[i + 1])\n",
    "    \n",
    "    print('%i: %0.2f' % (buckets[i], sum(mask1) / len(mask1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.probplot(df2.estimated_weight_g, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[0].annotation['leftCrop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_crops(left_image_f, right_image_f, ann, overlay_keypoints=True, show_labels=False):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    left_ann, right_ann = ann['leftCrop'], ann['rightCrop']\n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in left_ann}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in right_ann}\n",
    "    if overlay_keypoints:\n",
    "        for bp, kp in left_keypoints.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in right_keypoints.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "row = df1.iloc[1]\n",
    "print(row.new_lengths)\n",
    "print(row.estimated_weight_g)\n",
    "left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "left_crop_f, _, _ = s3.download_from_url(left_crop_url)\n",
    "right_crop_f, _, _ = s3.download_from_url(right_crop_url)\n",
    "ann = row.annotation\n",
    "display_crops(left_crop_f, right_crop_f, ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = df1.iloc[0].left_crop_url\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=url)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for point in df1.iloc[0].annotation['leftCrop']:\n",
    "    x.append(point['xCrop'])\n",
    "    y.append(point['yCrop'])\n",
    "    \n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = df1.iloc[0].right_crop_url\n",
    "\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
