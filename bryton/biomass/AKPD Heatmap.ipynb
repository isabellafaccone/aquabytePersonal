{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from research_lib.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from weight_estimation.dataset import prepare_gtsf_data, compute_akpd_score\n",
    "from weight_estimation.train import train, augment, normalize, get_data_split, train_model\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.load_csv('/root/data/alok/biomass_estimation/playground/gtsf_keypoints_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data')\n",
    "akpd_scorer_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/keypoint-detection-scorer/akpd_scorer_model_TF.h5'\n",
    "akpd_scorer_f, _, _ = s3.download_from_url(akpd_scorer_url)\n",
    "df1 = prepare_gtsf_data('2019-03-01', '2019-09-20', akpd_scorer_f, 0.5, 1.0)\n",
    "\n",
    "df2 = prepare_gtsf_data('2020-06-01', '2020-08-20', akpd_scorer_f, 0.5, 1.0)\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "gt_metadata = {'pen_id': 144,\n",
    " 'gutted_average_weight': 8000,\n",
    " 'gutted_weight_distribution': None,\n",
    " 'expected_loss_factor': 0.16,\n",
    " 'last_feeding_date': '2021-01-11',\n",
    " 'harvest_date': '2021-01-15',\n",
    " 'slaughter_date': '2021-01-15'}\n",
    "\n",
    "df2 = extract_biomass_data(gt_metadata['pen_id'], '2021-01-01', '2021-01-12', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1000].keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from functools import singledispatch\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def to_serializable(val):\n",
    "    \"\"\"Used by default.\"\"\"\n",
    "    return str(val)\n",
    "\n",
    "\n",
    "@to_serializable.register(np.float32)\n",
    "def ts_float32(val):\n",
    "    \"\"\"Used if *val* is an instance of numpy.float32.\"\"\"\n",
    "    return np.float64(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(keypoints_new, open('/root/data/alok/biomass_estimation/playground/keypoints_new.json', 'w'), default=to_serializable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keypoints_new'] = keypoints_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/root/data/alok/biomass_estimation/playground/gtsf_keypoints_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_new = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if count % 100 == 0:\n",
    "        print(count, 'out of', len(df))\n",
    "\n",
    "    count = count + 1\n",
    "    \n",
    "    if count < 0:\n",
    "        continue\n",
    "        \n",
    "    oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax = get_keypoints(row)\n",
    "    \n",
    "    newKeypoints = {\n",
    "        'leftCrop': [],\n",
    "        'rightCrop': []\n",
    "    }\n",
    "    \n",
    "    for i in np.arange(0, len(KP), 1):\n",
    "        newKeypoints['leftCrop'].append({\n",
    "            'xCrop': oL[i][0],\n",
    "            'yCrop': oL[i][1],\n",
    "            'xCropNew': kpL[i][0],\n",
    "            'yCropNew': kpL[i][1],\n",
    "            'xFrame': oL[i][0] + row.left_crop_metadata['x_coord'],\n",
    "            'yFrame': oL[i][1] + row.left_crop_metadata['y_coord'],\n",
    "            'xFrameNew': kpL[i][0] + row.left_crop_metadata['x_coord'],\n",
    "            'yFrameNew': kpL[i][1] + row.left_crop_metadata['y_coord'],\n",
    "            'score': kpLScore[i],\n",
    "            'scoreAvg': kpLScoreAvg[i],\n",
    "            'scoreMax': kpLScoreMax[i],\n",
    "            'keypointType': KP[i]\n",
    "        })\n",
    "        \n",
    "        newKeypoints['rightCrop'].append({\n",
    "            'xCrop': oR[i][0],\n",
    "            'yCrop': oR[i][1],\n",
    "            'xCropNew': kpR[i][0],\n",
    "            'yCropNew': kpR[i][1],\n",
    "            'xFrame': oR[i][0] + row.right_crop_metadata['x_coord'],\n",
    "            'yFrame': oR[i][1] + row.right_crop_metadata['y_coord'],\n",
    "            'xFrameNew': kpR[i][0] + row.right_crop_metadata['x_coord'],\n",
    "            'yFrameNew': kpR[i][1] + row.right_crop_metadata['y_coord'],\n",
    "            'score': kpRScore[i],\n",
    "            'scoreAvg': kpRScoreAvg[i],\n",
    "            'scoreMax': kpRScoreMax[i],\n",
    "            'keypointType': KP[i]\n",
    "        })\n",
    "        \n",
    "    keypoints_new.append(newKeypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_bak_path = '/root/data/bati/model/config.json'\n",
    "# config_bak = json.load(open(config_bak_path))\n",
    "# config_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/root/data/bati/model/production_config.json'\n",
    "# config_path = '/root/data/bati/model/config_4_stage.json'\n",
    "\n",
    "checkpoint_path = '/root/data/bati/model/model_499.pb'\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "class FLAGS(object):\n",
    "    input_size = tuple(config[\"input_size\"])\n",
    "    stages = config[\"cpm_stages\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    joints = config[\"num_of_joints\"]\n",
    "    model_path = checkpoint_path\n",
    "    cmap_radius = config[\"center_radius\"]\n",
    "    keypoints_order = config[\"keypoints_order\"]\n",
    "    normalize = config[\"normalize\"]\n",
    "    heatmap_size = 512#config[\"heatmap_size\"]\n",
    "    joint_gaussian_variance = config[\"joint_gaussian_variance\"]\n",
    "    crop = config[\"crop\"]\n",
    "    augmentation = None\n",
    "    \n",
    "def load_pb(path_to_pb):\n",
    "    with tf.io.gfile.GFile(path_to_pb, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, input_map=None,\n",
    "                                return_elements=None,\n",
    "                                name=\"\",\n",
    "                                op_dict=None,\n",
    "                                producer_op_list=None)\n",
    "        graph_nodes=[n for n in graph_def.node]\n",
    "#         for t in graph_nodes:\n",
    "#             print(t.name)\n",
    "        return graph\n",
    "\n",
    "mod = load_pb(checkpoint_path)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "sess.graph.as_default()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "tf_device = '/gpu:0'\n",
    "with tf.device(tf_device):\n",
    "    model = mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "\n",
    "def image_resize(image, FLAGS):\n",
    "    height, width, _ = image.shape\n",
    "    ratio_width = width / FLAGS.input_size[0]\n",
    "    ratio_height = height / FLAGS.input_size[1]\n",
    "    image = cv2.resize(image, FLAGS.input_size)\n",
    "    image  = image / 255.0 - 0.5\n",
    "    image = image[np.newaxis, ...]\n",
    "    return image\n",
    "\n",
    "def enhance(image, clip_limit=5):\n",
    "    # convert image to LAB color model\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # split the image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(image_lab)\n",
    "\n",
    "    # apply CLAHE to lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
    "    merged_channels = cv2.merge((cl, a_channel, b_channel))\n",
    "\n",
    "    # convert image from LAB color model back to RGB color model\n",
    "    final_image = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
    "    return final_image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(row):\n",
    "    imL = row['left_image_url']\n",
    "    imR = row['right_image_url']\n",
    "    lco = row['left_crop_metadata']\n",
    "    rco = row['right_crop_metadata']\n",
    "    meta = row['camera_metadata']\n",
    "\n",
    "    imageL = url_to_image(imL)\n",
    "    imageR = url_to_image(imR)\n",
    "\n",
    "    img1 = enhance(imageL)\n",
    "    img2 = enhance(imageR)\n",
    "\n",
    "    heightL, widthL, _ = img1.shape\n",
    "    img_input = image_resize(img1, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "    heightR, widthR, _ = img2.shape\n",
    "    img_input = image_resize(img2, FLAGS)\n",
    "    with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "        predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "    final_stage_heatmapR = predict_heatmap.squeeze()\n",
    "\n",
    "    oL = [] # original left key-points using max method\n",
    "    oR = [] # original right key-points using max method\n",
    "    kpL = [] # new left key-points using avg method\n",
    "    kpR = [] # new right key-points using avg method\n",
    "    kpLScore = []\n",
    "    kpRScore = []\n",
    "    kpLScoreAvg = []\n",
    "    kpRScoreAvg = []\n",
    "    kpLScoreMax = []\n",
    "    kpRScoreMax = []\n",
    "\n",
    "    for c in np.arange(0, len(KP), 1):\n",
    "        hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "        hm_maxL = list(np.where(hm == hm.max()))   \n",
    "        \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape) # 2D locations corresponding to highest 10000 heatmap values\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oL.append([int(hm_maxL[1][0]), int(hm_maxL[0][0])]) \n",
    "        kpL.append([int(np.rint(x)), int(np.rint(y))])\n",
    "        kpLScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpLScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpLScoreMax.append(hm.max())\n",
    "\n",
    "        hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "        hm_maxR = np.where(hm == hm.max())\n",
    "    \n",
    "        ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "        x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "        y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "\n",
    "        oR.append([int(hm_maxR[1][0]), int(hm_maxR[0][0])])\n",
    "        kpR.append([int(np.rint(x)), int(np.rint(y))])\n",
    "        kpRScore.append(hm[int(np.rint(y)), int(np.rint(x))])\n",
    "        kpRScoreAvg.append(np.mean(hm[ii]))\n",
    "        kpRScoreMax.append(hm.max())\n",
    "\n",
    "    return oL, oR, kpL, kpR, kpLScore, kpRScore, kpLScoreAvg, kpRScoreAvg, kpLScoreMax, kpRScoreMax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(int(np.rint(y)))\n",
    "# hm[np.rint(y), np.rint(x)]\n",
    "get_keypoints(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['input_name']='input_placeholder:0'\n",
    "config['output_name']='stage_3/mid_conv7/BiasAdd:0'\n",
    "\n",
    "# for row in df.iterrows():\n",
    "\n",
    "\n",
    "# def get_keypoints(row):\n",
    "    # row = df.iloc[0]\n",
    "imL = row['left_image_url']\n",
    "imR = row['right_image_url']\n",
    "lco = row['left_crop_metadata']\n",
    "rco = row['right_crop_metadata']\n",
    "meta = row['camera_metadata']\n",
    "\n",
    "# imL = row['left_crop_url']\n",
    "# imR = row['right_crop_url']\n",
    "# lco = row['left_crop_metadata']\n",
    "# rco = row['right_crop_metadata']\n",
    "# meta = row['camera_metadata']\n",
    "\n",
    "imageL = url_to_image(imL)\n",
    "imageR = url_to_image(imR)\n",
    "\n",
    "img1 = enhance(imageL)\n",
    "img2 = enhance(imageR)\n",
    "\n",
    "heightL, widthL, _ = img1.shape\n",
    "img_input = image_resize(img1, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapL = predict_heatmap.squeeze()\n",
    "\n",
    "heightR, widthR, _ = img2.shape\n",
    "img_input = image_resize(img2, FLAGS)\n",
    "with tf.compat.v1.Session(graph=model) as sess, tf.device(tf_device):\n",
    "    predict_heatmap = sess.run(config['output_name'], feed_dict = {config['input_name']: img_input})\n",
    "final_stage_heatmapR = predict_heatmap.squeeze()\n",
    "\n",
    "# SIFT matching\n",
    "MIN_MATCH_COUNT = 10\n",
    "GOOD_PERC = 0.7\n",
    "sift = cv2.KAZE_create()\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < GOOD_PERC*n.distance:\n",
    "        good.append(m)\n",
    "if len(good)>=MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "else:\n",
    "    print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "    matchesMask = None\n",
    "\n",
    "Hx=M\n",
    "Hy=np.linalg.inv(M)\n",
    "\n",
    "kpL = []\n",
    "kpR = []    \n",
    "kpL2R = []\n",
    "kpR2L = [] \n",
    "im1ps = []\n",
    "im2ps = []\n",
    "gtL2R = []\n",
    "gtR2L = [] \n",
    "gt1ps = []\n",
    "gt2ps = []\n",
    "\n",
    "for c in np.arange(0, len(KP), 1):\n",
    "    hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "\n",
    "    hm_maxL = list(np.where(hm == hm.max()))   \n",
    "    kpL.append([int(hm_maxL[1][0]), int(hm_maxL[0][0])]) \n",
    "    ptx=np.array([kpL[c][0],kpL[c][1],1])\n",
    "    zx=np.dot(Hx,ptx)\n",
    "    kpL2R.append([int(zx[0]/zx[2]), int(zx[1]/zx[2])]) \n",
    "\n",
    "    hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "    hm_maxR = np.where(hm == hm.max())\n",
    "    kpR.append([int(hm_maxR[1][0]), int(hm_maxR[0][0])])\n",
    "    pty=np.array([kpR[c][0],kpR[c][1],1])\n",
    "    zy=np.dot(Hy,pty)\n",
    "    kpR2L.append([int(zy[0]/zy[2]), int(zy[1]/zy[2])]) \n",
    "\n",
    "    im1ps.append([int((kpL[c][0]+kpR2L[c][0])/2), int((kpL[c][1]+kpR2L[c][1])/2)]) \n",
    "    im2ps.append([int((kpR[c][0]+kpL2R[c][0])/2), int((kpR[c][1]+kpL2R[c][1])/2)]) \n",
    "        \n",
    "#     return kpL, kpR, im1ps, im2ps, final_stage_heatmapL, final_stage_heatmapR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_keypoints_left = []\n",
    "refined_keypoints_right = []\n",
    "\n",
    "for c in np.arange(0, len(KP), 1):\n",
    "    hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "    ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "    # list(np.where(hm == hm.max()))\n",
    "    x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "    y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "    \n",
    "    refined_keypoints_left.append([np.rint(x), np.rint(y)])\n",
    "\n",
    "    hm = cv2.resize(final_stage_heatmapR[..., c], (widthR, heightR))\n",
    "    ii = np.unravel_index(np.argsort(hm.ravel())[-10000:], hm.shape)\n",
    "    # list(np.where(hm == hm.max()))\n",
    "    x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "    y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "    \n",
    "    refined_keypoints_right.append([np.rint(x), np.rint(y)])\n",
    "    \n",
    "    #ii\n",
    "# X = x / hmL.shape[0] * row.left_crop_metadata['width']\n",
    "# Y = y / hmL.shape[1] * row.left_crop_metadata['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.exp(hm[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_keypoints_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.where(hm == hm.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.sum(np.exp(hm[ii]) * ii[1]) / np.sum(np.exp(hm[ii]))\n",
    "y = np.sum(np.exp(hm[ii]) * ii[0]) / np.sum(np.exp(hm[ii]))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm[112, 1174], hm[111, 1154], hm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.arange(0, len(KP), 1):\n",
    "    hm = cv2.resize(final_stage_heatmapL[..., c], (widthL, heightL))\n",
    "    print(KP[c], hm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpL, kpR, im1ps, im2ps, final_stage_heatmapL, final_stage_heatmapR = get_keypoints(df2.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_crops(left_image_f, right_image_f, ann, overlay_keypoints=True, show_labels=False, custom_kps_left = [], custom_kps_right = []):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    left_image = plt.imread(left_image_f)\n",
    "    right_image = plt.imread(right_image_f)\n",
    "    axes[0].imshow(left_image)\n",
    "    axes[1].imshow(right_image)\n",
    "    left_ann, right_ann = ann['leftCrop'], ann['rightCrop']\n",
    "    left_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in left_ann}\n",
    "    right_keypoints = {item['keypointType']: [item['xCrop'], item['yCrop']] for item in right_ann}\n",
    "    if overlay_keypoints:\n",
    "        for bp, kp in left_keypoints.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in custom_kps_left.items():\n",
    "            axes[0].scatter([kp[0]], [kp[1]], color='cyan', s=10)\n",
    "            if show_labels:\n",
    "                axes[0].annotate(bp, (kp[0], kp[1]), color='cyan')\n",
    "        for bp, kp in right_keypoints.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='red', s=10)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='red')\n",
    "        for bp, kp in custom_kps_right.items():\n",
    "            axes[1].scatter([kp[0]], [kp[1]], color='cyan', s=10)\n",
    "            if show_labels:\n",
    "                axes[1].annotate(bp, (kp[0], kp[1]), color='cyan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, os\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "\n",
    "left_crop_url, right_crop_url = row.left_image_url, row.right_image_url\n",
    "ann, cm = row.keypoints, row.camera_metadata\n",
    "\n",
    "# left_crop_url, right_crop_url = row.left_crop_url, row.right_crop_url\n",
    "# ann, cm = row.annotation, row.camera_metadata\n",
    "\n",
    "left_crop_f, _, _ = s3.download_from_url(left_crop_url)\n",
    "right_crop_f, _, _ = s3.download_from_url(right_crop_url)\n",
    "wkps1 = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oDists = {}\n",
    "nDists = {}\n",
    "\n",
    "for kp in KP:\n",
    "    oDists[kp] = []\n",
    "    nDists[kp] = []\n",
    "\n",
    "cnt = 0\n",
    "stop = False\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    if stop:\n",
    "        continue\n",
    "    for i in np.arange(0, len(KP)):\n",
    "        newKeypointLeft = [kp for kp in keypoints_new[cnt]['leftCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        newKeypointRight = [kp for kp in keypoints_new[cnt]['rightCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "    \n",
    "        gtKeypoint = [kp for kp in row.keypoints['leftCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        gtX = gtKeypoint['xCrop']\n",
    "        gtY = gtKeypoint['yCrop']\n",
    "\n",
    "        oX = newKeypointLeft['xCrop']\n",
    "        oY = newKeypointLeft['yCrop']\n",
    "\n",
    "        nX = newKeypointLeft['xCropNew']\n",
    "        nY = newKeypointLeft['yCropNew']\n",
    "        \n",
    "        oDist = np.abs(gtX - oX) + np.abs(gtY - oY)\n",
    "        nDist = np.abs(gtX - nX) + np.abs(gtY - nY)\n",
    "        \n",
    "        if oDist > 1000:\n",
    "            print(KP[i], index, gtX, oX, gtY, oY)\n",
    "            stop = True\n",
    "\n",
    "        oDists[KP[i]].append(oDist)\n",
    "        nDists[KP[i]].append(nDist)\n",
    "\n",
    "        gtKeypoint = [kp for kp in row.keypoints['rightCrop'] if kp['keypointType'] == KP[i]][0]\n",
    "        gtX = gtKeypoint['xCrop']\n",
    "        gtY = gtKeypoint['yCrop']\n",
    "\n",
    "        oX = newKeypointRight['xCrop']\n",
    "        oY = newKeypointRight['yCrop']\n",
    "\n",
    "        nX = newKeypointRight['xCropNew']\n",
    "        nY = newKeypointRight['yCropNew']\n",
    "\n",
    "        oDist = np.abs(gtX - oX) + np.abs(gtY - oY)\n",
    "        nDist = np.abs(gtX - nX) + np.abs(gtY - nY)\n",
    "\n",
    "        oDists[KP[i]].append(oDist)\n",
    "        nDists[KP[i]].append(nDist)\n",
    "\n",
    "    cnt = cnt + 1\n",
    "    \n",
    "#     if cnt >= 449:\n",
    "#         break\n",
    "#     np.mean(oDists), np.mean(nDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
