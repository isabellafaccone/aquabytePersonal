{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json, os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Manager\n",
    "from research.utils.data_access_utils import S3AccessUtils, RDSAccessUtils\n",
    "from keras.models import load_model\n",
    "import boto3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute daily growth rate via fitting an exponential curve,\n",
    "# weighting each day by its sample size\n",
    "def compute_growth_rate(tdf, rdf, start_date, end_date):\n",
    "    x_values = [(dt.datetime.strptime(k, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(start_date, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    y = np.log(tdf.values)\n",
    "    reg = LinearRegression().fit(X, y, sample_weight=rdf.values)\n",
    "    growth_rate = reg.coef_[0]\n",
    "    trend_score = reg.score(X, y, sample_weight=rdf.values)\n",
    "    return growth_rate, trend_score\n",
    "\n",
    "\n",
    "# compute distribution confidence via looking at RMS of percent deviations for qq plot\n",
    "# of today's distribution against distribution in the remainder of the window\n",
    "def compute_distribution_confidence(df, start_date, end_date, date):\n",
    "    mean_adjustment = df[date:date].estimated_weight_g.mean() - df[start_date:end_date].estimated_weight_g.mean()\n",
    "    x = np.percentile(df[start_date:end_date].estimated_weight_g + mean_adjustment, list(range(100)))\n",
    "    y = np.percentile(df[date:date].estimated_weight_g, list(range(100)))\n",
    "    distribution_confidence = np.mean(np.square((x[1:99] - y[1:99]) / y[1:99])) ** 0.5\n",
    "    return distribution_confidence\n",
    "\n",
    "\n",
    "# NOTE: we need to think more carefully about this to understand how distribution \n",
    "# confidence and trend score affect the minimum sample size we want. Hardcoded for now. \n",
    "def compute_minimum_sample_size(distribution_confidence, trend_score):\n",
    "    return 5000\n",
    "    \n",
    "# Smart average is defined as a lookback to a maximum of window_size_d days (currently set to 7),\n",
    "# or until the minimum sample size is achieved\n",
    "def compute_smart_average(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = [str(d) for d in sorted(list(tdf.index.date.astype(str)))]\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(3, window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "def compute_smart_average_new(df, tdf, rdf, date, distribution_confidence, growth_rate, \n",
    "                          trend_score, window_size_d, bucket_size=0.1):\n",
    "    \n",
    "    dates = [str(d) for d in sorted(list(tdf.index.date.astype(str)))]\n",
    "    if len(dates) == 1:\n",
    "        growth_rate = 0.0\n",
    "    minimum_sample_size = compute_minimum_sample_size(distribution_confidence, trend_score)\n",
    "    x_values = [(dt.datetime.strptime(date, '%Y-%m-%d') - \\\n",
    "                 dt.datetime.strptime(k, '%Y-%m-%d')).days \\\n",
    "                 for k in tdf.index.date.astype(str)]\n",
    "    X = np.array(x_values).reshape(-1, 1)\n",
    "    Y = tdf.values\n",
    "    N = rdf.values\n",
    "    \n",
    "    for i in range(3, window_size_d):\n",
    "        if N[np.abs(np.squeeze(X)) <= i].sum() >= minimum_sample_size:\n",
    "            break\n",
    "    N[np.abs(np.squeeze(X)) > i] = 0\n",
    "    \n",
    "    smart_average = 0.0\n",
    "    sample_size = 0.0\n",
    "    adj_weights = []\n",
    "    total_days = 0\n",
    "    for x, y, n, this_date in zip(X, Y, N, dates):\n",
    "        smart_average += np.exp(x * growth_rate) * y * n\n",
    "        sample_size += n\n",
    "        if n > 0:\n",
    "            adj_weights_for_date = \\\n",
    "                list(np.exp(x * growth_rate) * df[this_date:this_date].estimated_weight_g.values)\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            total_days += 1\n",
    "        \n",
    "    smart_average /= sample_size\n",
    "    \n",
    "    adj_weights = np.array(adj_weights)\n",
    "    distribution = {}\n",
    "    buckets = [round(x, 1) for x in np.arange(0.0, 1e-3 * adj_weights.max(), bucket_size)]\n",
    "    for b in buckets:\n",
    "        low, high = 1e3 * b, 1e3 * (b + bucket_size)\n",
    "        count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "        distribution[b] = count / sample_size\n",
    "    \n",
    "    output = {\n",
    "        'weightMovingAvg': float(smart_average),\n",
    "        'weightMovingDist': distribution,\n",
    "        'numMovingAvgBatiFish': sample_size,\n",
    "        'numMovingAvgLookbackDays': total_days,\n",
    "        'dailyGrowthRate': growth_rate\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# generate date range given current date and window size. If future data\n",
    "# is available relative to current date, windows where the current date\n",
    "# is centered are preferred\n",
    "def compute_date_range(historical_dates, date, window_size_d):\n",
    "    FMT = '%Y-%m-%d'\n",
    "    max_num_days = 0\n",
    "    start_date, end_date = None, None\n",
    "    for i in range(window_size_d // 2 + 1):\n",
    "        lower_bound_date = (dt.datetime.strptime(date, FMT) - dt.timedelta(days=window_size_d-1) + \\\n",
    "                            dt.timedelta(days=i)).strftime(FMT)\n",
    "        upper_bound_date = (dt.datetime.strptime(date, FMT) + dt.timedelta(days=i)).strftime(FMT)\n",
    "        num_days = ((np.array(historical_dates)  >= lower_bound_date) & \\\n",
    "                    (np.array(historical_dates) <= upper_bound_date)).sum()\n",
    "        if num_days >= max_num_days:\n",
    "            start_date, end_date = lower_bound_date, upper_bound_date\n",
    "            max_num_days = num_days\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def get_new_average(df):\n",
    "    N, bins, _ = plt.hist(df['estimated_weight_g'], bins = 50)\n",
    "\n",
    "    binWidth = bins[1] - bins[0]\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    x_d = np.linspace(0, 2, 100)\n",
    "\n",
    "    average_weights = []\n",
    "    max_densities = []\n",
    "\n",
    "    for index in range(len(N)):\n",
    "        lowerBin = bins[index]\n",
    "        upperBin = bins[index + 1]\n",
    "        subset = (df['estimated_weight_g'] >= lowerBin) & (df['estimated_weight_g'] < upperBin) \n",
    "        depths = df[subset]['depth'].values\n",
    "        density = sum(norm.pdf((x_d - xi) / 0.1) for xi in depths)\n",
    "\n",
    "        average_weights.append(np.mean(df[subset]['estimated_weight_g']))\n",
    "        max_densities.append(np.max(density))\n",
    "    \n",
    "    max_densities = np.array(max_densities)\n",
    "    average_weights = np.nan_to_num(np.array(average_weights))\n",
    "    \n",
    "    return np.sum(max_densities * average_weights) / np.sum(max_densities)\n",
    "\n",
    "def compute_metrics_new(date, df, window_size_d=7):\n",
    "    \n",
    "#     records = json.loads(records_json)\n",
    "    \n",
    "#     dts, vals = [], []\n",
    "#     for iter_date in records:\n",
    "#         for val in records[iter_date]:\n",
    "#             dts.append(iter_date)\n",
    "#             vals.append(val)\n",
    "\n",
    "#     df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get raw statistics\n",
    "    raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    #raw_avg_weight = get_new_average(df[date:date])\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    print(df[start_date:end_date].resample('D'))\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata\n",
    "\n",
    "def compute_metrics(date, records_json, window_size_d=7):\n",
    "    \n",
    "    records = json.loads(records_json)\n",
    "    \n",
    "    dts, vals = [], []\n",
    "    for iter_date in records:\n",
    "        for val in records[iter_date]:\n",
    "            dts.append(iter_date)\n",
    "            vals.append(val)\n",
    "\n",
    "    df = pd.DataFrame(vals, index=pd.to_datetime(dts), columns=['estimated_weight_g'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get raw statistics\n",
    "    #raw_avg_weight = df[date:date].estimated_weight_g.mean()\n",
    "    raw_avg_weight = get_new_average(df[date:date])\n",
    "    raw_sample_size = df[date:date].shape[0]\n",
    "    \n",
    "    # compute relevant date range\n",
    "    historical_dates = sorted(list(set(df.index.date.astype(str))))\n",
    "    start_date, end_date = compute_date_range(historical_dates, date, window_size_d)\n",
    "    print(df[start_date:end_date].resample('D'))\n",
    "    rdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.shape[0])\n",
    "    tdf = df[start_date:end_date].estimated_weight_g.resample('D').agg(lambda x: x.mean())\n",
    "    tdf = tdf[rdf > 0].copy(deep=True)\n",
    "    rdf = rdf[rdf > 0].copy(deep=True)\n",
    "    \n",
    "    growth_rate, trend_score, distribution_confidence = None, None, None\n",
    "    if start_date < end_date:\n",
    "        growth_rate, trend_score = compute_growth_rate(tdf, rdf, start_date, end_date)\n",
    "        distribution_confidence = compute_distribution_confidence(df, start_date, end_date, date)\n",
    "    smart_average = compute_smart_average(df, tdf, rdf, date, \n",
    "                                          distribution_confidence, growth_rate, \n",
    "                                          trend_score, window_size_d)\n",
    "    metadata = {\n",
    "        'trend_score': trend_score,\n",
    "        'distribution_confidence': distribution_confidence\n",
    "    }\n",
    "\n",
    "    return raw_avg_weight, raw_sample_size, smart_average, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.concat([\n",
    "# #     pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-05,2019-06-12).csv'),\n",
    "# #     pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-12,2019-06-19).csv'),\n",
    "# #     pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-19,2019-06-26).csv'),\n",
    "# #     pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-06-26,2019-07-03).csv'),\n",
    "# #     pd.read_csv('/root/data/alok/biomass_estimation/playground/output-pen=5/biomass_output,pen=5,range=(2019-07-03,2019-07-04).csv')\n",
    "# # ])\n",
    "# rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "# query = \"\"\"\n",
    "#     SELECT * FROM (\n",
    "#       (SELECT * FROM prod.crop_annotation cas\n",
    "#       INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "#       WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "#       AND cas.annotation_state_id = 3\n",
    "#       AND cas.pen_id=5) a\n",
    "#     RIGHT JOIN \n",
    "#       (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "#       WHERE prod.biomass_computations.captured_at between '2019-06-05' and '2019-07-05'\n",
    "#       AND prod.biomass_computations.akpd_score > 0.9) bc \n",
    "#     ON \n",
    "#       (a.left_crop_url=bc.left_crop_url)\n",
    "#     ) x\n",
    "#     WHERE x.captured_at between '2019-06-05' and '2019-07-05'\n",
    "#     AND x.pen_id = 5\n",
    "#     AND x.group_id = '5';\n",
    "# \"\"\"\n",
    "# df = rds_access_utils.extract_from_database(query)\n",
    "\n",
    "# df = df.sort_values('captured_at')\n",
    "# # df['estimated_weight_g'] = df.weight\n",
    "# df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "# df.index = pd.to_datetime(df.captured_at)\n",
    "# df['hour'] = df.index.hour\n",
    "\n",
    "# depths = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     ann, cm = row.annotation, row.camera_metadata\n",
    "#     wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "#     depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "#     depths.append(depth)\n",
    "# df['depth'] = depths\n",
    "\n",
    "# hour_mask = (df.hour > 7) & (df.hour < 16)\n",
    "# df = df[hour_mask].copy(deep=True)\n",
    "\n",
    "# # get daily averages and sample sizes\n",
    "\n",
    "# # records = defaultdict(list)\n",
    "# # for date in sorted(list(set(df.index.date.astype(str)))):\n",
    "# #     records[date].extend(df[date].weight.values.tolist())\n",
    "    \n",
    "# # records_json = json.dumps(records)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=66) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at between '2020-06-01' and '2020-06-11'\n",
    "      AND prod.biomass_computations.akpd_score > 0.99) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at between '2020-06-01' and '2020-06-11'\n",
    "    AND x.pen_id = 66\n",
    "    AND x.group_id = '66';\n",
    "\"\"\"\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "\n",
    "df = df.sort_values('captured_at')\n",
    "# df['estimated_weight_g'] = df.weight\n",
    "df = df[df.akpd_score > 0.99].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "depths = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "df['depth'] = depths\n",
    "\n",
    "# hour_mask = (df.hour > 7) & (df.hour < 16)\n",
    "# df = df[hour_mask].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = [ 1, 2]\n",
    "b = [2, 3]\n",
    "\n",
    "pd.DataFrame([a, b], columns= ['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(list(set(df.index.date.astype(str))))\n",
    "raw_avg_weights, raw_sample_sizes, growth_rates, trend_scores, smart_averages, distribution_confidences = [], [], [], [], [], []\n",
    "for date in dates:\n",
    "    #raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics(date, records_json)\n",
    "    raw_avg_weight, raw_sample_size, smart_average, metadata = compute_metrics_new(date, df)\n",
    "    growth_rates.append(smart_average['dailyGrowthRate'])\n",
    "    trend_scores.append(metadata['trend_score'])\n",
    "    raw_avg_weights.append(raw_avg_weight)\n",
    "    raw_sample_sizes.append(raw_sample_size)\n",
    "    smart_averages.append(smart_average['weightMovingAvg'])\n",
    "    distribution_confidences.append(metadata['distribution_confidence'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 20))\n",
    "x_values = df.estimated_weight_g.resample('D').agg(lambda x: x.mean()).dropna().index\n",
    "axes[0].plot(x_values, raw_avg_weights, label='Raw Avg.')\n",
    "axes[0].plot(x_values, smart_averages, label='Smart Avg.')\n",
    "axes[0].plot(x_values, 1.02 * np.array(smart_averages), color='red', linestyle='--', label='Smart Avg. +/-2%')\n",
    "axes[0].plot(x_values, 0.98 * np.array(smart_averages), color='red', linestyle='--')\n",
    "axes[1].plot(x_values, raw_sample_sizes, label='Raw Daily Sample Size')\n",
    "axes[2].plot(x_values, growth_rates)\n",
    "axes[3].plot(x_values, trend_scores)\n",
    "axes[4].plot(x_values, distribution_confidences)\n",
    "for i, title in zip([0, 1, 2, 3, 4], ['Avg. weight', 'Raw Sample Size', 'Growth rate', 'Local trend score', 'Distribution Instability']):\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframe\n",
    "# s3_access_utils = S3AccessUtils('/root/data')\n",
    "# rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))\n",
    "# pen_id, group_id = 5, '5'\n",
    "# query = \"\"\"\n",
    "#     SELECT * FROM\n",
    "#     prod.biomass_computations bc\n",
    "#     WHERE bc.pen_id={0}\n",
    "#     AND bc.group_id='{1}';\n",
    "# \"\"\".format(pen_id, group_id)\n",
    "# query = \"\"\"\n",
    "#     SELECT * FROM (\n",
    "#       (SELECT * FROM prod.crop_annotation cas\n",
    "#       INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "#       WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "#       AND cas.annotation_state_id = 3\n",
    "#       AND cas.pen_id=5) a\n",
    "#     RIGHT JOIN \n",
    "#       (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "#       WHERE prod.biomass_computations.captured_at between '2019-07-02 00:00' and '2019-07-03 00:00'\n",
    "#       AND prod.biomass_computations.akpd_score > 0.9) bc \n",
    "#     ON \n",
    "#       (a.left_crop_url=bc.left_crop_url)\n",
    "#     ) x\n",
    "#     WHERE x.captured_at between '2019-07-02 00:00' and '2019-07-03 00:00'\n",
    "#     AND x.pen_id = 5\n",
    "#     AND x.group_id = '5';\n",
    "# \"\"\"\n",
    "\n",
    "pen_id = 83\n",
    "start_date = '2020-05-01 00:00'\n",
    "end_date = '2020-05-02 00:00'\n",
    "akpd_filter = 0.99\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=%i) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at >= '%s'\n",
    "      AND prod.biomass_computations.captured_at <= '%s'\n",
    "      AND prod.biomass_computations.akpd_score > %0.2f) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at >= '%s'\n",
    "    AND x.captured_at <= '%s'\n",
    "    AND x.pen_id = %i\n",
    "    AND x.group_id = '%i';\n",
    "\"\"\" % (pen_id, start_date, end_date, akpd_filter, start_date, end_date, pen_id, pen_id)\n",
    "\n",
    "df = rds_access_utils.extract_from_database(query)\n",
    "df = df.sort_values('captured_at')\n",
    "df = df[df.akpd_score > 0.9].copy(deep=True)\n",
    "df.index = pd.to_datetime(df.captured_at)\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "depths = []\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "df['depth'] = depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, bins, _ = plt.hist(df['estimated_weight_g'], bins = 50)\n",
    "\n",
    "binWidth = bins[1] - bins[0]\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5 * 3))\n",
    "\n",
    "x_d = np.linspace(0, 2, 100)\n",
    "\n",
    "average_weights = []\n",
    "max_densities = []\n",
    "\n",
    "for index in range(len(N)):\n",
    "    lowerBin = bins[index]\n",
    "    upperBin = bins[index + 1]\n",
    "    subset = (df['estimated_weight_g'] >= lowerBin) & (df['estimated_weight_g'] < upperBin) \n",
    "    depths = df[subset]['depth'].values\n",
    "    density = sum(norm.pdf((x_d - xi) / 0.1) for xi in depths)\n",
    "    \n",
    "    average_weights.append(np.mean(df[subset]['estimated_weight_g']))\n",
    "    #max_densities.append(np.percentile(density, 95))\n",
    "    #max_densities.append(np.percentile(density, 50))\n",
    "    max_densities.append(np.max(density))\n",
    "    #max_densities.append(np.mean(density))\n",
    "    #max_densities.append(len(depths))\n",
    "\n",
    "    #print(index, np.max(density))\n",
    "\n",
    "max_densities = np.array(max_densities)\n",
    "average_weights = np.nan_to_num(np.array(average_weights))\n",
    "\n",
    "axes[0].bar(bins[1:], N / np.max(N), width = binWidth, color = 'blue', alpha = 0.5)\n",
    "axes[0].set_title('Vikane Pen 1 (6/1/20 - 6/10/20): Original Histogram')\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[1].bar(bins[1:], max_densities / np.max(max_densities), width = binWidth, color = 'red', alpha = 0.5)\n",
    "axes[1].set_title('Vikane Pen 1 (6/1/20 - 6/10/20): Adjusted Histogram')\n",
    "axes[1].set_xlabel('Weight')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[2].bar(bins[1:], N / np.max(N), width = binWidth, color = 'blue', alpha = 0.5)\n",
    "axes[2].bar(bins[1:], max_densities / np.max(max_densities), width = binWidth, color = 'red', alpha = 0.5)\n",
    "axes[2].set_title('Vikane Pen 1 (6/1/20 - 6/10/20): Original vs Adjusted Histogram')\n",
    "axes[2].set_xlabel('Weight')\n",
    "axes[2].set_ylabel('Density')\n",
    "\n",
    "prior = np.mean(df['estimated_weight_g'])\n",
    "normalized = np.sum(max_densities * average_weights) / np.sum(max_densities)\n",
    "print(prior, normalized, (normalized - prior) / prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# raw_weights = df['estimated_weight_g'].values\n",
    "# raw_weights.sort()\n",
    "# weights = raw_weights[:,None]\n",
    "\n",
    "# kde = KernelDensity(bandwidth = 200, kernel='gaussian')\n",
    "# kde.fit(weights)\n",
    "\n",
    "# logprob = kde.score_samples(weights)\n",
    "\n",
    "# plt.fill_between(raw_weights, np.exp(logprob), alpha = 0.5)\n",
    "\n",
    "x_d = np.linspace(0, 8000, 1000)\n",
    "\n",
    "weights = df['estimated_weight_g'].values\n",
    "\n",
    "density = sum(norm.pdf((x_d - xi) / 200) for xi in weights)\n",
    "#density = sum(norm(xi).pdf(x_d) for xi in weights)\n",
    "\n",
    "plt.fill_between(x_d, density, alpha=0.5)\n",
    "\n",
    "# plt.hist(df['estimated_weight_g'], bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_weight = 2000\n",
    "bucket_size = bins[1] - bins[0]\n",
    "buckets = [bucket_size * x for x in list(range(8))]\n",
    "\n",
    "fig, axes = plt.subplots(nrows = len(buckets), ncols = 1, figsize = (10, 5 * len(buckets)))\n",
    "\n",
    "x_d = np.linspace(0, 2, 100)\n",
    "\n",
    "for index, bucket in enumerate(buckets):\n",
    "    startingWeight = base_weight + bucket\n",
    "    endingWeight = base_weight + bucket + bucket_size\n",
    "    subset = (df['estimated_weight_g'] > startingWeight) & (df['estimated_weight_g'] < endingWeight) \n",
    "    \n",
    "    depths = df[subset]['depth'].values\n",
    "#     raw_depths.sort()\n",
    "#     depths = raw_depths[:,None]\n",
    "\n",
    "#     kde = KernelDensity(bandwidth = 0.05, kernel='gaussian')\n",
    "#     kde.fit(depths)\n",
    "\n",
    "#     logprob = kde.score_samples(depths)\n",
    "\n",
    "#     axes[index].fill_between(raw_depths, np.exp(logprob), alpha=0.5)\n",
    "    density = sum(norm.pdf((x_d - xi) / 0.15) for xi in depths)\n",
    "    axes[index].fill_between(x_d, density, alpha=0.5)\n",
    "    axes[index].set_title('Vikane Pen 1 (6/1/20 - 6/10/20): Kernel Density by Depth [%0.0f -> %0.0f]' % (startingWeight, endingWeight))\n",
    "    axes[index].set_xlabel('Depth')\n",
    "    axes[index].set_ylabel('Density')\n",
    "    \n",
    "\n",
    "\n",
    "#     axes[index].hist(df[subset]['depth'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
