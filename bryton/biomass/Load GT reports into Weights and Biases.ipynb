{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from math import floor, ceil\n",
    "from pprint import pprint\n",
    "from traceback import print_exc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/root/data/alok/slaughterreports'\n",
    "pen_names = os.listdir(path)\n",
    "print(pen_names[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_names_new = [ pn.encode('utf-8', 'replace').decode() for pn in pen_names ]\n",
    "pen_names_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bounds(row):\n",
    "    bounds = row['bounds']\n",
    "    if row['category'] == 'Utkast' and bounds is np.nan:\n",
    "        return (0,10)\n",
    "    MISTYPES = {\n",
    "        '[3. 4]': '[3, 4]',\n",
    "        '[9+, ]': '[9, ]'\n",
    "    }\n",
    "    if bounds in MISTYPES:\n",
    "        bounds = MISTYPES[bounds]\n",
    "    if bounds is not np.nan:\n",
    "        bounds = [x for x in bounds[1:-1].split(',') if x.strip()]\n",
    "        bounds = [float(x) for x in bounds]\n",
    "        if len(bounds) == 1:\n",
    "            bounds = [bounds[0], 10]\n",
    "        out = (floor(bounds[0]), min(ceil(bounds[1]), 10))\n",
    "        return out\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def clean_weight(w):\n",
    "    if w is None:\n",
    "        return w\n",
    "    if isinstance(w, str):\n",
    "        w = w.replace('..', '.').replace(' ', '')\n",
    "        try:\n",
    "            return float(w)\n",
    "        except:\n",
    "            print('Could not clean')\n",
    "            print(w)\n",
    "            return None\n",
    "    elif isinstance(w, float):\n",
    "        return w\n",
    "    elif isinstance(w, int):\n",
    "        return float(w)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pen in pen_names_new:\n",
    "#     print('\\n\\n\\n')\n",
    "#     print(pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('here')\n",
    "\n",
    "out_d = dict()\n",
    "for index, pen_f in enumerate(pen_names):\n",
    "#     print('\\n\\n\\n\\n\\n\\n')\n",
    "#     print(index, pen_f)\n",
    "    pen_name = pen_names_new[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from traceback import print_exc\n",
    "# unred = []\n",
    "# unfixed = []\n",
    "\n",
    "# out_d = dict()\n",
    "# for index, pen_f in enumerate(pen_names):\n",
    "# #     if index not in unfixed:\n",
    "# #         continue\n",
    "#     print('\\n\\n\\n\\n\\n\\n')\n",
    "# #     print(index, pen_f)\n",
    "#     pen_name = pen_names_new[index]\n",
    "#     print(pen_name)\n",
    "    \n",
    "#     found = False\n",
    "#     this_pen_folder = os.path.join(path, pen_f)\n",
    "#     if not this_pen_folder.endswith('.DS_Store'):\n",
    "#         files = os.listdir(this_pen_folder)\n",
    "#         for f in files:\n",
    "# #             print(f)\n",
    "# #             print(pen_name, f)\n",
    "#             d = None\n",
    "#             if f.endswith('ods'):\n",
    "#                 d = pd.read_excel(os.path.join(this_pen_folder, f), engine='odf')\n",
    "#                 found = True\n",
    "#             elif f.endswith('csv'):\n",
    "#                 print(pen_name)\n",
    "#                 try:\n",
    "#                     p = os.path.join(this_pen_folder, f)\n",
    "                    \n",
    "#                     d = pd.read_csv(p)\n",
    "#                     found = True\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         with open(p) as inp:\n",
    "#                             d = pd.read_csv(inp)\n",
    "#                     except:\n",
    "\n",
    "#     #                     unfixed.append(index)\n",
    "#                         print('\\n\\nfailed')\n",
    "#                         print(pen_name)\n",
    "#     #                     print(f)\n",
    "#     #                     unred.append(p)\n",
    "# #                         print('RAW')\n",
    "#     #                     print(open(os.path.join(this_pen_folder, f)).read())\n",
    "#                         continue\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#             d.columns = [col.lower().strip() for col in d.columns]\n",
    "\n",
    "#             d = d.rename({'catergory': 'category',\n",
    "#                           'caterory': 'category',\n",
    "#                          'net wett.': 'net weight',\n",
    "#                          'net wett': 'net weight',\n",
    "#                          'number of fish': 'number of fish'}, axis=1)\n",
    "#             required = [\n",
    "#                 'Number of Fish',\n",
    "#                 'category',\n",
    "#                 'bounds',\n",
    "#                 'net weight'\n",
    "#             ]\n",
    "#             if 'net weight' not in d.columns:\n",
    "#                 d['net weight'] = None\n",
    "#             required = [r.lower() for r in required]\n",
    "#             has_required = True\n",
    "#             for col in required:\n",
    "#                 if col not in d.columns:\n",
    "#                     has_required = False\n",
    "#                     print('missing', pen_name, col, d.columns)\n",
    "#             if not has_required:\n",
    "#                 print(d)\n",
    "#                 continue\n",
    "\n",
    "#             d['category'] = d['category'].apply(str)\n",
    "#             d['net weight'] = d['net weight'].apply(clean_weight)\n",
    "#             d['reg'] = d['category'].apply(lambda s: s[0].isdigit())\n",
    "#             d['bounds'] = d.apply(clean_bounds, axis=1)\n",
    "#             d.drop_duplicates(inplace=True)\n",
    "#             if d['reg'].sum() > 2:\n",
    "#                 d = d[d['reg']]\n",
    "#             if len(d['category'].unique()) != len(d):\n",
    "#                 d = d.groupby(['category', 'bounds'])[['number of fish', 'net weight']].sum().reset_index()\n",
    "\n",
    "#             if  not len(d['category'].unique()) == len(d):\n",
    "#                 vcs = d['category'].value_counts()\n",
    "#                 print('nonunique cats', pen_name, d)\n",
    "#                 print(vcs[vcs>1])\n",
    "#             data = {\n",
    "#                  row['category']: (\n",
    "#                      row['bounds'],\n",
    "#                      row['number of fish'],\n",
    "#                      row['net weight']\n",
    "#                  ) for _, row in d.iterrows()\n",
    "#             }\n",
    "#             out_d[(pen_name, f)] = data\n",
    "# #             print((pen_name, f))\n",
    "# #             print(d)\n",
    "#         if not found:\n",
    "#             print('\\n\\nNo csv found/read for:')\n",
    "#             print(pen_name) \n",
    "#             print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import print_exc\n",
    "unred = []\n",
    "unfixed = []\n",
    "\n",
    "out_d = dict()\n",
    "for index, pen_f in enumerate(pen_names):\n",
    "#     if index not in unfixed:\n",
    "#         continue\n",
    "#     print('\\n\\n\\n\\n\\n\\n')\n",
    "#     print(index, pen_f)\n",
    "    pen_name = pen_names_new[index]\n",
    "    print(pen_name)\n",
    "    \n",
    "    found = False\n",
    "    this_pen_folder = os.path.join(path, pen_f)\n",
    "    if not this_pen_folder.endswith('.DS_Store'):\n",
    "        files = os.listdir(this_pen_folder)\n",
    "        for f in files:\n",
    "#             print(f)\n",
    "#             print(pen_name, f)\n",
    "            d = None\n",
    "            if f.endswith('ods'):\n",
    "                d = pd.read_excel(os.path.join(this_pen_folder, f), engine='odf')\n",
    "                found = True\n",
    "            elif f.endswith('csv'):\n",
    "#                 print(pen_name)\n",
    "                try:\n",
    "                    p = os.path.join(this_pen_folder, f)\n",
    "                    \n",
    "                    d = pd.read_csv(p)\n",
    "                    found = True\n",
    "                except:\n",
    "                    try:\n",
    "                        with open(p) as inp:\n",
    "                            d = pd.read_csv(inp)\n",
    "                    except:\n",
    "\n",
    "    #                     unfixed.append(index)\n",
    "#                         print('\\n\\nfailed')\n",
    "#                         print(pen_name)\n",
    "    #                     print(f)\n",
    "    #                     unred.append(p)\n",
    "#                         print('RAW')\n",
    "    #                     print(open(os.path.join(this_pen_folder, f)).read())\n",
    "                        continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            d.columns = [col.lower().strip() for col in d.columns]\n",
    "\n",
    "            d = d.rename({'catergory': 'category',\n",
    "                          'caterory': 'category',\n",
    "                         'net wett.': 'net weight',\n",
    "                         'net wett': 'net weight',\n",
    "                         'number of fish': 'number of fish'}, axis=1)\n",
    "            required = [\n",
    "                'Number of Fish',\n",
    "                'category',\n",
    "                'bounds',\n",
    "                'net weight'\n",
    "            ]\n",
    "            if 'net weight' not in d.columns:\n",
    "                d['net weight'] = None\n",
    "            required = [r.lower() for r in required]\n",
    "            has_required = True\n",
    "            for col in required:\n",
    "                if col not in d.columns:\n",
    "                    has_required = False\n",
    "#                     print('missing', pen_name, col, d.columns)\n",
    "            if not has_required:\n",
    "#                 print(d)\n",
    "                continue\n",
    "\n",
    "            d['category'] = d['category'].apply(str)\n",
    "            \n",
    "#             mask = d['category'].apply(lambda x: 'S' in x)\n",
    "#             if np.sum(mask) > 0:\n",
    "#                 d = d[mask]\n",
    "            \n",
    "            print(d['category'])\n",
    "            \n",
    "            d['net weight'] = d['net weight'].apply(clean_weight)\n",
    "            d['reg'] = d['category'].apply(lambda s: s[0].isdigit())\n",
    "            d['bounds'] = d.apply(clean_bounds, axis=1)\n",
    "            d.drop_duplicates(inplace=True)\n",
    "            if d['reg'].sum() > 2:\n",
    "                d = d[d['reg']]\n",
    "            if len(d['category'].unique()) != len(d):\n",
    "                d = d.groupby(['category', 'bounds'])[['number of fish', 'net weight']].sum().reset_index()\n",
    "\n",
    "            if  not len(d['category'].unique()) == len(d):\n",
    "                vcs = d['category'].value_counts()\n",
    "                print('nonunique cats', pen_name, d)\n",
    "                print(vcs[vcs>1])\n",
    "            data = {\n",
    "                 row['category']: (\n",
    "                     row['bounds'],\n",
    "                     row['number of fish'],\n",
    "                     row['net weight']\n",
    "                 ) for _, row in d.iterrows()\n",
    "            }\n",
    "            out_d[(pen_name, f)] = data\n",
    "#             print((pen_name, f))\n",
    "#             print(d)\n",
    "        if not found:\n",
    "            pass\n",
    "#             print('\\n\\nNo csv found/read for:')\n",
    "#             print(pen_name) \n",
    "#             print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_d[('BTI4_202006_ Bremnes Tittelsnes Pen 4 - June 2020',\n",
    "  'transcribed_buckets.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(out_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([k[0] for k in out_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for k in out_d:\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print(k)\n",
    "    print(pd.DataFrame(out_d[k]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "\n",
    "def convert_mixed_buckets_to_clean_buckets(report, loss_factor):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    report: dict\n",
    "        The raw harvest report data of the form:\n",
    "            {\n",
    "                <BUCKET1_NAME>: (\n",
    "                    (<BUCKET1_LOWERBOUND>[int], <BUCKET1_UPPERBOUND>[int]),\n",
    "                    <BUCKET_COUNT>[int],\n",
    "                    <BUCKET1_TOTAL_WEIGHT>[float],\n",
    "                    <BUCKET1_GUTTED_INDICATOR>[char(optional)]\n",
    "                ),\n",
    "                <BUCKET2_NAME>: (\n",
    "                    (<BUCKET2_LOWERBOUND>[int], <BUCKET2_UPPERBOUND>[int]),\n",
    "                    <BUCKET_COUNT>[int],\n",
    "                    <BUCKET2_TOTAL_WEIGHT>[float],\n",
    "                    <BUCKET2_GUTTED_INDICATOR>[char(optional)]\n",
    "                ), ...\n",
    "            }\n",
    "        Example: {\n",
    "           '0-10': ((2, 5), 213, 826.00, 'r'),\n",
    "           '2-3': ((2, 3), 159, 437.80),\n",
    "           '3-10': ((3, 5), 667, 1667.2),\n",
    "           '3-4': ((3, 4), 4089, 14856.00),\n",
    "           '3-6': ((3, 6), 4279, 19030.50),\n",
    "           '4.5-5.0': ((4, 5), 540, 2553.80),\n",
    "           '4-5': ((4, 5), 14872, 67174.40),\n",
    "           '5-6': ((5, 6), 14264, 77238.80),\n",
    "           '6+': ((6, 8), 618, 4071.80),\n",
    "           '6-7': ((6, 7), 4640, 29502.20),\n",
    "           '7-8': ((7, 8), 730, 5323.00),\n",
    "           '8-9': ((8, 9), 49, 402.20),\n",
    "           '9+': ((9, 10), 2, 18.30),\n",
    "       }\n",
    "    loss_factor: float\n",
    "        The loss factor you'd like.\n",
    "    Returns\n",
    "    -------\n",
    "    Updated Report\n",
    "    \"\"\"\n",
    "    all_n_indices = []\n",
    "    all_aw_indices = []\n",
    "\n",
    "    variable_n_indices = {}\n",
    "    variable_aw_indices = {}\n",
    "\n",
    "    index_to_bucket = {}\n",
    "\n",
    "    variables_n = { i: [] for i in range(0, 10) }\n",
    "    variables_aw = { i: [] for i in range(0, 10) }\n",
    "\n",
    "    conditions = []\n",
    "    bounds = []\n",
    "    guess = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    orig_all_weights = 0\n",
    "    orig_all_weights_nongutted = 0\n",
    "    orig_all_counts = 0\n",
    "\n",
    "    for constraint_name, constraint in report.items():\n",
    "        if len(constraint) == 4:\n",
    "            range_buckets, range_count, range_total_weight, gutted_indicator = constraint\n",
    "\n",
    "            if gutted_indicator == 'r':\n",
    "                orig_all_weights_nongutted = orig_all_weights_nongutted + range_total_weight\n",
    "                range_total_weight = range_total_weight * (1 - loss_factor)\n",
    "        else:\n",
    "            range_buckets, range_count, range_total_weight = constraint\n",
    "            orig_all_weights_nongutted = orig_all_weights_nongutted + range_total_weight\n",
    "\n",
    "        orig_all_weights = orig_all_weights + range_total_weight\n",
    "        orig_all_counts = orig_all_counts + range_count\n",
    "\n",
    "        range_count = range_count / 10000\n",
    "        range_total_weight = range_total_weight / 10000\n",
    "\n",
    "        variable_n_indices[constraint_name] = []\n",
    "        variable_aw_indices[constraint_name] = []\n",
    "\n",
    "        for bucket in range(range_buckets[0], range_buckets[1]):\n",
    "            if range_buckets[1] - range_buckets[0] > 1:\n",
    "                guess_n = range_count * (bucket + 0.5) / (np.sum(range(range_buckets[0], range_buckets[1])) + 0.5 * (range_buckets[1] - range_buckets[0]))\n",
    "                guess_aw = bucket + 0.5\n",
    "            else:\n",
    "                guess_n = range_count\n",
    "                guess_aw = range_total_weight / range_count\n",
    "\n",
    "            variable_n_indices[constraint_name].append(counter)\n",
    "            variables_n[bucket].append(counter)\n",
    "            all_n_indices.append(counter)\n",
    "            guess.append(guess_n)\n",
    "            bounds.append((0, range_count))\n",
    "            index_to_bucket[counter] = bucket\n",
    "\n",
    "            counter = counter + 1\n",
    "\n",
    "            variable_aw_indices[constraint_name].append(counter)\n",
    "            variables_aw[bucket].append(counter)\n",
    "            all_aw_indices.append(counter)\n",
    "            guess.append(guess_aw)\n",
    "            bounds.append((bucket, bucket + 1))\n",
    "            index_to_bucket[counter] = bucket\n",
    "\n",
    "            counter = counter + 1\n",
    "\n",
    "        variable_n_indices[constraint_name] = np.array(variable_n_indices[constraint_name])\n",
    "        variable_aw_indices[constraint_name] = np.array(variable_aw_indices[constraint_name])\n",
    "\n",
    "        n_indices = variable_n_indices[constraint_name].astype(int)\n",
    "        aw_indices = variable_aw_indices[constraint_name].astype(int)\n",
    "\n",
    "        conditions.append({ 'type': 'ineq', 'fun': lambda x: 0.000001 - np.abs(np.sum(x[n_indices]) - range_count) })\n",
    "        conditions.append({ 'type': 'ineq', 'fun': lambda x: 0.000001 - np.abs(np.sum(x[aw_indices] * x[n_indices]) - range_total_weight) })\n",
    "\n",
    "    for bucket in variables_n:\n",
    "        variables_n[bucket] = np.array(variables_n[bucket])\n",
    "        variables_aw[bucket] = np.array(variables_aw[bucket])\n",
    "\n",
    "        n_indices = variables_n[bucket].astype(int)\n",
    "        aw_indices = variables_aw[bucket].astype(int)\n",
    "\n",
    "        if len(n_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        conditions.append({ 'type': 'ineq', 'fun': lambda x: np.sum(x[aw_indices] * x[n_indices]) - bucket * np.sum(x[n_indices]) })\n",
    "        conditions.append({ 'type': 'ineq', 'fun': lambda x: (bucket + 1) * np.sum(x[n_indices]) - np.sum(x[aw_indices] * x[n_indices]) })\n",
    "\n",
    "    conditions.append({ 'type': 'ineq', 'fun': lambda x: 0.000001 - np.abs(np.sum(x[all_n_indices]) - orig_all_counts / 10000) })\n",
    "    conditions.append({ 'type': 'ineq', 'fun': lambda x: 0.000001 - np.abs(np.sum(x[all_aw_indices] * x[all_n_indices]) - orig_all_weights / 10000) })\n",
    "\n",
    "    def objective_function(x):\n",
    "        objective = 0\n",
    "\n",
    "        for constraint_name, constraint in report.items():\n",
    "            if len(constraint) == 4:\n",
    "                range_buckets, range_count, range_total_weight, gutted_indicator = constraint\n",
    "\n",
    "                if gutted_indicator == 'r':\n",
    "                    range_total_weight = range_total_weight * (1 - loss_factor)\n",
    "            else:\n",
    "                range_buckets, range_count, range_total_weight = constraint\n",
    "\n",
    "            range_count = range_count / 10000\n",
    "            range_total_weight = range_total_weight / 10000\n",
    "\n",
    "            n_indices = variable_n_indices[constraint_name].astype(int)\n",
    "            aw_indices = variable_aw_indices[constraint_name].astype(int)\n",
    "\n",
    "            total_subset_count = 0\n",
    "\n",
    "            for n_index in n_indices:\n",
    "                bucket = index_to_bucket[n_index]\n",
    "                bucket_indices = variables_n[bucket]\n",
    "                total_subset_count = total_subset_count + np.sum(x[bucket_indices])\n",
    "\n",
    "            for index, n_index in enumerate(n_indices):\n",
    "                aw_index = aw_indices[index]\n",
    "\n",
    "                bucket = index_to_bucket[n_index]\n",
    "                bucket_indices = variables_n[bucket]\n",
    "\n",
    "                objective = objective + np.abs((x[n_index] / np.sum(x[n_indices]) - np.sum(x[bucket_indices]) / total_subset_count))\n",
    "\n",
    "            objective = objective + np.abs(np.sum(x[n_indices]) - range_count) / range_count\n",
    "            objective = objective + np.abs(np.sum(x[aw_indices] * x[n_indices]) - range_total_weight) / range_total_weight\n",
    "\n",
    "        return objective\n",
    "\n",
    "    results = opt.minimize(objective_function, guess, method='SLSQP', bounds=bounds, constraints = conditions, options={'disp': True})\n",
    "\n",
    "    x = results.x\n",
    "\n",
    "    all_weight = 0\n",
    "    all_count = 0\n",
    "\n",
    "    for bucket in variables_n:\n",
    "        n_indices = variables_n[bucket].astype(int)\n",
    "        aw_indices = variables_aw[bucket].astype(int)\n",
    "\n",
    "        total_weight = np.sum(x[aw_indices] * x[n_indices]) * 10000\n",
    "        total_count = np.sum(x[n_indices]) * 10000\n",
    "\n",
    "        all_weight = all_weight + total_weight\n",
    "        all_count = all_count + total_count\n",
    "\n",
    "    out_buckets = []\n",
    "    for bucket in variables_n:\n",
    "        n_indices = variables_n[bucket].astype(int)\n",
    "        aw_indices = variables_aw[bucket].astype(int)\n",
    "\n",
    "        total_weight = np.sum(x[aw_indices] * x[n_indices]) * 10000\n",
    "        total_count = np.sum(x[n_indices]) * 10000\n",
    "\n",
    "        out_buckets.append([(bucket * 1000, (bucket + 1) * 1000), total_weight / total_count * 1000, total_count / all_count * 100, total_weight / all_weight * 100])\n",
    "    \n",
    "    report_avg_weight = 0\n",
    "\n",
    "    for val in out_buckets:\n",
    "        if not np.isnan(val[1]):\n",
    "            report_avg_weight = report_avg_weight + val[1] * val[2] / 100\n",
    "    \n",
    "    out_buckets = pd.DataFrame(out_buckets, columns=['bucket_bounds', 'avg_weight', 'bucket_count', 'total_weight']).to_dict()\n",
    "\n",
    "    out_buckets['report_avg_weight'] = report_avg_weight\n",
    "    \n",
    "    return out_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = dict()\n",
    "for key in out_d.keys(): \n",
    "    print(key)\n",
    "    try:\n",
    "        cleaned[key[0]] = convert_mixed_buckets_to_clean_buckets(out_d[key], 0.16)\n",
    "    except:\n",
    "        print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "report_ks = set([y[0] for y  in sorted(out_d.keys())])\n",
    "processed_ks = set(cleaned.keys())\n",
    "unprocessed = report_ks - processed_ks\n",
    "\n",
    "for x in unprocessed:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = cleaned['BTI4_202006_ Bremnes Tittelsnes Pen 4 - June 2020']\n",
    "\n",
    "# avg_weight = 0\n",
    "\n",
    "# for b in a:\n",
    "#     if not np.isnan(b[1]):\n",
    "#         avg_weight = avg_weight + b[1] * b[2] / 100\n",
    "    \n",
    "# print(avg_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('aquabyte-images-adhoc')\n",
    "folder = 'alok/production_datasets/'\n",
    "folders = [x for x in os.listdir(path) if x != '.DS_Store']\n",
    "for k in folders:\n",
    "    direc = os.path.join(path, k)\n",
    "    json_fs = [x for x in os.listdir(direc) if x.endswith('.json')]\n",
    "    if len(json_fs):\n",
    "        assert len(json_fs) == 1, json_fs\n",
    "\n",
    "        with open(os.path.join(direc, json_fs[0])) as f:\n",
    "            meta = json.load(f)\n",
    "        if k in cleaned:\n",
    "            meta['validated_data'] = cleaned[k]\n",
    "            out_d = os.path.join('ground_truth_slaughter_reports', k)\n",
    "            os.makedirs(out_d, exist_ok=True)\n",
    "            with open(os.path.join(out_d, 'ground_truth_metadata_validated_regular.json'), 'w') as out:\n",
    "                json.dump(meta, out)\n",
    "            print(k.encode('utf-8', 'replace').decode())\n",
    "        else:\n",
    "            pass\n",
    "#             print('Missing')\n",
    "#             print(k.encode('utf-8', 'replace').decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['BTI4_202006_ Bremnes Tittelsnes Pen 4 - June 2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "\n",
    "path = '/root/data/alok/slaughterreports'\n",
    "\n",
    "for k in os.listdir('ground_truth_slaughter_reports'):\n",
    "    direc = os.path.join(path, k)\n",
    "    json_fs = [x for x in os.listdir(direc) if x.endswith('.json')]\n",
    "    if len(json_fs):\n",
    "        assert len(json_fs) == 1, json_fs\n",
    "        \n",
    "        print(k)\n",
    "\n",
    "        with open(os.path.join(direc, json_fs[0])) as f:\n",
    "            meta = json.load(f)\n",
    "            print(meta)\n",
    "            \n",
    "            last_feeding_date = datetime.strptime(meta['last_feeding_date'], '%Y-%m-%d')\n",
    "            default_end_date = last_feeding_date + timedelta(days=3)\n",
    "            default_start_date = default_end_date - timedelta(days=14)\n",
    "            meta['data_end_date'] = default_end_date.strftime('%Y-%m-%d')\n",
    "            meta['data_start_date'] = default_start_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "#             print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "# import json\n",
    "# import wandb\n",
    "# from datetime import datetime, timedelta \n",
    "\n",
    "# from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "# wandb.init(project=\"my-project\")\n",
    "\n",
    "# path = '/root/data/alok/slaughterreports'\n",
    "\n",
    "# for k in os.listdir('ground_truth_slaughter_reports'):\n",
    "#     direc = os.path.join(path, k)\n",
    "#     json_fs = [x for x in os.listdir(direc) if x.endswith('.json')]\n",
    "#     if len(json_fs):\n",
    "#         assert len(json_fs) == 1, json_fs\n",
    "\n",
    "#         with open(os.path.join(direc, json_fs[0])) as f:\n",
    "#             meta = json.load(f)\n",
    "#             print(meta)\n",
    "            \n",
    "#             last_feeding_date = datetime.strptime(meta['last_feeding_date'], '%Y-%m-%d')\n",
    "#             default_end_date = last_feeding_date + timedelta(days=3)\n",
    "#             default_start_date = default_end_date - timedelta(days=14)\n",
    "#             meta['data_end_date'] = default_end_date.strftime('%Y-%m-%d')\n",
    "#             meta['data_start_date'] = default_start_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            \n",
    "#             df = extract_biomass_data(meta['pen_id'], meta['data_start_date'], meta['data_end_date'], 0.01)\n",
    "#             try:\n",
    "#                 table = wandb.Table(dataframe=df)\n",
    "\n",
    "#                 dataset_artifact = wandb.Artifact(k.split(' ')[0], type='dataset', metadata = meta)\n",
    "#                 dataset_artifact.add(table, 'biomass_estimations')\n",
    "#                 wandb.log_artifact(dataset_artifact)\n",
    "#             except:\n",
    "#                 print('Failed')\n",
    "#                 pass\n",
    "# wandb.finish()\n",
    "# #             table = wandb.Table(['captured_at', 'site_id', 'pen_id', 'left_crop_url', 'right_crop_url', 'estimated_weight_g', 'captured_at', 'akpd_score', 'estimated_length_mm', 'estimated_k_factor', 'process_info', 'annotation', 'camera_metadata', 'pair_id', 'left_crop_metadata', 'right_crop_metadata', 'date', 'hour'], df.values)\n",
    "            \n",
    "# #             for index, row in df.iterrows():\n",
    "# #                 table.add_data(row.captured_at, row.site_id, row.pen_id, row.left_crop_url, row.right_crop_url, row.estimated_weight_g, row.captured_at, row.akpd_score, row.estimated_length_mm, row.estimated_k_factor, row.process_info, row.annotation, row.camera_metadata, row.pair_id, row.left_crop_metadata, row.right_crop_metadata, row.date, row.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
