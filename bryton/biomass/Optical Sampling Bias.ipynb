{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen5 = pd.read_csv('blom_vikane_singleweights.csv')\n",
    "vikane_gt = pen5['weight'] * 1000 / 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('eide_langoy_singleweights.csv')\n",
    "langoy_gt = gt['weight'] * 1000 / 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vikane_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(langoy_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from research.weight_estimation.keypoint_utils import body_parts\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in body_parts.core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = body_parts.core_body_parts.index(body_parts.UPPER_LIP)\n",
    "    tail_notch_idx = body_parts.core_body_parts.index(body_parts.TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "# vdf = extract_biomass_data(60, '2020-07-01', '2020-09-15', 0.01)\n",
    "# df = extract_biomass_data(108, '2020-05-01', '2020-06-01', 0.01)\n",
    "ddf = extract_biomass_data(145, '2021-01-15', '2021-02-15', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.applymap(lambda x: x is None)['annotation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "lengths = []\n",
    "depths = []\n",
    "\n",
    "for idx, row in vdf.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "    lengths.append(np.linalg.norm(wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']))\n",
    "\n",
    "vdf['depth'] = depths\n",
    "vdf['length'] = lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "lengths = []\n",
    "depths = []\n",
    "\n",
    "for idx, row in ddf.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "    lengths.append(np.linalg.norm(wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']))\n",
    "\n",
    "ddf['depth'] = depths\n",
    "ddf['length'] = lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "lengths = []\n",
    "depths = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ann, cm = row.annotation, row.camera_metadata\n",
    "    wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "    depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "    depths.append(depth)\n",
    "    lengths.append(np.linalg.norm(wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']))\n",
    "\n",
    "df['depth'] = depths\n",
    "df['length'] = lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "\n",
    "model1_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/playground/nn_epoch_798_v2.pb'\n",
    "model2_url = 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb'\n",
    "\n",
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))\n",
    "\n",
    "weight_model1_f, _, _ = s3.download_from_url(model1_url)\n",
    "weight_model2_f, _, _ = s3.download_from_url(model2_url)\n",
    "kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "weight_estimator1 = WeightEstimator(weight_model1_f, kf_model_f)\n",
    "weight_estimator2 = WeightEstimator(weight_model2_f, kf_model_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = []\n",
    "weights2 = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for idx, row in vdf.iterrows():\n",
    "    count = count + 1\n",
    "    \n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    annotation = row.annotation\n",
    "    if not annotation:\n",
    "        weights.append(None)\n",
    "        continue\n",
    "    camera_metadata = row.camera_metadata\n",
    "\n",
    "    camera_metadata_obj = CameraMetadata(\n",
    "        focal_length=camera_metadata['focalLength'],\n",
    "        focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "        baseline_m=camera_metadata['baseline'],\n",
    "        pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "        pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "        image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "        image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "    )\n",
    "\n",
    "    weight1, length, kf = weight_estimator1.predict(annotation, camera_metadata_obj)\n",
    "    weight2, length, kf = weight_estimator2.predict(annotation, camera_metadata_obj)\n",
    "\n",
    "    weights1.append(weight1)\n",
    "    weights2.append(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weights1'] = weights1\n",
    "df['weights2'] = weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf['weights1'] = weights1\n",
    "vdf['weights2'] = weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dates = sorted(list(set(list(df.date))))\n",
    "plot_dates = [ datetime.strptime(d, '%Y-%m-%d') for d in dates ]\n",
    "\n",
    "weights1 = []\n",
    "weights2 = []\n",
    "weights3 = []\n",
    "\n",
    "for date in dates:\n",
    "#     mask1 = (df.akpd_score > 0.95) & (df.hour >= 8) & (df.hour <= 16) & (df.date == date)\n",
    "#     mask2 = (df.akpd_score > 0.95) & (df.hour >= 8) & (df.hour <= 16) & (df.date == date)\n",
    "#     mask3 = (df.akpd_score > 0.95) & (df.hour >= 8) & (df.hour <= 16) & (df.date == date)\n",
    "    mask1 = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == date)\n",
    "    mask2 = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == date)\n",
    "    mask3 = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == date)\n",
    "    \n",
    "    weights1.append(np.mean(df[mask1]['weights1']))\n",
    "    weights2.append(np.mean(df[mask2]['weights2']))\n",
    "    weights3.append(np.mean(df[mask3]['estimated_weight_g']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter, WeekdayLocator, MONDAY\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ax.xaxis.set_major_locator(WeekdayLocator(MONDAY))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%m/%d'))\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.plot(plot_dates, weights1, label = 'Old Model')\n",
    "ax.plot(plot_dates, weights2, label = 'New Model')\n",
    "ax.plot(plot_dates, weights3, label = 'Current Model')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (df.akpd_score > 0.95) & (df.date == '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.date == '2020-05-10')\n",
    "plt.hist(df[mask].hour, bins = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10')\n",
    "\n",
    "print(np.mean(df[mask]['weights2']))\n",
    "# print(np.mean(vikane_gt))\n",
    "print(np.mean(langoy_gt))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "count, bins, _ = ax.hist(langoy_gt, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "ax.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10')\n",
    "\n",
    "print(np.mean(df[mask]['weights2']))\n",
    "# print(np.mean(vikane_gt))\n",
    "print(np.mean(langoy_modified))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "count, bins, _ = ax.hist(langoy_modified, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "ax.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "\n",
    "print(np.mean(df[mask]['weights2']))\n",
    "print(np.mean(vikane_gt))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "ax.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "lower, upper = np.percentile(df[mask]['weights2'], 10), np.percentile(df[mask]['weights2'], 90)\n",
    "lower, upper = int(math.ceil(lower / 100.0)) * 100, int(math.ceil(upper / 100.0)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = (df[mask]['weights2'] >= lower) & (df[mask]['weights2'] <= upper)\n",
    "_, floc, _ = stats.weibull_min.fit(df[mask][mask2]['depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "buckets = np.linspace(lower, upper, 100)\n",
    "\n",
    "results = []\n",
    "\n",
    "for bucket in buckets:\n",
    "    min_bucket = bucket - 1000\n",
    "    max_bucket = bucket + 1000\n",
    "    mask3 = (df[mask].estimated_weight_g > min_bucket) & (df[mask].estimated_weight_g < max_bucket)\n",
    "    res = stats.weibull_min.fit(df[mask][mask3].depth, floc = floc)\n",
    "    results.append(res)\n",
    "    \n",
    "results = np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "Y0 = results[:,0]\n",
    "Y2 = results[:,2]\n",
    "X = buckets\n",
    "X = sm.add_constant(X)\n",
    "model0 = sm.OLS(Y0,X)\n",
    "model2 = sm.OLS(Y2,X)\n",
    "m0 = model0.fit()\n",
    "m2 = model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(buckets, results[:,0])\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(buckets, results[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(depth, weight):\n",
    "    v0 = m0.predict([1, weight])\n",
    "    v1 = floc\n",
    "    v2 = m2.predict([1, weight])\n",
    "    \n",
    "    x = np.linspace(0, 3, 5000)\n",
    "    x_pdf = stats.weibull_min.pdf(x, v0, v1, v2)\n",
    "    x_max = max(x_pdf)\n",
    "    avg_depth = x[np.argmax(x_pdf)]\n",
    "    \n",
    "    prob = stats.weibull_min.pdf(depth, v0, v1, v2)[0]\n",
    "#     if depth >= avg_depth:\n",
    "#         prob = x_max\n",
    "    \n",
    "    return prob / x_max\n",
    "\n",
    "weights = df[mask].weights2\n",
    "\n",
    "weights_weight = []\n",
    "weights_prob = []\n",
    "\n",
    "for i, row in df[mask].iterrows():\n",
    "    prob = get_prob(row['depth'], row['weights2'])\n",
    "    \n",
    "    if prob < 0.01:\n",
    "        print(row['depth'], row['weights2'])\n",
    "    else:\n",
    "        weights_weight.append(row['weights2'])\n",
    "        weights_prob.append(prob)\n",
    "    \n",
    "weights_weight = np.array(weights_weight)\n",
    "weights_prob = np.array(weights_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.mean(weights)\n",
    "w2 = np.sum(weights_weight / weights_prob) / np.sum(1 / weights_prob)\n",
    "wgt = np.mean(vikane_gt)\n",
    "\n",
    "print(w1, w2, wgt)\n",
    "print((w1 - wgt) / wgt)\n",
    "print((w2 - wgt) / wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "\n",
    "counts = []\n",
    "\n",
    "mask2 = df[mask].depth < 2\n",
    "\n",
    "for index, _ in enumerate(counts1):\n",
    "    if index == len(bins) - 1:\n",
    "        continue\n",
    "        \n",
    "    bin_lower = bins[index]\n",
    "    bin_upper = bins[index + 1]\n",
    "        \n",
    "    mask3 = (weights_weight[mask2] >= bin_lower) & (weights_weight[mask2] < bin_upper)\n",
    "    count = np.sum(1 / weights_prob[mask2][mask3])\n",
    "    counts.append(count)\n",
    "    \n",
    "counts = np.array(counts) / np.sum(1 / weights_prob[mask2])\n",
    "\n",
    "counts2, bins2, _ = plt.hist(bins[0:-1], weights = counts, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') & (df.depth <= np.median(df.depth))\n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') & (df.prob > 1)\n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') & (df.prob < 0.5)\n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') \n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "x = np.linspace(0, 3, 5000)\n",
    "plt.plot(x, stats.weibull_min.pdf(x, *stats.weibull_min.fit(df[mask]['depth'])))\n",
    "plt.hist(df[mask]['depth'], bins = 30, alpha = 0.5, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.probplot(df[mask].depth, dist=stats.weibull_min, sparams=stats.weibull_min.fit(df[mask]['depth']), plot=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-29')\n",
    "plt.hist(df[mask]['depth'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 15) & (df.hour <= 17) & (df.date == '2020-08-29')\n",
    "plt.hist(df[mask]['depth'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 7) & (df.hour <= 14) & (df.date == '2020-08-29')\n",
    "plt.hist(df[mask]['depth'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 6) & (df.date == '2020-08-25')\n",
    "plt.scatter(df[mask]['depth'], df[mask]['weights2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.weights2 > 7000)].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    L = row['length']\n",
    "    D = row['depth']\n",
    "    V = L\n",
    "    F = 1\n",
    "\n",
    "    p = (1.04 * D - L)*F / V\n",
    "    probs.append(p)\n",
    "    \n",
    "df['prob'] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "\n",
    "plt.scatter(df[mask]['depth'], df[mask]['weights2'], c = df[mask]['prob'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) \n",
    "\n",
    "plt.scatter(df[mask]['depth'], df[mask]['weights2'], c = df[mask]['prob'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) \n",
    "\n",
    "# Generate fake data\n",
    "x = df[mask]['depth']\n",
    "y = df[mask]['weights2']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date >= '2020-08-20') & (df.date <= '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "\n",
    "# Generate fake data\n",
    "x = df[mask]['depth']\n",
    "y = df[mask]['weights2']\n",
    "l = df[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (vdf.akpd_score > 0.95) & (vdf.hour >= 5) & (vdf.hour <= 16) & (vdf.date == '2020-08-25')\n",
    "\n",
    "# Generate fake data\n",
    "x = vdf[mask]['depth']\n",
    "y = vdf[mask]['weights2']\n",
    "l = vdf[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date >= '2020-08-20') & (df.date <= '2020-08-25')\n",
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date <= '2020-05-10')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10')\n",
    "# mask = mask & (df.depth < np.mean(df.depth))\n",
    "\n",
    "# Generate fake data\n",
    "x = df[mask]['depth']\n",
    "y = df[mask]['weights2']\n",
    "l = df[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (ddf.akpd_score > 0.95) & (ddf.date > '2021-01-20') & (ddf.date <= '2021-02-05')\n",
    "\n",
    "x = ddf[mask]['depth']\n",
    "y = ddf[mask]['estimated_weight_g']\n",
    "\n",
    "mask2 = (ddf.akpd_score > 0.95) & (ddf.date > '2021-02-05') & (ddf.date <= '2021-02-15')\n",
    "\n",
    "x2 = ddf[mask2]['depth']\n",
    "y2 = ddf[mask2]['estimated_weight_g']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x2, y2, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (ddf.akpd_score > 0.95) & (ddf.date > '2021-01-20') & (ddf.date <= '2021-02-05')\n",
    "\n",
    "# Generate fake data\n",
    "x = ddf[mask]['depth']\n",
    "y = ddf[mask]['estimated_weight_g']\n",
    "# l = ddf[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "\n",
    "d = np.arange(0.75, 2.75, 0.01)\n",
    "ax.plot(d, 600 + 600 * d ** 3, 'red')\n",
    "ax.plot(d, 1500 + 300 * d ** 3, 'cyan')\n",
    "\n",
    "plt.xlim([.5, 3])\n",
    "plt.ylim([0, 15000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (ddf.akpd_score > 0.95) & (ddf.date > '2021-01-20') & (ddf.date <= '2021-02-05') & (ddf.depth < 2.2)\n",
    "\n",
    "# Generate fake data\n",
    "x = ddf[mask]['depth']\n",
    "y = ddf[mask]['estimated_weight_g']\n",
    "# l = ddf[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "\n",
    "d = np.arange(0.75, 2.5, 0.01)\n",
    "ax.plot(d, 600 + 600 * d ** 3, 'red')\n",
    "ax.plot(d, 1500 + 300 * d ** 3, 'cyan')\n",
    "\n",
    "plt.xlim([.5, 3])\n",
    "plt.ylim([0, 15000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (ddf.akpd_score > 0.95) & (ddf.date > '2021-02-05') & (ddf.date <= '2021-02-15')\n",
    "\n",
    "# Generate fake data\n",
    "x = ddf[mask]['depth']\n",
    "y = ddf[mask]['estimated_weight_g']\n",
    "# l = ddf[mask]['length']\n",
    "\n",
    "# Calculate the point density\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "\n",
    "d = np.arange(0.75, 2.75, 0.01)\n",
    "ax.plot(d, 600 + 600 * d ** 3, 'red')\n",
    "ax.plot(d, 1500 + 300 * d ** 3, 'cyan')\n",
    "\n",
    "plt.xlim([.5, 3])\n",
    "plt.ylim([0, 15000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = .0001\n",
    "# cutoff = 0\n",
    "np.percentile(x[z > cutoff], 5), np.percentile(x[z > cutoff], 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = .0001\n",
    "# cutoff = 0\n",
    "np.percentile(x[z > cutoff], 5), np.percentile(x[z > cutoff], 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(z), np.mean(z), len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(z), np.mean(z), len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(z), np.mean(z), len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(z[(x > 1) & (x < 1.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(l, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(l, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "\n",
    "d = np.arange(0.6, 1.2, 0.01)\n",
    "ax.plot(d, 5000 * d ** 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x_sd = np.std(x)\n",
    "y_sd = np.std(y)\n",
    "\n",
    "xy_kd = [(x_ / x_sd, y[i] / y_sd) for i, x_ in enumerate(x) ]\n",
    "\n",
    "distance, index = spatial.KDTree(xy_kd).query([(1 / x_sd, 6000 / y_sd)], k = 10)\n",
    "print(distance)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in index[0]:\n",
    "    print(i)\n",
    "    x_, y_ = xy_kd[i]\n",
    "    print(x_ * x_sd, y_ * y_sd, z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x_sd = np.std(x)\n",
    "y_sd = np.std(y)\n",
    "\n",
    "xy_kd = [(x_ / x_sd, y[i] / y_sd) for i, x_ in enumerate(x) ]\n",
    "\n",
    "kd_tree = spatial.KDTree(xy_kd)\n",
    "\n",
    "def get_prob(weight, depth):\n",
    "    distance, index = kd_tree.query([(depth / x_sd, weight / y_sd)], k = 10)\n",
    "    \n",
    "    return np.mean([ z[i] for i in index[0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "\n",
    "weights = df[mask].weights2\n",
    "\n",
    "weights_weight = []\n",
    "weights_prob = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i, row in df[mask].iterrows():\n",
    "    count = count + 1\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print('%i out of %i' % (count, sum(mask)))\n",
    "        \n",
    "#     prob = get_prob(row['weights2'], row['depth'])\n",
    "    prob = get_final_prob(row['weights2'], row['depth'])\n",
    "    \n",
    "#     if prob < 0.01:\n",
    "#         print(row['depth'], row['weights2'])\n",
    "#     else:\n",
    "    weights_weight.append(row['weights2'])\n",
    "    weights_prob.append(prob)\n",
    "    \n",
    "weights_weight = np.array(weights_weight) * 1.1\n",
    "weights_prob = np.array(weights_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10') & (df.depth < 1.2)\n",
    "\n",
    "weights = df[mask].weights2\n",
    "\n",
    "weights_weight = []\n",
    "weights_prob = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i, row in df[mask].iterrows():\n",
    "    count = count + 1\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print('%i out of %i' % (count, sum(mask)))\n",
    "        \n",
    "    prob = get_prob(row['weights2'], row['depth'])\n",
    "#     prob = get_final_prob(row['weights2'], row['depth'])\n",
    "#     prob = 1\n",
    "    \n",
    "    if np.isnan(prob):\n",
    "        print(row['depth'], row['weights2'])\n",
    "    else:\n",
    "        weights_weight.append(row['weights2'])\n",
    "        weights_prob.append(prob)\n",
    "    \n",
    "weights_weight = np.array(weights_weight)\n",
    "weights_prob = np.array(weights_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.mean(weights)\n",
    "w2 = np.sum(weights_weight / weights_prob) / np.sum(1 / weights_prob)\n",
    "# wgt = np.mean(vikane_gt)\n",
    "wgt = np.mean(langoy_gt)\n",
    "\n",
    "print(w1, w2, wgt)\n",
    "print((w1 - wgt) / wgt)\n",
    "print((w2 - wgt) / wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1, bins, _ = plt.hist(vikane_gt, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "\n",
    "counts = []\n",
    "\n",
    "for index, _ in enumerate(counts1):\n",
    "    if index == len(bins) - 1:\n",
    "        continue\n",
    "        \n",
    "    bin_lower = bins[index]\n",
    "    bin_upper = bins[index + 1]\n",
    "        \n",
    "    mask3 = (weights_weight >= bin_lower) & (weights_weight < bin_upper)\n",
    "    count = np.sum(1 / weights_prob[mask3])\n",
    "    counts.append(count)\n",
    "    \n",
    "counts = np.array(counts) / np.sum(1 / weights_prob)\n",
    "\n",
    "counts2, bins2, _ = plt.hist(bins[0:-1], weights = counts, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1, bins, _ = plt.hist(vikane_modified, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "\n",
    "counts = []\n",
    "\n",
    "for index, _ in enumerate(counts1):\n",
    "    if index == len(bins) - 1:\n",
    "        continue\n",
    "        \n",
    "    bin_lower = bins[index]\n",
    "    bin_upper = bins[index + 1]\n",
    "        \n",
    "    mask3 = (weights_weight >= bin_lower) & (weights_weight < bin_upper)\n",
    "    count = np.sum(1 / weights_prob[mask3])\n",
    "    counts.append(count)\n",
    "    \n",
    "counts = np.array(counts) / np.sum(1 / weights_prob)\n",
    "\n",
    "counts2, bins2, _ = plt.hist(bins[0:-1], weights = counts, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1, bins, _ = plt.hist(langoy_modified, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "\n",
    "counts = []\n",
    "\n",
    "for index, _ in enumerate(counts1):\n",
    "    if index == len(bins) - 1:\n",
    "        continue\n",
    "        \n",
    "    bin_lower = bins[index]\n",
    "    bin_upper = bins[index + 1]\n",
    "        \n",
    "    mask3 = (weights_weight >= bin_lower) & (weights_weight < bin_upper)\n",
    "    count = np.sum(1 / weights_prob[mask3])\n",
    "    counts.append(count)\n",
    "    \n",
    "counts = np.array(counts) / np.sum(1 / weights_prob)\n",
    "\n",
    "counts2, bins2, _ = plt.hist(bins[0:-1], weights = counts, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10') & (df.depth >= 1.1) & (df.depth <= 1.6)\n",
    "\n",
    "a, b = np.mean(df[mask]['weights2']), np.mean(langoy_gt)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print((a - b) / b)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "# count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "count, bins, _ = ax.hist(langoy_modified, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "ax.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 16) & (df.date == '2020-08-25')\n",
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10')\n",
    "\n",
    "a, b = np.mean(df[mask]['weights2']), np.mean(langoy_modified)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print((a - b) / b)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "# count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "count, bins, _ = ax.hist(langoy_modified, density = True, bins = 30, alpha = 0.5, color = 'blue')\n",
    "ax.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (vdf.akpd_score > 0.95) & (vdf.hour >= 5) & (vdf.hour <= 16) & (vdf.date == '2020-08-25') & (vdf.depth >= 0.8) & (vdf.depth <= 1.4)\n",
    "\n",
    "a, b = np.mean(vdf[mask]['weights2']), np.mean(vikane_modified)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print((a - b) / b)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "ax.hist(vdf[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (vdf.akpd_score > 0.95) & (vdf.hour >= 5) & (vdf.hour <= 16) & (vdf.date == '2020-08-25')\n",
    "\n",
    "a, b = np.mean(vdf[mask]['weights2']), np.mean(vikane_modified)\n",
    "              \n",
    "print(a)\n",
    "print(b)\n",
    "print((a - b) / b)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "count, bins, _ = ax.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "ax.hist(vdf[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10')\n",
    "mask_alt = (df.akpd_score > 0.95) & (df.hour >= 5) & (df.hour <= 11) & (df.date == '2020-05-10') & (df.depth >= 1.1) & (df.depth <= 1.6)\n",
    "\n",
    "buckets = np.arange(0, 14000, 1000)\n",
    "\n",
    "num = 50\n",
    "\n",
    "sds = np.arange(0, 1000, 100)\n",
    "# sds = np.arange(0, .1, .01)\n",
    "\n",
    "# for sd in sds:\n",
    "#     diff1 = []\n",
    "#     diff2 = []\n",
    "        \n",
    "# for i in np.arange(0, num):\n",
    "sd = 500\n",
    "langoy_m = langoy_gt + np.random.normal(0, sd, len(langoy_gt))\n",
    "#     langoy_m = langoy_gt * np.random.normal(1, sd, len(langoy_gt))\n",
    "\n",
    "\n",
    "\n",
    "for bucket in buckets:\n",
    "    lower_bucket = bucket\n",
    "    upper_bucket = bucket + 1000\n",
    "\n",
    "    mask1 = (langoy_gt >= lower_bucket) & (langoy_gt < upper_bucket)\n",
    "    mask2 = (langoy_m >= lower_bucket) & (langoy_m < upper_bucket)\n",
    "\n",
    "    gt_pct1 = np.sum(mask1) / len(langoy_gt)\n",
    "    gt_pct2 = np.sum(mask2) / len(langoy_m)\n",
    "\n",
    "    mask3 = (df[mask]['weights2'] >= lower_bucket) & (df[mask]['weights2'] < upper_bucket)\n",
    "    a_pct = np.sum(mask3) / np.sum(mask)\n",
    "\n",
    "    mask4 = (df[mask_alt]['weights2'] >= lower_bucket) & (df[mask_alt]['weights2'] < upper_bucket)\n",
    "    b_pct = np.sum(mask4) / np.sum(mask_alt)\n",
    "\n",
    "#     print('%i %0.2f vs %0.2f vs %0.2f' % (bucket, a_pct * 100, gt_pct1 * 100, gt_pct2 * 100))\n",
    "    diff1.append(np.abs(a_pct - gt_pct1) * 100)\n",
    "    diff2.append(np.abs(b_pct - gt_pct1) * 100)\n",
    "\n",
    "    print('%i %0.2f vs %0.2f' % (bucket, (a_pct - gt_pct2) * 100, (b_pct - gt_pct2) * 100))\n",
    "\n",
    "#     print(sd, np.mean(diff1), np.mean(diff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(vikane_modified))\n",
    "\n",
    "print(np.sum(weights_weight / weights_prob) / np.sum(1 / weights_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') \n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 50, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'], bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.akpd_score > 0.95) & (df.hour >= 4) & (df.hour <= 17) & (df.date == '2020-08-25') \n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 50, alpha = 0.5, color = 'blue')\n",
    "plt.hist(df[mask]['weights2'] * 1.02, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vikane_modified = vikane_gt + np.random.normal(0, 500, len(vikane_gt))\n",
    "\n",
    "count, bins, _ = plt.hist(vikane_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(vikane_modified, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langoy_modified = langoy_gt + np.random.normal(0, 500, len(langoy_gt))\n",
    "\n",
    "count, bins, _ = plt.hist(langoy_gt, density = True, bins = 20, alpha = 0.5, color = 'blue')\n",
    "plt.hist(langoy_modified, bins = bins, density = True, alpha = 0.5, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "def get_prob(weight):\n",
    "    kernel_x = np.arange(depth - 0.25, depth + 0.25, .01) \n",
    "    \n",
    "    base_kernel = np.zeros(len(kernel_x))\n",
    "\n",
    "    mask2 = (y > weight - 500) & (y < weight + 500)\n",
    "\n",
    "    for index, curr_x in enumerate(x[mask2]):\n",
    "        my_pdf = stats.norm.pdf(kernel_x, loc=curr_x, scale = .1) \n",
    "        add_kernel = my_pdf / np.sum(my_pdf)\n",
    "        base_kernel = base_kernel + add_kernel * z[mask2][index]\n",
    "\n",
    "    base_kernel = base_kernel / sum(base_kernel)\n",
    "    \n",
    "    def a(depth):\n",
    "        return find_nearest(base_kernel, depth)\n",
    "    \n",
    "    return a\n",
    "# plt.plot(kernel_x, base_kernel)\n",
    "\n",
    "weight_models = np.arange(0, 9000, 100)\n",
    "prob_models = []\n",
    "\n",
    "for weight in weight_models:\n",
    "    model = get_prob(weight)\n",
    "    \n",
    "    prob_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def get_final_prob(weight, depth):\n",
    "    idx = find_nearest_idx(weight_models, weight)\n",
    "    prob_model = prob_models[idx]\n",
    "    return prob_model(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_final_prob(6000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
