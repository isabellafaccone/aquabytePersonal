{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from research.utils.data_access_utils import S3AccessUtils\n",
    "from report_generation.report_generator import generate_ts_data, SamplingFilter\n",
    "from research.utils.datetime_utils import add_days\n",
    "from report_generation.report_generator import gen_pm_base\n",
    "from population_metrics.smart_metrics import generate_smart_avg_weight, generate_smart_individual_values, ValidationError\n",
    "from filter_optimization.filter_optimization_task import _add_date_hour_columns\n",
    "from research.weight_estimation.keypoint_utils.optics import pixel2world\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3AccessUtils('/root/data', json.load(open(os.environ['AWS_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_names = [\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13',\n",
    "    'bolaks_pen_id_88_2020-02-28_2020-03-10',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17',\n",
    "    'tittelsnes_pen_id_37_2020-06-10_2020-06-24',\n",
    "    'aplavika_pen_id_95_2020-07-10_2020-07-26',\n",
    "#     'kjeppevikholmen_pen_id_5_2019-06-18_2019-07-02',\n",
    "    'silda_pen_id_86_2020-07-02_2020-07-19',\n",
    "    'vikane_pen_id_60_2020-08-10_2020-08-30',\n",
    "    'eldviktaren_pen_id_164_2020-09-21_2020-10-08',\n",
    "#     'habranden_pen_id_100_2020-08-10_2020-08-31',\n",
    "    'varholmen_pen_id_131_2020-08-15_2020-08-30',\n",
    "    'dale_pen_id_143_2020-10-07_2020-10-21',\n",
    "    'djubawik_pen_id_153_2020-11-10_2020-11-26',\n",
    "    'leivsethamran_pen_id_165_2020-10-18_2020-11-13',\n",
    "    'movikodden_pen_id_114_2020-11-03_2020-11-25',\n",
    "    'movikodden_pen_id_167_2020-10-13_2020-10-30',\n",
    "    'slapoya_pen_id_116_2020-10-18_2020-11-08',\n",
    "    'varholmen_pen_id_131_2020-08-15_2020-08-30',\n",
    "    'varholmen_pen_id_151_2020-10-02_2020-10-17',\n",
    "    'varholmen_pen_id_186_2020-10-18_2020-11-02'\n",
    "]\n",
    "\n",
    "cohort_names2 = [\n",
    "    'dale_pen_id_144_2020-12-20_2021-01-11'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_type = {\n",
    "    'seglberget_pen_id_66_2020-05-13_2020-06-13': 'sexton',\n",
    "    'bolaks_pen_id_88_2020-02-28_2020-03-10': 'sexton',\n",
    "    'langoy_pen_id_108_2020-05-07_2020-05-17': 'sexton',\n",
    "    'tittelsnes_pen_id_37_2020-06-10_2020-06-24': 'sexton',\n",
    "    'aplavika_pen_id_95_2020-07-10_2020-07-26': 'sexton',\n",
    "#     'kjeppevikholmen_pen_id_5_2019-06-18_2019-07-02': 'sexton',\n",
    "    'silda_pen_id_86_2020-07-02_2020-07-19': 'sexton',\n",
    "    'vikane_pen_id_60_2020-08-10_2020-08-30': 'atlas',\n",
    "    'eldviktaren_pen_id_164_2020-09-21_2020-10-08': 'atlas',\n",
    "#     'habranden_pen_id_100_2020-08-10_2020-08-31': 'imenco',\n",
    "    'varholmen_pen_id_131_2020-08-15_2020-08-30': 'imenco',\n",
    "    'dale_pen_id_143_2020-10-07_2020-10-21': 'atlas',\n",
    "    'djubawik_pen_id_153_2020-11-10_2020-11-26': 'atlas',\n",
    "    'leivsethamran_pen_id_165_2020-10-18_2020-11-13': 'atlas',\n",
    "    'movikodden_pen_id_114_2020-11-03_2020-11-25': 'imenco',\n",
    "    'movikodden_pen_id_167_2020-10-13_2020-10-30': 'imenco',\n",
    "    'slapoya_pen_id_116_2020-10-18_2020-11-08': 'imenco',\n",
    "    'varholmen_pen_id_131_2020-08-15_2020-08-30': 'imenco',\n",
    "    'varholmen_pen_id_151_2020-10-02_2020-10-17': 'imenco',\n",
    "    'varholmen_pen_id_186_2020-10-18_2020-11-02': 'atlas',\n",
    "    \n",
    "    \n",
    "    'dale_pen_id_144_2020-12-20_2021-01-11': 'atlas'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['varholmen_pen_id_186_2020-10-18_2020-11-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = 'test'\n",
    "\n",
    "ROOT_DIR = '/root/data/alok/biomass_estimation/playground'\n",
    "dfs, gt_metadatas = {}, {}\n",
    "for cohort_name in cohort_names:\n",
    "    s3_dir = os.path.join(\n",
    "        'https://aquabyte-images-adhoc.s3-eu-west-1.amazonaws.com/alok/production_datasets',\n",
    "        cohort_name\n",
    "    )\n",
    "\n",
    "    ground_truth_metadata_url = os.path.join(s3_dir, 'ground_truth_metadata.json')\n",
    "    ground_truth_key_base = os.path.join(batch_name, cohort_name, 'ground_truth_metadata.json')\n",
    "#     ground_truth_metadata_url = os.path.join(s3_dir, 'ground_truth_metadata_validated.json')\n",
    "#     ground_truth_key_base = os.path.join(batch_name, cohort_name, 'ground_truth_metadata_validated.json')\n",
    "    ground_truth_f = os.path.join(ROOT_DIR, ground_truth_key_base)\n",
    "    print(ground_truth_metadata_url)\n",
    "    s3.download_from_url(ground_truth_metadata_url, custom_location=ground_truth_f)\n",
    "    gt_metadata = json.load(open(ground_truth_f))\n",
    "    gt_metadatas[cohort_name] = gt_metadata\n",
    "    \n",
    "#     data_url = os.path.join(s3_dir, 'annotation_dataset.csv')\n",
    "#     data_f, _, _= s3.download_from_url(data_url)\n",
    "#     df = pd.read_csv(data_f)\n",
    "#     df = _add_date_hour_columns(df)\n",
    "#     dfs[cohort_name] = df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['varholmen_pen_id_186_2020-10-18_2020-11-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_optimization.filter_optimization_task import extract_biomass_data\n",
    "\n",
    "dfs2, gt_metadatas2 = {}, {}\n",
    "\n",
    "cohort_name = 'dale_pen_id_144_2020-12-20_2021-01-11'\n",
    "\n",
    "gt_metadata = {'pen_id': 144,\n",
    " 'gutted_average_weight': 8000,\n",
    " 'gutted_weight_distribution': None,\n",
    " 'expected_loss_factor': 0.16,\n",
    " 'last_feeding_date': '2021-01-11',\n",
    " 'harvest_date': '2021-01-15',\n",
    " 'slaughter_date': '2021-01-15'}\n",
    "\n",
    "gt_metadatas2[cohort_name] = gt_metadata\n",
    "\n",
    "df = extract_biomass_data(gt_metadata['pen_id'], '2021-01-01', '2021-01-12', 0.01)\n",
    "df = _add_date_hour_columns(df)\n",
    "dfs2[cohort_name] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate old / new model weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from research.weight_estimation.keypoint_utils import body_parts\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in body_parts.core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = body_parts.core_body_parts.index(body_parts.UPPER_LIP)\n",
    "    tail_notch_idx = body_parts.core_body_parts.index(body_parts.TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('weight_v1', 'curr-nonsynthetic', 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/playground/nn_epoch_798_v2.pb', True),\n",
    "    ('weight_v2', 'curr-synthetic', 'https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb', True),\n",
    "    ('weight_v3', 'nojitter-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton.pb', False),\n",
    "    ('weight_v4', 'jitter-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton2.pb', False),\n",
    "    ('weight_v5', 'jitter-nools', '/root/data/alok/biomass_estimation/playground/output_model_bryton3.pb', False),\n",
    "    ('weight_v6', 'augV1-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton4.pb', False),\n",
    "    ('weight_v7', 'augV1-nools', '/root/data/alok/biomass_estimation/playground/output_model_bryton5.pb', False),\n",
    "    ('weight_v8', 'augV2-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton6.pb', False),\n",
    "    ('weight_v9', 'augV3-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton7.pb', False),\n",
    "    ('weight_v10', 'augV1-bidir-ols', '/root/data/alok/biomass_estimation/playground/output_model_bryton8.pb', False),\n",
    "    ('weight_v11', 'augV2-ols-akpd', '/root/data/alok/biomass_estimation/playground/output_model_bryton9.pb', False),\n",
    "    ('weight_v12', 'augV1-ols-akpd-halfinfl', '/root/data/alok/biomass_estimation/playground/output_model_bryton10.pb', False),\n",
    "    ('weight_v13', 'augV2-ols-akpd-halfinfl', '/root/data/alok/biomass_estimation/playground/output_model_bryton11.pb', False),\n",
    "    ('weight_v14', 'augV3-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton12.pb', False),\n",
    "    ('weight_v15', 'augV4-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton13.pb', False),\n",
    "    ('weight_v16', 'augV4-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton14.pb', False),\n",
    "    ('weight_v17', 'augV4-o-a-h2-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton15.pb', False),\n",
    "    ('weight_v18', 'augV4-o-a-h3-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton16.pb', False),\n",
    "    ('weight_v19', 'augV4-o-a-h4-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton17.pb', False),\n",
    "    ('weight_v20', 'augV4-o-a-h5-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton18.pb', False),\n",
    "    ('weight_v21', 'augV4-j2-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton19.pb', False),\n",
    "    ('weight_v22', 'augV4-j3-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton20.pb', False),\n",
    "    ('weight_v23', 'augV4-j4-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton21.pb', False),\n",
    "    ('weight_v24', 'augV4-o-a-h-99#2', '/root/data/alok/biomass_estimation/playground/output_model_bryton22.pb', False),\n",
    "    ('weight_v25', 'noaugV4-o-a-h-99', '/root/data/alok/biomass_estimation/playground/output_model_bryton23.pb', False),\n",
    "    ('weight_v26', 'augV4-j2-o-a-h-99#2', '/root/data/alok/biomass_estimation/playground/output_model_bryton24.pb', False),\n",
    "    ('weight_v27', 'augV1-alokj-o-a-h-99#2', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a1.pb', False),\n",
    "    ('weight_v28', 'augV1-alokj-o-a-h-99-t', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a2.pb', False),\n",
    "    ('weight_v29', 'augV1-alokj-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a3.pb', False),\n",
    "    ('weight_v30', 'augV1-o-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a4.pb', False),\n",
    "    ('weight_v31', 'augV4-alokj-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a5.pb', False),\n",
    "    ('weight_v32', 'augV4-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a6.pb', False)\n",
    "]\n",
    "\n",
    "additional_models = [\n",
    "    ('weight_v31', 'augV4-alokj-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a5.pb', False),\n",
    "    ('weight_v32', 'augV4-o-a-h-90', '/root/data/alok/biomass_estimation/playground/output_model_bryton_a6.pb', False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsf = pd.read_csv('/root/data/alok/biomass_estimation/playground/gtsf_akpr2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, tag, model_url, is_url in additional_models:\n",
    "    if is_url:\n",
    "        weight_model_f, _, _ = s3.download_from_url(model_url)\n",
    "    else:\n",
    "        weight_model_f = model_url\n",
    "    kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "    weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "    \n",
    "    weights = []\n",
    "\n",
    "    for idx, row in gtsf.iterrows():\n",
    "        annotation = json.loads(row.keypoints.replace(\"'\", '\"'))\n",
    "        if not annotation:\n",
    "            weights.append(None)\n",
    "            continue\n",
    "        camera_metadata = json.loads(row.camera_metadata.replace(\"'\", '\"'))\n",
    "        if not camera_metadata:\n",
    "            camera_metadata = json.loads(rdf.camera_metadata.iloc[0].replace(\"'\", '\"'))\n",
    "\n",
    "        camera_metadata_obj = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "\n",
    "        weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "        weights.append(weight)\n",
    "\n",
    "    gtsf[key] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "for key, tag, model_url, is_url in models:\n",
    "# plt.scatter(gtsf.weight, gtsf['weight_v11'])\n",
    "    X = gtsf[key]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(gtsf.weight, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    print('%s: %0.2f, %0.2f' % (tag, 100 * results.rsquared, 100 * results.params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = rdf.iloc[0]\n",
    "\n",
    "annotation = json.loads(row.annotation.replace(\"'\", '\"'))\n",
    "camera_metadata = json.loads(row.camera_metadata.replace(\"'\", '\"'))\n",
    "if not camera_metadata:\n",
    "    camera_metadata = json.loads(rdf.camera_metadata.iloc[0].replace(\"'\", '\"'))\n",
    "\n",
    "camera_metadata_obj = CameraMetadata(\n",
    "    focal_length=camera_metadata['focalLength'],\n",
    "    focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "    baseline_m=camera_metadata['baseline'],\n",
    "    pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "    pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "    image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "    image_sensor_height=camera_metadata['imageSensorHeight']\n",
    ")\n",
    "\n",
    "weight = weight_estimator._get_model_input(annotation, camera_metadata_obj)\n",
    "weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, tag, model_url, is_url in additional_models:\n",
    "    # weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "    if is_url:\n",
    "        weight_model_f, _, _ = s3.download_from_url(model_url)\n",
    "    else:\n",
    "        weight_model_f = model_url\n",
    "    kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "    weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "\n",
    "    for k, rdf in dfs.items():\n",
    "        print(k)\n",
    "        weights = []\n",
    "        count = 0\n",
    "        for idx, row in rdf.iterrows():\n",
    "            if count % 100 == 0:\n",
    "                print('Percentage completion: {}%'.format(round(100 * count / rdf.shape[0], 2)))\n",
    "                print(count)\n",
    "            count += 1\n",
    "            annotation = json.loads(row.annotation.replace(\"'\", '\"'))\n",
    "            if not annotation:\n",
    "                weights.append(None)\n",
    "                continue\n",
    "            camera_metadata = json.loads(row.camera_metadata.replace(\"'\", '\"'))\n",
    "            if not camera_metadata:\n",
    "                camera_metadata = json.loads(rdf.camera_metadata.iloc[0].replace(\"'\", '\"'))\n",
    "\n",
    "            camera_metadata_obj = CameraMetadata(\n",
    "                focal_length=camera_metadata['focalLength'],\n",
    "                focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "                baseline_m=camera_metadata['baseline'],\n",
    "                pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "                pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "                image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "                image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "            )\n",
    "\n",
    "            weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "            weights.append(weight)\n",
    "        rdf[key] = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, tag, model_url, is_url in models:\n",
    "    # weight_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/biomass/trained_models/2020-11-27T00-00-00/weight_model_synthetic_data.pb')\n",
    "    if is_url:\n",
    "        weight_model_f, _, _ = s3.download_from_url(model_url)\n",
    "    else:\n",
    "        weight_model_f = model_url\n",
    "    kf_model_f, _, _ = s3.download_from_url('https://aquabyte-models.s3-us-west-1.amazonaws.com/k-factor/playground/kf_predictor_v2.pb')\n",
    "\n",
    "    weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "\n",
    "    for k, rdf in dfs2.items():\n",
    "        print(k)\n",
    "        weights = []\n",
    "        count = 0\n",
    "        for idx, row in rdf.iterrows():\n",
    "            if count % 100 == 0:\n",
    "                print('Percentage completion: {}%'.format(round(100 * count / rdf.shape[0], 2)))\n",
    "                print(count)\n",
    "            count += 1\n",
    "            annotation = row.annotation\n",
    "            if not annotation:\n",
    "                weights.append(None)\n",
    "                continue\n",
    "            camera_metadata = row.camera_metadata\n",
    "            if not camera_metadata:\n",
    "                camera_metadata = rdf.camera_metadata\n",
    "\n",
    "            camera_metadata_obj = CameraMetadata(\n",
    "                focal_length=camera_metadata['focalLength'],\n",
    "                focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "                baseline_m=camera_metadata['baseline'],\n",
    "                pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "                pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "                image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "                image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "            )\n",
    "\n",
    "            weight, length, kf = weight_estimator.predict(annotation, camera_metadata_obj)\n",
    "            weights.append(weight)\n",
    "        rdf[key] = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate average weight accuracy with old model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding):\n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    date = add_days(last_feeding_date, days_post_feeding)\n",
    "    weights, _ = generate_smart_individual_values(pm_base, date, max_day_diff, True, apply_growth_rate, 0.9)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def generate_average_weight_accuracy(weights, gt_metadata, loss_factor):\n",
    "    avg_weight_prediction = np.mean(weights)\n",
    "    gutted_weight_prediction = avg_weight_prediction * (1.0 - loss_factor)\n",
    "    gt_weight = gt_metadata['gutted_average_weight']\n",
    "    avg_weight_err = (gutted_weight_prediction - gt_weight) / gt_weight\n",
    "    return avg_weight_err, gutted_weight_prediction\n",
    "\n",
    "def generate_distribution_accuracy(weights, gt_metadata, loss_factor):\n",
    "    gutted_weights = weights * (1.0 - loss_factor)\n",
    "    gutted_weight_distribution = gt_metadata['gutted_weight_distribution']\n",
    "    \n",
    "    if gutted_weight_distribution is None:\n",
    "        return []\n",
    "    \n",
    "    count_distribution_errors = []\n",
    "    \n",
    "    for bucket in gutted_weight_distribution:\n",
    "        lower_bound, upper_bound = bucket.split('-')\n",
    "        pct = gutted_weight_distribution[bucket]\n",
    "        mask = (gutted_weights >= float(lower_bound) * 1000) & (gutted_weights < float(upper_bound) * 1000)\n",
    "\n",
    "        pct = np.sum(mask) / len(mask)\n",
    "        gt_pct = gutted_weight_distribution[bucket] / 100\n",
    "        \n",
    "        count_distribution_errors.append(pct - gt_pct)\n",
    "        \n",
    "    return count_distribution_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "start_index = np.where(bins == 10)[0][0]\n",
    "start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "lower_index = start_array\n",
    "upper_index = start_array\n",
    "\n",
    "while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "    lower_index = lower_index - 1\n",
    "while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "    upper_index = upper_index + 1\n",
    "    \n",
    "print(bins[idx_values[lower_index]], bins[idx_values[upper_index]])\n",
    "# min_idx, max_idx = idx_values[[0, -1]]\n",
    "# print(bins[min_idx], bins[max_idx])\n",
    "# bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "#count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "start_hour = np.min(df2.hour)\n",
    "end_hour = np.max(df2.hour)\n",
    "\n",
    "bins = np.arange(start_hour, end_hour + 1)\n",
    "\n",
    "weights = []\n",
    "\n",
    "for hour in np.arange(start_hour, end_hour + 1):\n",
    "    avg_weight = np.mean(df2[df2.hour == hour].estimated_weight_g)\n",
    "    weights.append(avg_weight)\n",
    "\n",
    "start_index = np.where(bins == 10)[0][0]\n",
    "\n",
    "lower_index = start_index\n",
    "upper_index = start_index\n",
    "\n",
    "is_iterating = True\n",
    "eps = 3\n",
    "\n",
    "while is_iterating:\n",
    "#     print(np.std(weights[lower_index:upper_index]))\n",
    "    if lower_index > 0 and upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index + 1]):\n",
    "        lower_index = lower_index - 1\n",
    "        upper_index = upper_index + 1\n",
    "    elif lower_index > 0 and np.abs(weights[upper_index] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index]):\n",
    "        lower_index = lower_index - 1\n",
    "    elif upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index]) < eps * np.std(weights[lower_index:upper_index + 1]):\n",
    "        upper_index = upper_index + 1\n",
    "    else:\n",
    "        is_iterating = False\n",
    "        \n",
    "start_hour, end_hour = bins[lower_index], bins[upper_index]\n",
    "\n",
    "plt.plot(bins, weights)\n",
    "\n",
    "print(start_hour, end_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['leivsethamran_pen_id_165_2020-10-18_2020-11-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "last_feeding_date = gt_metadatas['dale_pen_id_143_2020-10-07_2020-10-21']['last_feeding_date']\n",
    "slaughter_date = gt_metadatas['dale_pen_id_143_2020-10-07_2020-10-21']['slaughter_date']\n",
    "\n",
    "date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "date_diff.days\n",
    "# gt_metadatas['dale_pen_id_143_2020-10-07_2020-10-21']['expected_loss_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs1 = []\n",
    "all_dfs2 = []\n",
    "all_dfs3 = []\n",
    "all_dfs4 = []\n",
    "all_dfs5 = []\n",
    "all_dfs6 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, tag, _, _ in additional_models:\n",
    "    start_hours = [7]\n",
    "    end_hours = [15]\n",
    "    apply_growth_rate = True\n",
    "    max_day_diff = 3\n",
    "    days_post_feeding = 1\n",
    "    final_days_post_feeding = 1\n",
    "    loss_factors = [0.16, 'expected_loss_factor'] # need to determine the right values here\n",
    "    akpd_cutoffs = [0.01, 0.95]\n",
    "\n",
    "    hour_filter_methods = ['manual', 'hour_hist', 'u-shape'] #  'u-shape',\n",
    "\n",
    "    cohort_name_col = []\n",
    "    akpd_cutoff_col = []\n",
    "    hour_filter_method_col = []\n",
    "    start_hour_col = []\n",
    "    end_hour_col = []\n",
    "    loss_factor_col = []\n",
    "    starvation_days_col = []\n",
    "    avg_weight_col = []\n",
    "    avg_weight_error_col = []\n",
    "    gt_avg_weight_col = []\n",
    "    count_distribution_error_col = []\n",
    "    camera_col = []\n",
    "\n",
    "    for loss_factor in loss_factors:\n",
    "        avg_weight_error_col.append([])\n",
    "        avg_weight_error_col.append([])\n",
    "        count_distribution_error_col.append([])\n",
    "\n",
    "    for cohort_name in sorted(list(dfs.keys())):\n",
    "        print(cohort_name)\n",
    "        \n",
    "        gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "        last_feeding_date = gt_metadata['last_feeding_date']\n",
    "        slaughter_date = gt_metadata['slaughter_date']\n",
    "\n",
    "        if slaughter_date is not None and last_feeding_date is not None:\n",
    "            date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "            starvation_days = date_diff.days\n",
    "        else:\n",
    "            starvation_days = None\n",
    "\n",
    "        df = dfs[cohort_name]\n",
    "        df['estimated_weight_g'] = df[key]\n",
    "        final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "        tdf = df[df.date <= final_date_post_feeding]\n",
    "\n",
    "        start_end_hours = []\n",
    "\n",
    "        for method in hour_filter_methods:\n",
    "            if method == 'manual':\n",
    "                for start_hour in start_hours:\n",
    "                    for end_hour in end_hours:\n",
    "                        start_end_hours.append((method, start_hour, end_hour))\n",
    "            elif method == 'u-shape':\n",
    "                df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "                #count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "                start_hour = np.min(df2.hour)\n",
    "                end_hour = np.max(df2.hour)\n",
    "\n",
    "                bins = np.arange(start_hour, end_hour + 1)\n",
    "\n",
    "                weights = []\n",
    "\n",
    "                for hour in np.arange(start_hour, end_hour + 1):\n",
    "                    avg_weight = np.mean(df2[df2.hour == hour].estimated_weight_g)\n",
    "                    weights.append(avg_weight)\n",
    "\n",
    "                start_index = np.where(bins == 10)[0][0]\n",
    "\n",
    "                lower_index = start_index\n",
    "                upper_index = start_index\n",
    "\n",
    "                is_iterating = True\n",
    "                eps = 3\n",
    "\n",
    "                while is_iterating:\n",
    "                #     print(np.std(weights[lower_index:upper_index]))\n",
    "                    if lower_index > 0 and upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index + 1]):\n",
    "                        lower_index = lower_index - 1\n",
    "                        upper_index = upper_index + 1\n",
    "                    elif lower_index > 0 and np.abs(weights[upper_index] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index]):\n",
    "                        lower_index = lower_index - 1\n",
    "                    elif upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index]) < eps * np.std(weights[lower_index:upper_index + 1]):\n",
    "                        upper_index = upper_index + 1\n",
    "                    else:\n",
    "                        is_iterating = False\n",
    "\n",
    "                start_hour, end_hour = bins[lower_index], bins[upper_index]\n",
    "                \n",
    "                start_end_hours.append((method, start_hour, end_hour))\n",
    "            elif method == 'hour_hist':\n",
    "                df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "                count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "                idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "                start_index = np.where(bins == 10)[0][0]\n",
    "                start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "                lower_index = start_array\n",
    "                upper_index = start_array\n",
    "\n",
    "                while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "                    lower_index = lower_index - 1\n",
    "                while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "                    upper_index = upper_index + 1\n",
    "\n",
    "                start_hour, end_hour = bins[idx_values[lower_index]], bins[idx_values[upper_index]]\n",
    "\n",
    "                start_end_hours.append((method, start_hour, end_hour))\n",
    "\n",
    "        for akpd_cutoff in akpd_cutoffs:\n",
    "            for method, start_hour, end_hour in start_end_hours:\n",
    "                sampling_filter = SamplingFilter(\n",
    "                    start_hour=start_hour,\n",
    "                    end_hour=end_hour,\n",
    "                    kf_cutoff=0.0,\n",
    "                    akpd_score_cutoff=akpd_cutoff\n",
    "                )\n",
    "\n",
    "                pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "                try:\n",
    "                    weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "                except ValidationError as err:\n",
    "                    continue\n",
    "\n",
    "                akpd_cutoff_col.append(akpd_cutoff)\n",
    "                cohort_name_col.append(cohort_name)\n",
    "                hour_filter_method_col.append(method)\n",
    "                start_hour_col.append(start_hour)\n",
    "                end_hour_col.append(end_hour)\n",
    "                loss_factor_col.append(gt_metadata['expected_loss_factor'])\n",
    "                starvation_days_col.append(starvation_days)\n",
    "                avg_weight_col.append(np.mean(weights))\n",
    "                gt_avg_weight_col.append(gt_metadata['gutted_average_weight'])\n",
    "                camera_col.append(camera_type[cohort_name])\n",
    "\n",
    "                for index, loss_factor in enumerate(loss_factors):\n",
    "                    if loss_factor == 'expected_loss_factor':\n",
    "                        loss_factor = gt_metadata['expected_loss_factor'] or 0.165\n",
    "\n",
    "                        if loss_factor > 10:\n",
    "                            loss_factor = loss_factor / 100.0\n",
    "\n",
    "                    avg_weight_err, gutted_weight_prediction = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                    avg_weight_error_col[index].append(avg_weight_err)\n",
    "\n",
    "                    count_distribution_errors = generate_distribution_accuracy(weights, gt_metadata, loss_factor)\n",
    "                    count_distribution_error_col[index].append(count_distribution_errors)\n",
    "                    \n",
    "    columns = {\n",
    "        'cohort_name': cohort_name_col,\n",
    "        'hour_filter_method_col': hour_filter_method_col,\n",
    "        'akpd_cutoff_col': akpd_cutoff_col,\n",
    "        'start_hour_col': start_hour_col,\n",
    "        'end_hour_col': end_hour_col,\n",
    "        'loss_factor_col': loss_factor_col,\n",
    "        'starvation_days_col': starvation_days_col,\n",
    "        'avg_weight_col': avg_weight_col,\n",
    "        'gt_avg_weight_col': gt_avg_weight_col,\n",
    "        'camera_col': camera_col\n",
    "    }\n",
    "\n",
    "    for index, loss_factor in enumerate(loss_factors):\n",
    "        if loss_factor == 'expected_loss_factor':\n",
    "            col_name = 'avg_weight_error_exp'\n",
    "            col_abs_name = 'avg_weight_error_abs_exp'\n",
    "            col_abs_dist_name = 'avg_count_dist_error_abs_exp'\n",
    "        else:\n",
    "            col_name = 'avg_weight_error_%0.2f' % (loss_factor,)\n",
    "            col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "            col_abs_dist_name = 'avg_count_dist_error_abs_%0.2f' % (loss_factor,)\n",
    "\n",
    "        columns[col_name] = avg_weight_error_col[index]\n",
    "        columns[col_abs_name] = np.abs(avg_weight_error_col[index])\n",
    "        columns[col_abs_dist_name] = [np.mean(np.abs(l)) for l in count_distribution_error_col[index]]\n",
    "\n",
    "    tdf = pd.DataFrame(columns)\n",
    "    \n",
    "    df1 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'manual')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    df2 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    df3 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'u-shape')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    df4 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'manual')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    df5 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    df6 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'u-shape')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n",
    "    \n",
    "    all_dfs1.append(df1)\n",
    "    all_dfs2.append(df2)\n",
    "    all_dfs3.append(df3)\n",
    "    all_dfs4.append(df4)\n",
    "    all_dfs5.append(df5)\n",
    "    all_dfs6.append(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs21 = []\n",
    "all_dfs22 = []\n",
    "all_dfs23 = []\n",
    "all_dfs24 = []\n",
    "all_dfs25 = []\n",
    "all_dfs26 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, tag, _, _ in additional_models:\n",
    "    start_hours = [7]\n",
    "    end_hours = [15]\n",
    "    apply_growth_rate = True\n",
    "    max_day_diff = 3\n",
    "    days_post_feeding = 1\n",
    "    final_days_post_feeding = 1\n",
    "    loss_factors = [0.16, 'expected_loss_factor'] # need to determine the right values here\n",
    "    akpd_cutoffs = [0.01, 0.95]\n",
    "\n",
    "    hour_filter_methods = ['manual', 'hour_hist', 'u-shape'] #  'u-shape',\n",
    "\n",
    "    cohort_name_col = []\n",
    "    akpd_cutoff_col = []\n",
    "    hour_filter_method_col = []\n",
    "    start_hour_col = []\n",
    "    end_hour_col = []\n",
    "    loss_factor_col = []\n",
    "    starvation_days_col = []\n",
    "    avg_weight_col = []\n",
    "    avg_weight_error_col = []\n",
    "    gt_avg_weight_col = []\n",
    "    count_distribution_error_col = []\n",
    "    camera_col = []\n",
    "\n",
    "    for loss_factor in loss_factors:\n",
    "        avg_weight_error_col.append([])\n",
    "        avg_weight_error_col.append([])\n",
    "        count_distribution_error_col.append([])\n",
    "\n",
    "    for cohort_name in sorted(list(dfs2.keys())):\n",
    "        print(cohort_name)\n",
    "        \n",
    "        gt_metadata = gt_metadatas2[cohort_name]\n",
    "\n",
    "        last_feeding_date = gt_metadata['last_feeding_date']\n",
    "        slaughter_date = gt_metadata['slaughter_date']\n",
    "\n",
    "        if slaughter_date is not None and last_feeding_date is not None:\n",
    "            date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "            starvation_days = date_diff.days\n",
    "        else:\n",
    "            starvation_days = None\n",
    "\n",
    "        df = dfs2[cohort_name]\n",
    "        df['estimated_weight_g'] = df[key]\n",
    "        final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "        tdf = df[df.date <= final_date_post_feeding]\n",
    "\n",
    "        start_end_hours = []\n",
    "\n",
    "        for method in hour_filter_methods:\n",
    "            if method == 'manual':\n",
    "                for start_hour in start_hours:\n",
    "                    for end_hour in end_hours:\n",
    "                        start_end_hours.append((method, start_hour, end_hour))\n",
    "            elif method == 'u-shape':\n",
    "                df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "                #count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "                start_hour = np.min(df2.hour)\n",
    "                end_hour = np.max(df2.hour)\n",
    "\n",
    "                bins = np.arange(start_hour, end_hour + 1)\n",
    "\n",
    "                weights = []\n",
    "\n",
    "                for hour in np.arange(start_hour, end_hour + 1):\n",
    "                    avg_weight = np.mean(df2[df2.hour == hour].estimated_weight_g)\n",
    "                    weights.append(avg_weight)\n",
    "\n",
    "                start_index = np.where(bins == 10)[0][0]\n",
    "\n",
    "                lower_index = start_index\n",
    "                upper_index = start_index\n",
    "\n",
    "                is_iterating = True\n",
    "                eps = 3\n",
    "\n",
    "                while is_iterating:\n",
    "                #     print(np.std(weights[lower_index:upper_index]))\n",
    "                    if lower_index > 0 and upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index + 1]):\n",
    "                        lower_index = lower_index - 1\n",
    "                        upper_index = upper_index + 1\n",
    "                    elif lower_index > 0 and np.abs(weights[upper_index] - weights[lower_index - 1]) < eps * np.std(weights[lower_index - 1:upper_index]):\n",
    "                        lower_index = lower_index - 1\n",
    "                    elif upper_index < len(weights) - 1 and np.abs(weights[upper_index + 1] - weights[lower_index]) < eps * np.std(weights[lower_index:upper_index + 1]):\n",
    "                        upper_index = upper_index + 1\n",
    "                    else:\n",
    "                        is_iterating = False\n",
    "\n",
    "                start_hour, end_hour = bins[lower_index], bins[upper_index]\n",
    "                \n",
    "                start_end_hours.append((method, start_hour, end_hour))\n",
    "            elif method == 'hour_hist':\n",
    "                df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "                count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "                idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "                start_index = np.where(bins == 10)[0][0]\n",
    "                start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "                lower_index = start_array\n",
    "                upper_index = start_array\n",
    "\n",
    "                while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "                    lower_index = lower_index - 1\n",
    "                while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "                    upper_index = upper_index + 1\n",
    "\n",
    "                start_hour, end_hour = bins[idx_values[lower_index]], bins[idx_values[upper_index]]\n",
    "\n",
    "                start_end_hours.append((method, start_hour, end_hour))\n",
    "\n",
    "        for akpd_cutoff in akpd_cutoffs:\n",
    "            for method, start_hour, end_hour in start_end_hours:\n",
    "                sampling_filter = SamplingFilter(\n",
    "                    start_hour=start_hour,\n",
    "                    end_hour=end_hour,\n",
    "                    kf_cutoff=0.0,\n",
    "                    akpd_score_cutoff=akpd_cutoff\n",
    "                )\n",
    "\n",
    "                pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "                try:\n",
    "                    weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "                except ValidationError as err:\n",
    "                    continue\n",
    "\n",
    "                akpd_cutoff_col.append(akpd_cutoff)\n",
    "                cohort_name_col.append(cohort_name)\n",
    "                hour_filter_method_col.append(method)\n",
    "                start_hour_col.append(start_hour)\n",
    "                end_hour_col.append(end_hour)\n",
    "                loss_factor_col.append(gt_metadata['expected_loss_factor'])\n",
    "                starvation_days_col.append(starvation_days)\n",
    "                avg_weight_col.append(np.mean(weights))\n",
    "                gt_avg_weight_col.append(gt_metadata['gutted_average_weight'])\n",
    "                camera_col.append(camera_type[cohort_name])\n",
    "\n",
    "                for index, loss_factor in enumerate(loss_factors):\n",
    "                    if loss_factor == 'expected_loss_factor':\n",
    "                        loss_factor = gt_metadata['expected_loss_factor'] or 0.165\n",
    "\n",
    "                        if loss_factor > 10:\n",
    "                            loss_factor = loss_factor / 100.0\n",
    "\n",
    "                    avg_weight_err, gutted_weight_prediction = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                    avg_weight_error_col[index].append(avg_weight_err)\n",
    "\n",
    "                    count_distribution_errors = generate_distribution_accuracy(weights, gt_metadata, loss_factor)\n",
    "                    count_distribution_error_col[index].append(count_distribution_errors)\n",
    "                    \n",
    "    columns = {\n",
    "        'cohort_name': cohort_name_col,\n",
    "        'hour_filter_method_col': hour_filter_method_col,\n",
    "        'akpd_cutoff_col': akpd_cutoff_col,\n",
    "        'start_hour_col': start_hour_col,\n",
    "        'end_hour_col': end_hour_col,\n",
    "        'loss_factor_col': loss_factor_col,\n",
    "        'starvation_days_col': starvation_days_col,\n",
    "        'avg_weight_col': avg_weight_col,\n",
    "        'gt_avg_weight_col': gt_avg_weight_col,\n",
    "        'camera_col': camera_col\n",
    "    }\n",
    "\n",
    "    for index, loss_factor in enumerate(loss_factors):\n",
    "        if loss_factor == 'expected_loss_factor':\n",
    "            col_name = 'avg_weight_error_exp'\n",
    "            col_abs_name = 'avg_weight_error_abs_exp'\n",
    "            col_abs_dist_name = 'avg_count_dist_error_abs_exp'\n",
    "        else:\n",
    "            col_name = 'avg_weight_error_%0.2f' % (loss_factor,)\n",
    "            col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "            col_abs_dist_name = 'avg_count_dist_error_abs_%0.2f' % (loss_factor,)\n",
    "\n",
    "        columns[col_name] = avg_weight_error_col[index]\n",
    "        columns[col_abs_name] = np.abs(avg_weight_error_col[index])\n",
    "        columns[col_abs_dist_name] = [np.mean(np.abs(l)) for l in count_distribution_error_col[index]]\n",
    "\n",
    "    tdf = pd.DataFrame(columns)\n",
    "    \n",
    "    df1 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'manual')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    df2 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    df3 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'u-shape')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    df4 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'manual')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    df5 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    df6 = tdf[(tdf.akpd_cutoff_col == 0.95) & (tdf.hour_filter_method_col == 'u-shape')][['cohort_name', 'avg_weight_col', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp', 'start_hour_col', 'end_hour_col']]\n",
    "    \n",
    "    all_dfs21.append(df1)\n",
    "    all_dfs22.append(df2)\n",
    "    all_dfs23.append(df3)\n",
    "    all_dfs24.append(df4)\n",
    "    all_dfs25.append(df5)\n",
    "    all_dfs26.append(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_dfs21:\n",
    "    print(row['avg_weight_col'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "adj_avg = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    row = all_dfs22[index] \n",
    "    avg_weight = row['avg_weight_col'].values[0]\n",
    "    start_hour = row['start_hour_col'].values[0]\n",
    "    end_hour = row['end_hour_col'].values[0]\n",
    "    \n",
    "    avg.append(avg_weight)\n",
    "    adj_avg.append((1 - avg_under[model[1]]) * avg_weight)\n",
    "    \n",
    "    print(model[1], start_hour, end_hour, (1 - avg_under[model[1]]), avg_weight, (1 - avg_under[model[1]]) * avg_weight, avg_weight / (1 + avg_under[model[1]]))\n",
    "\n",
    "print(np.mean(avg))\n",
    "print(np.mean(adj_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "adj_avg = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    row = all_dfs25[index] \n",
    "    avg_weight = row['avg_weight_col'].values[0]\n",
    "    start_hour = row['start_hour_col'].values[0]\n",
    "    end_hour = row['end_hour_col'].values[0]\n",
    "    \n",
    "    avg.append(avg_weight)\n",
    "    adj_avg.append((1 - avg_under[model[1]]) * avg_weight)\n",
    "    \n",
    "    print(model[1], start_hour, end_hour, (1 - avg_under[model[1]]), avg_weight, (1 - avg_under[model[1]]) * avg_weight, avg_weight / (1 + avg_under[model[1]]))\n",
    "\n",
    "print(np.mean(avg))\n",
    "print(np.mean(adj_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr-synthetic -0.033101807184804044\n",
    "# augV4-o-a-h-99 -0.020492502857156494\n",
    "# augV4-o-a-h-99#2 -0.026963895675136098\n",
    "\n",
    "7895 * 1.033, 8243 * 1.02, 8233 * 1.027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_dfs23:\n",
    "    print(row['avg_weight_col'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_dfs24:\n",
    "    print(row['avg_weight_col'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_dfs25:\n",
    "    print(row['avg_weight_col'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_dfs26:\n",
    "    print(row['avg_weight_col'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['leivsethamran_pen_id_165_2020-10-18_2020-11-13'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in additional_models:\n",
    "#     models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "color=iter(cm.rainbow(np.linspace(0,1,len(models))))\n",
    "\n",
    "all_dfs = all_dfs5\n",
    "\n",
    "avg_under = {}\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    c = next(color)\n",
    "    _, tag, _, _ = model\n",
    "    plt.scatter(all_dfs[index].gt_avg_weight_col, all_dfs[index]['avg_weight_error_0.16'], color = c, label=tag)\n",
    "\n",
    "    mask = all_dfs[index].gt_avg_weight_col > 5000\n",
    "                   \n",
    "    avg_under[model[1]] = np.mean(all_dfs[index]['avg_weight_error_exp'][mask])\n",
    "    \n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "my_models = ['curr-synthetic', 'augV4-o-a-h-99', 'augV4-o-a-h-99#2']\n",
    "\n",
    "color=iter(cm.rainbow(np.linspace(0,1,len(my_models))))\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    \n",
    "    if tag not in my_models: # 'augV1-ols-akpd-halfinfl',\n",
    "        continue\n",
    "    c = next(color)\n",
    "    plt.scatter(all_dfs[index].gt_avg_weight_col, all_dfs2[index]['avg_weight_error_exp'], color = c, label=tag)\n",
    "    \n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "my_models = ['curr-synthetic', 'augV1-ols', 'jitter-ols'] # , 'augV4-o-a-h-99', 'augV4-o-a-h-99#2'\n",
    "\n",
    "color=iter(cm.rainbow(np.linspace(0,1,len(my_models))))\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    \n",
    "    if tag not in my_models: # 'augV1-ols-akpd-halfinfl',\n",
    "        continue\n",
    "    c = next(color)\n",
    "    plt.scatter(all_dfs[index].gt_avg_weight_col, all_dfs5[index]['avg_weight_error_exp'], color = c, label=tag)\n",
    "    \n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs21\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs1\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs2\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs3\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs4\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs5\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = all_dfs6\n",
    "\n",
    "metric = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    _, tag, _, _ = model\n",
    "    print('%-*s: %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f' % (25, tag, 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 90), 100 * np.percentile(np.abs(all_dfs[index]['avg_weight_error_exp']), 50), np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)), 100 * np.mean((all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_0.16'])), 100 * np.mean(np.abs(all_dfs[index]['avg_weight_error_exp'])), 100 * np.std((all_dfs[index]['avg_weight_error_0.16']))))\n",
    "    metric.append(np.sqrt(np.mean((50 * np.abs(all_dfs[index]['avg_weight_error_exp'])) ** 2)))\n",
    "    \n",
    "print(np.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0])\n",
    "all_dfs[4]\n",
    "\n",
    "# dfs['aplavika_pen_id_95_2020-07-10_2020-07-26'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[1])\n",
    "all_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(np.mean(np.abs(df4['avg_weight_error_0.16'])), np.mean(np.abs(df3['avg_weight_error_0.16'])), np.mean(np.abs(df5['avg_weight_error_0.16'])), np.mean(np.abs(df1['avg_weight_error_0.16'])), np.mean(np.abs(df6['avg_weight_error_0.16'])), np.mean(np.abs(df7['avg_weight_error_0.16'])))\n",
    "# # print(np.std((df4['avg_weight_error_0.16'])), np.std((df3['avg_weight_error_0.16'])), np.std((df5['avg_weight_error_0.16'])), np.std((df1['avg_weight_error_0.16'])), np.std((df6['avg_weight_error_0.16'])), np.std((df7['avg_weight_error_0.16'])))\n",
    "\n",
    "# print(np.mean(np.abs(df5['avg_weight_error_0.16'])), np.mean(np.abs(df1['avg_weight_error_0.16'])), np.mean(np.abs(df6['avg_weight_error_0.16'])), np.mean(np.abs(df7['avg_weight_error_0.16'])), np.mean(np.abs(df8['avg_weight_error_0.16'])))\n",
    "# print(np.std((df5['avg_weight_error_0.16'])), np.std((df1['avg_weight_error_0.16'])), np.std((df6['avg_weight_error_0.16'])), np.std((df7['avg_weight_error_0.16'])), np.std((df8['avg_weight_error_0.16'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(np.mean(np.abs(df4['avg_weight_error_exp'])), np.mean(np.abs(df3['avg_weight_error_exp'])), np.mean(np.abs(df5['avg_weight_error_exp'])), np.mean(np.abs(df1['avg_weight_error_exp'])), np.mean(np.abs(df6['avg_weight_error_exp'])), np.mean(np.abs(df7['avg_weight_error_exp'])))\n",
    "# # print(np.std((df4['avg_weight_error_exp'])), np.std((df3['avg_weight_error_exp'])), np.std((df5['avg_weight_error_exp'])), np.std((df1['avg_weight_error_exp'])), np.std((df6['avg_weight_error_exp'])), np.std((df7['avg_weight_error_exp'])))\n",
    "\n",
    "# print(np.mean(np.abs(df5['avg_weight_error_exp'])), np.mean(np.abs(df1['avg_weight_error_exp'])), np.mean(np.abs(df6['avg_weight_error_exp'])), np.mean(np.abs(df7['avg_weight_error_exp'])), np.mean(np.abs(df8['avg_weight_error_exp'])))\n",
    "# print(np.std((df5['avg_weight_error_exp'])), np.std((df1['avg_weight_error_exp'])), np.std((df6['avg_weight_error_exp'])), np.std((df7['avg_weight_error_exp'])), np.std((df8['avg_weight_error_exp'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(tdf[tdf.akpd_cutoff_col == 0.01])\n",
    "# np.std(tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'manual')])\n",
    "# np.std(tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')])\n",
    "# np.mean(tdf[tdf.akpd_cutoff_col == 0.95])\n",
    "# np.std(tdf[tdf.akpd_cutoff_col == 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "tdf1 = tdf[tdf['starvation_days_col'] > 0]\n",
    "\n",
    "plt.scatter(tdf1['starvation_days_col'], tdf1['avg_weight_error_0.14'])\n",
    "\n",
    "X = tdf1['starvation_days_col']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(tdf1['avg_weight_error_0.14'], X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tdf['avg_weight_col'], tdf['avg_weight_error_0.14'])\n",
    "\n",
    "X = tdf['avg_weight_col']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(tdf['avg_weight_error_0.14'], X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(tdf1['avg_weight_error_0.14'])), np.mean(np.abs(tdf1['avg_weight_error_0.14'] - 0.0292 + 0.0028 * tdf1['starvation_days_col']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tdf['avg_weight_error_0.14'], tdf['avg_weight_error_0.14'], color = 'blue')\n",
    "plt.scatter(tdf['avg_weight_error_0.14'], tdf['avg_weight_error_0.16'], color = 'green')\n",
    "plt.scatter(tdf['avg_weight_error_0.14'], tdf.avg_weight_error_exp, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf.avg_weight_error_exp)\n",
    "plt.hist(tdf['avg_weight_error_0.16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf1 = tdf[tdf.hour_filter_method_col == 'manual']\n",
    "mask1 = tdf1.akpd_cutoff_col == 0.95\n",
    "counts, bins, _ = plt.hist(tdf1[mask1]['avg_weight_error_0.14'], alpha = 0.5, color = 'cyan')\n",
    "plt.hist(tdf1[mask1]['avg_weight_error_exp'], alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generate average weight accuracy with new model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tdf[tdf.akpd_cutoff_col == 0.01])\n",
    "np.std(tdf[tdf.akpd_cutoff_col == 0.01])\n",
    "np.mean(tdf[tdf.akpd_cutoff_col == 0.95])\n",
    "np.std(tdf[tdf.akpd_cutoff_col == 0.95])\n",
    "tdf[tdf.hour_filter_method_col == 'hour_hist']\n",
    "tdf[tdf.hour_filter_method_col == 'manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf1 = tdf[tdf.hour_filter_method_col == 'hour_hist']\n",
    "mask1 = tdf1.akpd_cutoff_col == 0.01\n",
    "counts, bins, _ = plt.hist(tdf1[mask1]['avg_weight_error_0.14'], alpha = 0.5, color = 'cyan')\n",
    "plt.hist(tdf1[~mask1]['avg_weight_error_0.14'], alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [7]\n",
    "end_hours = [15]\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "days_post_feeding = 1\n",
    "final_days_post_feeding = 1\n",
    "loss_factors = [0.16, 'expected_loss_factor'] # need to determine the right values here\n",
    "akpd_cutoffs = [0.01, 0.95]\n",
    "\n",
    "hour_filter_methods = ['manual', 'hour_hist'] #  'u-shape',\n",
    "\n",
    "cohort_name_col = []\n",
    "akpd_cutoff_col = []\n",
    "hour_filter_method_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "loss_factor_col = []\n",
    "starvation_days_col = []\n",
    "avg_weight_col = []\n",
    "avg_weight_error_col = []\n",
    "gt_avg_weight_col = []\n",
    "count_distribution_error_col = []\n",
    "\n",
    "for loss_factor in loss_factors:\n",
    "    avg_weight_error_col.append([])\n",
    "    count_distribution_error_col.append([])\n",
    "\n",
    "for cohort_name in sorted(list(dfs.keys())):\n",
    "    print(cohort_name)\n",
    "# for cohort_name in ['dale_pen_id_143_2020-10-07_2020-10-21']:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "    \n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    slaughter_date = gt_metadata['slaughter_date']\n",
    "\n",
    "    if slaughter_date is not None and last_feeding_date is not None:\n",
    "        date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "        starvation_days = date_diff.days\n",
    "    else:\n",
    "        starvation_days = None\n",
    "        \n",
    "    df = dfs[cohort_name]\n",
    "    \n",
    "    mask = df['weight_v1'] > 0\n",
    "    \n",
    "    df['estimated_weight_g'][mask] = df['weight_v6'][mask]\n",
    "    df['estimated_weight_g'][~mask] = df['weight_v1'][~mask]    \n",
    "    \n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "    \n",
    "    start_end_hours = []\n",
    "    \n",
    "    for method in hour_filter_methods:\n",
    "        if method == 'manual':\n",
    "            for start_hour in start_hours:\n",
    "                for end_hour in end_hours:\n",
    "                    start_end_hours.append((method, start_hour, end_hour))\n",
    "        elif method == 'u-shape':\n",
    "            pass\n",
    "        elif method == 'hour_hist':\n",
    "            df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "            count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "            idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "            start_index = np.where(bins == 10)[0][0]\n",
    "            start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "            lower_index = start_array\n",
    "            upper_index = start_array\n",
    "\n",
    "            while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "                lower_index = lower_index - 1\n",
    "            while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "                upper_index = upper_index + 1\n",
    "    \n",
    "            start_hour, end_hour = bins[idx_values[lower_index]], bins[idx_values[upper_index]]\n",
    "            \n",
    "            start_end_hours.append((method, start_hour, end_hour))\n",
    "    \n",
    "    for akpd_cutoff in akpd_cutoffs:\n",
    "        for method, start_hour, end_hour in start_end_hours:\n",
    "            sampling_filter = SamplingFilter(\n",
    "                start_hour=start_hour,\n",
    "                end_hour=end_hour,\n",
    "                kf_cutoff=0.0,\n",
    "                akpd_score_cutoff=akpd_cutoff\n",
    "            )\n",
    "\n",
    "            pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "            try:\n",
    "                weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "            except ValidationError as err:\n",
    "                continue\n",
    "\n",
    "            akpd_cutoff_col.append(akpd_cutoff)\n",
    "            cohort_name_col.append(cohort_name)\n",
    "            hour_filter_method_col.append(method)\n",
    "            start_hour_col.append(start_hour)\n",
    "            end_hour_col.append(end_hour)\n",
    "            loss_factor_col.append(gt_metadata['expected_loss_factor'])\n",
    "            starvation_days_col.append(starvation_days)\n",
    "            avg_weight_col.append(np.mean(weights))\n",
    "            gt_avg_weight_col.append(gt_metadata['gutted_average_weight'])\n",
    "\n",
    "            for index, loss_factor in enumerate(loss_factors):\n",
    "                if loss_factor == 'expected_loss_factor':\n",
    "                    loss_factor = gt_metadata['expected_loss_factor'] or 0.165\n",
    "                    \n",
    "                    if loss_factor > 10:\n",
    "                        loss_factor = loss_factor / 100.0\n",
    "                    \n",
    "                avg_weight_err = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                avg_weight_error_col[index].append(avg_weight_err)\n",
    "\n",
    "                count_distribution_errors = generate_distribution_accuracy(weights, gt_metadata, loss_factor)\n",
    "                count_distribution_error_col[index].append(count_distribution_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'cohort_name': cohort_name_col,\n",
    "    'hour_filter_method_col': hour_filter_method_col,\n",
    "    'akpd_cutoff_col': akpd_cutoff_col,\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'loss_factor_col': loss_factor_col,\n",
    "    'starvation_days_col': starvation_days_col,\n",
    "    'avg_weight_col': avg_weight_col,\n",
    "    'gt_avg_weight_col': gt_avg_weight_col\n",
    "}\n",
    "\n",
    "for index, loss_factor in enumerate(loss_factors):\n",
    "    if loss_factor == 'expected_loss_factor':\n",
    "        col_name = 'avg_weight_error_exp'\n",
    "        col_abs_name = 'avg_weight_error_abs_exp'\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_exp'\n",
    "    else:\n",
    "        col_name = 'avg_weight_error_%0.2f' % (loss_factor,)\n",
    "        col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_%0.2f' % (loss_factor,)\n",
    "        \n",
    "    columns[col_name] = avg_weight_error_col[index]\n",
    "    columns[col_abs_name] = np.abs(avg_weight_error_col[index])\n",
    "    columns[col_abs_dist_name] = [np.mean(np.abs(l)) for l in count_distribution_error_col[index]]\n",
    "\n",
    "tdf = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [7]\n",
    "end_hours = [15]\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "days_post_feeding = 1\n",
    "final_days_post_feeding = 1\n",
    "loss_factors = [0.16, 'expected_loss_factor'] # need to determine the right values here\n",
    "akpd_cutoffs = [0.01, 0.95]\n",
    "\n",
    "hour_filter_methods = ['manual', 'hour_hist'] #  'u-shape',\n",
    "\n",
    "cohort_name_col = []\n",
    "akpd_cutoff_col = []\n",
    "hour_filter_method_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "loss_factor_col = []\n",
    "starvation_days_col = []\n",
    "avg_weight_col = []\n",
    "avg_weight_error_col = []\n",
    "gt_avg_weight_col = []\n",
    "count_distribution_error_col = []\n",
    "\n",
    "for loss_factor in loss_factors:\n",
    "    avg_weight_error_col.append([])\n",
    "    count_distribution_error_col.append([])\n",
    "\n",
    "for cohort_name in sorted(list(dfs.keys())):\n",
    "    print(cohort_name)\n",
    "# for cohort_name in ['dale_pen_id_143_2020-10-07_2020-10-21']:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "    \n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    slaughter_date = gt_metadata['slaughter_date']\n",
    "\n",
    "    if slaughter_date is not None and last_feeding_date is not None:\n",
    "        date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "        starvation_days = date_diff.days\n",
    "    else:\n",
    "        starvation_days = None\n",
    "        \n",
    "    df = dfs[cohort_name]\n",
    "    \n",
    "    mask = df['weight_v5'] > 8000\n",
    "    \n",
    "    df['estimated_weight_g'][mask] = df['weight_v7'][mask]\n",
    "    df['estimated_weight_g'][~mask] = df['weight_v5'][~mask]    \n",
    "    \n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "    \n",
    "    start_end_hours = []\n",
    "    \n",
    "    for method in hour_filter_methods:\n",
    "        if method == 'manual':\n",
    "            for start_hour in start_hours:\n",
    "                for end_hour in end_hours:\n",
    "                    start_end_hours.append((method, start_hour, end_hour))\n",
    "        elif method == 'u-shape':\n",
    "            pass\n",
    "        elif method == 'hour_hist':\n",
    "            df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "            count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "            idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "            start_index = np.where(bins == 10)[0][0]\n",
    "            start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "            lower_index = start_array\n",
    "            upper_index = start_array\n",
    "\n",
    "            while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "                lower_index = lower_index - 1\n",
    "            while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "                upper_index = upper_index + 1\n",
    "    \n",
    "            start_hour, end_hour = bins[idx_values[lower_index]], bins[idx_values[upper_index]]\n",
    "            \n",
    "            start_end_hours.append((method, start_hour, end_hour))\n",
    "    \n",
    "    for akpd_cutoff in akpd_cutoffs:\n",
    "        for method, start_hour, end_hour in start_end_hours:\n",
    "            sampling_filter = SamplingFilter(\n",
    "                start_hour=start_hour,\n",
    "                end_hour=end_hour,\n",
    "                kf_cutoff=0.0,\n",
    "                akpd_score_cutoff=akpd_cutoff\n",
    "            )\n",
    "\n",
    "            pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "            try:\n",
    "                weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "            except ValidationError as err:\n",
    "                continue\n",
    "\n",
    "            akpd_cutoff_col.append(akpd_cutoff)\n",
    "            cohort_name_col.append(cohort_name)\n",
    "            hour_filter_method_col.append(method)\n",
    "            start_hour_col.append(start_hour)\n",
    "            end_hour_col.append(end_hour)\n",
    "            loss_factor_col.append(gt_metadata['expected_loss_factor'])\n",
    "            starvation_days_col.append(starvation_days)\n",
    "            avg_weight_col.append(np.mean(weights))\n",
    "            gt_avg_weight_col.append(gt_metadata['gutted_average_weight'])\n",
    "\n",
    "            for index, loss_factor in enumerate(loss_factors):\n",
    "                if loss_factor == 'expected_loss_factor':\n",
    "                    loss_factor = gt_metadata['expected_loss_factor'] or 0.165\n",
    "                    \n",
    "                    if loss_factor > 10:\n",
    "                        loss_factor = loss_factor / 100.0\n",
    "                    \n",
    "                avg_weight_err = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                avg_weight_error_col[index].append(avg_weight_err)\n",
    "\n",
    "                count_distribution_errors = generate_distribution_accuracy(weights, gt_metadata, loss_factor)\n",
    "                count_distribution_error_col[index].append(count_distribution_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'cohort_name': cohort_name_col,\n",
    "    'hour_filter_method_col': hour_filter_method_col,\n",
    "    'akpd_cutoff_col': akpd_cutoff_col,\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'loss_factor_col': loss_factor_col,\n",
    "    'starvation_days_col': starvation_days_col,\n",
    "    'avg_weight_col': avg_weight_col,\n",
    "    'gt_avg_weight_col': gt_avg_weight_col\n",
    "}\n",
    "\n",
    "for index, loss_factor in enumerate(loss_factors):\n",
    "    if loss_factor == 'expected_loss_factor':\n",
    "        col_name = 'avg_weight_error_exp'\n",
    "        col_abs_name = 'avg_weight_error_abs_exp'\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_exp'\n",
    "    else:\n",
    "        col_name = 'avg_weight_error_%0.2f' % (loss_factor,)\n",
    "        col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_%0.2f' % (loss_factor,)\n",
    "        \n",
    "    columns[col_name] = avg_weight_error_col[index]\n",
    "    columns[col_abs_name] = np.abs(avg_weight_error_col[index])\n",
    "    columns[col_abs_dist_name] = [np.mean(np.abs(l)) for l in count_distribution_error_col[index]]\n",
    "\n",
    "tdf = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hours = [7]\n",
    "end_hours = [15]\n",
    "apply_growth_rate = True\n",
    "max_day_diff = 3\n",
    "days_post_feeding = 1\n",
    "final_days_post_feeding = 1\n",
    "loss_factors = [0.16, 'expected_loss_factor'] # need to determine the right values here\n",
    "akpd_cutoffs = [0.01, 0.95]\n",
    "\n",
    "hour_filter_methods = ['manual', 'hour_hist'] #  'u-shape',\n",
    "\n",
    "cohort_name_col = []\n",
    "akpd_cutoff_col = []\n",
    "hour_filter_method_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "loss_factor_col = []\n",
    "starvation_days_col = []\n",
    "avg_weight_col = []\n",
    "avg_weight_error_col = []\n",
    "gt_avg_weight_col = []\n",
    "count_distribution_error_col = []\n",
    "\n",
    "for loss_factor in loss_factors:\n",
    "    avg_weight_error_col.append([])\n",
    "    count_distribution_error_col.append([])\n",
    "\n",
    "for cohort_name in sorted(list(dfs.keys())):\n",
    "    print(cohort_name)\n",
    "# for cohort_name in ['dale_pen_id_143_2020-10-07_2020-10-21']:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "    \n",
    "    last_feeding_date = gt_metadata['last_feeding_date']\n",
    "    slaughter_date = gt_metadata['slaughter_date']\n",
    "\n",
    "    if slaughter_date is not None and last_feeding_date is not None:\n",
    "        date_diff = datetime.strptime(slaughter_date, '%Y-%m-%d') - datetime.strptime(last_feeding_date, '%Y-%m-%d')\n",
    "        starvation_days = date_diff.days\n",
    "    else:\n",
    "        starvation_days = None\n",
    "        \n",
    "    df = dfs[cohort_name]\n",
    "    \n",
    "    mask = df['weight_v5'] > 6000\n",
    "    \n",
    "    df['estimated_weight_g'][mask] = df['weight_v8'][mask]\n",
    "    df['estimated_weight_g'][~mask] = df['weight_v5'][~mask]    \n",
    "    \n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "    \n",
    "    start_end_hours = []\n",
    "    \n",
    "    for method in hour_filter_methods:\n",
    "        if method == 'manual':\n",
    "            for start_hour in start_hours:\n",
    "                for end_hour in end_hours:\n",
    "                    start_end_hours.append((method, start_hour, end_hour))\n",
    "        elif method == 'u-shape':\n",
    "            pass\n",
    "        elif method == 'hour_hist':\n",
    "            df2 = df[(df.hour >= 3) & (df.hour <= 20)]\n",
    "\n",
    "            count, bins, _ = plt.hist(df2.hour, density = True, bins = (np.max(df2.hour) - np.min(df2.hour)))\n",
    "\n",
    "            idx_values = np.where(count > 1.0 / 18)[0]\n",
    "\n",
    "            start_index = np.where(bins == 10)[0][0]\n",
    "            start_array = np.where(idx_values == start_index)[0][0]\n",
    "\n",
    "            lower_index = start_array\n",
    "            upper_index = start_array\n",
    "\n",
    "            while lower_index > 0 and (idx_values[lower_index] - idx_values[lower_index - 1] == 1):\n",
    "                lower_index = lower_index - 1\n",
    "            while upper_index < len(idx_values) - 1 and (idx_values[upper_index + 1] - idx_values[upper_index] == 1):\n",
    "                upper_index = upper_index + 1\n",
    "    \n",
    "            start_hour, end_hour = bins[idx_values[lower_index]], bins[idx_values[upper_index]]\n",
    "            \n",
    "            start_end_hours.append((method, start_hour, end_hour))\n",
    "    \n",
    "    for akpd_cutoff in akpd_cutoffs:\n",
    "        for method, start_hour, end_hour in start_end_hours:\n",
    "            sampling_filter = SamplingFilter(\n",
    "                start_hour=start_hour,\n",
    "                end_hour=end_hour,\n",
    "                kf_cutoff=0.0,\n",
    "                akpd_score_cutoff=akpd_cutoff\n",
    "            )\n",
    "\n",
    "            pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "            try:\n",
    "                weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "            except ValidationError as err:\n",
    "                continue\n",
    "\n",
    "            akpd_cutoff_col.append(akpd_cutoff)\n",
    "            cohort_name_col.append(cohort_name)\n",
    "            hour_filter_method_col.append(method)\n",
    "            start_hour_col.append(start_hour)\n",
    "            end_hour_col.append(end_hour)\n",
    "            loss_factor_col.append(gt_metadata['expected_loss_factor'])\n",
    "            starvation_days_col.append(starvation_days)\n",
    "            avg_weight_col.append(np.mean(weights))\n",
    "            gt_avg_weight_col.append(gt_metadata['gutted_average_weight'])\n",
    "\n",
    "            for index, loss_factor in enumerate(loss_factors):\n",
    "                if loss_factor == 'expected_loss_factor':\n",
    "                    loss_factor = gt_metadata['expected_loss_factor'] or 0.165\n",
    "                    \n",
    "                    if loss_factor > 10:\n",
    "                        loss_factor = loss_factor / 100.0\n",
    "                    \n",
    "                avg_weight_err = generate_average_weight_accuracy(weights, gt_metadata, loss_factor)\n",
    "                avg_weight_error_col[index].append(avg_weight_err)\n",
    "\n",
    "                count_distribution_errors = generate_distribution_accuracy(weights, gt_metadata, loss_factor)\n",
    "                count_distribution_error_col[index].append(count_distribution_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'cohort_name': cohort_name_col,\n",
    "    'hour_filter_method_col': hour_filter_method_col,\n",
    "    'akpd_cutoff_col': akpd_cutoff_col,\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'loss_factor_col': loss_factor_col,\n",
    "    'starvation_days_col': starvation_days_col,\n",
    "    'avg_weight_col': avg_weight_col,\n",
    "    'gt_avg_weight_col': gt_avg_weight_col\n",
    "}\n",
    "\n",
    "for index, loss_factor in enumerate(loss_factors):\n",
    "    if loss_factor == 'expected_loss_factor':\n",
    "        col_name = 'avg_weight_error_exp'\n",
    "        col_abs_name = 'avg_weight_error_abs_exp'\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_exp'\n",
    "    else:\n",
    "        col_name = 'avg_weight_error_%0.2f' % (loss_factor,)\n",
    "        col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "        col_abs_dist_name = 'avg_count_dist_error_abs_%0.2f' % (loss_factor,)\n",
    "        \n",
    "    columns[col_name] = avg_weight_error_col[index]\n",
    "    columns[col_abs_name] = np.abs(avg_weight_error_col[index])\n",
    "    columns[col_abs_dist_name] = [np.mean(np.abs(l)) for l in count_distribution_error_col[index]]\n",
    "\n",
    "tdf = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = tdf[(tdf.akpd_cutoff_col == 0.01) & (tdf.hour_filter_method_col == 'hour_hist')][['cohort_name', 'gt_avg_weight_col', 'avg_weight_error_0.16', 'avg_weight_error_exp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('eide_langoy_singleweights.csv')\n",
    "langoy_gt = gt['weight'] * 1000 / 0.86\n",
    "\n",
    "pen5 = pd.read_csv('blom_vikane_singleweights.csv')\n",
    "vikane_gt = pen5['weight'] * 1000 / 0.86\n",
    "\n",
    "# imr_gt = pd.read_csv('imr.csv').weight\n",
    "\n",
    "single_weights = [\n",
    "    ('langoy_pen_id_108_2020-05-07_2020-05-17', langoy_gt),\n",
    "    ('vikane_pen_id_60_2020-08-10_2020-08-30', vikane_gt)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(langoy_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vikane_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name, gt_weights in single_weights:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "    df = dfs[cohort_name]\n",
    "    df['estimated_weight_g'] = df['weight_v2']\n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=7,\n",
    "        end_hour=15,\n",
    "        kf_cutoff=0.0,\n",
    "        akpd_score_cutoff=0.01\n",
    "    )\n",
    "\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    try:\n",
    "        weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "    except ValidationError as err:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    count, bins, _ = plt.hist(gt_weights, density = True, color = 'blue', bins = 50)\n",
    "    plt.hist(weights, density = True, alpha = 0.5, color = 'red', bins = bins)\n",
    "    print(np.mean(gt_weights), np.mean(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name, gt_weights in single_weights:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "    df = dfs[cohort_name]\n",
    "    df['estimated_weight_g'] = df['weight_v5']\n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=7,\n",
    "        end_hour=15,\n",
    "        kf_cutoff=0.0,\n",
    "        akpd_score_cutoff=0.01\n",
    "    )\n",
    "\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    try:\n",
    "        weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "    except ValidationError as err:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    count, bins, _ = plt.hist(gt_weights, density = True, color = 'blue', bins = 50)\n",
    "    plt.hist(weights, density = True, alpha = 0.5, color = 'red', bins = bins)\n",
    "    print(np.mean(gt_weights), np.mean(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_name, gt_weights in single_weights:\n",
    "    gt_metadata = gt_metadatas[cohort_name]\n",
    "\n",
    "    df = dfs[cohort_name]\n",
    "    df['estimated_weight_g'] = df['weight_v4']\n",
    "    final_date_post_feeding = add_days(gt_metadata['last_feeding_date'], final_days_post_feeding)\n",
    "    tdf = df[df.date <= final_date_post_feeding]\n",
    "\n",
    "    sampling_filter = SamplingFilter(\n",
    "        start_hour=7,\n",
    "        end_hour=15,\n",
    "        kf_cutoff=0.0,\n",
    "        akpd_score_cutoff=0.01\n",
    "    )\n",
    "\n",
    "    pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "\n",
    "    try:\n",
    "        weights = generate_raw_individual_values(pm_base, gt_metadata, start_hour, end_hour, apply_growth_rate, max_day_diff, days_post_feeding, final_days_post_feeding)\n",
    "    except ValidationError as err:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    count, bins, _ = plt.hist(gt_weights, density = True, color = 'blue', bins = 50)\n",
    "    plt.hist(weights, density = True, alpha = 0.5, color = 'red', bins = bins)\n",
    "    print(np.mean(gt_weights), np.mean(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = dfs['tittelsnes_pen_id_37_2020-06-10_2020-06-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(kdf.weight_v1 - kdf.weight_v2, bins=200)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "loss_factor = loss_factors[index]\n",
    "col_abs_name = 'avg_weight_error_abs_%0.2f' % (loss_factor,)\n",
    "error = tdf[col_abs_name]\n",
    "\n",
    "print('Loss factor', loss_factor)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Average Weight Error')\n",
    "print('Avg %0.1f' % (np.mean(error) * 100, ))\n",
    "print('90th Pct %0.1f' % (np.percentile(error, 90) * 100, ))\n",
    "print('Max %0.1f' % (np.max(error) * 100, ))\n",
    "\n",
    "print()\n",
    "\n",
    "dist_errors = [item for sublist in count_distribution_error_col[index] for item in sublist]\n",
    "\n",
    "print('Count Distribution Error')\n",
    "print('Avg %0.1f' % (np.mean(np.abs(dist_errors)) * 100, ))\n",
    "print('90th Pct %0.1f' % (np.percentile(np.abs(dist_errors), 90) * 100, ))\n",
    "print('Max %0.1f' % (np.max(np.abs(dist_errors)) * 100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cohort_name in cohort_names:\n",
    "#     mask = tdf.cohort_name == cohort_name\n",
    "#     print(tdf[mask].sort_values('avg_weight_error_abs', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metadatas['vikane_pen_id_60_2020-08-05_2020-08-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.cohort_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.cohort_name == 'tittelsnes_pen_id_37_2020-05-23_2020-06-24') & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3) & (tdf.loss_factor == 0.17)\n",
    "tdf[mask].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.start_hour_col == 6) & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3)\n",
    "tdf[mask].avg_weight_error_abs.median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (tdf.start_hour_col == 7) & (tdf.days_post_feeding == 1) & (tdf.final_days_post_feeding == 1) & (tdf.max_day_diff == 3)\n",
    "tdf[mask].avg_weight_error_abs.median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name_col = []\n",
    "start_hour_col = []\n",
    "end_hour_col = []\n",
    "apply_growth_rate_col = []\n",
    "max_day_diff_col = []\n",
    "days_post_feeding_col = []\n",
    "final_days_post_feeding_col = []\n",
    "loss_factor_col = []\n",
    "std_avg_weight_error_col = []\n",
    "abs_avg_weight_error_col = []\n",
    "mean_avg_weight_error_col = []\n",
    "\n",
    "for start_hour in start_hours:\n",
    "    for end_hour in end_hours:\n",
    "        for apply_growth_rate in apply_growth_rate_list:\n",
    "            for max_day_diff in max_day_diff_list:\n",
    "                for days_post_feeding in days_post_feeding_list:\n",
    "                    for final_days_post_feeding in final_days_post_feeding_list:\n",
    "                        for loss_factor in loss_factors:\n",
    "                            mask = (tdf.start_hour_col == start_hour) & \\\n",
    "                            (tdf.end_hour_col == end_hour) & \\\n",
    "                            (tdf.apply_growth_rate == apply_growth_rate) & \\\n",
    "                            (tdf.max_day_diff == max_day_diff) & \\\n",
    "                            (tdf.days_post_feeding == days_post_feeding) & \\\n",
    "                            (tdf.final_days_post_feeding == final_days_post_feeding) & \\\n",
    "                            (tdf.loss_factor == loss_factor)\n",
    "                            \n",
    "                            start_hour_col.append(start_hour)\n",
    "                            end_hour_col.append(end_hour)\n",
    "                            apply_growth_rate_col.append(apply_growth_rate)\n",
    "                            max_day_diff_col.append(max_day_diff)\n",
    "                            days_post_feeding_col.append(days_post_feeding)\n",
    "                            final_days_post_feeding_col.append(final_days_post_feeding)\n",
    "                            loss_factor_col.append(loss_factor)\n",
    "                            std_avg_weight_error_col.append(tdf[mask].avg_weight_error.std())\n",
    "                            abs_avg_weight_error_col.append(tdf[mask].avg_weight_error_abs.mean())\n",
    "                            mean_avg_weight_error_col.append(tdf[mask].avg_weight_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame({\n",
    "    'start_hour_col': start_hour_col,\n",
    "    'end_hour_col': end_hour_col,\n",
    "    'apply_growth_rate': apply_growth_rate_col,\n",
    "    'max_day_diff': max_day_diff_col,\n",
    "    'days_post_feeding': days_post_feeding_col,\n",
    "    'final_days_post_feeding': final_days_post_feeding_col,\n",
    "    'loss_factor': loss_factor_col,\n",
    "    'abs_avg_weight_error': abs_avg_weight_error_col,\n",
    "    'std_avg_weight_error': std_avg_weight_error_col,\n",
    "    'mean_avg_weight_error': mean_avg_weight_error_col,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (rdf.loss_factor == 0.16)\n",
    "rdf[mask].sort_values('abs_avg_weight_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('/root/data/alok/biomass_estimation/playground/smart_average_param_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.cohort_name == 'bolaks_pen_id_88_2020-02-10_2020-03-10')].sort_values('avg_weight_error_abs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Vikane average weight and distribution error - explore basic parameters\n",
    "\n",
    "ground_truth_metadata = json.load(open(ground_truth_f))\n",
    "day_after_feeding_stop = add_days(ground_truth_metadata['last_feeding_date'], 1)\n",
    "start_date, end_date = add_days(day_after_feeding_stop, -2), add_days(day_after_feeding_stop, -1)\n",
    "tdf = df[(df.date >= start_date) & (df.date <= end_date)].copy(deep=True)\n",
    "\n",
    "sampling_filter = SamplingFilter(\n",
    "    start_hour=7,\n",
    "    end_hour=15,\n",
    "    akpd_score_cutoff=0.95,\n",
    "    kf_cutoff=0.0\n",
    ")\n",
    "pm_base = gen_pm_base(tdf, sampling_filter)\n",
    "weights, _ = generate_smart_individual_values(pm_base, day_after_feeding_stop, 3, True, True, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
