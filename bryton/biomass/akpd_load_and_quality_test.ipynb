{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKPD Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import multiprocessing.dummy\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "\n",
    "FORMAT = '%(asctime)-15s %(levelname)-5s %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd; from pandas import Series, DataFrame; pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weight as W\n",
    "importlib.reload(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_keypoints import predict\n",
    "importlib.reload(predict)\n",
    "\n",
    "model = predict.Predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AKPD Request Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.load(open(\"tests/resources/akpd_local.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [predict.Item(\n",
    "    Id             = row[\"id\"],\n",
    "    left_crop_url  = row[\"leftCropUrl\"],\n",
    "    right_crop_url = row[\"rightCropUrl\"],\n",
    "    lcm            = {\"x_coord\": row[\"leftCropMetadata\"][\"x_coord\"], \"y_coord\": row[\"leftCropMetadata\"][\"y_coord\"]},\n",
    "    rcm            = {\"x_coord\": row[\"rightCropMetadata\"][\"x_coord\"], \"y_coord\": row[\"rightCropMetadata\"][\"y_coord\"]},\n",
    "    cm             = {\"pixelCountWidth\": row[\"cameraMetadata\"][\"pixelCountWidth\"], \"pixelCountHeight\": row[\"cameraMetadata\"][\"pixelCountHeight\"]},\n",
    ") for i, row in enumerate(body)]\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.load_images(items)\n",
    "logging.info(f\"loaded images: {len(items)}\")\n",
    "result = model.predict(items)\n",
    "result[0][\"leftCrop\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akpd_vector(ann):\n",
    "    assert len(ann['leftCrop']) == len(ann['rightCrop']) == 8\n",
    "    all_crops = ann['leftCrop'] + ann['rightCrop']\n",
    "    x = [kp['xFrame'] for kp in all_crops] + \\\n",
    "        [kp['yFrame'] for kp in all_crops]\n",
    "    hmax_arr = [kp.get('hm_max',0) for kp in all_crops]\n",
    "    return np.array(x), np.array(hmax_arr)\n",
    "\n",
    "\n",
    "def akpd_distance(a1, a2):\n",
    "    x1, hmax_arr1 = akpd_vector(a1)\n",
    "    x2, hmax_arr2 = akpd_vector(a2)\n",
    "    dx = x1 - x2\n",
    "    return np.sqrt(np.dot(dx, dx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_log_time = 0\n",
    "\n",
    "def variance_of_laplacian(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    \n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "\n",
    "def run_row_iter(item, camera_metadata):\n",
    "    items = [item]\n",
    "    predict.load_images(items)\n",
    "    result = model.predict(items)[0]\n",
    "    weight, length, kf = W.weight_estimator.predict(annotation=result, camera_metadata=camera_metadata)\n",
    "    return (result, weight, length, kf)\n",
    "    d_weight = (float(row[\"estimated_weight_g\"]) - weight) / (weight or 1e-9) * 100\n",
    "    \n",
    "    \n",
    "def run_row(args):\n",
    "    _id, row = args\n",
    "    basedir    = Path(f\".data/{row['pen_id']}/{row['pair_id']}/\")\n",
    "    left_path  = basedir / \"left.jpg\"\n",
    "    right_path = basedir / \"right.jpg\"\n",
    "    lcm        = json.loads(row[\"left_crop_metadata\"])\n",
    "    rcm        = json.loads(row[\"right_crop_metadata\"])\n",
    "    cm         = json.loads(row[\"camera_metadata\"])\n",
    "    annotation = json.loads(row[\"annotation\"])\n",
    "\n",
    "    item = predict.Item(\n",
    "        Id             = str(_id),\n",
    "        left_crop_url  = str(left_path),\n",
    "        right_crop_url = str(right_path),\n",
    "        lcm            = {\"x_coord\": lcm[\"x_coord\"], \"y_coord\": lcm[\"y_coord\"]},\n",
    "        rcm            = {\"x_coord\": rcm[\"x_coord\"], \"y_coord\": rcm[\"y_coord\"]},\n",
    "        cm             = {\"pixelCountWidth\": cm[\"pixelCountWidth\"], \"pixelCountHeight\": cm[\"pixelCountHeight\"]},\n",
    "        _down_sample   = 0.95,\n",
    "    )\n",
    "\n",
    "    camera_metadata = W.CameraMetadata(\n",
    "        focal_length        = cm['focalLength'      ],\n",
    "        focal_length_pixel  = cm['focalLengthPixel' ],\n",
    "        baseline_m          = cm['baseline'         ],\n",
    "        pixel_count_width   = cm['pixelCountWidth'  ],\n",
    "        pixel_count_height  = cm['pixelCountHeight' ],\n",
    "        image_sensor_width  = cm['imageSensorWidth' ],\n",
    "        image_sensor_height = cm['imageSensorHeight'],\n",
    "    )\n",
    "\n",
    "    akpd_score_vec = []\n",
    "    weight_vec = []\n",
    "    NUM_ITER = 1\n",
    "    for i in range(NUM_ITER):\n",
    "        item._down_sample = 1 - (NUM_ITER - i - 1) * 0.06\n",
    "        result, weight, length, kf = run_row_iter(item, camera_metadata)\n",
    "        akpd_score_vec.append(result['probability'])\n",
    "        weight_vec.append(weight)\n",
    "    \n",
    "    akpd_dist = akpd_distance(annotation, result)\n",
    "    x, hm = akpd_vector(result)\n",
    "    \n",
    "    try:\n",
    "        d_weight = (float(row[\"estimated_weight_g\"]) - weight) / (weight or 1e-9) * 100\n",
    "        d_weight = np.abs(d_weight)\n",
    "    except:\n",
    "        # some weird error happen here???\n",
    "        logging.exception(f\"Error weight={weight}\")\n",
    "        d_weight = 0\n",
    "    \n",
    "    weights = [w for w, s in zip(weight_vec, akpd_score_vec) if s >= 0.5]\n",
    "    if weights:\n",
    "        akpd_score_rate = len(weights) / NUM_ITER\n",
    "        weight_std = np.std(weights)\n",
    "    else:\n",
    "        akpd_score_rate = 0\n",
    "        weight_std = 0\n",
    "        \n",
    "    llv = variance_of_laplacian(item.l_img)\n",
    "    rlv = variance_of_laplacian(item.r_img)\n",
    "    \n",
    "    # LOGGING\n",
    "    global next_log_time\n",
    "    t = time.time()\n",
    "    if t > next_log_time:\n",
    "        next_log_time = t + LOG_INTERVAL        \n",
    "        good = '*' if float(row[\"akpd_score\"]) >= 0.5 else ' '\n",
    "        logging.info(\"{:04d} {:40s} dKP={}; dw={:0.2f}%; score={:0.3f}{}; akpd_score_rate={:0.1f}; weight_std={:6.1f}; hm={:0.4f}, {:0.4f}\".format(\n",
    "            _id, \n",
    "            row['pair_id'], \n",
    "            akpd_dist, \n",
    "            d_weight, \n",
    "            float(row[\"akpd_score\"]),\n",
    "            good,\n",
    "            akpd_score_rate,                                                                                          \n",
    "            weight_std,                                                                                          \n",
    "            hm.mean(), hm.min())\n",
    "        )\n",
    "\n",
    "    result['pair_id'            ] = row['pair_id']\n",
    "    result['basedir'            ] = str(basedir)\n",
    "    result['lx_coord'           ] = lcm[\"x_coord\"]\n",
    "    result['ly_coord'           ] = lcm[\"y_coord\"]\n",
    "    result['l_width'            ] = lcm[\"width\"]\n",
    "    result['l_height'           ] = lcm[\"height\"]\n",
    "    result['rx_coord'           ] = rcm[\"x_coord\"]\n",
    "    result['ry_coord'           ] = rcm[\"y_coord\"]\n",
    "    result['r_width'            ] = rcm[\"width\"]\n",
    "    result['r_height'           ] = rcm[\"height\"]\n",
    "    result['pixelCountWidth'    ] = cm[\"pixelCountWidth\"]\n",
    "    result['pixelCountHeight'   ] = cm[\"pixelCountHeight\"]\n",
    "    result['estimated_weight_g' ] = weight\n",
    "    result['estimated_k_factor' ] = kf\n",
    "    result['annotation0'        ] = row[\"annotation\"]\n",
    "    result['akpd_score0'        ] = row[\"akpd_score\"]\n",
    "    result['estimated_weight_g0'] = row[\"estimated_weight_g\"]\n",
    "    result['estimated_k_factor0'] = row[\"estimated_k_factor\"]\n",
    "    result['akpd_dist'          ] = akpd_dist\n",
    "    result['hm'                 ] = list(hm)\n",
    "    result['hm_mean'            ] = hm.mean()\n",
    "    result['hm_min'             ] = hm.min()\n",
    "    result['akpd_score_vec'     ] = akpd_score_vec\n",
    "    result['weight_vec'         ] = weight_vec\n",
    "    result['akpd_score_rate'    ] = akpd_score_rate\n",
    "    result['weight_std'         ] = weight_std\n",
    "    result['llv'                ] = llv\n",
    "    result['rlv'             ] = rlv\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 30\n",
    "LOG_INTERVAL = 10\n",
    "WORKERS = 3\n",
    "N = 10000\n",
    "\n",
    "def load_test(n=N, workers=WORKERS):\n",
    "    filename = '.data/2020-12-20_biomass.csv.gz'\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as fp:\n",
    "        start_time = time.time()\n",
    "        reader = csv.DictReader(fp)\n",
    "\n",
    "        logging.info(f\"N={n} workers={workers}\")\n",
    "\n",
    "        if workers == 0:\n",
    "            results = list(map(run_row, zip(range(n), reader)))\n",
    "        else:        \n",
    "            with multiprocessing.dummy.Pool(workers) as pool:\n",
    "                results = pool.map(run_row, zip(range(n), reader), chunksize=4)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed {n} rows with workers={workers} avg={elapsed_time/n:0.3f}\")\n",
    "    \n",
    "    with open('.data/2020-12-20_biomass.re-akpd.csv', 'w') as fp:\n",
    "        fields = \"id,pair_id,basedir,lx_coord,ly_coord,l_width,l_height,rx_coord,ry_coord,r_width,r_height,pixelCountWidth,pixelCountHeight,\" \\\n",
    "            \"estimated_weight_g,estimated_k_factor,\"\\\n",
    "            \"annotation0,akpd_score0,estimated_weight_g0,estimated_k_factor0,\"\\\n",
    "            \"akpd_dist,hm,hm_mean,hm_min,weight_vec,akpd_score_vec,akpd_score_rate,weight_std,llv,rlv,\" \\\n",
    "            \"leftCrop,rightCrop,probability,modelVerion,scorerVersion\".split(\",\")\n",
    "        writer = csv.DictWriter(fp, fields)        \n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            r1 = result.copy()\n",
    "            r1['leftCrop'] = json.dumps(r1['leftCrop'])\n",
    "            r1['rightCrop'] = json.dumps(r1['rightCrop'])\n",
    "            writer.writerow(r1)\n",
    "        print(f\"Written {len(results)} rows to {fp.name}\")\n",
    "                \n",
    "    return results\n",
    "\n",
    "#results = load_test(10000,workers=3)\n",
    "results = load_test(200,workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = json.loads(r0[0]['orig_annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0,_=akpd_vector(a0) \n",
    "x1,_=akpd_vector(r0[0])\n",
    "x1-x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KP= [\"TAIL_NOTCH\", \"ADIPOSE_FIN\", \"UPPER_LIP\", \"ANAL_FIN\", \"PELVIC_FIN\", \"EYE\", \"PECTORAL_FIN\", \"DORSAL_FIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hma = np.array([r['hm'] for r in results])\n",
    "hma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm1 = pd.DataFrame(hma[:,0:8],columns=KP)\n",
    "hm1['side'] = 'left'\n",
    "hm2 = pd.DataFrame(hma[:,8:],columns=KP)\n",
    "hm2['side'] = 'right'\n",
    "hm3 = pd.concat([hm1, hm2])\n",
    "hm3 = hm3[hm3.median(axis=0).sort_values(ascending=False).index]\n",
    "\n",
    "sorted_kp = list(hm3.index)\n",
    "#sorted_kp = list(hm3.median(axis=0).sort_values(ascending=False).index)\n",
    "\n",
    "hmm = hm3.melt(id_vars=['side'])\n",
    "hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm3.median(axis=0).sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.violinplot(x='variable', y='value', hue='side', split=True, data=hmm, order=sorted_kp, bw=.05, cut=0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.violinplot(x='variable', y='value', hue='side', split=True, data=hmm, order=sorted_kp, bw=.05, cut=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KP  visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('Set3')\n",
    "cmap = plt.cm.get_cmap('Pastel1')\n",
    "cmap = plt.cm.get_cmap('Pastel2')\n",
    "cmap = plt.cm.get_cmap('Accent')\n",
    "colors = np.array(cmap.colors[:8])\n",
    "\n",
    "def _enhance(image, clip_limit=5):\n",
    "    # convert image to LAB color model\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    # split the image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(image_lab)\n",
    "    # apply CLAHE to lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
    "    merged_channels = cv2.merge((cl, a_channel, b_channel))\n",
    "    # convert image from LAB color model back to RGB color model\n",
    "    final_image = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
    "    return final_image\n",
    "\n",
    "def parse_kps(ann):\n",
    "    def _kps(points):\n",
    "        return np.array([[p['xCrop'], p['yCrop']] for p in points])\n",
    "    def _hm(points):\n",
    "        return np.array([p.get('hm_max',0.0) for p in points])\n",
    "    left_crop = json.loads(ann['leftCrop'])\n",
    "    right_crop = json.loads(ann['rightCrop'])\n",
    "    return _kps(left_crop), _hm(left_crop), _kps(right_crop), _hm(right_crop),\n",
    "\n",
    "def show_img(ax, image_path, pts, hm):\n",
    "    with image_path.open('rb') as fp:\n",
    "        np_image = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "    img = cv2.imdecode(np_image, cv2.IMREAD_COLOR)\n",
    "#    if 'right' in str(image_path):\n",
    "#        img = _enhance(img)\n",
    "    img = _enhance(img)\n",
    "    ax.imshow(img, alpha=0.6)\n",
    "\n",
    "    s = 2000*(1-hm)**2 + 8\n",
    "    alpha = np.clip(hm**1.5, 0.2, 1)\n",
    "    rgba = np.hstack([colors, alpha.reshape((-1,1))])\n",
    "    ax.scatter(x=pts[:,0], y=pts[:,1], marker='o', c=rgba, s=s)\n",
    "#    ax.scatter(x=pts[:,0], y=pts[:,1], marker='o', facecolors='none', edgecolors=rgba, s=s)\n",
    "#    ax.scatter(x=pts[:,0], y=pts[:,1], marker=',', s=1)\n",
    "\n",
    "    outline = pts[[0,1,7,2,5,6,4,3]]\n",
    "    ax.plot(outline[:,0], outline[:,1], c='orange', ls=\"-\", lw=1.1, alpha=0.99)\n",
    "    \n",
    "    \n",
    "def render_row(df, idx, figsize=(36,12)):\n",
    "    row = df.iloc[idx]\n",
    "    lkps, lhm, rkps, rhm = parse_kps(row)\n",
    "    basedir = Path(row['basedir'])\n",
    "    fig, ((ax1, ax2)) = plt.subplots(figsize=figsize, ncols=2, nrows=1, constrained_layout=False)\n",
    "    #plt.subplots_adjust(hspace=50)\n",
    "    show_img(ax1, basedir/'left.jpg', lkps, lhm)\n",
    "    show_img(ax2, basedir/'right.jpg', rkps, rhm)\n",
    "    #fig.subplots_adjust(top=0.6)\n",
    "    fig.suptitle(f\"akpd_score={row['akpd_score0']}\", y=0.7)\n",
    "    return ax1, ax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1, ax2 = render_row(df_score, 5381, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = df.sort_values(by='akpd_score0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax1, ax2 = render_row(results0, 284)\n",
    "ax1, ax2 = render_row(df, 914)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 106 -  3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk=parse_kps(results[12])\n",
    "lc=pk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc[[0,1,7,2,5,6,4,3]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=np.array([.01, .1, .5 ,.8, .99])\n",
    "print(np.clip(hm, 0.1, 1)) # alpha\n",
    "print(1000*(1-hm)**2+3)\n",
    "print(hm**.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_colors = np.zeros((10,4))\n",
    "# for red the first column needs to be one\n",
    "rgba_colors[:,0] = 1.0\n",
    "# the fourth column needs to be your alphas\n",
    "rgba_colors[:, 3] = alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blurries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pair(pair_id, figsize=(36,12)):\n",
    "    pair_id = pair_id.strip()\n",
    "    pen_id = pair_id.partition('-')[0][1:]\n",
    "    basedir = Path(f'.data/{pen_id}/{pair_id}')\n",
    "    assert basedir.exists()\n",
    "    fig, ((ax1, ax2)) = plt.subplots(figsize=figsize, ncols=2, nrows=1)\n",
    "    img = _enhance(cv2.imread(str(basedir/'left.jpg')))\n",
    "    ax1.imshow(img)\n",
    "    ax2.imshow(_enhance(cv2.imread(str(basedir/'right.jpg'))))\n",
    "    return ax1, ax2\n",
    "\n",
    "def variance_of_laplacian(p):\n",
    "    image = cv2.imread(str(p))\n",
    "    image = _enhance(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)    \n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_pair(\"\"\"\n",
    "p198-t20201220_025809.455983-c1l4r\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".data/2020-12-28_biomass.1000.weight_vec.csv\") as fp:\n",
    "    reader = csv.DictReader(fp)\n",
    "    for i, row in zip(range(80),reader):\n",
    "        basedir = Path(row['basedir']) \n",
    "        llv = variance_of_laplacian(basedir/'left.jpg')\n",
    "        rlv = variance_of_laplacian(basedir/'right.jpg')\n",
    "        logging.info(\"{:40s} s={:0.3f} hm={:0.4f} {:7.2f} {:-9.5f} {:-9.5f}\".format(\n",
    "            row['pair_id'],\n",
    "            float(row['akpd_score0']),\n",
    "            float(row['hm_min']),\n",
    "            float(row['weight_std']),\n",
    "            llv,\n",
    "            rlv,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_quality.skip_classifier import SkipPredictor\n",
    "\n",
    "skip_pred = SkipPredictor(\n",
    "    '.data/skip-classifier/model.pt',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_left = lambda row: skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'left.jpg')))\n",
    "skip_right = lambda row: skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'right.jpg')))\n",
    "\n",
    "%time df['l_skip_score'] = df.apply(skip_left, axis=1)\n",
    "%time df['r_skip_score'] = df.apply(skip_right, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.data/2020-12-30_biomass.re-akpd.skip_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_left = lambda row: skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'left.jpg')))\n",
    "skip_right = lambda row: skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'right.jpg')))\n",
    "\n",
    "row=df.iloc[4]\n",
    "print(\"{} {}\".format(row.pair_id, row.akpd_score0))\n",
    "print(skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'left.jpg'))))\n",
    "print(skip_pred.predict(cv2.imread(str(Path(row.basedir) / 'right.jpg'))))\n",
    "render_pair(row.pair_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AKPD on thumb nail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.data/2020-12-30_biomass.re-akpd.skip_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_log_time = 0\n",
    "\n",
    "def show_img_arr(ax, img, pts, hm, enhance_flag=False):\n",
    "    if enhance_flag:\n",
    "        img = _enhance(img)\n",
    "    ax.imshow(img, alpha=0.999)\n",
    "\n",
    "    # plot keypoints\n",
    "    if pts is not None:\n",
    "        s = 2000*(1-hm)**2 + 8\n",
    "        alpha = np.clip(hm**1.5, 0.2, 1)\n",
    "        rgba = np.hstack([colors, alpha.reshape((-1,1))])\n",
    "        ax.scatter(x=pts[:,0], y=pts[:,1], marker='o', c=rgba, s=s)\n",
    "\n",
    "        outline = pts[[0,1,7,2,5,6,4,3]]\n",
    "        ax.plot(outline[:,0], outline[:,1], c='orange', ls=\"-\", lw=1.1, alpha=0.9)\n",
    "    \n",
    "    \n",
    "def render_row_tb(row, ann, l_img, r_img, figsize=(36,10)):\n",
    "    fig, ((ax1, ax2)) = plt.subplots(figsize=figsize, ncols=2, nrows=1)\n",
    "    lkps = lhm = rkps = rhm = None\n",
    "    if ann:\n",
    "        lkps, lhm, rkps, rhm = parse_kps(ann)\n",
    "    show_img_arr(ax1, l_img, lkps, lhm)\n",
    "    show_img_arr(ax2, r_img, rkps, rhm, enhance_flag=True)\n",
    "    akpd_score1 = ann['probability'] if ann else -1\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    fig.suptitle(f\"akpd_score 0->1\\n\\n{float(row.akpd_score0):0.3f}\\n{akpd_score1:0.3f}\\n\\nsize = {row.l_crop_size:.0%}, {row.r_crop_size:.0%}\", y=0.8)\n",
    "    return ax1, ax2\n",
    "\n",
    "\n",
    "def run_row_tb(args):\n",
    "  try:\n",
    "    _id, row = args\n",
    "    basedir    = Path(row.basedir)\n",
    "    left_path  = basedir / \"left_tb.jpg\"\n",
    "    right_path = basedir / \"right_tb.jpg\"\n",
    "    item = predict.Item(\n",
    "        Id             = str(_id),\n",
    "        left_crop_url  = str(left_path),\n",
    "        right_crop_url = str(right_path),\n",
    "        lcm            = {\"x_coord\": row.lx_coord, \"y_coord\": row.ly_coord},\n",
    "        rcm            = {\"x_coord\": row.rx_coord, \"y_coord\": row.ry_coord},\n",
    "        cm             = {\"pixelCountWidth\": row.pixelCountWidth, \"pixelCountHeight\": row.pixelCountHeight},\n",
    "        _down_sample   = 1,\n",
    "    )\n",
    "    items = [item]\n",
    "    predict.load_images(items)\n",
    "    \n",
    "    xscale = 512 / row.pixelCountWidth\n",
    "    yscale = 512 / row.pixelCountHeight\n",
    "\n",
    "    l_img0 = item.l_img\n",
    "    r_img0 = item.r_img\n",
    "    \n",
    "    _xscale = lambda x: int(x*xscale)\n",
    "    _yscale = lambda x: int(x*yscale)\n",
    "\n",
    "    xs0 = _xscale(row.lx_coord)\n",
    "    xs1 = _xscale(row.lx_coord + row.l_width)\n",
    "    ys0 = _yscale(row.ly_coord)\n",
    "    ys1 = _yscale(row.ly_coord + row.l_height)\n",
    "    item.l_img = l_img0[ys0:ys1, xs0:xs1, :]\n",
    "    item.l_img = cv2.resize(item.l_img, (row.l_width, row.l_height))\n",
    "    \n",
    "    xs0 = _xscale(row.rx_coord)\n",
    "    xs1 = _xscale(row.rx_coord + row.r_width)\n",
    "    ys0 = _yscale(row.ry_coord)\n",
    "    ys1 = _yscale(row.ry_coord + row.r_height)\n",
    "    item.r_img = r_img0[ys0:ys1, xs0:xs1, :]\n",
    "    item.r_img = cv2.resize(item.r_img, (row.r_width, row.r_height))\n",
    "    \n",
    "    if RENDER:\n",
    "        ax1, ax2 = render_row_tb(row, None, l_img0, r_img0)\n",
    "\n",
    "        rect = patches.Rectangle((row.lx_coord*xscale, row.ly_coord*yscale), row.l_width*xscale, row.l_height*yscale, \n",
    "                                 linewidth=1, edgecolor='y', facecolor=\"None\")\n",
    "        ax1.add_patch(rect)     \n",
    "        rect = patches.Rectangle((row.rx_coord*xscale, row.ry_coord*yscale), row.r_width*xscale, row.r_height*yscale, \n",
    "                                 linewidth=1, edgecolor='y', facecolor=\"None\")\n",
    "        ax2.add_patch(rect)     \n",
    "\n",
    "    akpd_result = model.predict(items)[0]\n",
    "\n",
    "    pts, hm = akpd_vector(akpd_result)\n",
    "    result = dict(\n",
    "        pts           = pts,\n",
    "        tb_hm         = hm,\n",
    "        tb_hm_min     = hm.min(),\n",
    "        tb_akpd_score = akpd_result['probability'],\n",
    "        akpd_score0   = row.akpd_score0\n",
    "    )   \n",
    "    \n",
    "    if RENDER:\n",
    "        ax1, ax2 = render_row_tb(row, akpd_result, item.l_img, item.r_img)\n",
    "    \n",
    "    # LOGGING\n",
    "    global next_log_time\n",
    "    t = time.time()\n",
    "    if t > next_log_time:\n",
    "        next_log_time = t + LOG_INTERVAL        \n",
    "        logging.info(\"{:04d} {:40s} tb_score={:0.3f}; tb_hm_min={:0.4f}\".format(\n",
    "            _id, \n",
    "            row.pair_id, \n",
    "            result['tb_akpd_score'],\n",
    "            result['tb_hm_min'],\n",
    "        ))\n",
    "\n",
    "    return result\n",
    "\n",
    "  except:\n",
    "    logging.exception(f\"id={_id} something breaks\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run_row_tb\n",
    "RENDER = 1\n",
    "row = df.iloc[5]\n",
    "print(akpd_vector(json.loads(row.annotation0))[0])\n",
    "run_row_tb((-1, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = 0\n",
    "LOG_INTERVAL = 15\n",
    "\n",
    "def run_tb_akpd(n, workers):\n",
    "    start_time = time.time()\n",
    "    logging.info(f\"N={n} workers={workers}\")\n",
    "\n",
    "    if workers == 0:\n",
    "        results = list(map(run_row_tb, zip(range(n), df.itertuples(index=False))))\n",
    "    else:        \n",
    "        with multiprocessing.dummy.Pool(workers) as pool:\n",
    "            results = pool.map(run_row_tb, zip(range(n), df.itertuples(index=False)), chunksize=4)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed {n} rows with workers={workers} avg={elapsed_time/n:0.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_tb_akpd(10000, workers=3)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(results)\n",
    "df['tb_hm'] = df1['tb_hm']\n",
    "df['tb_hm_min'] = df1['tb_hm_min']\n",
    "df['tb_akpd_score'] = df1['tb_akpd_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.data/2020-12-30_biomass.re-akpd.tb_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('tb.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AKPD score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter, itemgetter\n",
    "\n",
    "akpd_scores = []\n",
    "filename = '.data/2020-12-20_biomass.csv.gz'\n",
    "with gzip.open(filename, 'rt', encoding='utf-8') as fp:\n",
    "        reader = csv.DictReader(fp)\n",
    "        for row in reader:\n",
    "            akpd_scores.append(float(row['akpd_score']))\n",
    "\n",
    "for r, s in zip(results, akpd_scores):\n",
    "    r['akpd_score'] = s\n",
    "\n",
    "results1 = [r for r in results if r['akpd_score'] >= 0.5]\n",
    "results0 = [r for r in results if r['akpd_score'] < 0.5]\n",
    "\n",
    "results1.sort(key=itemgetter('akpd_score'))\n",
    "results0.sort(key=itemgetter('akpd_score'), reverse=True)\n",
    "print(f\"results1={len(results1)}, results0={len(results0)}\")\n",
    "\n",
    "akpd_scores = np.array(akpd_scores)\n",
    "akpd_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(akpd_scores > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(akpd_scores, bins=30)\n",
    "ax.set_title('akpd scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.data/2020-12-29_biomass.re-akpd.dimensions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lx_off'] = (df.lx_coord + df.l_width / 2) / df.pixelCountWidth                                                     \n",
    "df['ly_off'] = (df.ly_coord + df.l_height / 2) / df.pixelCountHeight \n",
    "df['l_trim_l'] = (df.lx_coord == 0).astype(int)\n",
    "df['l_trim_r'] = (df.lx_coord + df.l_width + 1 >= df.pixelCountWidth).astype(int)\n",
    "df['l_trim_t'] = (df.ly_coord == 0).astype(int)\n",
    "df['l_trim_b'] = (df.ly_coord + df.l_height + 1 >= df.pixelCountHeight).astype(int)\n",
    "df['l_crop_size'] = ((df.l_width + 1) * (df.l_height + 1)) / (df.pixelCountWidth * df.pixelCountHeight)\n",
    "\n",
    "df['rx_off'] = (df.rx_coord + df.r_width / 2) / df.pixelCountWidth                                                     \n",
    "df['ry_off'] = (df.ry_coord + df.r_height / 2) / df.pixelCountHeight \n",
    "df['r_trim_l'] = (df.rx_coord == 0).astype(int)\n",
    "df['r_trim_r'] = (df.rx_coord + df.r_width + 1 >= df.pixelCountWidth).astype(int)\n",
    "df['r_trim_t'] = (df.ry_coord == 0).astype(int)\n",
    "df['r_trim_b'] = (df.ry_coord + df.r_height + 1 >= df.pixelCountHeight).astype(int)\n",
    "df['r_crop_size'] = ((df.r_width + 1) * (df.r_height + 1)) / (df.pixelCountWidth * df.pixelCountHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.akpd_score0 >= 0.5\n",
    "HM = np.array(list(df.hm.apply(json.loads)))\n",
    "TB_HM = np.array(list(df.tb_hm))\n",
    "TB_AKPD_OK = df.tb_akpd_score >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_misc = df[[\n",
    "#'lx_off',\n",
    "#'ly_off',\n",
    "#'l_trim_l',\n",
    "#'l_trim_r',\n",
    "#'l_trim_t',\n",
    "#'l_trim_b',\n",
    "#'l_crop_size',\n",
    "#'rx_off',\n",
    "#'ry_off',\n",
    "#'r_trim_l',\n",
    "#'r_trim_r',\n",
    "#'r_trim_t',\n",
    "#'r_trim_b',\n",
    "#'r_crop_size',    \n",
    "#'llv',\n",
    "#'rlv',\n",
    "#'l_skip_score',\n",
    "#'r_skip_score',\n",
    "#'hm_min',\n",
    "#'hm_mean',\n",
    "'tb_hm_min',\n",
    "'tb_akpd_score',\n",
    "]].to_numpy()\n",
    "\n",
    "X = HM\n",
    "X = np.hstack([TB_AKPD_OK.astype(float).to_numpy().reshape(-1,1),X_misc])\n",
    "X = TB_AKPD_OK.astype(float).to_numpy().reshape(-1,1)\n",
    "X = TB_AKPD_OK.astype(float).to_numpy().reshape(-1,1)\n",
    "X = TB_AKPD_OK.astype(float).to_numpy().reshape(-1,1)\n",
    "X = np.hstack([TB_AKPD_OK.astype(float).to_numpy().reshape(-1,1),X_misc])\n",
    "X = TB_HM\n",
    "X = X_misc\n",
    "X = np.hstack([TB_HM,X_misc])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=437)\n",
    "print((X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(scaler.transform(X_train), y_train)\n",
    "print(clf.coef_)\n",
    "train_score = clf.score(scaler.transform(X_train), y_train)\n",
    "test_score = clf.score(scaler.transform(X_test), y_test)\n",
    "print(f\"training score {train_score}\")\n",
    "print(f\"test score {test_score}\")\n",
    "sklearn.metrics.confusion_matrix(clf.predict(scaler.transform(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(clf, X_test, y_test, title):\n",
    "    lr_probs = clf.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    yhat = clf.predict(X_test)\n",
    "    lr_precision, lr_recall, _ = sklearn.metrics.precision_recall_curve(y_test, lr_probs)\n",
    "    lr_f1 = sklearn.metrics.f1_score(y_test, yhat)\n",
    "    lr_auc = sklearn.metrics.auc(lr_recall, lr_precision)\n",
    "\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall\\n\\n' + 'f1=%.5f auc=%.5f' % (lr_f1, lr_auc))\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    return lr_recall, lr_precision, _\n",
    "\n",
    "lr_recall, lr_precision, _ = plot_precision_recall_curve(clf, X_test, y_test, 'AKPD score on Thumbnail + min heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(scaler.transform(X_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_pred)\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "plot_roc(fpr, tpr, auc, plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test[0:10])\n",
    "#sklearn.metrics.confusion_matrix(clf.predict(scaler.transform(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.coef_[0,:8])\n",
    "print(clf.coef_[0,8:16])\n",
    "print(clf.coef_[0,16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, auc, ax):\n",
    "    lw = 2\n",
    "    ax.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUC = %0.2f' % auc)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate (skiprate)')\n",
    "    ax.set_ylabel('Recall (KPI)')\n",
    "    ax.set_title(f'ROC Curve', size=20)\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = -df.r_skip_score\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, score)\n",
    "auc = sklearn.metrics.roc_auc_score(y, score)\n",
    "plot_roc(fpr, tpr, auc, plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run(\"load_test(200,0)\", \"restats\")\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats(SortKey.CUMULATIVE).print_stats(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sort_stats(SortKey.TIME).print_stats(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Orig\n",
    "2020-12-24 18:32:06,106 INFO  N=500 WORKERS=6\n",
    "2020-12-24 18:32:07,924 INFO  000 p67-t20201220_000004.170682-c1l2r        l0=0.653 r1=0.6793\n",
    "2020-12-24 18:32:18,476 INFO  050 p193-t20201220_000416.820198-c1l2r       l0=0.728 r1=0.6188\n",
    "2020-12-24 18:32:35,205 INFO  100 p86-t20201220_000900.746319-c1l2r        l0=0.699 r1=0.7934\n",
    "2020-12-24 18:32:43,916 INFO  150 p86-t20201220_001215.793702-c1l2r        l0=0.612 r1=0.0114\n",
    "2020-12-24 18:32:59,820 INFO  200 p138-t20201220_001653.210747-c1l2r       l0=0.630 r1=0.1509\n",
    "2020-12-24 18:33:12,730 INFO  250 p4-t20201220_002520.027040-c1l3r         l0=0.465 r1=0.0393\n",
    "2020-12-24 18:33:24,633 INFO  300 p85-t20201220_003124.881337-c1l2r        l0=0.008 r1=0.0094\n",
    "2020-12-24 18:33:37,326 INFO  350 p85-t20201220_003625.595207-c1l2r        l0=0.687 r1=0.8297\n",
    "2020-12-24 18:33:49,364 INFO  400 p85-t20201220_004024.373046-c1l2r        l0=0.648 r1=0.9228\n",
    "2020-12-24 18:34:01,708 INFO  450 p86-t20201220_004418.193731-c1l2r        l0=0.690 r1=0.7138\n",
    "\n",
    "Pair\n",
    "2020-12-24 18:46:31,496 INFO  N=500 WORKERS=1\n",
    "2020-12-24 18:46:32,058 INFO  000 p67-t20201220_000004.170682-c1l2r        l0=0.653 r1=0.6793\n",
    "2020-12-24 18:46:56,986 INFO  050 p193-t20201220_000416.820198-c1l2r       l0=0.728 r1=0.6188\n",
    "2020-12-24 18:47:25,557 INFO  100 p86-t20201220_000900.746319-c1l2r        l0=0.699 r1=0.7934\n",
    "2020-12-24 18:47:51,579 INFO  150 p86-t20201220_001215.793702-c1l2r        l0=0.612 r1=0.0114\n",
    "2020-12-24 18:48:17,300 INFO  200 p138-t20201220_001653.210747-c1l2r       l0=0.630 r1=0.1509\n",
    "2020-12-24 18:48:42,029 INFO  250 p4-t20201220_002520.027040-c1l3r         l0=0.465 r1=0.0393\n",
    "2020-12-24 18:49:08,200 INFO  300 p85-t20201220_003124.881337-c1l2r        l0=0.008 r1=0.0094\n",
    "2020-12-24 18:49:34,432 INFO  350 p85-t20201220_003625.595207-c1l2r        l0=0.687 r1=0.8297\n",
    "2020-12-24 18:50:00,873 INFO  400 p85-t20201220_004024.373046-c1l2r        l0=0.648 r1=0.9228\n",
    "2020-12-24 18:50:24,729 INFO  450 p86-t20201220_004418.193731-c1l2r        l0=0.690 r1=0.7138\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left and right separate\n",
    "\n",
    "- Processed 500 rows with workers=1 avg=0.560 -> 500\n",
    "- Processed 500 rows with workers=2 avg=0.330 -> 500\n",
    "- Processed 500 rows with workers=3 avg=0.275 -> 500\n",
    "- Processed 500 rows with workers=4 avg=0.271 -> 500\n",
    "- Processed 500 rows with workers=5 avg=0.270 -> 500\n",
    "- Processed 500 rows with workers=6 avg=0.269 -> 500\n",
    "\n",
    "### Input as pair\n",
    "\n",
    "- Processed 500 rows with workers=1 avg=0.519 -> 500\n",
    "- Processed 500 rows with workers=2 avg=0.303 -> 500\n",
    "- Processed 500 rows with workers=3 avg=0.266 -> 500\n",
    "- Processed 500 rows with workers=4 avg=0.257 -> 500\n",
    "- Processed 500 rows with workers=5 avg=0.259 -> 500\n",
    "- Processed 500 rows with workers=6 avg=0.251 -> 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_path, row['left_crop_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['left_crop_url'].partition('.amazonaws.com')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "l_input = self.model.get_tensor_by_name(self.config['input_name'])\n",
    "l_output = self.model.get_tensor_by_name(self.config['output_name'])\n",
    "item = items[0]\n",
    "#self.process(item.l_img, self._sess, item.lcm['x_coord'], item.lcm['y_coord'], l_input, l_output)\n",
    "img = item.l_img\n",
    "height, width, _ = img.shape\n",
    "img0 = self.prepare(item.l_img)\n",
    "imgs = np.stack([img0])\n",
    "predict_heatmap = self._sess.run(l_output, feed_dict = {l_input: imgs})\n",
    "final_stage_heatmap = predict_heatmap.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs.shape)\n",
    "predict_heatmap.shape, final_stage_heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps, img_height, img_width, x_offset, y_offset = \\\n",
    "  final_stage_heatmap, height, width, item.lcm['x_coord'], item.lcm['y_coord']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxindex = hm.argmax()\n",
    "np.unravel_index(hm.argmax(), hm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(hm == hm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=6\n",
    "hm = cv2.resize(heatmaps[..., c], (img_width, img_height))\n",
    "hm_max = np.where(hm == hm.max())\n",
    "coordinates = [int(np.mean(hm_max[1])), int(np.mean(hm_max[0]))]\n",
    "keypoint_x = int(coordinates[0])\n",
    "keypoint_y = int(coordinates[1])\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.shape, hm_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog, ax = plt.subplots(ncols=4, figsize=(15,5))\n",
    "ax[0].imshow(heatmaps[..., 0])\n",
    "ax[1].imshow(heatmaps[..., 1])\n",
    "ax[2].imshow(heatmaps[..., 2])\n",
    "ax[3].imshow(heatmaps[..., 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(img0[0,...] + 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "self = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [predict.Item(\n",
    "    Id             = row[\"id\"],\n",
    "    left_crop_url  = row[\"leftCropUrl\"],\n",
    "    right_crop_url = row[\"rightCropUrl\"],\n",
    "    lcm            = {\"x_coord\": row[\"leftCropMetadata\"][\"x_coord\"], \"y_coord\": row[\"leftCropMetadata\"][\"y_coord\"]},\n",
    "    rcm            = {\"x_coord\": row[\"rightCropMetadata\"][\"x_coord\"], \"y_coord\": row[\"rightCropMetadata\"][\"y_coord\"]},\n",
    "    cm             = {\"pixelCountWidth\": row[\"cameraMetadata\"][\"pixelCountWidth\"], \"pixelCountHeight\": row[\"cameraMetadata\"][\"pixelCountHeight\"]},\n",
    ") for i, row in enumerate(body)]\n",
    "item = items[0]\n",
    "predict.load_images(items)\n",
    "logging.info(f\"loaded images: {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(\".data/model.pb\", \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "converted_graph_def = trt.create_inference_graph(\n",
    "    input_graph_def = graph_def,\n",
    "    outputs = [\"stage_3/mid_conv7/BiasAdd:0\"],\n",
    "    precision_mode='FP16',\n",
    "#    precision_mode=\"INT8\",\n",
    "#    use_calibration=True,\n",
    ")\n",
    "\n",
    "\n",
    "gd0 = graph_def\n",
    "gd1 = converted_graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(gd1,\n",
    "                        input_map=None,\n",
    "                        return_elements=None,\n",
    "                        name=\"\",\n",
    "                        op_dict=None,\n",
    "                        producer_op_list=None)\n",
    "self._model = graph\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1/2\n",
    "self._sess = tf.Session(graph=self.model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(items)\n",
    "result[0][\"leftCrop\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(items)\n",
    "result[0][\"leftCrop\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer()\n",
    "l_input = self._model.get_tensor_by_name(self.config['input_name'])\n",
    "l_output = self._model.get_tensor_by_name(self.config['output_name'])\n",
    "img0 = self.prepare(item.l_img, 1)\n",
    "img1 = self.prepare(item.r_img, 1)\n",
    "imgs = np.stack([img0, img1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = self._sess.run(l_output, feed_dict = {l_input: imgs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit H = self._sess.run(l_output, feed_dict = {l_input: imgs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "170000*.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26000-_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_/170000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
