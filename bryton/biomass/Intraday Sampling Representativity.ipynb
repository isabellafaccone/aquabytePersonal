{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from research.utils.datetime_utils import day_difference, add_days\n",
    "from research.utils.datetime_utils import get_dates_in_range\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationMetricsEstimator(object):\n",
    "\n",
    "    def __init__(self, biomass_computations, use_kernel_weight_algo = False, kernel_details=(50, 100, 0.1)):\n",
    "        self.biomass_computations = biomass_computations\n",
    "        self.use_kernel_weight_algo = use_kernel_weight_algo\n",
    "        self.kernel_bins = kernel_details[0]\n",
    "        self.kernel_split = kernel_details[1]\n",
    "        self.kernel_sd = kernel_details[2]\n",
    "        self.bcs_by_date = defaultdict(list)\n",
    "        self.unique_dates_nr = []\n",
    "        self.unique_dates = []\n",
    "        self.average_weights = []\n",
    "        self.sample_sizes = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.biomass_computations = sorted(self.biomass_computations, key=lambda x: x[0])\n",
    "        self.generate_daily_values()\n",
    "        \n",
    "    def get_average_weight_on_date(self, weights, depths):\n",
    "        if self.use_kernel_weight_algo:\n",
    "            df = pd.DataFrame(list(zip(weights, depths)), columns = ['weight', 'depth'])\n",
    "            \n",
    "            N, bins, _ = plt.hist(weights, bins = self.kernel_bins)\n",
    "            \n",
    "            x_d = np.linspace(0, 2, self.kernel_split)\n",
    "            \n",
    "            average_weights = []\n",
    "            max_densities = []\n",
    "\n",
    "            for index in range(len(N)):\n",
    "                lowerBin = bins[index]\n",
    "                upperBin = bins[index + 1]\n",
    "                subset = (df['weight'] >= lowerBin) & (df['weight'] < upperBin) \n",
    "                depths = df[subset]['depth'].values\n",
    "                density = sum(norm.pdf((x_d - xi) / self.kernel_sd) for xi in depths)\n",
    "\n",
    "                average_weights.append(np.mean(df[subset]['weight']))\n",
    "                max_densities.append(np.max(density))\n",
    "                \n",
    "            max_densities = np.array(max_densities)\n",
    "            average_weights = np.nan_to_num(np.array(average_weights))\n",
    "            average_weight = np.sum(max_densities * average_weights) / np.sum(max_densities)\n",
    "\n",
    "            return average_weight\n",
    "        \n",
    "        return np.mean(weights)\n",
    "\n",
    "    def generate_daily_values(self):\n",
    "        weights_for_date = []\n",
    "        depths_for_date = []\n",
    "        curr_date = self.biomass_computations[0][0]\n",
    "        for date, weight, kf, depth in self.biomass_computations:\n",
    "            self.bcs_by_date[date].append((weight, kf))\n",
    "            if date != curr_date:\n",
    "                self.unique_dates.append(curr_date)\n",
    "                self.average_weights.append(self.get_average_weight_on_date(weights_for_date, depths_for_date))\n",
    "                self.sample_sizes.append(len(weights_for_date))\n",
    "                weights_for_date = [weight]\n",
    "                depths_for_date = [depth]\n",
    "                curr_date = date\n",
    "            else:\n",
    "                weights_for_date.append(weight)\n",
    "                depths_for_date.append(depth)\n",
    "\n",
    "        self.unique_dates.append(curr_date)\n",
    "        self.average_weights.append(self.get_average_weight_on_date(weights_for_date, depths_for_date))\n",
    "        self.sample_sizes.append(len(weights_for_date))\n",
    "        self.unique_dates_nr = [day_difference(date, self.unique_dates[0]) for date in self.unique_dates]\n",
    "\n",
    "    def generate_raw_daily_metrics_on_date(self, date):\n",
    "        if date in self.unique_dates:\n",
    "            idx = self.unique_dates.index(date)\n",
    "            return self.average_weights[idx], self.sample_sizes[idx]\n",
    "        return None, None\n",
    "\n",
    "    def generate_raw_weights_kfs_on_date(self, date):\n",
    "        if len(self.bcs_by_date[date]) > 0:\n",
    "            weights, kfs = [list(l) for l in list(zip(*self.bcs_by_date[date]))]\n",
    "            return weights, kfs\n",
    "        return [], []\n",
    "\n",
    "    def get_start_end_idx(self, start_date, end_date):\n",
    "        if start_date > self.unique_dates[-1] or end_date < self.unique_dates[0]:\n",
    "            return -1, -1\n",
    "        start_idx = [idx for idx, date in enumerate(self.unique_dates) if date >= start_date][0]\n",
    "        end_idx = [idx for idx, date in enumerate(self.unique_dates) if date <= end_date][-1] + 1\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def compute_growth_rate(self, date, start_date, end_date, decay=0.1):\n",
    "        if not any([date in self.unique_dates for date in get_dates_in_range(start_date, end_date)]):\n",
    "            return None, None\n",
    "        start_idx, end_idx = self.get_start_end_idx(start_date, end_date)\n",
    "        X = np.array([day_difference(d, date) for d in self.unique_dates[start_idx:end_idx]]).reshape(-1, 1)\n",
    "        y = np.log(np.array(self.average_weights[start_idx:end_idx]))\n",
    "        n = np.array(self.sample_sizes[start_idx:end_idx])\n",
    "\n",
    "        if X.shape[0] < 4:\n",
    "            return None, None\n",
    "\n",
    "        sample_weights = np.multiply(n, np.exp(-decay * np.abs(X.squeeze())))\n",
    "        reg = LinearRegression().fit(X, y, sample_weight=sample_weights)\n",
    "        growth_rate = reg.coef_[0]\n",
    "        y_pred = reg.predict(X)\n",
    "\n",
    "        error_magnitude_pct = np.average(((np.exp(y) - np.exp(y_pred)) / np.exp(y_pred))**2,\n",
    "                                         weights=sample_weights)**0.5\n",
    "        return growth_rate, error_magnitude_pct\n",
    "\n",
    "    def compute_local_growth_rate(self, date, incorporate_future, window=7):\n",
    "        # compute local growth rate\n",
    "        day_diffs = np.array([day_difference(d, date) for d in self.unique_dates])\n",
    "        if incorporate_future:\n",
    "            start, end = add_days(date, -window), add_days(date, window // 2)\n",
    "            if not any([date in get_dates_in_range(start, end) for date in self.unique_dates]):\n",
    "                return None, None\n",
    "            end_idx = np.where(day_diffs <= window // 2)[0][-1]\n",
    "            end_date = self.unique_dates[end_idx]\n",
    "            start_date = add_days(end_date, -window)\n",
    "        else:\n",
    "            start_date, end_date = add_days(date, -window), date\n",
    "        growth_rate, error_magnitude_pct = self.compute_growth_rate(date, start_date, end_date)\n",
    "        return growth_rate, error_magnitude_pct\n",
    "\n",
    "    def generate_historical_weights(self, date, window=7):\n",
    "        seven_days_ago, yesterday = add_days(date, -window), add_days(date, -1)\n",
    "        dates = get_dates_in_range(seven_days_ago, yesterday)\n",
    "        historical_weights = []\n",
    "        for curr_date in dates:\n",
    "            weights, _ = self.generate_raw_weights_kfs_on_date(curr_date)\n",
    "            historical_weights.extend(weights)\n",
    "\n",
    "        return historical_weights\n",
    "\n",
    "    def generate_distribution_consistency(self, date, window=7):\n",
    "        raw_weights, _ = self.generate_raw_weights_kfs_on_date(date)\n",
    "        historical_weights = self.generate_historical_weights(date, window=window)\n",
    "        if not raw_weights or not historical_weights:\n",
    "            return None\n",
    "        raw_weights = np.array(raw_weights)\n",
    "        historical_weights = np.array(historical_weights)\n",
    "        mean_adjustment = np.mean(raw_weights) - np.mean(historical_weights)\n",
    "        x = np.percentile(historical_weights + mean_adjustment, list(range(100)))\n",
    "        y = np.percentile(raw_weights, list(range(100)))\n",
    "        distribution_confidence = 1.0 - 10.0 * (np.mean(np.abs(y[1:99] - x[1:99]) ** 2) ** 0.5 / 10000.0)\n",
    "        return distribution_confidence\n",
    "\n",
    "    def generate_smart_metrics_on_date(self, date, max_day_difference=3, bucket_size=100, incorporate_future=True,\n",
    "                                       apply_growth_rate=True):\n",
    "\n",
    "        # compute metrics on this date\n",
    "        distribution_consistency = self.generate_distribution_consistency(date, window=7)\n",
    "        raw_average_weight, raw_sample_size = self.generate_raw_daily_metrics_on_date(date)\n",
    "        _, raw_kfs = self.generate_raw_weights_kfs_on_date(date)\n",
    "        raw_average_kf = np.mean([kf for kf in raw_kfs if kf is not None])\n",
    "\n",
    "        # compute local growth rate\n",
    "        growth_rate, error_magnitude_pct = self.compute_local_growth_rate(date, incorporate_future)\n",
    "\n",
    "        # compute smart average\n",
    "\n",
    "        look_ahead = max_day_difference if incorporate_future else 0\n",
    "        start_date, end_date = add_days(date, -max_day_difference), add_days(date, look_ahead)\n",
    "        if not any([date in get_dates_in_range(start_date, end_date) for date in self.unique_dates]):\n",
    "            return {}\n",
    "        start_idx, end_idx = self.get_start_end_idx(start_date, end_date)\n",
    "\n",
    "        if growth_rate and apply_growth_rate and raw_sample_size and error_magnitude_pct < 0.02 and \\\n",
    "                abs(growth_rate) < 0.02:\n",
    "            growth_rate_for_smart_avg = growth_rate\n",
    "        else:\n",
    "            growth_rate_for_smart_avg = 0.0\n",
    "\n",
    "        x = np.array([day_difference(d, date) for d in self.unique_dates[start_idx:end_idx]])\n",
    "        y = np.array(self.average_weights[start_idx:end_idx])\n",
    "        n = np.array(self.sample_sizes[start_idx:end_idx])\n",
    "        sample_size = int(np.sum(n))\n",
    "        smart_average = np.sum(np.exp(-x * growth_rate_for_smart_avg) * y * n) / sample_size\n",
    "\n",
    "        # compute smart distribution\n",
    "        adj_weights, kfs = [], []\n",
    "        for date_idx, date in enumerate(self.unique_dates[start_idx:end_idx]):\n",
    "            weights_for_date, kfs_for_date = self.generate_raw_weights_kfs_on_date(date)\n",
    "            adj_weights_for_date = np.array(weights_for_date) * np.exp(growth_rate_for_smart_avg * x[date_idx])\n",
    "            adj_weights.extend(adj_weights_for_date)\n",
    "            kfs.extend(kfs_for_date)\n",
    "\n",
    "        assert len(adj_weights) == np.sum(n), 'Inconsistent sample sizes!'\n",
    "\n",
    "        adj_weights, kfs = np.array(adj_weights), np.array(kfs)\n",
    "        smart_distribution = dict()\n",
    "\n",
    "        bucket_size_kg = 1e-3 * bucket_size\n",
    "        buckets = [round(x, 2) for x in np.arange(0.0, 1e-3 * np.max(adj_weights), bucket_size_kg)]\n",
    "        for b in buckets:\n",
    "            low, high = 1e3 * b, 1e3 * (b + bucket_size_kg)\n",
    "            count = adj_weights[(adj_weights >= low) & (adj_weights < high)].shape[0]\n",
    "            kfs_for_bucket = [kf if kf else np.nan for kf in kfs[(adj_weights >= low) & (adj_weights < high)]]\n",
    "            mean_kf = np.mean(kfs_for_bucket) if count > 0 else np.nan\n",
    "            smart_distribution[str(b)] = {\n",
    "                'count': count,\n",
    "                'avgKFactor': None if np.isnan(mean_kf) else mean_kf\n",
    "            }\n",
    "\n",
    "        metrics = dict(\n",
    "            raw_average_weight=raw_average_weight,\n",
    "            raw_average_kf=raw_average_kf,\n",
    "            raw_sample_size=raw_sample_size,\n",
    "            smart_average_weight=smart_average,\n",
    "            smart_average_kf=np.mean([kf for kf in kfs if kf is not None]),\n",
    "            smart_distribution=smart_distribution,\n",
    "            smart_sample_size=sample_size,\n",
    "            adj_weights=adj_weights,\n",
    "            kfs=kfs,\n",
    "            growth_rate=growth_rate,\n",
    "            error_magnitude_pct=error_magnitude_pct,\n",
    "            distribution_consistency=distribution_consistency\n",
    "        )\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryCache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_id = 60\n",
    "start_date = '2020-04-25'\n",
    "end_date = '2020-05-25'\n",
    "# pen_id = 66\n",
    "# start_date = '2020-05-20'\n",
    "# end_date = '2020-06-10'\n",
    "# akpd_filter = 0.99\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=%i) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at >= '%s'\n",
    "      AND prod.biomass_computations.captured_at <= '%s'\n",
    "      AND prod.biomass_computations.akpd_score > %0.4f) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.left_crop_url)\n",
    "    ) x\n",
    "    WHERE x.captured_at >= '%s'\n",
    "    AND x.captured_at <= '%s'\n",
    "    AND x.pen_id = %i\n",
    "    AND x.group_id = '%i';\n",
    "\"\"\" % (pen_id, start_date, end_date, akpd_filter, start_date, end_date, pen_id, pen_id)\n",
    "\n",
    "if query in queryCache:\n",
    "    df = queryCache[query].copy()\n",
    "else:\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "    \n",
    "    depths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ann, cm = row.annotation, row.camera_metadata\n",
    "        wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "        depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "        depths.append(depth)\n",
    "    df['depth'] = depths\n",
    "    \n",
    "    df = df.sort_values('captured_at').copy(deep=True)\n",
    "    df.index = pd.to_datetime(df.captured_at)\n",
    "    dates = df.index.date.astype(str)\n",
    "    df['date'] = dates\n",
    "    df['hour'] = df.index.hour\n",
    "\n",
    "    if 'estimated_k_factor' not in df.columns.tolist():\n",
    "        df['estimated_k_factor'] = 0.0\n",
    "    \n",
    "    queryCache[query] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_computations = []\n",
    "for idx, row in df.iterrows():\n",
    "    biomass_computations.append((row.date, row.estimated_weight_g, row.estimated_k_factor, row.depth))\n",
    "\n",
    "kernel_details = (50, 100, 0.1)\n",
    "pme = PopulationMetricsEstimator(biomass_computations, use_kernel_weight_algo = False, kernel_details = kernel_details)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "\n",
    "# startDate, startHour = datetime.strptime(df.ix[0]['date'], '%Y-%m-%d'), df.ix[0]['hour']\n",
    "# endDate, endHour = datetime.strptime(df.ix[-1]['date'], '%Y-%m-%d'), df.ix[-1]['hour']\n",
    "\n",
    "startDate = df.index[0]\n",
    "endDate = df.index[-1]\n",
    "\n",
    "maxWeight = max(df['estimated_weight_g'])\n",
    "maxWeightInt = int(maxWeight / 1000)\n",
    "\n",
    "diff = endDate - startDate\n",
    "days, seconds = diff.days, diff.seconds\n",
    "hours = int((days * 24 + seconds // 3600) / 1)\n",
    "\n",
    "a = np.zeros((hours + 1, maxWeightInt + 1))\n",
    "print(a.shape)\n",
    "\n",
    "count = 0\n",
    "\n",
    "dateStrings = []\n",
    "dates = []\n",
    "distConfs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row.date not in dateStrings:\n",
    "        dateStrings.append(row.date)\n",
    "        dates.append(datetime.strptime(row.date, '%Y-%m-%d'))\n",
    "        \n",
    "        distConf = pme.generate_distribution_consistency(row.date)\n",
    "        \n",
    "        if distConf and distConf < .1:\n",
    "            distConfs.append(distConfs[-1])\n",
    "        else:\n",
    "            distConfs.append(distConf)\n",
    "\n",
    "    diff = idx - startDate\n",
    "    days, seconds = diff.days, diff.seconds\n",
    "    hours = int((days * 24 + seconds // 3600) / 1)\n",
    "    \n",
    "    weight = row['estimated_weight_g']\n",
    "    weightInt = int(weight / 1000)\n",
    "    \n",
    "    a[hours, weightInt] = a[hours, weightInt] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pysal as ps\n",
    "\n",
    "# coefs = []\n",
    "\n",
    "# window = 24\n",
    "# skip = 24\n",
    "\n",
    "# for i in np.arange(window, hours, skip):\n",
    "#     b = a[(i - window):i,2:6]\n",
    "#     w = ps.lib.weights.lat2W(b.shape[0], b.shape[1])\n",
    "#     mi = ps.explore.esda.Moran(b, w)\n",
    "#     coefs.append(mi.I)\n",
    "    \n",
    "# fig, axes = plt.subplots(2, 1, figsize=(15, 20))\n",
    "# axes[0].plot(dates[1:], coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ps.lib.weights.lat2W(a.shape[0], a.shape[1])\n",
    "mi = ps.explore.esda.Moran_Local(a, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = np.log(1 + np.mean(a, 1))\n",
    "f = np.mean(a, 1)\n",
    "\n",
    "w = ps.lib.weights.lat2W(f.shape[0], 1)\n",
    "mi = ps.explore.esda.Moran_Local(f, w)\n",
    "\n",
    "plt.plot(mi.Is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(f, mi.Is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24\n",
    "skip = 24\n",
    "\n",
    "coefs = []\n",
    "\n",
    "for i in np.arange(window, hours, skip):\n",
    "    data = f[(i - window):i]\n",
    "    \n",
    "    if np.sum(data) < 1:\n",
    "        coefs.append(coefs[-1])\n",
    "        continue\n",
    "\n",
    "    a = (np.percentile(data, 90) - np.percentile(data, 10)) / (np.mean(data))\n",
    "    \n",
    "    coefs.append(1 - a / 5)\n",
    "    #coefs.append(np.percentile(data, 95) / np.mean(data))\n",
    "    \n",
    "Y = coefs\n",
    "X = distConfs[1:]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 20))\n",
    "axes[0].bar(dates[1:], coefs)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Sampling Representativity')\n",
    "axes[0].set_title('Pen %i Sampling Representativity' % (pen_id, ))\n",
    "\n",
    "axes[1].bar(dates[1:], distConfs[1:])\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Distribution Consistency')\n",
    "axes[1].set_title('Pen %i Distribution Consistency (%0.2f)' % (pen_id, results.rsquared, ))\n",
    "axes[1].set_ylim((.8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mi.Is.reshape(a.shape)\n",
    "d = np.mean(c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24\n",
    "skip = 24\n",
    "\n",
    "coefs = []\n",
    "\n",
    "for i in np.arange(window, hours, skip):\n",
    "    coefs.append(np.std(d[(i - window):i]))\n",
    "    \n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 20))\n",
    "axes[0].plot(dates[1:], coefs)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Sampling Representativity')\n",
    "axes[0].set_title('Pen 60 Sampling Representativity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM (\n",
    "          (SELECT * FROM prod.crop_annotation cas\n",
    "          INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "          WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "          AND cas.annotation_state_id = 3\n",
    "          AND cas.pen_id=%i) a\n",
    "        RIGHT JOIN \n",
    "          (SELECT left_crop_url, estimated_weight_g, akpd_score FROM prod.biomass_computations\n",
    "          WHERE prod.biomass_computations.captured_at >= '%s'\n",
    "          AND prod.biomass_computations.captured_at <= '%s'\n",
    "          AND prod.biomass_computations.akpd_score > %0.4f) bc \n",
    "        ON \n",
    "          (a.left_crop_url=bc.left_crop_url)\n",
    "        ) x\n",
    "        WHERE x.captured_at >= '%s'\n",
    "        AND x.captured_at <= '%s'\n",
    "        AND x.pen_id = %i\n",
    "        AND x.group_id = '%i';\n",
    "    \"\"\" % (pen_id, start_date, end_date, akpd_filter, start_date, end_date, pen_id, pen_id)\n",
    "\n",
    "    if query in queryCache:\n",
    "        df = queryCache[query].copy()\n",
    "    else:\n",
    "        df = rds_access_utils.extract_from_database(query)\n",
    "\n",
    "        depths = []\n",
    "        for idx, row in df.iterrows():\n",
    "            ann, cm = row.annotation, row.camera_metadata\n",
    "            wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], cm)\n",
    "            depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "            depths.append(depth)\n",
    "        df['depth'] = depths\n",
    "\n",
    "        df = df.sort_values('captured_at').copy(deep=True)\n",
    "        df.index = pd.to_datetime(df.captured_at)\n",
    "        dates = df.index.date.astype(str)\n",
    "        df['date'] = dates\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        if 'estimated_k_factor' not in df.columns.tolist():\n",
    "            df['estimated_k_factor'] = 0.0\n",
    "\n",
    "        queryCache[query] = df.copy()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical = [\n",
    "    {\n",
    "        pen_id: 60,\n",
    "        start_date: '2020-04-25',\n",
    "        end_date: '2020-05-25'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
