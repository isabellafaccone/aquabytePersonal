{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains constants representing core & auxiliary fish body parts.\n",
    "\"\"\"\n",
    "\n",
    "UPPER_LIP = 'UPPER_LIP'\n",
    "EYE = 'EYE'\n",
    "PECTORAL_FIN = 'PECTORAL_FIN'\n",
    "DORSAL_FIN = 'DORSAL_FIN'\n",
    "PELVIC_FIN = 'PELVIC_FIN'\n",
    "ADIPOSE_FIN = 'ADIPOSE_FIN'\n",
    "ANAL_FIN = 'ANAL_FIN'\n",
    "TAIL_NOTCH = 'TAIL_NOTCH'\n",
    "UPPER_PRECAUDAL_PIT = 'UPPER_PRECAUDAL_PIT'\n",
    "LOWER_PRECAUDAL_PIT = 'LOWER_PRECAUDAL_PIT'\n",
    "HYPURAL_PLATE = 'HYPURAL_PLATE'\n",
    "\n",
    "core_body_parts = sorted([UPPER_LIP,\n",
    "                          EYE,\n",
    "                          PECTORAL_FIN,\n",
    "                          DORSAL_FIN,\n",
    "                          PELVIC_FIN,\n",
    "                          ADIPOSE_FIN,\n",
    "                          ANAL_FIN,\n",
    "                          TAIL_NOTCH])\n",
    "\n",
    "auxiliary_body_parts = sorted([UPPER_PRECAUDAL_PIT,\n",
    "                               LOWER_PRECAUDAL_PIT,\n",
    "                               HYPURAL_PLATE])\n",
    "\n",
    "all_body_parts = sorted(core_body_parts + auxiliary_body_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module contains utility helper functions for the WeightEstimator class.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "CameraMetadata = namedtuple('CameraMetadata',\n",
    "                            ['focal_length', 'focal_length_pixel', 'baseline_m',\n",
    "                             'pixel_count_width', 'pixel_count_height', 'image_sensor_width',\n",
    "                             'image_sensor_height'])\n",
    "\n",
    "\n",
    "def get_left_right_keypoint_arrs(annotation: Dict[str, List[Dict]]) -> Tuple:\n",
    "    \"\"\"Gets numpy array of left and right keypoints given input keypoint annotation.\n",
    "    Args:\n",
    "        annotation: dict with keys 'leftCrop' and 'rightCrop'. Values are lists where each element\n",
    "        is a dict with keys 'keypointType', 'xCrop' (num pixels from crop left edge),\n",
    "        'yCrop' (num pixels from crop top edge), 'xFrame' (num pixels from full frame left edge),\n",
    "        and 'yFrame' (num pixels from full frame top edge).\n",
    "    Returns:\n",
    "        X_left: numpy array containing left crop (xFrame, yFrame) for each key-point ordered\n",
    "        alphabetically.\n",
    "        X_right: same as above, but for right crop.\n",
    "    \"\"\"\n",
    "\n",
    "    left_keypoints, right_keypoints = {}, {}\n",
    "    for item in annotation['leftCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        left_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    for item in annotation['rightCrop']:\n",
    "        body_part = item['keypointType']\n",
    "        right_keypoints[body_part] = (item['xFrame'], item['yFrame'])\n",
    "\n",
    "    left_keypoint_arr, right_keypoint_arr = [], []\n",
    "    for body_part in core_body_parts:\n",
    "        left_keypoint_arr.append(left_keypoints[body_part])\n",
    "        right_keypoint_arr.append(right_keypoints[body_part])\n",
    "\n",
    "    X_left = np.array(left_keypoint_arr)\n",
    "    X_right = np.array(right_keypoint_arr)\n",
    "    return X_left, X_right\n",
    "\n",
    "\n",
    "def normalize_left_right_keypoint_arrs(X_left: np.ndarray, X_right: np.ndarray) -> Tuple:\n",
    "    \"\"\"Normalizes input left and right key-point arrays. The normalization involves (1) 2D\n",
    "    translation of all keypoints such that they are centered, (2) rotation of the 2D coordiantes\n",
    "    about the center such that the line passing through UPPER_LIP and fish center is horizontal.\n",
    "    \"\"\"\n",
    "\n",
    "    # translate key-points, perform reflection if necessary\n",
    "    upper_lip_idx = core_body_parts.index(UPPER_LIP)\n",
    "    tail_notch_idx = core_body_parts.index(TAIL_NOTCH)\n",
    "    if X_left[upper_lip_idx, 0] > X_left[tail_notch_idx, 0]:\n",
    "        X_center = 0.5 * (np.max(X_left, axis=0) + np.min(X_left, axis=0))\n",
    "        X_left_centered = X_left - X_center\n",
    "        X_right_centered = X_right - X_center\n",
    "    else:\n",
    "        X_center = 0.5 * (np.max(X_right, axis=0) + np.min(X_right, axis=0))\n",
    "        X_left_centered = X_right - X_center\n",
    "        X_right_centered = X_left - X_center\n",
    "        X_left_centered[:, 0] = -X_left_centered[:, 0]\n",
    "        X_right_centered[:, 0] = -X_right_centered[:, 0]\n",
    "\n",
    "    # rotate key-points\n",
    "    upper_lip_x, upper_lip_y = tuple(X_left_centered[upper_lip_idx])\n",
    "    theta = np.arctan(upper_lip_y / upper_lip_x)\n",
    "    R = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    D = X_left_centered - X_right_centered\n",
    "    X_left_rot = np.dot(X_left_centered, R)\n",
    "    X_right_rot = X_left_rot - D\n",
    "    return X_left_rot, X_right_rot\n",
    "\n",
    "\n",
    "def convert_to_world_point_arr(X_left: np.ndarray, X_right: np.ndarray,\n",
    "                               camera_metadata: CameraMetadata) -> np.ndarray:\n",
    "    \"\"\"Converts input left and right normalized keypoint arrays into world coordinate array.\"\"\"\n",
    "\n",
    "    y_world = camera_metadata.focal_length_pixel * camera_metadata.baseline_m / \\\n",
    "              (X_left[:, 0] - X_right[:, 0])\n",
    "\n",
    "    # Note: the lines commented out below are technically the correct formula for conversion\n",
    "    # x_world = X_left[:, 0] * y_world / camera_metadata.focal_length_pixel\n",
    "    # z_world = -X_left[:, 1] * y_world / camera_metadata.focal_length_pixel\n",
    "    x_world = ((X_left[:, 0] * camera_metadata.image_sensor_width / camera_metadata.pixel_count_width) * y_world) / (camera_metadata.focal_length)\n",
    "    z_world = (-(X_left[:, 1] * camera_metadata.image_sensor_height / camera_metadata.pixel_count_height) * y_world) / (camera_metadata.focal_length)\n",
    "    X_world = np.vstack([x_world, y_world, z_world]).T\n",
    "    return X_world\n",
    "\n",
    "\n",
    "def stabilize_keypoints(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transforms world coordinate array so that neural network inputs are stabilized\"\"\"\n",
    "    X_new = np.zeros(X.shape)\n",
    "    X_new[:, 0] = 0.5 * X[:, 0] / X[:, 1]\n",
    "    X_new[:, 1] = 0.5 * X[:, 2] / X[:, 1]\n",
    "    X_new[:, 2] = 0.05 / X[:, 1]\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def convert_to_nn_input(annotation: Dict[str, List[Dict]], camera_metadata: CameraMetadata) \\\n",
    "        -> torch.Tensor:\n",
    "    \"\"\"Convrts input keypoint annotation and camera metadata into neural network tensor input.\"\"\"\n",
    "    X_left, X_right = get_left_right_keypoint_arrs(annotation)\n",
    "    X_left_norm, X_right_norm = normalize_left_right_keypoint_arrs(X_left, X_right)\n",
    "    X_world = convert_to_world_point_arr(X_left_norm, X_right_norm, camera_metadata)\n",
    "    X = stabilize_keypoints(X_world)\n",
    "    nn_input = torch.from_numpy(np.array([X])).float()\n",
    "    return nn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the WeightEstimator class for estimating fish weight (g), length (mm), and\n",
    "k-factor given input keypoint coordinates and camera metadata.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network class defines neural-network architecture for both weight and k-factor estimation\n",
    "    (currently both neural networks share identical architecture).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on input keypoint tensor.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightEstimator:\n",
    "    \"\"\"WeightEstimator class is used to predict fish weight, k-factor, and length\n",
    "    given input keypoint annotations and camera metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_model_f: str, kf_model_f: str) -> None:\n",
    "        \"\"\"Initializes class with input weight and k-factor neural-networks.\"\"\"\n",
    "        self.weight_model = Network()\n",
    "        self.weight_model.load_state_dict(torch.load(weight_model_f))\n",
    "        self.weight_model.eval()\n",
    "\n",
    "        self.kf_model = Network()\n",
    "        self.kf_model.load_state_dict(torch.load(kf_model_f))\n",
    "        self.kf_model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model_input(annotation: Dict, camera_metadata: CameraMetadata) -> torch.Tensor:\n",
    "        \"\"\"Generates neural-network input tensor given annotation and camera_metadata.\"\"\"\n",
    "        X = convert_to_nn_input(annotation, camera_metadata)\n",
    "        return X\n",
    "\n",
    "    def predict_weight(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates weight prediction given input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        weight = 1e4 * self.weight_model(X).item()\n",
    "        return weight\n",
    "\n",
    "    def predict_kf(self, annotation: Dict, camera_metadata: CameraMetadata) -> float:\n",
    "        \"\"\"Generates k-factor prediction gievn input annotation and camera metadata.\"\"\"\n",
    "        X = self._get_model_input(annotation, camera_metadata)\n",
    "        kf = self.kf_model(X).item()\n",
    "        return kf\n",
    "\n",
    "    def predict(self, annotation: Dict, camera_metadata: CameraMetadata) -> Tuple:\n",
    "        \"\"\"Generates weight, k-factor, and length predictions given input annotation and camera\n",
    "        metadata.\"\"\"\n",
    "        weight = self.predict_weight(annotation, camera_metadata)\n",
    "        kf = self.predict_kf(annotation, camera_metadata)\n",
    "        if weight * kf > 0:\n",
    "            length = (1e5 * weight / kf) ** (1.0 / 3)\n",
    "        else:\n",
    "            length = 0\n",
    "        return weight, length, kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta, time\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import AutoDateFormatter, AutoDateLocator\n",
    "\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryCache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('eide_langoy_singleweights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.hist(gt['weight'] * 1000 / 0.83, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(kp1, kp2):\n",
    "    vector = kp1 - kp2\n",
    "    x, y, z = vector / np.linalg.norm(vector)\n",
    "\n",
    "    theta = math.atan(y / x) * np.sign(y)\n",
    "    phi = math.acos(z)\n",
    "    dtheta = math.degrees(theta)\n",
    "    dphi = 90 - math.degrees(phi)\n",
    "    \n",
    "    return dtheta, dphi\n",
    "\n",
    "# cm2 = CameraMetadata(\n",
    "#     focal_length=4000 * 3.45e-6,\n",
    "#     focal_length_pixel=4000,\n",
    "#     baseline_m=0.1,\n",
    "#     pixel_count_width=4000,\n",
    "#     pixel_count_height=3096,\n",
    "#     image_sensor_width=0.0141312,\n",
    "#     image_sensor_height=0.01035\n",
    "# )\n",
    "\n",
    "weight_model_f = 'weight_model.pb'\n",
    "kf_model_f = 'kf_model.pb'\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "\n",
    "def getPenDF(pen, akpd_filter):\n",
    "    query = \"\"\"\n",
    "       SELECT * FROM prod.biomass_computations\n",
    "        WHERE prod.biomass_computations.captured_at >= '%s'\n",
    "        AND prod.biomass_computations.captured_at <= '%s'\n",
    "        AND prod.biomass_computations.akpd_score > %0.4f\n",
    "        AND prod.biomass_computations.pen_id = %i;\n",
    "    \"\"\" % (pen['start_date'], pen['end_date'], akpd_filter, pen['pen_id'])\n",
    "\n",
    "    if query in queryCache:\n",
    "        df = queryCache[query].copy()\n",
    "    else:\n",
    "        df = rds_access_utils.extract_from_database(query)\n",
    "        queryCache[query] = df.copy()\n",
    "\n",
    "    df = df.sort_values('captured_at').copy(deep=True)\n",
    "    df.index = pd.to_datetime(df.captured_at)\n",
    "    dates = df.index.date.astype(str)\n",
    "    df['date'] = dates\n",
    "    df['hour'] = df.index.hour\n",
    "    \n",
    "    depths = []\n",
    "    lengths = []\n",
    "    widths = []\n",
    "    lengths_adj = []\n",
    "#     lengths_adj2 = []\n",
    "    \n",
    "    weights2 = []\n",
    "    lengths2 = []\n",
    "    kfs2 = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        ann, camera_metadata = row.annotation, row.camera_metadata\n",
    "        wkps = pixel2world(ann['leftCrop'], ann['rightCrop'], camera_metadata)\n",
    "        depth = np.median([wkp[1] for wkp in wkps.values()])\n",
    "        vector = wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']\n",
    "        \n",
    "        cm2 = CameraMetadata(\n",
    "            focal_length=camera_metadata['focalLength'],\n",
    "            focal_length_pixel=camera_metadata['focalLengthPixel'],\n",
    "            baseline_m=camera_metadata['baseline'],\n",
    "            pixel_count_width=camera_metadata['pixelCountWidth'],\n",
    "            pixel_count_height=camera_metadata['pixelCountHeight'],\n",
    "            image_sensor_width=camera_metadata['imageSensorWidth'],\n",
    "            image_sensor_height=camera_metadata['imageSensorHeight']\n",
    "        )\n",
    "        \n",
    "        weight, length, kf = weight_estimator.predict(row.annotation, cm2)\n",
    "        \n",
    "        weights2.append(weight)\n",
    "        lengths2.append(length / 1000)\n",
    "        kfs2.append(kf)\n",
    "        \n",
    "        centroid = .5 * (wkps['DORSAL_FIN'] + wkps['PELVIC_FIN'])\n",
    "        angle = np.linalg.norm(np.array(get_angles(wkps['UPPER_LIP'], centroid)) - np.array(get_angles(centroid, wkps['TAIL_NOTCH'])))\n",
    "        a = (wkps['UPPER_LIP'] - centroid) / np.linalg.norm(wkps['UPPER_LIP'] - centroid)\n",
    "        b = (wkps['TAIL_NOTCH'] - centroid) / np.linalg.norm(wkps['TAIL_NOTCH'] - centroid)\n",
    "\n",
    "        lengths_adj.append(np.linalg.norm(wkps['UPPER_LIP'] - centroid) + np.linalg.norm(centroid - wkps['TAIL_NOTCH']))\n",
    "#         lengths_adj.append(np.linalg.norm((wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']) * a))\n",
    "#         lengths_adj2.append(np.linalg.norm((wkps['UPPER_LIP'] - wkps['TAIL_NOTCH']) * b))\n",
    "        \n",
    "        depths.append(depth)\n",
    "        lengths.append(np.linalg.norm(vector))\n",
    "        widths.append(np.linalg.norm(wkps['DORSAL_FIN'] - wkps['PELVIC_FIN']))\n",
    "        \n",
    "    df['depth'] = depths\n",
    "    df['length'] = lengths\n",
    "    df['width'] = widths\n",
    "    df['length_adj'] = lengths_adj\n",
    "#     df['length_adj2'] = lengths_adj2\n",
    "    \n",
    "    df['weight2'] = weights2\n",
    "    df['length2'] = lengths2\n",
    "    df['kf2'] = kfs2\n",
    "    \n",
    "    df['estimated_weight'] = get_weight(df.length)\n",
    "    df['estimated_weight2'] = get_weight(df.length2)\n",
    "    df['estimated_weight_adj'] = get_weight(df.length_adj)\n",
    "#     df['estimated_weight'] = get_weight(df.length, df.width)\n",
    "#     df['estimated_weight2'] = get_weight(df.length2, df.width)\n",
    "#     df['estimated_weight_adj'] = get_weight(df.length_adj, df.width)\n",
    "#     df['estimated_weight_adj2'] = get_weight(df.length_adj2, df.width2)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pens = [\n",
    "    {\n",
    "        'pen_id': 108,\n",
    "        'start_date': '2020-05-07',\n",
    "        'end_date': '2020-05-10'\n",
    "    },\n",
    "    {\n",
    "        'pen_id': 60,\n",
    "        'start_date': '2020-08-22',\n",
    "        'end_date': '2020-08-25'\n",
    "    },\n",
    "    {\n",
    "        'pen_id': 145,\n",
    "        'start_date': '2020-11-20',\n",
    "        'end_date': '2020-11-23'\n",
    "    },\n",
    "    {\n",
    "        'pen_id': 61,\n",
    "        'start_date': '2019-11-30',\n",
    "        'end_date': '2019-12-02'\n",
    "    },\n",
    "    {\n",
    "        'pen_id': 153,\n",
    "        'start_date': '2020-11-18',\n",
    "        'end_date': '2020-11-21'\n",
    "    }\n",
    "]\n",
    "\n",
    "akpd_score = 0.95\n",
    "\n",
    "_df1 = getPenDF(pens[0], akpd_score)\n",
    "_df2 = getPenDF(pens[1], akpd_score)\n",
    "_df3 = getPenDF(pens[2], akpd_score)\n",
    "_df4 = getPenDF(pens[3], akpd_score)\n",
    "_df5 = getPenDF(pens[4], akpd_score)\n",
    "\n",
    "df1 = _df1[(_df1.hour >= 6) & (_df1.hour <= 12)]\n",
    "df2 = _df2[(_df2.hour >= 5) & (_df2.hour <= 15)]\n",
    "df3 = _df3[(_df3.hour >= 7) & (_df3.hour <= 15)]\n",
    "df4 = _df4[(_df4.hour >= 8) & (_df4.hour <= 14)]\n",
    "df5 = _df5[(_df5.hour >= 7) & (_df5.hour <= 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[['estimated_weight_g', 'weight2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df5.estimated_weight_g), np.mean(df5.weight2)\n",
    "\n",
    "mask = df5.estimated_weight_g > 8000\n",
    "df6 = df5.copy()\n",
    "df6.loc[mask, 'estimated_weight_g'] = df6[mask].weight2\n",
    "np.mean(df6.estimated_weight_g), np.mean(df5.estimated_weight_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df1.iloc[0]\n",
    "\n",
    "cm = CameraMetadata(\n",
    "    focal_length=4000 * 3.45e-6,\n",
    "    focal_length_pixel=4000,\n",
    "    baseline_m=0.1,\n",
    "    pixel_count_width=4000,\n",
    "    pixel_count_height=3096,\n",
    "    image_sensor_width=0.0141312,\n",
    "    image_sensor_height=0.01035\n",
    ")\n",
    "\n",
    "weight_model_f = 'weight_model.pb'\n",
    "kf_model_f = 'kf_model.pb'\n",
    "weight_estimator = WeightEstimator(weight_model_f, kf_model_f)\n",
    "weight, length, kf = weight_estimator.predict(row.annotation, cm)\n",
    "\n",
    "print(weight, length, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(_df4.hour, bins = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count, bins, _ = plt.hist(df3.estimated_weight_g, density = True, alpha = 0.5, color = 'red', bins = 20)\n",
    "weight2 = get_weight(df3.length_adj)\n",
    "\n",
    "count, bins, _ = plt.hist(weight2[weight2 < 12000], density = True, alpha = 0.5, color = 'blue', bins = 20)\n",
    "# plt.hist(df3.estimated_weight_g, density = True, alpha = 0.5, color = 'red', bins = bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(length):\n",
    "    return (length * 23.6068) ** 3\n",
    "\n",
    "# def get_weight(length, width):\n",
    "#     return (length * 12.5471 + width * 46.1998) ** 3\n",
    "\n",
    "# def get_weight(length, width):\n",
    "#     return (length * 16.06328176 + width * 31.74291343) ** 3\n",
    "\n",
    "# def get_weight(length, width):\n",
    "#     return (length ** 3 * 13071.99583268 + width ** 3 * 7460.4478138)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight3(length, factor):\n",
    "    return ((length * 23.6068) ** 3) * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df1.estimated_weight_g), np.mean(gt.weight / .83 * 1000), np.mean(get_weight(df1.length_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df2.estimated_weight_g), np.mean(pen5.weight / .83 * 1000), np.mean(get_weight(df2.length_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df1.estimated_weight_g, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df1.estimated_weight_g, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df1.weight2, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df1.estimated_weight2, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'][gt.weight < (5 * .83)] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df1.estimated_weight_g[df1.estimated_weight_g < 5000], density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.mean(gt['weight'][gt.weight < (5 * .83)] * 1000 / 0.83), np.mean(df1.estimated_weight_g[df1.estimated_weight_g < 5000])\n",
    "\n",
    "a, b, (a - b) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 30)\n",
    "plt.hist(get_weight(df1.length, df1.width), density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df1.estimated_weight_g, df1.weight2)\n",
    "plt.plot(df1.estimated_weight_g, df1.estimated_weight_g, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df2.estimated_weight_g, df2.weight2)\n",
    "plt.plot(df2.estimated_weight_g, df2.estimated_weight_g, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df2.weight2, df2.weight2 - df2.estimated_weight_g)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df2.estimated_weight_g\n",
    "X = sm.add_constant(X)\n",
    "Y = df2.weight2 - df2.estimated_weight_g\n",
    "model = sm.OLS(Y,X)\n",
    "m = model.fit()\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df3.estimated_weight_g, df3.weight2)\n",
    "plt.plot(df3.estimated_weight_g, df3.estimated_weight_g, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df4.estimated_weight_g, df4.weight2)\n",
    "plt.plot(df4.estimated_weight_g, df4.estimated_weight_g, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(gt['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 30)\n",
    "plt.hist(get_weight(df1.length_adj, df1.width), density = True, alpha = 0.5, color = 'red', bins = bins)\n",
    "\n",
    "# my_dist2 = pdf_mvsk([np.mean(get_weight(df1.length_adj)), (np.std(get_weight(df1.length_adj)) / 2) ** 2, 0, 1])\n",
    "# vec2 = np.arange(start=0, stop = 10000, step = .01)\n",
    "\n",
    "# dist3 = my_dist2(vec2)\n",
    "\n",
    "# plt.plot(vec2, dist3, lw = 4, color = 'green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# res = stats.probplot(gt.weight, dist=stats.norm, plot=plt)\n",
    "res = stats.probplot(gt.weight, dist=stats.lognorm, sparams=(0.15), plot=plt)\n",
    "# res = stats.probplot(get_weight(df1.length_adj), dist=stats.lognorm, sparams=(0.1), plot=plt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df1.akpd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df1[get_weight(df1.length_adj) > 12000].akpd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = np.mean(gt['weight'] * 1000 / 0.83), np.mean(get_weight(df1.length)), np.mean(get_weight(df1.length_adj))\n",
    "\n",
    "a, b, c, (a - b) / a, (a - c) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['estimated_weight'] = get_weight(df1.lengths)\n",
    "\n",
    "count, bins, _ = plt.hist(gt['weight'][gt.weight > (5 * 0.83)] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df1['estimated_weight'][df1['estimated_weight'] > 5000], density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen5 = pd.read_csv('blom_vikane_singleweights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df2.estimated_weight_g, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df2.weight2, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df2.estimated_weight_g, df2.weight2)\n",
    "plt.plot(df2.estimated_weight_g, df2.estimated_weight_g, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(get_weight(df2.length, df2.width), density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(get_weight(df2.length_adj), density = True, alpha = 0.5, color = 'red', bins = bins)\n",
    "\n",
    "from statsmodels.sandbox.distributions.extras import pdf_mvsk\n",
    "\n",
    "my_dist2 = pdf_mvsk([np.mean(get_weight(df2.length_adj)), (np.std(get_weight(df2.length_adj)) / 2) ** 2, 0, 1])\n",
    "vec2 = np.arange(start=0, stop = 10000, step = .01)\n",
    "\n",
    "dist3 = my_dist2(vec2)\n",
    "\n",
    "# plt.plot(vec2, dist3, lw = 4, color = 'green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(get_weight(df2.length_adj), density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df1.estimated_k_factor, density = True, alpha = 0.5, color = 'red')\n",
    "plt.hist(df2.estimated_k_factor, density = True, alpha = 0.5, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight2(length, factor):\n",
    "    return (length * 23.6068 * factor) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(get_weight2(df2.length_adj, 0.8) * 2, density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, _ = plt.hist(pen5['weight'] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(get_weight3(df2.lengths, df2.estimated_k_factor ** 0.3 / 0.9), density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = np.mean(pen5['weight'] * 1000 / 0.83), np.mean(get_weight(df2.length)), np.mean(get_weight(df2.length_adj))\n",
    "\n",
    "a, b, c, (a - b) / a, (a - c) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['estimated_weight'] = get_weight(df2.lengths)\n",
    "\n",
    "count, bins, _ = plt.hist(pen5['weight'][pen5.weight > (5 * 0.83)] * 1000 / 0.83, density = True, color = 'blue', bins = 50)\n",
    "plt.hist(df2['estimated_weight'][df2['estimated_weight'] > 5000], density = True, alpha = 0.5, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imr_gt = pd.read_csv('imr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins, _ = plt.hist(imr_gt.weight, alpha = 0.5, density = True, color = 'blue', bins = 30)\n",
    "plt.hist(df4.estimated_weight_g, alpha = 0.5, density = True, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins, _ = plt.hist(imr_gt.weight, alpha = 0.5, density = True, color = 'blue', bins = 30)\n",
    "plt.hist(get_weight(df4.length, df4.width), alpha = 0.5, density = True, color = 'red', bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
