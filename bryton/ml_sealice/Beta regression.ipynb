{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "u\"\"\"\n",
    "Beta regression for modeling rates and proportions.\n",
    "References\n",
    "----------\n",
    "GrÃ¼n, Bettina, Ioannis Kosmidis, and Achim Zeileis. Extended beta regression\n",
    "in R: Shaken, stirred, mixed, and partitioned. No. 2011-22. Working Papers in\n",
    "Economics and Statistics, 2011.\n",
    "Smithson, Michael, and Jay Verkuilen. \"A better lemon squeezer?\n",
    "Maximum-likelihood regression with beta-distributed dependent variables.\"\n",
    "Psychological methods 11.1 (2006): 54.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.special import gammaln as lgamma\n",
    "\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from statsmodels.genmod.families import Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only need while #2024 is open.\n",
    "class Logit(sm.families.links.Logit):\n",
    "\n",
    "    \"\"\"Logit tranform that won't overflow with large numbers.\"\"\"\n",
    "\n",
    "    def inverse(self, z):\n",
    "        return 1 / (1. + np.exp(-z))\n",
    "\n",
    "_init_example = \"\"\"\n",
    "    Beta regression with default of logit-link for exog and log-link\n",
    "    for precision.\n",
    "    >>> mod = Beta(endog, exog)\n",
    "    >>> rslt = mod.fit()\n",
    "    >>> print rslt.summary()\n",
    "    We can also specify a formula and a specific structure and use the\n",
    "    identity-link for phi.\n",
    "    >>> from sm.families.links import identity\n",
    "    >>> Z = patsy.dmatrix('~ temp', dat, return_type='dataframe')\n",
    "    >>> mod = Beta.from_formula('iyield ~ C(batch, Treatment(10)) + temp',\n",
    "    ...                         dat, Z=Z, link_phi=identity())\n",
    "    In the case of proportion-data, we may think that the precision depends on\n",
    "    the number of measurements. E.g for sequence data, on the number of\n",
    "    sequence reads covering a site:\n",
    "    >>> Z = patsy.dmatrix('~ coverage', df)\n",
    "    >>> mod = Beta.from_formula('methylation ~ disease + age + gender + coverage', df, Z)\n",
    "    >>> rslt = mod.fit()\n",
    "\"\"\"\n",
    "\n",
    "class Beta(GenericLikelihoodModel):\n",
    "\n",
    "    \"\"\"Beta Regression.\n",
    "    This implementation uses `phi` as a precision parameter equal to\n",
    "    `a + b` from the Beta parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endog, exog, Z=None, link=Logit(),\n",
    "            link_phi=sm.families.links.Log(), **kwds):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        endog : array-like\n",
    "            1d array of endogenous values (i.e. responses, outcomes,\n",
    "            dependent variables, or 'Y' values).\n",
    "        exog : array-like\n",
    "            2d array of exogeneous values (i.e. covariates, predictors,\n",
    "            independent variables, regressors, or 'X' values). A nobs x k\n",
    "            array where `nobs` is the number of observations and `k` is\n",
    "            the number of regressors. An intercept is not included by\n",
    "            default and should be added by the user. See\n",
    "            `statsmodels.tools.add_constant`.\n",
    "        Z : array-like\n",
    "            2d array of variables for the precision phi.\n",
    "        link : link\n",
    "            Any link in sm.families.links for `exog`\n",
    "        link_phi : link\n",
    "            Any link in sm.families.links for `Z`\n",
    "        Examples\n",
    "        --------\n",
    "        {example}\n",
    "        See Also\n",
    "        --------\n",
    "        :ref:`links`\n",
    "        \"\"\".format(example=_init_example)\n",
    "        assert np.all((0 < endog) & (endog < 1))\n",
    "        if Z is None:\n",
    "            extra_names = ['phi']\n",
    "            Z = np.ones((len(endog), 1), dtype='f')\n",
    "        else:\n",
    "            extra_names = ['precision-%s' % zc for zc in \\\n",
    "                        (Z.columns if hasattr(Z, 'columns') else range(1, Z.shape[1] + 1))]\n",
    "        kwds['extra_params_names'] = extra_names\n",
    "\n",
    "        super(Beta, self).__init__(endog, exog, **kwds)\n",
    "        self.link = link\n",
    "        self.link_phi = link_phi\n",
    "        \n",
    "        self.Z = Z\n",
    "        assert len(self.Z) == len(self.endog)\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood.\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            Parameter estimates\n",
    "        \"\"\"\n",
    "        return -self._ll_br(self.endog, self.exog, self.Z, params)\n",
    "\n",
    "    def fit(self, start_params=None, maxiter=100000, maxfun=5000, disp=False,\n",
    "            method='bfgs', **kwds):\n",
    "        \"\"\"\n",
    "        Fit the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_params : array-like\n",
    "            A vector of starting values for the regression\n",
    "            coefficients.  If None, a default is chosen.\n",
    "        maxiter : integer\n",
    "            The maximum number of iterations\n",
    "        disp : bool\n",
    "            Show convergence stats.\n",
    "        method : str\n",
    "            The optimization method to use.\n",
    "        \"\"\"\n",
    "\n",
    "        if start_params is None:\n",
    "            start_params = sm.GLM(self.endog, self.exog, family=Binomial()\n",
    "                                 ).fit(disp=False).params\n",
    "            start_params = np.append(start_params, [0.5] * self.Z.shape[1])\n",
    "\n",
    "        return super(Beta, self).fit(start_params=start_params,\n",
    "                                        maxiter=maxiter, maxfun=maxfun,\n",
    "                                        method=method, disp=disp, **kwds)\n",
    "\n",
    "    def _ll_br(self, y, X, Z, params):\n",
    "        nz = self.Z.shape[1]\n",
    "\n",
    "        Xparams = params[:-nz]\n",
    "        Zparams = params[-nz:]\n",
    "\n",
    "        mu = self.link.inverse(np.dot(X, Xparams))\n",
    "        phi = self.link_phi.inverse(np.dot(Z, Zparams))\n",
    "        # TODO: derive a and b and constrain to > 0?\n",
    "\n",
    "        if np.any(phi <= np.finfo(float).eps): return np.array(-np.inf)\n",
    "\n",
    "        ll = lgamma(phi) - lgamma(mu * phi) - lgamma((1 - mu) * phi) \\\n",
    "                + (mu * phi - 1) * np.log(y) + (((1 - mu) * phi) - 1) \\\n",
    "                * np.log(1 - y)\n",
    "\n",
    "        return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/bryton/github/cv_research/bryton/ml_sealice/2016.csv')\n",
    "\n",
    "my_df = df.loc[:,'avgAdultFemaleLice':'seaTemperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices_1 = my_df['avgAdultFemaleLice'] < 1\n",
    "good_indices_2 = my_df['avgMobileLice'] < 1\n",
    "good_indices_3 = my_df['avgStationaryLice'] < 1\n",
    "good_indices_4 = my_df['avgAdultFemaleLice'] > 0\n",
    "good_indices_5 = my_df['avgMobileLice'] > 0\n",
    "good_indices_6 = my_df['avgStationaryLice'] > 0\n",
    "\n",
    "good_indices = good_indices_1 & good_indices_2 & good_indices_3 & good_indices_4 & good_indices_5 & good_indices_6\n",
    "\n",
    "print(np.sum(good_indices) / len(good_indices))\n",
    "\n",
    "my_df_X = my_df.loc[good_indices, :]\n",
    "\n",
    "m = Beta.from_formula('avgStationaryLice ~ avgMobileLice + avgAdultFemaleLice', my_df_X)\n",
    "\n",
    "model = m.fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
