{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta, time\n",
    "from research.utils.data_access_utils import RDSAccessUtils\n",
    "from research.weight_estimation.keypoint_utils.optics import euclidean_distance, pixel2world, depth_from_disp, convert_to_world_point\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import AutoDateFormatter, AutoDateLocator\n",
    "\n",
    "rds_access_utils = RDSAccessUtils(json.load(open(os.environ['DATA_WAREHOUSE_SQL_CREDENTIALS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryCache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen = {\n",
    "    'pen_id': 108,\n",
    "    'start_date': '2020-05-09 00:00',\n",
    "    'end_date': '2020-05-12 00:00'\n",
    "}\n",
    "\n",
    "pen = {\n",
    "    'pen_id': 56,\n",
    "    'start_date': '2020-08-12 00:00',\n",
    "    'end_date': '2020-08-17 00:00'\n",
    "}\n",
    "\n",
    "pen = {\n",
    "    'pen_id': 88,\n",
    "    'start_date': '2020-02-12 00:00',\n",
    "    'end_date': '2020-02-19 00:00'\n",
    "}\n",
    "\n",
    "akpd_filter = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * FROM (\n",
    "      (SELECT * FROM prod.crop_annotation cas\n",
    "      INNER JOIN prod.annotation_state pas on pas.id=cas.annotation_state_id\n",
    "      WHERE cas.service_id = (SELECT ID FROM prod.service where name='BATI')\n",
    "      AND cas.annotation_state_id = 3\n",
    "      AND cas.pen_id=%i) a\n",
    "    RIGHT JOIN \n",
    "      (SELECT left_crop_url as lcu, estimated_weight_g, akpd_score, estimated_k_factor FROM prod.biomass_computations\n",
    "      WHERE prod.biomass_computations.captured_at >= '%s'\n",
    "      AND prod.biomass_computations.captured_at <= '%s'\n",
    "      AND prod.biomass_computations.akpd_score > %0.4f) bc \n",
    "    ON \n",
    "      (a.left_crop_url=bc.lcu)\n",
    "    ) x\n",
    "    WHERE x.captured_at >= '%s'\n",
    "    AND x.captured_at <= '%s'\n",
    "    AND x.pen_id = %i\n",
    "    AND x.group_id = '%i';\n",
    "\"\"\" % (pen['pen_id'], pen['start_date'], pen['end_date'], akpd_filter, pen['start_date'], pen['end_date'], pen['pen_id'], pen['pen_id'])\n",
    "\n",
    "if query in queryCache:\n",
    "    df = queryCache[query].copy()\n",
    "else:\n",
    "    df = rds_access_utils.extract_from_database(query)\n",
    "    queryCache[query] = df.copy()\n",
    "\n",
    "captures = df\n",
    "\n",
    "captures.captured_at = pd.to_datetime(captures['captured_at'])\n",
    "captures = df.sort_values(['captured_at'])\n",
    "captures = captures.reset_index(drop=True)\n",
    "\n",
    "print(len(captures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captures = pd.read_csv('bolaks_pen_id_88_2020-02-10_2020-03-10.csv')\n",
    "captures.ix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)\n",
    "\n",
    "def get_eye_wkps(row1, row2):\n",
    "    ann1, cm1 = row1.annotation, row1.camera_metadata\n",
    "    wkps1 = pixel2world(ann1['leftCrop'], ann1['rightCrop'], cm1)\n",
    "    ann2, cm2 = row2.annotation, row2.camera_metadata\n",
    "    wkps2 = pixel2world(ann2['leftCrop'], ann2['rightCrop'], cm2)\n",
    "    \n",
    "    return wkps1['EYE'], wkps2['EYE']\n",
    "\n",
    "body_parts = ['TAIL_NOTCH', 'ADIPOSE_FIN', 'UPPER_LIP', 'ANAL_FIN', 'PELVIC_FIN', 'EYE', 'PECTORAL_FIN', 'DORSAL_FIN']\n",
    "\n",
    "def get_median_distance(row1, row2):\n",
    "    ann1, cm1 = row1.annotation, row1.camera_metadata\n",
    "    wkps1 = pixel2world(ann1['leftCrop'], ann1['rightCrop'], cm1)\n",
    "    ann2, cm2 = row2.annotation, row2.camera_metadata\n",
    "    wkps2 = pixel2world(ann2['leftCrop'], ann2['rightCrop'], cm2)\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for body_part in body_parts:\n",
    "        if wkps1[body_part][1] < 0 or wkps2[body_part][1] < 0:\n",
    "            return None\n",
    "    \n",
    "        distances.append(np.linalg.norm(wkps1[body_part] - wkps2[body_part]))\n",
    "    \n",
    "    return np.median(distances)\n",
    "\n",
    "def get_median_distance_adj(row1, row2):\n",
    "    ann1, cm1 = row1.annotation, row1.camera_metadata\n",
    "    ann2, cm2 = row2.annotation, row2.camera_metadata\n",
    "    \n",
    "    leftCrop1 = json.loads(ann1.replace(\"\\'\", \"\\\"\"))['leftCrop']\n",
    "    rightCrop1 = json.loads(ann1.replace(\"\\'\", \"\\\"\"))['rightCrop']\n",
    "    cm1 = json.loads(cm1.replace(\"\\'\", \"\\\"\"))\n",
    "    leftCrop2 = json.loads(ann2.replace(\"\\'\", \"\\\"\"))['leftCrop']\n",
    "    rightCrop2 = json.loads(ann2.replace(\"\\'\", \"\\\"\"))['rightCrop']\n",
    "    cm2 = json.loads(cm2.replace(\"\\'\", \"\\\"\"))\n",
    "    \n",
    "    wkps1 = pixel2world(leftCrop1, rightCrop1, cm1)\n",
    "    wkps2 = pixel2world(leftCrop2, rightCrop2, cm2)\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for body_part in body_parts:\n",
    "        if wkps1[body_part][1] < 0 or wkps2[body_part][1] < 0:\n",
    "            return None\n",
    "    \n",
    "    return np.median(distances)\n",
    "\n",
    "def get_eye_distance(row1, row2):\n",
    "    ann1, cm1 = row1.annotation, row1.camera_metadata\n",
    "    wkps1 = pixel2world(ann1['leftCrop'], ann1['rightCrop'], cm1)\n",
    "    ann2, cm2 = row2.annotation, row2.camera_metadata\n",
    "    wkps2 = pixel2world(ann2['leftCrop'], ann2['rightCrop'], cm2)\n",
    "    \n",
    "    if wkps1['EYE'][1] < 0 or wkps2['EYE'][1] < 0:\n",
    "        return None\n",
    "    \n",
    "    return np.linalg.norm(wkps1['EYE'] - wkps2['EYE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures.captured_at = pd.to_datetime(captures['captured_at'])\n",
    "captures = captures.sort_values(['captured_at'])\n",
    "\n",
    "startDate = np.min(captures.captured_at).date()\n",
    "endDate = np.max(captures.captured_at).date()\n",
    "delta = endDate - startDate\n",
    "\n",
    "dates = []\n",
    "for i in range(delta.days + 1):\n",
    "    day = startDate + timedelta(days = i)\n",
    "    day = datetime.combine(day, time.min).replace(tzinfo=timezone.utc)\n",
    "    dates.append(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pairs.p1_captured_at = pd.to_datetime(pairs['p1_captured_at'])\n",
    "# pairs.p2_captured_at = pd.to_datetime(pairs['p2_captured_at'])\n",
    "\n",
    "#captures.ix[0]['annotation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = captures['captured_at'].diff()\n",
    "\n",
    "np.mean(gaps < timedelta(seconds=0.5)), np.mean(gaps < timedelta(seconds=1.5)), np.mean(gaps < timedelta(seconds=2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    daily_captures = (captures['captured_at'] > date) & (captures['captured_at'] < (date + timedelta(days = 1)))\n",
    "    #daily_pairs = (pairs['p1_captured_at'] > datetime(2020, 2, i, tzinfo=timezone.utc)) & (pairs['p1_captured_at'] < datetime(2020, 2, i + 1, tzinfo=timezone.utc))\n",
    "\n",
    "    print(date, np.sum(daily_captures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_pairs = []\n",
    "linkages = []\n",
    "singles = []\n",
    "weights_all = []\n",
    "weights_pairs = []\n",
    "k_factor_all = []\n",
    "k_factor_pairs = []\n",
    "existing_indices = []\n",
    "\n",
    "num_sec = 10\n",
    "pixel_threshold = 300\n",
    "weight_g_threshold = 500\n",
    "k_factor_treshold = 0.3\n",
    "\n",
    "for date in dates:\n",
    "    mask = (captures['captured_at'] > date) & (captures['captured_at'] < (date + timedelta(days = 1)))\n",
    "\n",
    "    old_datas = []\n",
    "    \n",
    "#     old_rows = []\n",
    "#     old_captured_ats = []\n",
    "#     old_ids = []\n",
    "#     old_tails = []\n",
    "#     old_eyes = []\n",
    "\n",
    "    count = 0\n",
    "    bad_count = 0\n",
    "#     correct_count = 0\n",
    "\n",
    "    for index, row in captures[mask].iterrows():\n",
    "        current_captured_at = row.captured_at\n",
    "\n",
    "        # Delete old captures\n",
    "        old_datas = [ old_data for old_data in old_datas if np.abs(current_captured_at - old_data['captured_at']) < timedelta(seconds = num_sec) ]\n",
    "#         old_captured_at_indices = [i for i, ca in enumerate(old_captured_ats) if np.abs(current_captured_at - ca) < timedelta(seconds=10)]\n",
    "#         old_captured_ats = [ca for ca in  enumerate(old_captured_ats) if j in old_captured_at_indices ]\n",
    "#         old_ids = [_id for j, _id in enumerate(old_ids) if j in old_captured_at_indices ]\n",
    "#         old_tails = [row for j, row in enumerate(old_tails) if j in old_captured_at_indices ]\n",
    "#         old_eyes = [row for j, row in enumerate(old_eyes) if j in old_captured_at_indices ]\n",
    "#         old_rows = [row for j, row in enumerate(old_rows) if j in old_captured_at_indices ]\n",
    "        \n",
    "        #print(current_captured_at)\n",
    "\n",
    "        # Check captures\n",
    "#         left_crops = json.loads(row['annotation'].replace(\"\\'\", \"\\\"\"))['leftCrop']\n",
    "        left_crops = row['annotation']['leftCrop']\n",
    "        left_tail = [crop for crop in left_crops if crop['keypointType'] == 'ANAL_FIN'][0]\n",
    "        left_eye = [crop for crop in left_crops if crop['keypointType'] == 'EYE'][0]\n",
    "    #     right_crops = json.loads(row['annotation'].replace(\"\\'\", \"\\\"\"))['rightCrop']\n",
    "    #     right_eye = [crop for crop in right_crops if crop['keypointType'] == 'EYE'][0]\n",
    "\n",
    "        k_factor = row['estimated_k_factor']\n",
    "        weight_g = row['estimated_weight_g']\n",
    "        \n",
    "        weights_all.append(weight_g)\n",
    "        k_factor_all.append(k_factor)\n",
    "        singles.append([index, weight_g, k_factor])\n",
    "    \n",
    "        for old_data in old_datas:\n",
    "#             ca = old_captured_ats[i]\n",
    "#             old_tail = old_tails[i]\n",
    "#             old_row = old_rows[i]\n",
    "            old_eye = old_data['eye']\n",
    "            old_tail = old_data['tail']\n",
    "            old_captured_at = old_data['captured_at']\n",
    "            #old_row = old_data['row']\n",
    "            old_index = old_data['index']\n",
    "            old_k_factor = old_data['k_factor']\n",
    "            old_weight_g = old_data['weight_g']\n",
    "    \n",
    "            #distance = get_eye_distance(captures.ix[old_index], captures.ix[index])\n",
    "            distance = get_median_distance(captures.ix[old_index], captures.ix[index])\n",
    "#             distance = get_median_distance_adj(captures.ix[old_index], captures.ix[index])\n",
    "            time_seconds = (current_captured_at - old_captured_at).total_seconds()\n",
    "    \n",
    "            weight_g_diff = np.abs(weight_g - old_weight_g)\n",
    "            k_factor_diff = np.abs(k_factor - old_k_factor)\n",
    "            \n",
    "            if distance is None or time_seconds < .1:\n",
    "                bad_count = bad_count + 1\n",
    "                continue\n",
    "            \n",
    "            speed = distance / time_seconds\n",
    "\n",
    "            shift_condition = np.abs((old_eye['xFrame'] - left_eye['xFrame']) - (old_tail['xFrame'] - left_tail['xFrame'])) < pixel_threshold\n",
    "            vertical_condition = np.abs((old_eye['yFrame'] - left_eye['yFrame']) - (old_tail['yFrame'] - left_tail['yFrame'])) < pixel_threshold #np.abs(old_eye['yFrame'] - left_eye['yFrame']) < 200\n",
    "\n",
    "            if left_tail['xFrame'] < left_eye['xFrame']:\n",
    "                horizontal_condition = old_tail['xFrame'] < old_eye['xFrame'] and old_eye['xFrame'] < left_eye['xFrame'] and old_tail['xFrame'] < left_tail['xFrame']\n",
    "                \n",
    "                if horizontal_condition and shift_condition and vertical_condition:\n",
    "                    if weight_g_diff > weight_g_threshold or k_factor_diff > k_factor_treshold:\n",
    "                        bad_count = bad_count + 1\n",
    "                        continue\n",
    "\n",
    "                    count = count + 1\n",
    "\n",
    "#                     if np.sum((pairs.p2 == index) & (pairs.p1 == old_ids[i])) > 0:\n",
    "#                         correct_count = correct_count + 1\n",
    "#                     else:\n",
    "#                         pass\n",
    "                    weights_pairs.append(weight_g)\n",
    "                    weights_pairs.append(old_weight_g)\n",
    "                    k_factor_pairs.append(k_factor)\n",
    "                    k_factor_pairs.append(old_k_factor)\n",
    "                    singles = [ s for s in singles if s[0] not in (index, old_index) ]\n",
    "                    \n",
    "                    if speed > 2:\n",
    "                        print(old_index, index, speed, distance, time_seconds)\n",
    "                    \n",
    "                    if not (index in existing_indices or old_index in existing_indices):\n",
    "                        found_pairs.append([old_index, index, old_captured_at, current_captured_at, speed, distance, time_seconds, weight_g, k_factor, old_weight_g, old_k_factor, weight_g_diff, k_factor_diff, np.abs(old_eye['xFrame'] - left_eye['xFrame']), np.abs(old_eye['yFrame'] - left_eye['yFrame']), np.abs(old_tail['xFrame'] - left_tail['xFrame']), np.abs(old_tail['yFrame'] - left_tail['yFrame']), old_eye['xFrame'], old_eye['yFrame'], old_tail['xFrame'], old_tail['yFrame'], left_eye['xFrame'], left_eye['yFrame'], left_tail['xFrame'], left_tail['yFrame']])\n",
    "                        linkages.append([[old_index, index], [weight_g, old_weight_g], [k_factor, old_k_factor], [ speed ], (weight_g + old_weight_g) / 2, (k_factor + old_k_factor) / 2, speed, 2])\n",
    "                        existing_indices.append(old_index)\n",
    "                        existing_indices.append(index)\n",
    "                    else: # in the future, instead of throwing away. Average over new speed\n",
    "                        foundLinkage = [ linkage for linkage in linkages if (old_index in linkage[0] or index in linkage[0]) ][0]\n",
    "                        if old_index not in foundLinkage[0]:\n",
    "                            foundLinkage[0].append(old_index)\n",
    "                            foundLinkage[1].append(old_weight_g)\n",
    "                            foundLinkage[2].append(old_k_factor)\n",
    "                            existing_indices.append(old_index)\n",
    "                        if index not in foundLinkage[0]:\n",
    "                            foundLinkage[0].append(index)\n",
    "                            foundLinkage[1].append(weight_g)\n",
    "                            foundLinkage[2].append(k_factor)\n",
    "                            existing_indices.append(index)\n",
    "                        foundLinkage[3].append(speed)\n",
    "                        foundLinkage[4] = np.mean(foundLinkage[1])\n",
    "                        foundLinkage[5] = np.mean(foundLinkage[2])\n",
    "                        foundLinkage[6] = np.mean(foundLinkage[3])\n",
    "                        foundLinkage[7] = len(foundLinkage[0])\n",
    "                        \n",
    "                #print(index, old_ids[i])\n",
    "                #print(index, old_ids[i])\n",
    "                #print(left_eye['xFrame'])\n",
    "            else:\n",
    "                horizontal_condition = old_tail['xFrame'] > old_eye['xFrame'] and old_eye['xFrame'] > left_eye['xFrame'] and old_tail['xFrame'] > left_tail['xFrame']\n",
    "                \n",
    "                if horizontal_condition and shift_condition and vertical_condition:\n",
    "                    if weight_g_diff > weight_g_threshold or k_factor_diff > k_factor_treshold:\n",
    "                        bad_count = bad_count + 1\n",
    "                        continue\n",
    "\n",
    "                    count = count + 1\n",
    "\n",
    "#                     if np.sum((pairs.p2 == index) & (pairs.p1 == old_ids[i])) > 0:\n",
    "#                         correct_count = correct_count + 1\n",
    "#                     else:\n",
    "#                         pass\n",
    "                    weights_pairs.append(weight_g)\n",
    "                    weights_pairs.append(old_weight_g)\n",
    "                    singles = [ s for s in singles if s[0] not in (index, old_index) ]\n",
    "                    \n",
    "                    if speed > 2:\n",
    "                        print(old_index, index, speed, distance, time_seconds)\n",
    "                \n",
    "                    if not (index in existing_indices or old_index in existing_indices):\n",
    "                        found_pairs.append([old_index, index, old_captured_at, current_captured_at, speed, distance, time_seconds, weight_g, k_factor, old_weight_g, old_k_factor, weight_g_diff, k_factor_diff, np.abs(old_eye['xFrame'] - left_eye['xFrame']), np.abs(old_eye['yFrame'] - left_eye['yFrame']), np.abs(old_tail['xFrame'] - left_tail['xFrame']), np.abs(old_tail['yFrame'] - left_tail['yFrame']), old_eye['xFrame'], old_eye['yFrame'], old_tail['xFrame'], old_tail['yFrame'], left_eye['xFrame'], left_eye['yFrame'], left_tail['xFrame'], left_tail['yFrame']])\n",
    "                        linkages.append([[old_index, index], [weight_g, old_weight_g], [k_factor, old_k_factor], [ speed ], (weight_g + old_weight_g) / 2, (k_factor + old_k_factor) / 2, speed, 2])\n",
    "                        existing_indices.append(old_index)\n",
    "                        existing_indices.append(index)\n",
    "                    else: # in the future, instead of throwing away. Average over new speed\n",
    "                        foundLinkage = [ linkage for linkage in linkages if (old_index in linkage[0] or index in linkage[0]) ][0]\n",
    "                        if old_index not in foundLinkage[0]:\n",
    "                            foundLinkage[0].append(old_index)\n",
    "                            foundLinkage[1].append(old_weight_g)\n",
    "                            foundLinkage[2].append(old_k_factor)\n",
    "                            existing_indices.append(old_index)\n",
    "                        if index not in foundLinkage[0]:\n",
    "                            foundLinkage[0].append(index)\n",
    "                            foundLinkage[1].append(weight_g)\n",
    "                            foundLinkage[2].append(k_factor)\n",
    "                            existing_indices.append(index)\n",
    "                        foundLinkage[3].append(speed)\n",
    "                        foundLinkage[4] = np.mean(foundLinkage[1])\n",
    "                        foundLinkage[5] = np.mean(foundLinkage[2])\n",
    "                        foundLinkage[6] = np.mean(foundLinkage[3])\n",
    "                        foundLinkage[7] = len(foundLinkage[0])\n",
    "                    \n",
    "        old_datas.append({\n",
    "            'eye': left_eye,\n",
    "            'tail': left_tail,\n",
    "            'captured_at': current_captured_at,\n",
    "            #'row': row,\n",
    "            'index': index,\n",
    "            'k_factor': k_factor,\n",
    "            'weight_g': weight_g\n",
    "        })\n",
    "#         old_captured_ats.append(current_captured_at)\n",
    "#         old_ids.append(index)\n",
    "#         old_tails.append(left_tail)\n",
    "#         old_eyes.append(left_eye)\n",
    "#         old_rows.append(row)\n",
    "\n",
    "#     print(count, correct_count)\n",
    "    print('Count', date, bad_count, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newLinkages[:, 0]) / len(weights_all), len(weights_all), len(weights_pairs), len(newLinkages[:, 0]), len(newSingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLinkages = np.array([ linkage[4:] for linkage in linkages ])\n",
    "newSingles = np.array([ s[1] for s in singles ])\n",
    "KFSingles = np.array([ s[2] for s in singles ])\n",
    "\n",
    "#plt.hist(newLinkages[:,2])\n",
    "\n",
    "overallDedup = np.concatenate((newLinkages[:,0], newSingles))\n",
    "KFDedup = np.concatenate((newLinkages[:,1], KFSingles))\n",
    "\n",
    "print(np.mean(weights_all), np.mean(weights_pairs), np.mean(newLinkages[:,0]), np.mean(newSingles), np.mean(overallDedup))\n",
    "print(np.mean(k_factor_all), np.mean(k_factor_pairs), np.mean(newLinkages[:,1]), np.mean(KFSingles), np.mean(KFDedup))\n",
    "\n",
    "all_pct, new_pct = [], []\n",
    "\n",
    "for i in np.arange(0, 100, 1):\n",
    "    all_pct.append(np.percentile(weights_all, i))\n",
    "    #new_pct.append(np.percentile(newLinkages[:,0], i))\n",
    "    new_pct.append(np.percentile(overallDedup, i))\n",
    "    \n",
    "# plt.scatter(all_pct, new_pct)\n",
    "# xpoints = ypoints = plt.xlim()\n",
    "# plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n",
    "# plt.xlabel('All weights')\n",
    "# plt.ylabel('De-duplicated weights')\n",
    "# plt.title('All weights vs deduplicated weights')\n",
    "\n",
    "# fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "# axes.hist(weights_all, bins = 50, density = True, alpha = 0.5, label = 'All')\n",
    "# axes.hist(overallDedup, bins = 50, density = True, alpha = 0.5, label = 'Singles + Deduplicated Duplicates')\n",
    "# axes.legend()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "axes.hist(k_factor_all, bins = 50, density = True, alpha = 0.5, label = 'All')\n",
    "axes.hist(KFDedup, bins = 50, density = True, alpha = 0.5, label = 'Singles + Deduplicated Duplicates')\n",
    "axes.legend()\n",
    "\n",
    "np.mean(overallDedup) / np.mean(weights_all), np.mean(KFDedup) / np.mean(k_factor_all), np.std(overallDedup) / np.std(weights_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "axes.hist(KFSingles, bins = 20, density = True, alpha = 0.5, label = 'Singles')\n",
    "axes.hist(newLinkages[:,1], bins = 20, density = True, alpha = 0.5, label = 'Deduplicated Duplicates')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('duplicate_detections.csv', 'w') as outcsv:   \n",
    "    writer = csv.writer(outcsv, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    writer.writerow(['p1', 'p2', 'p1_captured_at', 'p2_captured_at', 'speed', 'distance', 'time', 'weight_g', 'k_factor', 'old_weight_g', 'old_k_factor', 'weight_g_diff', 'k_factor_diff', 'eye_to_eye_xFrame', 'eye_to_eye_yFrame', 'tail_to_tail_xFrame', 'tail_to_tail_yFrame', 'old_eye_xFrame', 'old_eye_yFrame', 'old_tail_xFrame', 'old_tail_yFrame', 'left_eye_xFrame', 'left_eye_yFrame', 'left_tail_xFrame', 'left_tail_yFrame'])\n",
    "    for item in found_pairs:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_detections = pd.read_csv('duplicate_detections.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_detections\n",
    "#duplicate_detections[duplicate_detections['speed'] > 1.5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(duplicate_detections.time, duplicate_detections.speed)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('Swimmng speed')\n",
    "plt.title('Time between frames vs Swimming speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    mask = (duplicate_detections.time > i + 0.5) & (duplicate_detections.time < (i + 1.5))\n",
    "    avg_speed = np.mean(duplicate_detections.speed[mask])\n",
    "    print(i, np.sum(mask), avg_speed * (i - 1))\n",
    "    avgs.append(avg_speed * (i - 1))\n",
    "\n",
    "#plt.scatter(duplicate_detections.time, duplicate_detections.k_factor)\n",
    "plt.plot(np.arange(10) + 1, avgs)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Time between frames vs distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    mask = (duplicate_detections.time > i + 0.5) & (duplicate_detections.time < (i + 1.5))\n",
    "    avg_speed = np.mean(duplicate_detections.speed[mask])\n",
    "    print(i, np.sum(mask), avg_speed * (i - 1))\n",
    "    avgs.append(avg_speed * (i - 1))\n",
    "\n",
    "#plt.scatter(duplicate_detections.time, duplicate_detections.k_factor)\n",
    "plt.plot(np.arange(10) + 1, avgs)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Time between frames vs distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    mask = (duplicate_detections.time > i + 0.5) & (duplicate_detections.time < (i + 1.5))\n",
    "    avg_k_factor = np.mean(duplicate_detections.k_factor[mask])\n",
    "    print(i, np.sum(mask), avg_k_factor)\n",
    "    avgs.append(avg_k_factor)\n",
    "\n",
    "#plt.scatter(duplicate_detections.time, duplicate_detections.k_factor)\n",
    "plt.plot(np.arange(10) + 1, avgs)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('K-factor')\n",
    "plt.title('Time between frames vs K-factor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    mask = (duplicate_detections.time > i + 0.5) & (duplicate_detections.time < (i + 1.5))\n",
    "    avg_weight_g = np.mean(duplicate_detections.weight_g[mask])\n",
    "    print(i, np.sum(mask), avg_weight_g)\n",
    "    avgs.append(avg_weight_g)\n",
    "\n",
    "#plt.scatter(duplicate_detections.time, duplicate_detections.k_factor)\n",
    "plt.plot(np.arange(10) + 1, avgs)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('K-factor')\n",
    "plt.title('Time between frames vs Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(duplicate_detections.weight_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    mask = (duplicate_detections.weight_g > (i * 1000)) & (duplicate_detections.weight_g < ((i + 1) * 1000))\n",
    "    avg_speed = np.mean(duplicate_detections.speed[mask])\n",
    "    print(i, np.sum(mask), avg_speed)\n",
    "    avgs.append(avg_speed)\n",
    "\n",
    "#plt.scatter(duplicate_detections.time, duplicate_detections.k_factor)\n",
    "plt.plot(np.arange(10) * 1000, avgs)\n",
    "plt.xlabel('Time between frames')\n",
    "plt.ylabel('Speed')\n",
    "plt.title('Time between Weight vs Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row1 = captures.ix[1994]\n",
    "# row2 = captures.ix[1995]\n",
    "\n",
    "# print(get_eye_wkps(row1, row2))\n",
    "# print(get_eye_wkps(row2, row1))\n",
    "# print(get_eye_distance(row1, row2))\n",
    "# print(get_eye_distance(row2, row1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(duplicate_detections.replace([np.inf, -np.inf], np.nan).dropna(subset=['speed'], how=\"all\")['speed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speed = duplicate_detections.replace([np.inf, -np.inf], np.nan).dropna(subset=['speed'], how=\"all\")['speed']\n",
    "duplicate_detections.p1_captured_at = pd.to_datetime(duplicate_detections['p1_captured_at'])\n",
    "\n",
    "fig, axes = plt.subplots(len(dates), 1, figsize=(10, len(dates) * 5))\n",
    "\n",
    "for index, date in enumerate(dates):\n",
    "    mask = (duplicate_detections['p1_captured_at'] > date) & (duplicate_detections['p1_captured_at'] < (date + timedelta(days = 1)))\n",
    "    \n",
    "    speed = duplicate_detections[mask]['speed']\n",
    "    axes[index].hist(speed, bins = 20)\n",
    "    axes[index].set_xlabel('Speed (meters per second)')\n",
    "    axes[index].set_ylabel('Frequency')\n",
    "    axes[index].set_title('Swimming Speed Distribution for Pen %i on %s (%i samples)' % (pen['pen_id'], date.strftime('%m/%d/%Y'), np.sum(mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(duplicate_detections['p1_captured_at'], duplicate_detections['speed'])\n",
    "plt.plot(duplicate_detections['speed'].rolling(window=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "xtick_locator = AutoDateLocator(minticks=50)\n",
    "xtick_formatter = AutoDateFormatter(xtick_locator)\n",
    "\n",
    "axes.xaxis.set_major_locator(xtick_locator)\n",
    "axes.xaxis.set_major_formatter(xtick_formatter)\n",
    "\n",
    "ma = duplicate_detections['speed'].rolling(window=10).mean()\n",
    "axes.plot(duplicate_detections['p1_captured_at'], ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(duplicate_detections['speed'].values, index = duplicate_detections['p1_captured_at'])\n",
    "ma = ser.rolling('6h').mean().resample('h').apply(lambda x:x.tail(1) if x.shape[0] else np.nan).fillna(method='ffill')\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "xtick_locator = AutoDateLocator(minticks=50)\n",
    "xtick_formatter = AutoDateFormatter(xtick_locator)\n",
    "\n",
    "axes.xaxis.set_major_locator(xtick_locator)\n",
    "axes.xaxis.set_major_formatter(xtick_formatter)\n",
    "\n",
    "axes.plot(ma, color = 'red')\n",
    "axes.set_xlabel('Time')\n",
    "axes.set_ylabel('Average Speed (meters per second)')\n",
    "axes.set_title('Average Swimming Speed for Pen %i (%i samples)' % (pen['pen_id'], len(duplicate_detections)))\n",
    "axes.scatter(duplicate_detections['p1_captured_at'], duplicate_detections['speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(duplicate_detections['eye_to_eye_xFrame'] - duplicate_detections['tail_to_tail_xFrame'] < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pair = (24, 25)\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=captures.ix[wrong_pair[0], 'left_crop_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=captures.ix[wrong_pair[1], 'left_crop_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
